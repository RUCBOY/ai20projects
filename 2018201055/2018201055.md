# Task 7

Group4 王逢源 2018201055 劳动人事学院

## 10月进度

- 学习了机器学习、自然语言处理的相关知识并选择了算法。主要是《Python机器学习基础教程》和《精通Python自然语言处理》两本书的学习。

- 学习了网络爬虫的相关知识，并尝试了编写代码（ieeexplore.jpynb, 第一次提交) ，爬取IEEE上的英文论文。

## 11月进度

- #### 爬取计算机科学英文论文

  - 选择了DBLP这一计算机类英文文献集成数据库，爬取CS论文的DOI链接和标题并保存，再通过SCI-HUB搜索到论文并进行批量下载。共爬取285篇论文。

- #### 提取pdf并分词

  - 使用pdfminer、pdfplumber提取出pdf的文本内容并保存为txt格式。

- #### 问题

  - 数据量少：由于是通过sci-hub爬取CS期刊论文，并需要连接vpn，常出现网址失效、网络不稳定、无权限等问题，导致目前爬取量较少，爬取速度慢。
  - 格式问题：有一些CS论文布局特殊，一页上有多栏多块，目前pdfplumber无法识别出论文的布局信息。后续希望通过设计正则表达式解决这一问题。
  - 进度较慢：由于团队成员基础较为薄弱，前期在学习和选择算法方面花费了大量时间，导致进展较慢，项目需要加速完成。

- #### 未来计划

  - 爬取更多CS论文，通过设计正则表达式将论文各个部分提取出来，使之成为可以处理的数据。
  - 用朴素贝叶斯模型将论文数据集分类，生成分类器。
  - 用tf-idf构建句子的n元语法模型。

## 12月进度

- #### 发现问题：
  - 部分论文存在分栏等布局信息，部分论文中包含大量数学公式和符号，导致提取出的txt格式混乱、存在乱码，阻碍下一步分词和模型的构建。
  - 论文复杂多样，缺少规律，难以设计通用的正则表达式批量提取论文内容。
  - 尝试使用OCR字符识别技术，但识别率不高，效果不好。（先利用wand和PIL，将pdf转换成图像，再利用PyOCR与Tesseract-OCR结合进行字符识别，并将识别内容保存到txt中。）

- #### 解决问题：
  - 听取老师的建议，改用Elsevier Sciencedirect数据库，直接从网页版论文中提取文本内容。
  - 好处：
    - 省去pdf转换txt的麻烦。
    - 网页结构清晰，可分类提取abstract、keywords、introduction、conclusion等部分，减少了工作量。
    - 论文中包含的数学公式较少，爬取方便，样本量足够大。
  - 结果：爬取了sciencedirect上5000篇论文的abstract、keywords、introduction、conclusion部分，数据规范，易于进行下一步分词和模型构建。

## 项目总结

- #### 整体思路

  - 数据收集：Python网络爬虫，爬取足够多的CS论文，提取出论文的各个部分。
  - 数据清洗、对论文分句、分词
  - 构建模型：统计不同论文章节中每个单词的词频，计算词在此章节中出现的概率。将一个句子当作一个序列文本，按照1阶马尔可夫链，即二元语法计算句子的概率。再以句子为单位去构建二元句法模型。

- #### 分工

  我负责了数据收集部分，参与了模型构建思路的确定，负责每次汇报展示的ppt制作。

- #### 项目特点

  网络爬虫与人工智能的结合

## 个人收获

​		我作为其他学院的同学，此前仅学习过python基础。在做项目的过程中，学会了网络爬虫、pdf批处理、ocr字符识别，并自行编写了代码。自己debug的过程让我的编程能力有了很大的进步。

​		在课堂上学习了机器学习、深度学习等人工智能方面的知识。特别是对自然语言处理、语言模型、RNN、CNN有了深入的了解。囿于能力不足，在构建模型方面没能够将理论知识通过代码实现，在模型方面的贡献较少。但基于这些知识，我希望在今后继续提高自己的编程能力。

​		在每次展示课和最终项目汇报中，从同学们的项目经历中学习了很多知识，也得到了很多启发。
