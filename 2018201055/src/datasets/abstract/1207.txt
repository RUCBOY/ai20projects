Some authors recently underlined the existence of programs generating invalid responses in online surveys as an emerging threat for the different crowdsourced research fields (e.g., botnets, form fillers or survey bots). Accordingly, online data research might include computer-generated sets of responses representing invalid data at risk of largely distorting study results. Several statistical indices exist in order to detect problematic data. In line with a previous study that compared these indices in Likert-type scale questionnaire data, this study purported to extend the analyses with dichotomous-itemed questionnaires. Three samples of about more than 2,000 participants were mixed with different proportions (i.e., 5% to 50%) of simulated data to mimic their effect. Then, seven indices were compared in terms of correct detections of non-human response sets. Consistent with former findings, three indices resulted in superior correct detection rates: response coherence, the Mahalanobis distance and the person-total correlation. Two of them can easily be computed using basic statistical software. The current study findings represent an encouragement to use them in priority as routine for data screening.
