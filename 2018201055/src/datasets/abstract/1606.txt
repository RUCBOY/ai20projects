Image semantic segmentation is a basic and challenging computer vision task, where each pixel in an image is classified into a semantic class. In recent years, deep neural networks have settled many computer vision problems such as semantic segmentation, image classification, object detection. A lot of approaches based on deep neural networks have achieved outstanding performance on different benchmarks. However, most of them are restricted by the scale of elaborate labeled data to train a deep neural network, especially on image semantic segmentation. Collecting pixel-level annotated images is an extremely time-consuming process. Thus, utilization of synthetic data is becoming prevalent. Nevertheless, simply applying the models trained on synthetic data leads to a dramatic performance drop on real images due to the domain shift. In this paper, we propose an adversarial learning method for domain adaptation from the perspectives of both image-level and feature-level. The former adapts images from two domains to appear as if drawn from one domain and the latter attempts to learn similarities between the source and target domains in feature-level common space. To further enhance the adapted model, we introduce a self-training strategy using pseudo-labels. Extensive experiments and ablation studies are conducted under various settings and experiment results indicate our unsupervised method outperforms other state-of-the-art methods.
