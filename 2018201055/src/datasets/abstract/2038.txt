Five decades ago advances in integrated circuits and time-sharing operating systems made interactive use of computers economically feasible. Visions of man-computer symbiosis and the augmentation of man’s intellect became realistic research objectives. The initial focus was on facilitating interactivity through improved interface technology, and supporting its application through good practices based on experience and psychological principles. Within a decade technology advances made low cost personal computers commonly available in the home, office and industry, and these were rapidly enhanced with software that made them attractive in a wide range of applications from games to office automation and industrial process control. Within three decades the Internet enabled human–computer interaction to extend across local, national and international networks, and, within four, smartphones and tablets had made access to computers and networks almost ubiquitous to any person, at any time and any place. Banking, commerce, institutional and company operations, utility and government infrastructures, took advantage of, and became dependent on, interactive computer and networking capabilities to such an extent that they have now been assimilated in our society and are taken for granted. This hyperconnectivity has been a major economic driver in the current millennium, but it has also raised new problems as malevolent users anywhere in the world have become able to access and interfere with critical personal, commercial and national resources. A major issue for human–computer studies now is to maintain and enhance functionality, usability and likeability for legitimate users whilst protecting them from malevolent users. Understanding the issues involved requires a far broader consideration of socio-economic issues than was required five decades ago. This article reviews various models of the role of technology in human civilization that can provide insights into our current problématique.
