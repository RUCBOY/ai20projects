The recognition of emotions in spoken words is one of the most important aspects in human communication and social relationships. Traditional approaches to the study of vocal emotional recognition involve instructing listeners to choose which one of several words describing emotion categories best characterize linguistically neutral utterances or vocalizations uttered by actors portraying various emotional states. To this end, generic experiment control software is usually used, which has some disadvantages. In this paper, we present a system that digitalizes the whole process involved in understanding how people perceive and understand vocal emotions, improving data collection, processing and analysis. Moreover, this system provides a new group of features that allows a more comprehensive characterization of the behavioral dimension underlying vocal emotional recognition. In this paper we describe this system and analyze the relationship between emotional perception, gender, age and Humanâ€“Computer Interaction.
