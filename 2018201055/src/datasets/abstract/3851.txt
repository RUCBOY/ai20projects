In artificial intelligence, many temporal processing tasks such as speech recognition, video analysis, and natural language processing depend on not only spatial contents of the current sensory input frame but also the relevant context in the attended past. It is illusive how brains use temporal contexts. Many computer methods, such as Hidden Markov chains and recurrent neural networks, require the human programmer to handcraft contexts as symbolic contexts. It has been proved that our Developmental Networks (DN) are capable of learning any emergent Turing Machine (TM), they can learn patterns as their states from human teachersâ€™ scrupulous supervision. In this paper, we explain why contexts are important for temporal processing. We study how agent actions are natural sources of contexts and enable muscle neurons to autonomously generate actions as contexts. In humans, muscle actions correspond to the firings of muscle neurons. They are information-dense in time and correlated with the cognitive and motor skills of the individual. Some actions are meant to handle time warping, while others are not (e.g., for time duration counting). We model actions as information-dense action patterns. We use entropy to define the new concept of information-dense. We also introduce the free-of-labeling technique. We experiment with DN for recognition of audio sequences as an example of modality, but the principles are modality independent. Our experimental results show how the information-dense actions and the free-of-labeling mechanisms help DN to generate temporal contexts. This work is a necessary step toward our goal to enable machines to autonomously abstract contexts from actions through life-long development.
