Ultra-wideband radio networks enable low-cost, low-computation robot localization in semi-structured environments; however, previous results have shown that these localization systems suffer from spatially-varying measurement biases, leading to a spatially-varying offset between the physical and the estimated position. In tasks where absolute positioning or high tracking accuracy is required, this offset can lead to failure of the task. This paper proposes augmenting ultra-wideband-based localization with visual localization to improve estimation accuracy for critical tasks. It also presents a control strategy that takes the camera measurement process into account, and allows the ultra-wideband systemâ€™s measurement biases to be learned and compensated over multiple executions of the task. This bias compensation can be used to improve the accuracy of the task in the case of visual impairment. The effectiveness of the proposed framework is demonstrated by accurately flying a quadrocopter to a landing platform using on-board estimation and control.
