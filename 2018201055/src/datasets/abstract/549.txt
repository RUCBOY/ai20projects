Gaussian processes (GPs) are powerful tools for regression and classification tasks, and have been used widely in various fields of machine learning. However, GPs are barely applied directly to the scenario of multi-view learning so far. In this paper, we propose two kinds of multi-view GPs frameworks called MVGP1 and MVGP2 with posterior consistency, which combine multiple views by regularizing the marginal likelihood. Utilizing the posterior distribution consistency criterion, the MVGP1 is first presented. Then to further improve classification accuracy, we present the MVGP2, in which we introduce a trade-off parameter to govern the relative importance of the likelihood term compared with the regularization term. The MVGP1 significantly reduces the training time of the multi-view GPs with acceptable performance on the accuracy, and the MVGP2 achieves state-of-the-art classification performance. Experimental results on multiple real-world classification data sets show the effectiveness of the proposed frameworks.
