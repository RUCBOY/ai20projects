In the last few decades, genetic algorithms (GAs) demonstrated to be an effective approach for solving real-world optimization problems. However, it is known that, in presence of a huge solution space and many local optima, GAs cannot guarantee the achievement of global optimality. In this work, in order to make GAs more effective in finding the global optimal solution, we propose a hybrid GA which combines the classical genetic mechanisms with the gradient-descent (GD) technique for local searching and constraints management. The basic idea is to exploit the GD capability in finding local optima to refine search space exploration and to place individuals in areas that are more favorable for achieving convergence. This confers to GAs the capability of escaping from the discovered local optima, by progressively moving towards the global solution. Experimental results on a set of test problems from well-known benchmarks showed that our proposal is competitive with other more complex and notable approaches, in terms of solution precision as well as reduced number of individuals and generations.
