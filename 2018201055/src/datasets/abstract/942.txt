Partial Multi-label Learning (PML) tackles the problem where each training instance is associated with a set of candidate labels that include both the relevant ground-truth labels and irrelevant false positive labels. Most of the existing PML methods try to iteratively update the confidence of each candidate label, while the estimated label confidence may be not reliable due to the cumulative error induced in the confidence updating process, especially when false positive labels dominate. In this paper, we propose a simple yet effective model called PML-MT (Partial Multi-label Learning with Mutual Teaching), in which a couple of prediction networks as well as the corresponding teacher networks are adopted to learn collaboratively and teach each other throughout the training process. Specially, the proposed PML-MT model iteratively refines the label confidence matrix through a couple of self-ensemble teacher networks and trains two prediction networks simultaneously in a mutual teaching manner. Moreover, we propose a novel regularization term to further exploit label correlations from the outputs of the prediction networks under the supervision of the refined label confidence matrix. In addition, a co-regularization term is introduced to maximize the agreement on the outputs of the couple prediction networks, so that the predictions of each network would be more reliable. Extensive experiments on synthesized and real-world PML datasets demonstrate that the proposed approach outperforms the state-of-the-art counterparts.
