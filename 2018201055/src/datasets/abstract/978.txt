Event extraction aims to determine entity mentions, event triggers and argument roles in text, such as news articles. For this task, dependency syntax has been recognized as a valuable source of information. Previous work integrates dependency trees by using one-best results from a parser, where potential incorrect dependencies may impact the event extraction performance. We propose to use syntax features in an implicit approach, by adopting Soft Head Vectors (SHV) drawn from a well-trained parser as an adjacency matrix, densely connecting all words with the probabilistic head scores. SHV can be also regarded as dependency forests where multiple possible structures can be taken into account. A two-phase graph neural network is adopted to represent the forests, automatically differentiating the salient syntactic information from noisy parsers. Experiments on the two public datasets show that our model outperforms various baselines including graph-based one-best tree as well as recent transition-based decoding, giving the state-of-the-art results in the literature.
