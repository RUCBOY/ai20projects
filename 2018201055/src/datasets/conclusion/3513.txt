The purpose of this work was to assess the ability of convolutional neural networks to correctly classify synthetically generated and real silhouettes, using an extensive synthetic training set of binary silhouettes. Such a classification system would be of high practical value, since it could classify the very important 3 generic poses in real time, provided that silhouette segmentation has been performed. Furthermore, the use of a fisheye camera adds to the value of the proposed system, since its 180Â° field of view allows the imaging of a whole room with only a single camera.
Our initial results show that the CNN trained only on synthetic silhouettes was unable to correctly classify the three generic poses in silhouettes segmented from real videos. Similarly, the CNN enhanced with the fusion of GZMI at the FNN level, achieved low classification accuracy, although slightly better than the standard CNN. On the other hand, a simple classifier using the GZMI (Zernike moment invariants, defined using the geodetic properties of the calibration of the specific acquiring camera) achieved significantly better accuracy, although still not high enough to be practically useful. The GZMI features appear to be more immune to the artefacts induced by the segmentation than the CNN. On the other hand, all three-classification setups achieved very high accuracy when tested on synthetic silhouettes (higher that 97%). This result points to the conclusion that imperfections of the segmentation results, which are unavoidable in any real-life conditions of video acquisition, strongly affect the performance of the classifiers. Thus, the use of transfer learning is a very feasible approach. Indeed, the effect of transfer learning is very clearly demonstrated by the marked increase to the accuracy obtained when transfer learning was active.
The classification accuracy achieved with transfer learning is close to practically useful levels. However, further work is required to establish the usefulness of this approach, which includes investigation of the structure and the parameters of the utilized CNN, spatial distribution of the misclassifications or exploiting views of the same subject taken from different cameras and appropriately fusing the information before classification.
