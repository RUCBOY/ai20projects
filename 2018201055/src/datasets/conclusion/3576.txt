Before the advent of the World Wide Web, even before the current Internet, it was arguably easier to find existing software for personal computers—there was less of it, and there were simply fewer places to look. Community bulletin boards and archive sites using FTP made software available for copying by anonymous users over telephone networks; later, the Usenet culture (Emerson, 1983) of the1980′s encouraged widespread sharing and even devoted a newsgroup (comp.sources) to the exchange of software source code. Communities created manually-curated lists of software (e.g., Boisvert, Howe, Kahaner, 1985, Brand, 1984) and some journals regularly published published surveys of topical software (e.g., Martinez, 1988). The breadth of software we have today did not exist then, but one could feel reasonably sure to have found and examined the available options in a finite amount of time. Fast-forward to today, and the staggering wealth of software resources available to users is both a blessing and a curse: one can simultaneously feel that for any given task, “surely someone has already written software to do this,” and yet an attempt to find suitable software can seem like falling into a rabbit hole.
6.1. How scientists and engineers find softwareSo what do users do today when they want to find software? This survey was an attempt to gain insight into the approaches used by people working in science and engineering, including criteria that they apply to select between alternative software choices. Our participants were experienced researchers worked primarily in the physical, computing, mathematical and biological sciences; the majority were involved in software development and had a mean of 20 years of experience; most worked in small groups; and all had some degree of choice in the software they used. The majority spent over 50% of their day using software; this is somewhat higher than some other studies have reported (e.g., Hannay et al., 2009, found scientists spent 40% of their time using scientific software).The survey results help identify a number of current community practices in searching for both ready-to-use software and source code:
1.When searching for ready-to-run software (RQ1), the top three approaches overall were are: (i) search the Web with general-purpose search engines, (ii) ask colleagues, (iii) look in the scientific literature. After these top three, the next most commonly stated approaches differed between those respondents who self-identified as being involved in software development and those did not: more developers in our sample indicated asking on social help sites such as Stack Overflow and searching in public software repositories such as GitHub (in that order), while nondevelopers indicated following their organization’s guidelines and a tie between asking on public mailing lists and asking on social media. We found statistically significant differences between the subgroups’ uses of social help sites, software project repositories, software catalogs, and organization-specific mailing lists or forums.2.Our RQ2 revealed that the top five criteria given above-average weight when searching for ready-to-run software are: (i) availability of specific features, (ii) support for specific data standards and file formats, (iii) price, (iv) apparent quality of the software, and (v) operating system requirements. On the other hand, the least important criteria were (a) size of software, (b) software architecture, and (c) programming languages used in implementation.3.Regarding information that workers in scientific and engineering fields would like to see in a catalog of ready-to-run software, a total of 15 features were indicated as having above-average value by at least 50% of the respondents; of these characteristics, the operating system supported, purpose of software, name of software, domain/field of application, and licensing terms were the five most-often requested features. Software developers different from nondevelopers in our sample of scientists and engineers in that they rated the name and purpose of the software as the most important information to provide. Another slight difference involved information about the availability of support or help for a given software product, but on the whole, both subgroups displayed similar preferences.4.The top five approaches used by software developers in science and engineering to search for source code are almost identical to those they use to find ready-to-run software. They are: (i) search the Web with general-purpose search engines, (ii) ask colleagues, (iii) look in the scientific literature, (iv) search in public software project repository sites such as GitHub, and (v) look in social help sites such as Stack Overflow.5.The top three reasons given by the developers in our sample for why they were sometimes unable to find source code are: (i) unable to locate suitable software, (ii) requirements are too unique, and (iii) insufficient time to search or evaluate options. Conversely, concerns about intellectual property issues ranked low.The results above have implications for the development of better resources for locating software. In common with other surveys, we found that more people indicate they use general Web search engines than any other approach for finding both ready-to-run software and source code. This implies that for any specialized resource such as a software catalog to gain popularity, it must be indexed by Google and other search engines so that users can find its content via general Web searches. Our results for RQ2 (Fig. 8) also point out information that people consider important when looking for software; this can be used to inform the development of more effective software search systems. For example, if one were creating a software search engine, providing direct access to information about data formats supported by different software tools could help scientists and engineers to find and select tools more quickly. Finally, software cataloging efforts would benefit by focusing on presenting the most desirable information revealed by RQ3 in our survey (Fig. 9).Though our survey considered only general resources, there also exist a number of source code finding systems today integrated into specialized software development tools (e.g. Hoffmann, Fogarty, Weld, 2007, Linstead, Bajracharya, Ngo, Rigor, Lopes, Baldi, 2009, Ossher, Bajracharya, Linstead, Baldi, Lopes, 2009, Linstead, Rigor, Bajracharya, Lopes, Baldi, 2008, Zagalsky, Barzilay, Yehudai, 2012, Martie, LaToza, d. Hoek, 2015, Ye, Fischer, 2002). Software developers can take advantage of these systems to find software during development activities. Though our survey did not specifically examine the use of these tools, we would expect that the attributes rated most important in Fig. 8 would also be relevant in the context of using such integrated code-finding facilities. However, this hypothesis is untested, and constitutes a question that future studies could explore.
6.2. Lessons for future surveysAnalyzing the survey results has led us to recognize aspects of the survey that could have been improved. First, in the demographic profile questions (Section 4.1), it would have been useful to gather more specific data. For example, the work fields question could have offered finer-grained options, and additional questions could have asked participants about their institutional affiliation (e.g., educational, government, industry) as well as their work roles (e.g., student, staff, faculty). Of course, the benefits of additional questions must be weighed against respondents’ patience for filling out long surveys.Second, the questions asking about software search could have had an explicit answer choice about the use of scientific gateways. The survey questions generally did not mention gateways or portals explicitly; the closest was the question discussed in Fig. 9, which included workflow environments as an answer choice. Based on the responses reported in Fig. 9, one quarter of the respondents consider support for workflow environments a criterion in selecting software. Since we did not ask about it explicitly, it is unclear whether any of the participants had the use of gateways in mind and framed their responses accordingly. It is also not clear what effect this would have had on their responses. Gateways concentrate software resources in one location and typically provide an index or other means of finding software provided by the gateway, and it is conceivable that this may change the nature of how users think of finding software or the criteria they use to discriminate between available alternatives. It is therefore possible that this is a confounding factor in our results. Future surveys should address this aspect explicitly.Third, future work must strive to increase the response rate. While we believe the present survey’s results are accurate for the sample of people who finished the survey, we must also acknowledge that a response rate of 3% is disappointing. It is widely asserted that Web-based surveys often encounter low rates (e.g., Kitchenham and Pfleeger, 2008; Couper, 2000; Couper and Miller, 2008); in our experience, many studies even fail to disclose the response rate, or claim a rate without reporting the number of potential recipients, leaving in question the accuracy of the rate. However, of the published surveys that disclose both the number of potential recipients and the number of completed responses received (e.g., Wu et al., 2007; Bauer, Eckhardt, Hauptmann, Klimek, 2014, Kalliamvakou, Gousios, Blincoe, Singer, German, Damian, 2014, Lawrence, Zentner, Wilkins-Diehr, Wernert, Pierce, Marru, Michael, 2015; Sojer, 2010), the values often have been higher. For example, Sojer (2010) reported 9.7% and Lawrence et al. (2015) obtained 17%, albeit with a highly motivated population. One possible cause for our lower response rate may be the venues where we advertised the survey. Our primary venues for soliciting participation were certain mailing lists and Facebook groups. With respect to the mailing lists, some recipients may not have received the survey messages because automatic spam filters may have blocked the messages from their electronic mail inboxes. This would mean that fewer people saw the invitations than the number of people subscribed to the mailing lists, artificially reducing the apparent response rate. With respect to Facebook, some users may be have signed up long ago but they may rarely or never check the group we targeted. The latter is especially plausible when we consider two other results of our survey: as shown in Fig. 4, respondents had a mean of 20 years of experience, and in Fig. 12, social media of Twitter/Facebook/LinkedIn variety were little-used by participants for finding software. If that reflects the overall population we reached and their broader pattern of social media use, then they may simply be of a generation that spends less time on Facebook than a younger generation of researchers. Again, this would cause our estimated number of recipients to be higher than the actual number of people who saw the announcements in that venue. Finally, it is possible that our announcements and/or the front page of the survey were simply not sufficiently motivational.
