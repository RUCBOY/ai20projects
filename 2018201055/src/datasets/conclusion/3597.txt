In this work, we have introduced the novel concept of High Performance Robotic Computing (HPRC), an augmentation of the ideas behind traditional High Performance Computing, for the world of robotics. Previous works have suggested the use of HPC parallel software libraries such as CUDA or MPI for computationally demanding robotic tasks. In such works, HPC has always been understood as a set of tools and infrastructures for speeding up tasks that are too slow running on single computing units. However, HPRC goes beyond this by proposing means of achieving scalability, centralization/distribution, user-transparency, etc., in missions carried out by multi-robot systems, where tasks could be or not computationally demanding.
The scalability feature requires further research if multi-robot systems composed of hundreds or even thousands of robots are to be controlled by MPI software. The results shown in this work were obtained with a small number of parallel processes and while each Raspberry Pi was set with an SITL autopilot, MPI communications strategies need to be addressed if the number of robots increases. Nevertheless, two aspects are promising. First, MPI implementations have been compiled for ARM architectures such as the Raspberry Pi and second, MPI research is targeting the exascale computing era, where millions of cores are expected to be used for single simulations. This research approach follows the success of using MPI in the current petascale era.
Within the scope of HPRC, a robot becomes a general multi-purpose computing device, which opens a completely new spectrum of possible robotic applications. In this work, we have demonstrated that a full HPRC cluster can be deployed with traditional HPC software such as MPI, PBS torque, etc., on embedded computing devices such as the Raspberry PI. This way, with new embedded computing devices and communications technologies appearing constantly, full HPC cluster of robots, resembling those with wired technologies are a true possibility. The emergence of 5G communications in the nearby future is a very promising aspect that could allow solving the performance decay issues shown in Section 4.1. However, nowadays the design of robotic missions using HPRC should take into consideration current network limitations. Regardless, as future work, we plan to run the same experiments, i.e. running HPL and parallelMotion on real unmanned vehicles while flying using Wi-Fi and 4G LTE connectivity.
To demonstrate that HPRC is more than a speeding up tool for robotic tasks, we have introduced a parallel motion software for multi-robot systems depicted as parallelMotion. The software requires further development to be used in real robots but it potentially demonstrates that MPI software can be used for controlling the motion of all kind and number of robots such as UAVs and UGVs. Furthermore, it shows that HPRC is not only usable for computationally demanding tasks but it can be actually be a bedrock for scalable multi-robot systems exploiting all HPC features. The Python library mpi4py used by parallelMotion does not currently allow user interaction at execution time. Given such limitation, we are revising other Python libraries for MPI in order to improve parallelMotion by allowing pilot dynamic control during vehicles’ motion while maintaining autonomous behavior.
As a new emergent field, HPRC is still in its infant phase and requires more research to be implemented in real world applications but its potential is enormous. In fact, an integration between traditional HPC and HPRC systems is an interesting path to follow. Computing is everywhere nowadays, from sensors up to powerful servers and while HPC is used in several endeavors, we believe that it is time to propose the appearance of Ubiquitous Supercomputing, as the merging of HPC and HPRC.
In future works, we aim at introducing Ubiquitous Supercomputing and define its philosophy, infrastructures, technologies, etc. Such approach requires a set of services to guarantee the HPC features in ubiquitous supercomputing systems. The custom Python libraries, used by parallelMotion, already implement such of those services but they required further development. Moreover, the concept of general-purpose computing mission is interesting because it binds a robotic task with a software, which favors the reutilization of a multi-robot system by simply changing the software associated to each task and the reconfiguration of the mission network.
Future work includes as well testing parallelMotion with real robots. The transition for the simulated scenario presented in here is a straightforward endeavor because of the use of DroneKit, which was designed to allow easy deployment from SITL to real robots. Furthermore, parallelMotion was developed to facilitate such transition.
In this work, we introduced the HPRC cluster as an adaptation of traditional HPC infrastructures such as HPC cluster of computers. However, other HPC infrastructures such as Grid or Big data could also be exploited to build powerful robotic settings. In fact, Wan et al. [52] described cloud robotics as a combination between multi-robot settings and cloud computing where the former delegates complex computing tasks to the latter. HPRC does not deny the use of external computing power; it could be actually an interesting combination to have a HPRC cluster interacting with computing resources located elsewhere, i.e. in the cloud or any other computing infrastructure specially those exhibiting HPC.
As a final thought, the combination of standard technologies such as MPI and CUDA within the scope of HPRC could lead to the development of fascinating multi-robot systems where artificial intelligence finds HPC/HPRC as powerful allies.
