In this paper, we proposed a novel framework to find procedural texture models and corresponding parameters for generating texture images consistent with user-defined perceptual scales based on an example image. Through extensive psychophysical experiments with a procedural texture dataset, we established a perceptual texture space. The perceptual distance measured in this space improved texture similarity measurement in a manner consistent with human perception. The most similar texture retrieved in the PTS provided parameter values for the predicted texture generation model. Promising results had been obtained and demonstrated the effectiveness of the proposed approach.
Nonetheless, there are some limitations in our current work. The textures in the dataset do not sufficiently cover the full range of parameter space of procedural models we choose. Constrained by the time-consuming rendering process, we only sampled in the parameter space with the purpose of producing visually different textures. In addition, in the psychophysical experiments, the number of samples for the observers is limited due to the available time and labor. The texture samples in the perceptual space are relatively sparse. Moreover, the parameter space is discretized at a roughly uniform scale; it may not reflect all changes in texture perception. The other limitation is that if exemplars are not procedural textures, e.g. a natural photograph, the appearance difference between the photograph and the training samples may dramatically affect the performance of the method. Specifically, the photograph might be taken under unknown lighting conditions, it may affect the accuracy of the prediction of the perceptual scales. The generation result will not be good either.
It should be noted that the proposed framework is general and open to all types of procedural models. When more procedural models are added, the training database will be enlarged and the consistency and stability of the learned PTS will be improved accordingly. The mapping from computational features to perceptual features will also be more accurate. Furthermore, better sampling methods of the parameter values should be investigated in order to optimize the balance between accuracy and efficiency.
Based on the performance of the proposed method, we believe that we are getting close to the actual applications of generating procedural textures according perceptual scales or even semantic description, which is a goal we would like to pursue further.
