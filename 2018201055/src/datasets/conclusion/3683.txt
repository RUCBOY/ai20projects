In this paper we discuss the challenging problem of inferring wearer’s activity from a first-person perspective. Our goal is set to learning generic multiple deep feature from different types of original data for this task. Thus, we proposed multiple deep learning pipelines to learn appearance and motion patterns that are used to predict wearer’s activity. Through quantitative and qualitative comparison with the state-of-the-art methods on challenging video datasets, we demonstrate that our DAML achieves encouraging results especially for long-term activity task while using a simple classifier. However, we also note that these learned generic features have limitation on short-term activity task, which means that our DAML feature can better benefit from long-term and stable activities instead of short-term and instantaneous actions.
