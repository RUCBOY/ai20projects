Nature-inspired metaheuristics such as Ant Colony Optimization have been successfully applied to many NP-complete optimization problems. We concluded that coarse-grained parallelism implemented in previous designs does not fit perfectly on GPUs architecture, and thus, to overcome this problem, different parallelization strategies are analyzed. First,we propose an agnostic vectorization of the tour construction stage of ACO, specifically designed for massively parallel architectures; establishing an ant to both 32-width vector (i.e. CUDA warp) and 64-width (called super-warp). To implement the latter, we use partial synchronization and different communication schemes based on shuffle instructions combined with shared memory. Our results reveal that the best design option is assigning a Warp to develop the ant task as having 64-threads does not take advantage of the lock step execution as threads within a warp do. However, it is expected that future agnostic vectorization platforms that enable hardware implementations of these techniques may provide better algorithmic performance. Actually, our fusion proposal where both, tour construction and pheromone update, are executed in the same kernel, benefits from super-warp implementation. Moreover, we introduce the SS-Roulette, a novel parallel implementation that mimics the behavior of the classical roulette selection procedure. This method is an order of magnitude faster than our previousI-Roulette procedure. Finally, for the pheromone update stage, we analyzed the impact of using atomic operations in terms of both performance and quality. We identified potential trade-offs and investigated several alternatives on different GPU generations. From our analysis, we can conclude that using atomic instructions guarantees the sequential results and also leads to performance gains compared to the non-atomic approach.
ACO on GPUs is at a relatively mature stage. However, the hardware features included in upcoming GPU generations may require an algorithm redefinition to obtain peak performance. As ACO is a massively parallel algorithm by its definition, this field seems to offer a potentially fruitful and promising area of research. From the point of view of hardware, it is expected to reach higher accelerations on GPUs whenever new hardware features come along. Among them, we may highlight hardware support for establishing the warp size dynamically, improvement of atomic operations hardware on device memory to deal with new 3D-stacked memory hierarchy – just to mention a few. These architectural advances can lift performance into unprecedented gains where parallelism is called to play a decisive role. Moreover, we may ensure that the benefits of our implementations and designs would also be extensive to other accelerators that relies on vectorization to improve performance/watt ratio such as Intel Xeon Phi architecture. The use of SSE/AVX SIMD instructions can be used to implement the parallel designs proposed here. Similar to assigning an ant to one Warp, the ant computation could be distributed to SSE/AVX SIMD vectors, where threads within a Warp correspond to lanes per SIMD vectors. At the upper level, we can also benefit from the multiple cores via multithreading.
