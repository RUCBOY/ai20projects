More than thirty percent of contracts in the treatment with the computer agents exhibited non-monotonicity; thus, a large portion of the population understands these unusual contracts may be optimal in particular situations. Furthermore, slightly less than one is six contracts were non-monotonic with the human agents, indicating principals recognized a difference in using non-monotonic contracts when paired with another person as compared to a computer.
In every round, principals offered fewer non-monotonic contracts to humans than the computer agents. Although some of these non-monotonic contracts failed to create the incentive to choose high effort, these numbers indicate that more principals used the correct qualitative structure of the theoretically optimal contract when matched with computer agents. The survey principals completed at the end of the experiment, asking how they would decide if interacting with the agent from the other treatment, indicates that some principals with human agents believed that humans would fail to act as risk-neutral profit-maximizers. Roughly fifty-seven percent of principals stated in our survey that they would offer different contracts to the two different types of agents.
Survey responses suggest that several of the principals had concerns regarding the rationality of the human agents, a few seemed worried about fairness, and none seemed concerned with risk attitudes (likely because our design accounts for such factors). Unlike other causes of poor decisions by the players, a lack of understanding of the game, or the “rationality” of a player, can be corrected with further instruction. The following quotes demonstrate the concern of some of the principals regarding the rationality of the human agents:24
“People aren’t as rational [as computers]; some will be swayed by a slightly higher round number without calculating the exact probabilistic payoffs of each scenario.”
“I changed it [the contract] because I know the computer program will calculate which decision makes the most money on average, but a person may not or might make errors.”
The principals above had reason to worry about the rationality of human agents as many failed to respond optimally, even in one case with a contract that fully accounts for risk-aversion, (0,16,12). Agents may have responded this way due to the inequity in the realized payments with such contracts. Principals, according to the data, responded by offering fewer non-monotonic contracts in later periods.
As the experiment progressed, principals created more non-monotonic contracts with computer agents and fewer with humans ones. Given the decisions of the human agents, offering fewer non-monotonic contracts was not necessarily suboptimal for the principal. Below is a quote expressing the frustration of one of the principals that contracted with human agents:25
“I would make different payments to the computer program because it understands how to maximize its profit unlike the morons playing today unless they based their answers on how much more money I would make as player 1 [the principal].”
The quote also demonstrates that some principals seem aware of inequity as possible issue. A few principals from Type (C) even discussed their own preferences for fairness.26
“I would pay slightly more for each higher payment, that way both player 2 [the agent] and myself end up earning more.”
“I would like to be more fair with a person.”
Because the contracts failed to shift towards theoretically profitable contracts (that paid at least as well as the null) as the rounds progressed in Type (H), in contrast to Type (C), it does not appear more periods would lead players towards contracts derived from traditional economic theory in Type (H). Our experiment shows how agents responded, often sub-optimally, but not why they often selected the action that earned them less money. Whether the human agents made poor decisions due to a lack of understanding, a desire to reward generous principals, or other reasons is outside the scope of this experiment. However, their actions are somewhat consistent with a concern for fairness (ex-post) and efficiency. Future work could use our methodology and design a similar experiment to ours with a larger focus on the agents, such as by having the computer act as a principal that randomly creates contracts for each agent. One could also reduce the concerns on rationality of the agent by including a communication treatment where the principal explains why a given contract was offered via text. Presumably, agents may behave differently when they understand the reasoning behind the structure of the contract, whether one is constructed with fairness in mind or because it has an unusual non-monotonic structure. Our computer treatment is similar to a communication treatment where human agents text the principals regarding their own preferences for contracts, except the computer treatment obviously had a homogeneous preference and an inflexible decision rule while the human agents may have heterogeneous preferences over the contract space and flexibility when deciding.27 A treatment where the agent communicates would allow us to better understand the motivation behind their decisions.
The suboptimal responses of agents, however, increased efficiency when they selected Effort = 12 even though Effort = 0 provided a larger payoff. This behavior caused the overall rate of efficiency in Type (H) to roughly equal that of Type (C). Furthermore, the principals in the experiment actually earned more with human agents than with computer agents, though not a statistically significant difference. Therefore, agent behavior could mitigate the loss of surplus in scenarios with non-monotonic contracts because human agents either respond with other-regarding preferences in mind, or they lack the comprehension skills to choose optimally. Since more than sixty percent of contracts were not non-monotonic with computer agents, many subjects seem not to comprehend the optimality of non-monotonic contracts. Furthermore, human agents responded negatively to non-monotonic contracts in a few cases.
Our work shows that non-monotonic contracts are indeed unintuitive to many principals as the majority of contracts in each treatment fail to have a non-monotonic structure. However, since roughly half as many principals offered human agents non-monotonic contracts compared to computer agents, there seems to be rationality concerns or other-regarding preferences, such as fairness on either the part of the principal or agent, affecting the decisions of the principals in this treatment. Because agents often responded favorably to theoretically suboptimal contracts that roughly compensated for cost of effort or could be perceived as “fair”, efficiency was almost the same between treatments. Given our current results, we find no evidence to recommend the wide adoption of non-monotonic contracts, though more work needs to be completed, especially research focusing more on the motivation and choice of the agent. In his work on contracts and limited liability, Innes (1990) included a constraint that payments to agents must increase with profit. This constraint is often included in principal-agent problems, because the principal and agent sometimes have an incentive to manipulate the profit or output of the firm in order to decrease or increase the wage to the agent. For example, a salesperson may avoid making additional sales to prevent profit from increasing if given a non-monotonic contract. Our work shows that this constraint may seem reasonable, even when such profit manipulation is not feasible, as principals either do not comprehend such contracts, worry about agent response, believe efficiency concerns motivate agents, or prefer equitable outcomes, which would prevent the use of non-monotonic contracts.
