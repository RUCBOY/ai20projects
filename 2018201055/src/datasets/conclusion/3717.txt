In this paper, we propose a new attention mechanism that leverages the gate system of LSTM to compute the attention weights for action recognition. The proposed attention mechanism is embedded in a recurrent attention network that can explore the spatial-temporal relations between different local regions to concentrate important ones. For more accurate attention, we derive a new attention unit from the standard LSTM unit so as how important the local part is only depends on its input gate. We conduct a series of experiments on three datasets and the results demonstrate our model achieves a significant improvement compared with other attention models. We also visualize attention maps learned by our model and soft attention model. The visualization results show our model can focus on the important regions more correctly and roundly for action recognition.
