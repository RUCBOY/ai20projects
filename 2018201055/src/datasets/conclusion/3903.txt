Through detailing an account of pig genome sequencing using a thick sequencing perspective, this paper has demonstrated that sequencing can be understood as a process that is open-ended: spatially, temporally and intellectually. Sequencing as an ongoing process involves the creation of libraries and maps, the working and workings of automated sequencing machines, and associated decision-making related to the use of them. The process also involves the assembly of the sequence, the development and improvement of statistical and computational tools, of chemistry and machinery, annotation, extra sequencing of certain parts of the genome, improvement of the contiguity and quality of the data, new reads, uploading, circulation and interpretation of data, and the management, curation and maintenance of data and data infrastructures.
There may be instances at which start or end-points of the sequencing process can be ascertained. One might conceive the approval by a body such as the Wellcome Trust, USDA or NIH of a proposal to sequence a particular species as a start. The initial receipt of clone libraries at sequencing centres, and the first entry into automated sequencing machines, may both be conceived as starting points. Or one might wish to identify how the tools, organisational capacity and desire by the community to sequence some or all of the genome came to be. The endpoints might be a published paper, or the online publication of a completed sequence.
Yet these starting points and endpoints are less discrete and definitive than on first inspection. In culinary terms, genome sequencing is more like cooking a perpetual stew in which ingredients can be added and the pot kept constantly on the boil, never fully complete. Firstly, the product is almost always incomplete. Gaps may remain, and there remain some errors in final published sequences. Secondly, either the product is an abstraction (purporting to be a reference sequence for a species, breed or strain where there is known to be genomic variation) or the product incorporates (or is built to allow the incorporation of) genomic variants such as single nucleotide polymorphisms (SNPs). The former is not definitive and is therefore subject to contestation and revision; the latter can never be definitive.36 As well as not being able to (historiographically or philosophically) privilege one stage of the sequencing process over any other, it is not possible to determine a priori the start and end points of sequencing.
Any attempt to methodologically or epistemically delimit sequencing therefore requires a specific historiographical or philosophical basis, and the limitations of this choice need to be acknowledged and used to inform any conclusions drawn. In the case of pig genome sequencing, an attempt to reduce sequencing to thin sequencing precludes one from understanding or appreciating many of the key decisions and research directions, especially concerning the purported ‘logic’ of the location and strategy of the thin-centred sequencing. The Sanger Institute was chosen – and seemed ‘logical’ – because significant parts of the physical mapping work had been conducted there, the clones were already there, the prior conducting of human genome sequencing there and the adoption of the human model by the pig community, and the relationships that had been established. So even to understand the objects of a thin perspective of sequencing, one must invoke the work and actors revealed by the thick perspective. In its attention to the production of a sequence with added value and usability, the thick perspective will also allow us to apprehend how genomic research may contribute to strategic policy directions concerning translation. It also helps one to recognise key differences between institutions and their consequences, for example of the production of sequence data at institutions that devote different levels of resources to adding value to the sequence through comprehensive evaluation and annotation.
An attention to the thickness of sequencing leads one to characterise the geography, the temporality and the nature of sequencing work in a fundamentally different way than for the thin perspective. Understood more thickly, sequencing takes longer, has less well-defined start and end points, is more institutionally-diverse, involves a plethora of different skill sets and background knowledge, and involves considerably more actors in general. A thick examination of sequencing reveals the active interpretation, intervention, assessment, evaluation and creativity of scientists. It requires an appreciation of the relationships between scientists, technical staff, project managers, administrators, industries and funders. Throughout the sequencing process detailed in this paper, there was an interplay and interpenetration between adapting and refining protocols and processes and using standardised tools and procedures. Where elements of work have been automated, the manual, creative and interpretive work of scientists may still be required both in and around the automated processes. These scientists work in the processes to evaluate, maintain and refine them, and around them to take advantage of the ‘black-boxing’ in order to concentrate on new problems. In sequencing interpreted in a thicker manner, some of the features of this reconfiguration may be discerned in the apparently centralised work conducted in massive genome centres. For example, in the pig genome project described above, the development of the principles and processes of assembly and annotation had culminated in the use of automated pipelines, yet there was still room for manual intervention both in the later stages of assembly and also in annotation.
For automated sequencing, lower costs have made the geography and concentration of it more diffuse and less centralised. Citing the sequencing services offered by shared facilities in research institutions as enabling sequencing to be “reconfigured as a small-scale, slower and artisanal form of work, subordinated to concrete research necessities,” García-Sancho observes that “other sequencing is possible” (2012, pp. 176–177). Even thin sequencing requires attention to the particular (often-shifting) assemblages of people, institutions, machines and materials that are involved in any particular project. We may therefore develop Fortun's (1999) analysis of the temporalities of genomics. In that, he drew a connection between speed and other factors such as concentration, scale, capital intensivity, and the organisation of labour and space that accelerate the speed of sequencing as well as driving the development and intensification of particular organisational forms such as large-scale sequencing centres.
When one considers sequencing activities more thickly, we may observe different drivers of temporality. Rather than the ever-enhanced speed driven in the thin parts of sequencing by the factors Fortun identifies, alternative priorities may be exhibited. Different organisation of projects and different temporal regimes may be apparent depending on whether we interpret sequencing in a thick or thin manner. In the pig project at the Sanger Institute, the speed of sequencing was halved due to issues of scale and some institutional opposition to the project. This was viewed by many in the pig genome research community as beneficial, as processing and analysis had become – according to SGSC Technical Committee member Craig Beattie – the “rate-limiting step.” The speed of production of sequence data meant that they “were overwhelming the information pipeline.”37 So a reduction in speed of production was not a problem. This was, still, fast science, although it was not necessarily so at all stages of the sequencing described in the thick sense. By attending to the thicker understanding of sequencing, one is able to grasp the institutional, collaborative, translational and infrastructural contexts more fully.
In addition, the particularities of the sequencing work in a given community are defined in a sharper and more finely-grained manner, enabling one to identify the conditions that guided particular decisions and actions. In so doing, one can make comparisons between particular objects of study with the aim of defining more precisely how, and to what extent, the conclusions drawn from one may be applicable to the other. To provide one example of this, we may consider two potential objects of study for historians, philosophers and sociologists of science: pig genome research and human genome research. If we were to conduct research based on a thin interpretation of sequencing, both of these objects of study look much the same. The work forming the focus of the thin perspective on sequencing was conducted by specialist teams at large-scale, highly-automated, high-throughput sequencing centres (one of them, the Sanger Institute, participated in both human and pig genome sequencing). One might therefore expect that findings concerning one project will likely be transferable to the other; to re-quote Alan Archibald “to produce a humanised pig genome.” Yet based on a thicker study of sequencing, we not only de-humanise the pig genome research but genome research altogether. We reveal important differences in: library construction, the continuing and leading role of the pig genetics community in the sequencing work (as against the marginalisation of medical geneticists in the human genome project), the rationale for the production of sequence data and the use of annotation. The thick perspective also leads us to different characterisations of the projects in terms of scale and velocity.
In this paper I do not claim to establish what sequencing or genomics is, nor to base any of the claims that I do make on a supposed representativeness or significance of pig genome sequencing. I would suggest, however, that the characterisation of sequencing and genomics in much of the scholarly literature is – understandably – dominated by human genome sequencing, and in particular, the efforts that fall under the narrative umbrella of ‘The Human Genome Project’. In human genome sequencing, the kinds of work and objects foregrounded by a thin account of sequencing appear to be central, the object of the competition and race between the ‘private’ and the ‘public’ initiatives, the area of the work most associated with charismatic and forceful individuals such as John Sulston and Craig Venter, who themselves have helped shape the narratives dominating journalistic and activist discourse (e.g. Sulston & Ferry, 2002; Venter, 2008; see Hilgartner, 2017, chapter 7, for an acute dissection of the narratives; see also a discussion of the “narrative gap” in accounts of the Human Genome Project in Bartlett, 2008, pp. 124–125).
It is precisely those prominent aspects of human genome sequencing that have been associated with scale, automation, speed and many other properties attributed to genomics. Due to the level of funding and the political stakes involved, the imperative to produce sequence data as quickly as possible was more acute for this project than any other sequencing initiative. As the objects and processes highlighted by a thin interpretation of sequencing are proximate to the immediate production of sequence data – the traces transmitted to computers from the bases – it is the stage encompassing these that has gained prominence. To a lesser extent, assembly garnered attention insofar as it was the draft products of this that were announced at the White House in June 2000. Thus, the human genome project was primarily understood in a thin way, and this is consequently how sequencing has become understood.
Finally, I want to emphasise the iterative and recursive nature of sequencing. Sequencing is the production of a tool as well as a dataset. The products of sequencing are intended to be used in scientific investigation for the production of knowledge claims, but also to further improve tools that can be used in investigation and intervention. It is in this sense that further investigation of the development of sequences and sequencing practices towards their intended use and re-use as tools for research and intervention can potentially be fruitful in improving understanding of translational research processes. A thick perspective enables us to open up those processes.
