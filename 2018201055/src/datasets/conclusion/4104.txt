This study examined the design of CBA and its effects on student engagement in the VLE, satisfaction, and pass rates using a combination of visualisations, and mixed methods on 74 first-level undergraduate modules at the OU. The first finding indicated that, on average, assessment activities accounted for 25% of the total workload, with great variability across modules. Moreover, the workload on other activities decreased when assessment activities were introduced. This implied that educators aimed to balance the total workload when designing CBA.
Secondly, assessment activities were associated with more time spent in the VLE, since the majority of assessment activities required computer use. Learning design in general could explain up to 69% of the variance in VLE engagement. Modules with higher relative frequency of assessment activities are associated with higher pass rates, while no clear relation was found with regard to satisfaction.
Finally, the six case study modules illustrated the diversity of assessment strategies across disciplines in terms of assessment types and approaches. Further visualisations support our findings above as educators reduced the workload on other activities when introducing assessment, and the time spent in VLE increased considerably in assessment weeks.
Our study contributes to CBA innovation by approaching CBA from a learning design perspective using learning analytics. While CBA has been studied extensively in terms of micro-analysis on individual assessment items (Greiff et al., 2015, Greiff et al., 2016) and assessment strategies within a module (Brito and de Sá-Soares, 2014, Tempelaar et al., 2015, Terzis and Economides, 2011), the connection between CBAs and learning designs across a large number of distance learning modules has not been explicitly examined in the past. Therefore, by investigating CBA and other learning activities in tandem, we provided a broader picture of how teachers design assessment activities within their modules and across time.
In line with recent reviews of learning analytics (Ferguson et al., 2016, Papamitsiou and Economides, 2014, Papamitsiou and Economides, 2016), we encourage researchers to look beyond “cold” learning analytics data (such as weekly learning design activities, CBAs and student engagement), as a rich diversity of practice seemed to be present when contrasting six case studies. While our findings highlight a strong relation between weekly learning design activities and student engagement, it is important to bring teachers and educational researchers on board to unpack the complexities of learning.
In terms of practical implications, assessment and feedback are high on the priority list for students and educators, as these link directly to student success and to the success of a course, programme, faculty and university. Some policy makers have already made moves intended to improve the effectiveness of teaching (Ferguson et al., 2016). For example, a Teaching Excellence Framework has been introduced in the UK, and it is likely that measures related to assessment will be used as key indicators. In order to explain how satisfaction and assessment activities are linked and which elements of assessment (balance of activities, spread through module material or assessment methods) have a significant impact on student outcomes, we need to combine research data and institutional data and work together in order to solve this complex puzzle.
