In this work, we introduced a recurrent convolutional neural network architecture, which in addition to learning spatial relations is also able to exploit temporal relations from video. We started with a series of toy examples that showed that our networks are able to solve tasks that require denoising, detecting movement, and retaining uncertainty.
We further carried out experiments on sequences of indoor RGB-D video sequences from the NYUD dataset. Combined with dropout, unsupervised weight initialization, covering windows and conditional random fields, our proposed model improves performance when compared to other non-recurrent baseline models and random forests, obtaining the best results obtained so far without transfer of features from ImageNet.
