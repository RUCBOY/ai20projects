We have developed a simple model for estimating the speedup from vectorization in the event-based particle transport algorithm under two sets of assumptions. First, we assumed that each event type has constant execution time. We then relaxed that assumption and allowed each event type to have a distribution of event times. These assumptions result in two different models for the vector speedup in Eqs. (4), (5). Data collected from simulations of four reactor problems using OpenMC was then used in conjunction with the models to calculate the vector speedup as a function of two parameters: the size of the particle bank and the vector width. The results demonstrate that when events are assumed to have constant execution time, increasing the bank size is sufficient to obtain a speedup close to the maximum possible. We observed that the bank size generally needs to be at least 20 times greater than vector size in order to achieve vector efficiency greater than 90%. When the execution times for events are allowed to vary, however, the vector speedup is also limited by the differences in execution time for events being carried out in a single event iteration. For some problems (for example, the VHTR and the BEAVRS model with many nuclides in fuel), this implies that vector efficiencies over 50% may not be attainable.
We emphasize that this study is only a first step toward understanding the performance of event-based algorithms. A number of assumptions were made to arrive at simple expressions for the time to solution and would not hold for a realistic simulation. For example, we have assumed that there is no cost to data movement inherent in the event-based algorithm through the use of particle stacks. This aspect may be difficult to capture in a theoretical model. We have also completely neglected tallies. For large-scale simulations, tallying physical quantities can consume a significant fraction of the execution time, so any successful model or analysis must account for this. This aspect is sufficiently complicated that a focused study on tallies would be worthwhile.
Another limitation of our study is that we have looked at only one choice for how to divide particle histories into events; namely, we considered only three events: free flight, collision, and boundary crossing. In reality, each of these events is sufficiently complicated that it may make sense to use finer-grained events. When using coarse-grained events, not all the logic will be vectorizable, especially for collision events in a continuous-energy code because of the inherently deep branching logic. Our assumption that the execution time of an event iteration is limited by the maximum serial time of an individual event is perhaps not realistic. Notwithstanding, it does provide a means for modeling the fact that the entire event is not vectorizable. Our analysis points to this being a limiting factor in the achievable speedup.
One must keep in mind that data-level parallelism is only one aspect of achieving performance on modern computer architectures. While achieving good use of vectorization is often important in maximizing performance, other optimizations may be more crucial, especially for codes that tend to be limited by memory latency and bandwidth. MC neutron transport is one such application that has been characterized as being limited by the memory subsystem [16], [17]. In practice, the use of event-based algorithms could yield better performance than the history-based algorithm by virtue of better cache utilization. Capturing such an effect in a theoretical model is notoriously difficult, however.
