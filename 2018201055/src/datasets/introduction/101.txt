Vision-based human–computer interaction is a combined research effort of both Machine Vision Technology and Human–Computer Interaction (HCI) technology. The application of vision-based human–computer interaction technology not only excels in facilitating people’s daily life and improving work efficacy but also completely alters people’s work and life experiences, thus profoundly affects people’s thinking and communication forms. HCI technology contains both aspects of science and technology that enable humans and machines to communicate, interact, and talk to each other. The most common and earliest implementation of the human–computer interaction form in the computer system. Tasks or commands could be transferred to computers in the forms of a written code through the keyboard or mouse. Computers calculate or execute tasks and display the outcomes at the end. The HCI technology benefits from a computer-based science and technology whose key characteristic is defined through input and output devices or called User Interface (UI). To realize the communication and dialogue between people and computers, the research of user interface has always been an active research area in the HCI technology.
Although researchers make full use of the respective advantages of human–computer interaction and image recognition (or machine vision) technology and probe on more new fields in engineering practices, new requirements, and challenges in technology such as exact recognition and understanding of the complicated human body movements, the universality of the real natural environment, the ability to learn and comprehend strange movements, etc. also are put forward when the visual human–computer interaction is a concern. Two aspects could be mentioned. On the one hand, the same type of gestures or actions, which are often recognized as the same outcome by image recognition technology, could be different in diverse environments or “contexts” in a human–computer interaction system. In other words, the meanings they convey could differ much. On the other hand, human social communication is a very complex and time-varying systematic activity. Both the scope and environment of activities are also rich and diverse, which is very difficult for the machine to mimic human beings through a visual simulation. The key to the vision-based HCI technology is the spatial–temporal independence and high universality of human motion recognition with related algorithms. Therefore, several active research efforts have been directed to the area.
In previous researches, the traditional RGB video on human motion recognition was examined by utilizing various methods [1], [2], [3]. However, processing RGB video data encounters several difficulties. For example, it has no visual angle invariance, is sensitive to both light and background alterations, and is not robust to noise. Although many eloquent outcomes have been achieved through the efforts of the researchers for the past few years, the research on human motion recognition has still been very challenging.
In recent years, the release of Microsoft Kinect has brought new opportunities in this field. Kinect devices can collect depth maps in real-time. Hence, they have many advantages over traditional color images. Firstly, the depth map sequence consists of essentially a four-dimensional space and is not sensitive to alterations in lighting conditions. Moreover, it can contain more action information and estimate human contours and bones more reliably. In this manuscript, a multi-modal depth neural network utilizing joint cost function is proposed for human motion recognition combining with two kinds of the DNN architectures to process different types of images. Hence, the mechanism of feature extraction and classification of deep neural networks (DNN) is conducted. The extracted multi-modal features of behavior recognition can effectively improve the accuracy and robustness of motion classification. The main contributions of the manuscript are summarized as follows:
(1) Both Rank Pooling and Back Rank Pooling methods are conventional approaches to preprocess the deep image. In the time domain, the previous method mainly encodes the prominent global features, nevertheless, ignoring the distinguishable motion mode in the space–time domain. Due to the motion information in the space being low, the motion information with higher discrimination to motion recognition is suppressed by that with larger granularity. To extract information on Spatial–temporal alterations during the actions, we select the Spatially Structured Dynamic Depth Images (SSDDI) method as a preprocessing approach for the deep image.
(2) From the application point of view of deep neural networks, different architectures of the DNN accommodate different types of images. In the architecture, the features of the RGB video frames are extracted by the 3D CNN architecture while the characteristics of human motion recognition in the SSDDI graphics based on depth map are extracted by the LSTM.
(3) In addition to the cross-entropy loss, the distance constraint between the feature space of training samples and their center values within each category is added that helps consider the intra-group aggregation and inter-group separation of motion features. The guidance for the deep convolutional neural network to find out features with strong discrimination potential can promote the accuracy of motion classification.
