MPI (Message Passing Interface) is a portable, standard API for writing parallel programs using a distributed-memory programming model. MPI began as an effort in 1992 by a group known as the MPI Forum to define a standard API for message passing instead of the plethora of different APIs provided by existing vendor and research message-passing libraries at the time. Version 1.0 of the MPI specification was released in 1994, and MPI has been widely used since then for developing parallel scientific applications on all kinds of parallel computer systems. Several new versions of the specification, with additional features, have been released over the years, the latest being MPI 3.1 in 2015. MPI 4.0 is expected to be released in a year. MPI has been essential to the success of supercomputing in the past 25 years. It has provided a standard message-passing interface for scalable parallel systems and clusters. It is supported by all supercomputer and cluster vendors, and it is used by essentially all science and engineering parallel simulation codes running on supercomputers and is expected to continue to do so in the next decade.
MPICH [14] began as a research project in 1992 with the goal of developing a portable implementation of the evolving MPI standard as it was being defined. The idea was to provide early feedback on decisions being made by the MPI Forum and provide an early implementation to allow users to experiment with the specification even as it was being developed. In many cases, the MPI design pushed features for new capabilities not seen in previous systems, and MPICH became the reference implementation that proved that these features can be implemented efficiently. Targets for the implementation included all systems capable of supporting the message-passing model—from single-node, shared-memory systems to clusters of systems and the largest supercomputers in the world. MPICH has been and continues to be the de facto implementation driver leading to the success of MPI.
MPICH was always intended as both a research project and a software development project. As a research project, its goal is to explore methods for delivering the highest possible performance for communication in parallel applications on all available hardware. As a software project, its goal is to promote the adoption of the MPI Standard by providing users with a high-quality, high-performance implementation on a diversity of platforms, while aiding vendors in providing their own customized implementations.
MPICH was designed from the ground up to serve as a reference for other derived implementations. It was cleverly layered so that vendors could replace the lowermost “driver” level of MPICH, thus customizing it to their own hardware, while reusing most code. More important, vendors could choose to customize fewer or more layers in MPICH, and all did so.
While the full picture is more complex, MPICH has three major layers [6]. The top layer provides the implementation of the MPI API and provides code for features that are part of MPI but not critical to performance, such as MPI attributes (a way for software libraries and tools to attach information to MPI objects) and error reporting. The next layer, called the “device,” provides architecture-specific optimizations for performance-critical portions of the MPI interface, including point-to-point communication, collective communication with an arbitrary collection of MPI processes, one-sided (remote-memory) communication, and communication with noncontiguous data.1 This layer exposes an internal interface called the MPICH Abstract Device Interface (ADI). Each version of MPICH has provided a third layer within the device: a simple network or shared-memory “module” that is required to provide as little as a simple movement of bytes between processes, thus making initial ports to new architectures easy, but is allowed to override as much of the MPI functionality as it desires.
All layers have a modular design (based mostly on inlining code and sharing data structures in keeping with the capabilities of compilers and runtimes when MPICH was designed) as well as a focus on low latency. These features allow vendors to take advantage of particular capabilities of their systems, such as hardware support for a subset of collective operations (e.g., the IBM BlueGene for reduction operations (MPI_Allreduce) [2]). A full ADI implementation lets vendors optimize nearly the entire stack, leveraging MPICH code to handle the MPI API, internal state, and support for features not critical for performance. This layered design has been key to the longevity of MPICH. Today, most supercomputing vendors use MPICH in order to develop their own proprietary implementation of MPI.
MPICH has always had a focus on performance, as nearly all applications make use of parallelism in order to access performance not otherwise obtainable. Performance in this case includes both low latency (including overhead in the LogP model [3]) as well as high bandwidth of data communications. One early example was a port of MPICH to the NEC SX-4 vector supercomputer [4]; the layered design made it easy to port to a very different CPU and interconnect architecture.
MPICH has continued to evolve over three decades in order to support the evolving MPI standard (now over 800 pages long) and new computer architectures. MPICH has continued successfully in its triple role:
•As a reference implementation that supports the latest MPI features as soon as they are standardized.•As the leading implementation used by vendors (Intel, Cray, Microsoft, Tianhe, Sunway, etc.) for their own products. MPICH-derived implementations are used on practically all the largest supercomputers today as well as on clusters. The popular Intel MPI, Cray MPI, and MVAPICH [15] implementations of MPI are also derived from MPICH.•As a research platform that is used to study how one can handle large-scale parallelism (MPICH has scaled to over 10 million processes) and heavy multithreading and hybrid programming for extreme-scale systems.
MPICH won an R&D 100 Award in 2005.
