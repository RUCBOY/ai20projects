On October 25, 2018, a portrait generated by a machine learning (ML) algorithm called a generative adversarial network (or GAN) (Goodfellow et al., 2014) sold at Christie's art auction for $432,500. As Christie's initial estimate for the piece was $10,000, its sale for over 40 times this expectation shocked the art world. Marketed by Christie's as “the first portrait generated by an algorithm to come up for auction,’’ the painting—entitled Edmond De Belamy (see Figure 1)—struck a chord about the nature of authorship and artificial intelligence (AI) (Cohn, 2018).Download : Download high-res image (1MB)Download : Download full-size imageFigure 1. Image of the Painting Edmond de Belamy, which Sold for $432,500 at Christie's Art Auction
Yet the reality of the painting's creation is not as clear as Christie's purports. Even though AI played a role in generating the artwork, Edmond de Belamy would never have been produced without the help of humans. It was the Parisian art collective Obvious who selected, printed, marketed, and sold the image; but the human involvement does not stop there. The algorithm was trained on the paintings of Renaissance masters, sourced from WikiArt. Ian Goodfellow invented the original GAN architecture, and Alec Radford, Luke Metz, and Soumith Chintala innovated the DCGAN that actually generated the artwork. But perhaps the most relevant here is the then-19-year-old artist and technologist, Robbie Barrat, who wrote code to produce Renaissance-style images with DCGAN (Learn more about his GitHub repo here: https://github.com/robbiebarrat/art-DCGAN) and which was ostensibly lightly repurposed to produce Edmond de Belamy. Barrat noted that Obvious “almost immediately started producing work identical to the outputs of the pre-trained portrait and landscape networks” he had put online (Vincent, 2018). Neither Barrat nor the ML researchers received any of the $432,500, which all went to Obvious.
Although the humans involved in the creation of Edmond de Belamy were essentially cut out of the art's creation narrative, the AI itself was often spoken about as having human-like characteristics. In a press release, Obvious told reporters that “an artificial intelligence managed to create art,” which underpinned their motto that “creativity isn't only for humans.” When Christie's was raising awareness about the impending auction of Edmond De Belamy, they also employed anthropomorphic language to increase hype for the work: “This portrait … is not the product of a human mind. It was created by an artificial intelligence, an algorithm defined by that algebraic formula with its many parentheses” (Anonymous, 2018). Another spokesperson went further saying, “We are offering a public platform to exhibit an artwork that has entirely been realised by an algorithm,” (Hitti, 2018). The media ran with this narrative, creating a discourse that emphasized the autonomy and agency of the algorithm (Table 1 contains further examples.).Table 1. Media Snippets from the Edmond de Belamy CaseQuoteSourceThis portrait … is not the product of a human mind. It was created by an artificial intelligence, an algorithm defined by that algebraic formula with its many parenthesesChristie's (Anonymous, 2018)AI has already been incorporated as a tool by contemporary artists and as this technology further develops, we are excited to participate in these continued conversations. To best engage in the dialogue, we are offering a public platform to exhibit an artwork that has entirely been realised by an algorithm,Christie's (Hitti, 2018)Christie's, the auction house that has sold paintings by picasso and monet at record prices, was poised on Tuesday to set another milestone with the first-ever auction of art created by artificial intelligence.Reuters (Goldberg, 2018)The painting, titled “the portrait of edmond belamy,” was completed by artificial intelligence managed by a Paris-based collective called Obvious, Christie's said.USA Today (Molina, 2018)Whether art or not, the signature of the ‘artist’ at the bottom of the painting gives away its origin as a product of machine learning rather than human hand.PC Mag (Smith, 2018)Once the software “understood the rules of portraiture” using a new algorithm developed by Google researcher Ian Goodfellow, it then generated a series of new images by itself, Fautrel said.NDTV (France-Presse, 2018)Agentic language is bolded.
The story of Edmond de Belamy underscores two general obstacles for the accountability and governance of AI systems, which are critical to understanding the complexity of assigning credit and responsibility in AI art cases. The first obstacle is knowing what the set of possibly relevant human stakeholders are and how they are relatively positioned within an AI system. Indeed, AI is a diffuse term that corresponds to a web of human actors and computational processes interacting in complex ways (Seaver, 2017). This complexity may lead to situations wherein individual responsibility and accountability is obfuscated due to a lack of clear understanding of who the relevant actors are and how they interact. Such lack of understanding can manifest as the Moral Crumple Zone, whereby disproportional outrage is channeled toward a peripheral person of an AI system simply because the person is closest to the transgression (think about an upset customer yelling at the employee at the flight kiosk when their flight is canceled, despite the fact that the employee had nothing to do with the cancellation itself) (Elish, 2019). Our intuitive moral understanding of actors and transgressions may be at odds with the inherent complexity of AI systems.
Previous studies of the social impact of AI have considered a wide range of possible human stakeholders. In the context of autonomous vehicles (AVs), Waytz et al. consider the human passenger, the car itself, the people who designed the car, and the company that developed the car (Waytz et al., 2014), whereas Awad and Levine et al. consider the human passenger, the car itself, the company who created it, and the programmer who implemented the car's software (Awad et al., 2018). In the context of AI art, Eshraghian distinguishes between the programmer, the trainer, and the user (Eshraghian, 2020), whereas McCormack et al. similarly distinguish between the creators of the software, curators of datasets, and those who train the algorithm and modify parameters (McCormack et al., 2019).
A second obstacle is the phenomenon of anthropomorphizing AI systems. With the recent boom of suprahuman performance on such tasks as Atari games (Mnih et al., 2015), Go (Silver et al., 2016), and lung cancer detection (Ardila et al., 2019), we have seen a proliferation of the anthropomorphization of AI in the media (Proudfoot, 2011; Watson, 2019; Salles et al., 2020). This has been exacerbated by the ML literature itself (Lipton and Steinhardt, 2018), where many ML tasks and techniques are described using the same language we would use for a human doing the task—sreading comprehension (Hermann et al., 2015), music composition (Mozer, 1994), curiosity (Schmidhuber, 1991), fear (Lipton et al., 2016), “thought” vectors (Kiros et al., 2015), and “consciousness” priors (Bengio, 2017).
But what is at stake when we anthropomorphize AI? Recent work reveals how anthropomorphization can affect trust. Through a series of experiments involving an unavoidable crash in a driving simulator with cars of varying complexity (i.e., a normal car versus a self-driving car versus an anthropomorphized self-driving car with a human voice and name), Waytz et al. show that increases in the anthropomophization of a car predicts trust in the car (Waytz et al., 2014). Although they mostly focused on the psychological construct of trust, they also found that anthropomorphization affects attributions of responsibility and punishment for the car's mistakes, which is consistent with the established relationship between the agency and perceived responsibility (Epley et al., 2007; Waytz et al., 2014). This builds on a growing body of work that our “mind perception” (which manifests as inferences of intentions, beliefs, and values) meaningfully varies across individuals and shapes our moral judgments (Epley et al., 2007; Gray et al., 2007, 2012; Waytz et al., 2010).
There is also the concern that anthropomorphizing AI systems can “undermine our ability to hold powerful individuals and groups accountable for their technologically-mediated actions” (Watson, 2019). When an AI system causes a moral transgression, it may be the case that the programmer or systems architect can eschew personal responsibility by blaming the “unexpected behavior” of the system, downplaying their own involvement. Along these lines in the context of AVs, Gill found that participants thought harming a pedestrian was more permissible for an AV when compared with a human in a regular car and that the attribution of responsibility to the AV drove the shift in moral judgment (Gill, 2020).
Ultimately, as AI systems become further integrated into human decision-making, it is likely that they will be increasingly anthropomorphized. Thus, understanding the psychological mechanics of this “absorption of responsibility” by the AI is important for the accountability and governance of AI systems. In particular, in line with Watson, one might expect that increased anthropomorphicity of an AI system may diminish the perceived responsibility of all human actors involved (Watson, 2019). Yet ultimately this is an empirical question subject to inquiry.
In this article, we use the case of Edmond de Belamy to explore these questions in the context of AI art: not only was there ambiguity about the humans involved in the creation process but also rampant anthropormorphizaton of the process itself.
To those ends, we focus on two main research questions:
1.How do people think credit and responsibility should be allocated to various actors in the production of AI art?2.How do these intuitions vary based on people's perceptions of the anthropomorphicity of the AI system?
These research questions are closely related to, but distinct from, the broader philosophical questions related to AI art, such as “Can computers create art?” Hertzmann traces the histories of several art automation technologies (such as the camera and animation) to argue that generative AI technologies are yet another artistic tool, with their own distinct affordances (Hertzmann, 2018). As such, he contends that art is necessarily authored by social agents, and thus AI algorithms (as understood today) cannot be credited with authorship of art. McCormack et al. build on these ideas in the context of the Edmond de Belamy phenomenon (McCormack et al., 2019). They conclude that “The creator of the software and person who trained and modified parameters to produce the work can both be considered authors,” but that “AI systems are not broadly accepted as authors by artistic or general public communities.”
These scholars make convincing arguments for why AI systems ought not be credited with authorship. Our investigation concerns a different question, namely, how does the public assign credit to an AI involved in making art? In particular, we use a series of vignette studies to directly explore the relationship between anthropomorphicity of the AI and the levels of responsibility assigned to various actors in an AI system. By focusing on peoples' intuitions in these vignettes, we consider credit and responsibility in the broad sense of public perception, rather than in the legal or prescriptive sense (Colton, 2008; Eshraghian, 2020).
The Terminology of AI ArtComputer-generated artwork has a long and diverse history and involves a wide range of AI tools and AI-human interaction paradigms. Some use interactive evolutionary algorithms to crowd-source the creation and curation of artifacts (Draves, 2005; Epstein et al., 2020; Secretan et al., 2011; Sims, 1991), whereas others have created platforms for artists and practitioners to use AI models, such as RunwayML, GANPaint (Bau et al., 2018), and DeepAngel (Groh et al., 2019). In addition to GANs, many other visual generative algorithms have been explored, such as neural style transfer (Gatys et al., 2016), Computational Aesthetics (Machado et al., 2008), Fractal Flame (Draves, 2005; Draves and Reckase, 2003), deep learning-powered adversarial evolution (Blair, 2019), and hybrid methods (Colton, 2008, 2012). Here, following the case study of Edmond de Belamy, we trace a particular type of AI art, where the system is presented with human artwork and attempts to mimic the style of the human artists. This process involves both a specific AI technology (e.g., the GAN) and a corresponding workflow, which inspired our vignettes (described in full in Tables S1 and S2).
