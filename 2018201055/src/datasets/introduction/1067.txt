Sign language (SL) is the main method of communication among the hearing-impaired or deaf individuals and members of their community who interact with them. Definitely, there are alternative forms of communication, however, these methods may not support natural interaction, where the receiver of the communicative message does not have to learn sign language or any other method of communication.
It is here that assistive technologies become a viable solution. Technologies that support natural interaction with/among users can be utilized to assist in the hearing-impaired or deaf individuals’ successful social integration and to communicate with others in natural ways.
Assistive solutions which can interpret SL and translate it into verbal or textual cues is required to facilitate the inclusion of the hearing-impaired or deaf individuals in society. Nonetheless, and in spite of the growing research in this area, there is still a persistent need to develop efficient and easy to use assistive devices and specialized SL interpreters that perform such tasks.
Arabic Sign Language (ArSL) research acknowledges the demand for developing technologies which can be used to address the needs of the hearing-impaired or deaf people in Arabic communities as well as target the creation of inclusive environments for persons suffering from hearing loss. Various social institutions like health, education, transportation, etc. encourage and support this kind of research in Arabic countries, but the research outcomes reported until now leave much to be desired especially regarding the reliability and practicality of the developed solutions which eventually are rarely being adopted in real life situations. For example, one of the challenges for the deaf seeking the completion of their higher education is the absence of general admission to most university majors due to the small number of specialized solutions (interpreters, for example) that facilities their communication and effective inclusion.
One of the most prominent approaches in research addressing hearing-impaired or deaf individuals’ inclusion in society makes use of technologies with natural interfaces that can be utilized as SL recognition systems, which incorporate advancements in gesture recognition, depth sensors, and machine learning algorithms.
Hand gestures in SL use the palms, finger positions and shapes to generate forms that refer to different letters and phrases with different meanings in different cultural contexts [1,2]. Hence, gesture recognition refers to recognizing meaningful expressions of a motion produced by a human, including the hands, face, head, and body. Basically, gesture recognition is essential in application development ranging from ones used to interpret sign language to applications in the field of virtual reality [3]. More specifically, there are two types of hand gestures: time independent static hand gestures are those in which the hand position does not change during the gesturing period, and time dependent dynamic hand gestures, where the hand position changes continuously with respect to time [4].
In SL recognition systems that rely on natural interaction, depth sensors are a requirement. Depth sensors provide essential data about every object near the device, which will help in extracting many of the user body's features, such as the neckline, thumb and index fingers. In real life, hand and finger recognition needs extra extraction of features and uses complex procedures to achieve accurate gesture recognition using machine learning approaches [5].
In such recognition systems, machine learning algorithms are used to translate SL based on human a gesture recognition process, which passes through a human detection phase and a gesture recognition or classification phase. Moreover, in the science of statistics and machine learning, basic learning algorithms that work on part of the data do not provide superior predictive performance than the ensemble methods that use several learning algorithms. The main reason for combining models of learning algorithms is their collaborations, where all models break down to capturing the high dimensional data in a more profound comprehension [6,7]
The remaining content of this article is organized as follows: Section 2 presents the research objective, followed by Section 3, which introduces a literature review of some of the related works. Then, a thorough description of the gesture recognition pipeline is provided in Section 4. Section 5 details the proposed model (DPM). Section 6 highlights the results and discussion of important findings. Finally, Section 7 reiterates the importance of the study, the results and suggestions for future work.
