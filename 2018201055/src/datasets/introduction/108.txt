Lung cancer is one of the fatal disease, which has killed 1.6 million people, about 19% of all cancer deaths in 2017 worldwide [1]. It is known that earlier-stage lung cancers have more possibility of being cured, but only 15% of all the diagnosed cases are preliminary, almost detected with chest computed tomography (CT) in the routine checkup. So far the CT is the best way to detect early-stage pulmonary nodules because of non-invasive and inexpensive.
Recall the entire reading process of CT image. The radiologists always screen all the CT sequences images, in which the early-stage instances are just imaging with the limited pixels, even if increasing X-ray dose [2]. When the radiologists find a suspected lesion with low resolution (LR), they always advise the examinees to review invasive examinations (e.g., bronchoscope) to get more rigorous diagnosis. However, the examinations are uncomfortable and risky (e.g., infection and asphyxia). Hence, the image enhancement is a suboptimal solution for early-stage pulmonary nodule detection, and worth being boosted in the applications of Computer-Aided Diagnosis/Detection system (CADs).
uper-resolution (SR) is a conventional solution for enhancing image quality. The methods utilize the pixel and features of low-resolution (LR) image to reconstruct its high-resolution (HR) versions, such as linear and cubic-spline interpolation methods. For most of the SR methods, their optimization targets minimize the mean squared error (MSE) between the recovered HR image and the ground truth (GT). This category of methods is ill-posed for capturing high texture detail. The smaller MSE can only increase the peak signal-to-noise ratio (PSNR), but not improve the perceptual experience. To overcome the drawback, SRGAN (Super Resolved Generative Adversarial Networks) [3] defines a perceptual loss function for its generator to focus on better perceptual information of the generated HR counterparts. Its results fulfill the user demands for photo-realistic but are also questionable. Are the generated images objective enough for more rigorous generative tasks, such as nodule detection? As the exploratory works, some medical SR methods are used for data augmentation to solve the imbalance ratio of the positive/negative samples in the original dataset. They can be roughly divided into two representative methods, including noise-to-image methods and image-to-image methods. The former group is fully referred to as the idea of the original GAN [4], generating the pathological image by random noise samples. Another is always used to produce a malignant derivative from a benign image by appending some malignant pathological conditions. Compared with the noise-to-image methods, the latter group can significantly reduce computational cost. Moreover, its procedure can be understood as a semantic editing process. Different from the photo-realistic constraints, the editing the process needs the higher-level semantic constraints to map the features extracted from the input LR images to those in the generated HR counterparts. Fig. 1 shows a series of the generative cases, in which the first column contains two original low-resolution inputs, the other columns list the generative 4× SR results of SRGAN and the proposed work, respectively.
We observe that the SRGAN-based super-resolved images are realistic enough to make them indistinguishable from their ground truths, which reflect the same visual response as the definition of photo-realism [5]. Besides, we believe another realism need to be followed to emphasize the functional cognition of the generated HR images, called functional-realism. It could quantify the medical image’s functional structures to supply more reliable imaging information as the visual criteria for suspected nodule detection. We use functional-realism as an editing criterion of an image-to-image GAN architecture called functional-realistic GAN (FRGAN) to ensure the more exact visual semantic information for the paper’s generated HR images. We try to generate high-resolution counterparts for an input early-stage nodule image. Meanwhile, we also learn the semantic rules of the generator. For the first goal alone, it has lots of SISR solutions, such as SRResnet, SRGAN [3] and WGAN-GP. Both for PSNR and SSIM, these methods have reached higher performance while gaining unsatisfactory MOS. Some improved image-to-image methods are proposed to reconstruct ROI/VOI with the additional, conditional setting, including size, location, and background context. However, the generator cannot yield its desired results than its expensive cost. The main reason for the non-ideal results come the conditional setting without semantic production rules which fit with the doctors’ knowledge. Therefore, we supplement an assistant generating process for structural-semantic feature sequences.Download : Download high-res image (138KB)Download : Download full-size imageFig. 1. (a) Original LR images; (b) SRGAN-based SR images and (c) FRGAN-based SR images, generated from the LR counterparts after 20 training epochs.
We also need an explicable mode of the fundamental semantic unit and the corresponding grammar model for a better structural-semantic representation. To meet the requirements of the model’s scalability and flexibility, attributed relational graph. (ARG) [6] has almost become the optimal solution for the editable semantic representation. Its interesting follow-up study, named AOGNets [7] can split the input feature map into different feature channels as a sentence of words, and then hierarchically select and integrate the words into a word group (e.g., phrase/sentence) based on a structure/dependency grammar, respectively. A more convenient and logical editable word group offers better performance for feature exploration and reuse. Due to the natures of the pulmonary nodule, its image features and the semantic features of its EMR are the different ways to describe the identical image in a standard diagnostic procedure, where the parse tree for semantic structure, the AOGs for visual feature structure. FRGAN is also an image-to-image GAN, parallels two generators for semantics and visual features, respectively. Its generative stage likes a sentence making, and the discriminator is used to point out the ill-formed sentences. The two main contributions of this paper include:
(1) It proposes a novel image-to-image GAN architecture for early-stage pulmonary nodule detection, called FRGAN. Its generative stage is abstracted as a process of sentence completion based on AOGNets. More complicated features are explored, reused, and then gradually socketed into the original inputs’ suitable fields in a hierarchical and compositional way. Due to the grammar-guided functional combination, the generator owns a better performance to fabricate the HR nodules’ image with the potential malignant pathologies of its LR inputs;
(2) It implies a proper data augmentation (DA) method to balance the positive/negative ratio (e.g., 1:1), which may boost the detection performance of the CNN-based discriminator for the false positive reduction task in pulmonary nodule detection. On 1034 scans merging with the self-collected dataset and LUNA16 dataset, the proposed architecture can achieve a CPM score of 0.915.
The remainder of this paper is organized as follows: we detail our methodology in Section 3, and report our experimental results and discuss some critical options in Section 4, followed by concluding our contribution in Section 5.
