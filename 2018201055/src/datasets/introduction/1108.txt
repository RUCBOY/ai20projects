It is widely agreed that machine learning technique is a critical branch of artificial intelligence. The importance originates from its powerful applications ranging from playing go [1] to facial recognition [2] and web searches [3]. A variety of machine learning techniques, such as support vector machine [4], decision trees [5], and deep learning [1], are emerging for dealing with the problems encounted in our life related to artificial intelligence. Among them, an intriguing one known as reservoir computing approach has recently attracted increasing attention [6], [7], [8], [9], [10]. In particular, intensive studies demonstrate that this approach has been used successfully for chaotic system prediction [7], inferring unmeasured variables [11], and attractor reconstruction [12]. In this sense, reservoir computing technique offers an alternative way for modeling and characterizing nonlinear dynamics, especially for chaotic systems.
When using reservoir computing technique for modeling a given chaotic time series, a number of reservoir parameters, such as a “leakage” rate, size of the reservoir network, and the number of the training data, need to be given in advance. However, different experts will typically construct not identical reservoir computers for capturing its evolutionary mechanism. In this context, an intriguing problem is what is the common feature linking these superficially distinct reservoir computers? In this paper, we demonstrate that synchronization is a common feature connecting reservoir computers upon learning the same chaotic system. Specifically, we show that by sending a common signal, the synchronization phenomenon will be present in the trained reservoir computers. In fact, synchronization is a universal concept in nonlinear sciences [13], [14]. Moreover, we find that even when the transmitted signal is contaminated by extensive white or colored noise, synchronization will still occur. We further show that this synchronization feature has a potential application for chaotic masking.
