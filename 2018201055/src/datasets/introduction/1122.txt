High-performance computers continue to increase in speed and capacity, with exascale machines expected to be delivered in 2021. Alongside these developments, architectures are becoming progressively more complex, with multi-socket, multi-core central processing units (CPUs), multiple graphics processing unit (GPU) accelerators, and multiple network interfaces per node. This new complexity leaves existing software unable to make efficient use of the increased processing power.
For decades, processor performance has been improving in each generation consistent with Moore's Law doubling transistor counts every two years and Dennard Scaling enabling increases in clock frequency. Combined, these doubled peak performance every 18 months. Since Dennard Scaling ceased around 2006 due to physical limits, the push has been to multi-core architectures. Instead of getting improved performance for free, software had to be adapted to parallel, multi-threaded architectures.
In addition to multi-threaded CPU architectures, hybrid computing has also become a popular approach to increasing parallelism, with the introduction of CUDA in 2007 and OpenCL in 2009. Hybrid computing couples heavyweight CPU cores (using out-of-order execution, branch prediction, hardware prefetching, etc.) with comparatively lighter weight (using in-order execution) but heavily vectorized GPU accelerator cores. There is also heterogeneity in memory: large, relatively slow CPU DDR memory coupled with smaller but faster GPU memory such as 3-D stacked high-bandwidth memory (HBM). To take advantage of these capabilities, modern software has to explicitly program for multi-core CPUs and GPU accelerators while also managing data movement between CPU and GPU memories and across the network to multiple nodes.
The compute speed, memory and network bandwidth, and memory and network latency increase at different exponential rates, leading to an increasing gap between data movement speeds and computation speeds. For decades, the machine balance of compute speed to memory bandwidth has increased 15–30% per year (Fig. 1). Hiding communication costs is thus becoming increasingly more difficult. Instead of just relying on hardware caches, new algorithms must be designed to minimize and hide communication, sometimes at the expense of duplicating memory and computation.Download : Download high-res image (255KB)Download : Download full-size imageFig. 1. Processor and machine balance increasing, making communication relatively more expensive. Data from vendor specs and STREAM benchmark [1].
Very high levels of parallelism also mean that synchronization becomes increasingly expensive. With processors at around 1–2 GHz, exascale machines, with 1018 floating point operations per second, must have billion-way parallelism. This is currently anticipated to be achieved by roughly 1.5 GHz × 10,000 nodes × 100,000 thread-level and vector-level parallelism. Thus parallelism must become asynchronous and dynamically scheduled.
Mathematical libraries are, historically, among the first software adapted to the hardware changes occurring over time, both because these low-level workhorses are critical to the accuracy and performance of many different types of applications, and because they have proved to be outstanding vehicles for finding and implementing solutions to the problems that novel architectures pose. We have seen architectures change from scalar to vector to symmetric multiprocessing to distributed parallel to heterogeneous hybrid designs over the last 40 years. Each of these changes has forced the underlying implementations of the mathematical libraries to change. Vector computers used Level 1 and Level 2 basic linear algebra subprograms (BLAS); with the change to cache-based memory hierarchies, algorithms were reformulated with block operations using Level 3 BLAS matrix multiply. Task-based scheduling has addressed multicore CPUs, while more recently—as the compute-speed-to-bandwidth ratio increases—algorithms have again been reformulated as communication avoiding. In all of these cases, ideas that were first expressed in research papers were subsequently implemented in open-source software, to be integrated into scientific and engineering applications, both open-source and commercial.
Developing numerical libraries that enable a broad spectrum of applications to exploit the power of next-generation hardware platforms is a mission-critical challenge for scientific computing generally, and for HPC specifically. But this challenge raises a variety of difficult issues. For instance, programming models and hardware architectures are still in a state of flux, and this uncertainty is bound to inhibit the development of libraries as new configurations and abstractions are tried. At the same time, it seems prudent, if possible, to build on top of existing libraries instead of developing entirely new ones, since this will amortize some of the software maintenance costs, provide backward compatibility, and make transition for applications easier; and yet including radically different algorithms and methods at a low level, without radically altering usage characteristics of familiar packages at a high level, is a difficult software engineering problem. Moreover, many HPC applications will need to run on platforms ranging from leadership-class machines to smaller-scale clusters and workstations. These architectural changes have come every decade or so, thereby creating a need to rewrite or refractor the software for the emerging architectures. Scientific libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications can be rapidly constructed—with improved robustness, portability, and sustainability.
This process of writing new generations of numerical software for new architectures has, informally, led to the translational process illustrated in Fig. 2, which starts with basic research to develop high performance, numerically stable methods. This research grew out of a motivation to have efficient and stable algorithms on state-of-the-art architectures. Out of that research comes new mathematical algorithms that are developed into robust software libraries that are portable across platforms and include an extensive testing suite and documentation. Applications start to use these libraries, which are eventually adopted by system vendors such as AMD, Cray, IBM, and Intel for inclusion in their system software. Ideally, software goes through a standardization process, as in the case of MPI and BLAS, while other software becomes a de facto standard, like LAPACK. With this standardization comes widespread acceptance. Throughout this process, feedback is exchanged between the math library developers, application developers, and vendors. Underlying this process is an environment that includes: community involvement; an emphasis on high performance, efficiency, and portability; development of software that is freely available under a liberal open-source license; and ongoing software maintenance of the libraries. This general translational process was published by Abramson and Parashar [2]. Here, basic research and robust software corresponds to the lab in their concept; early adoption by applications and vendors corresponds to the locale, and standardization and widespread acceptance corresponds to the community. In this paper, we will look at how this translational research has affected the development of mathematical software libraries.Download : Download high-res image (197KB)Download : Download full-size imageFig. 2. Translational approach for mathematical software.
