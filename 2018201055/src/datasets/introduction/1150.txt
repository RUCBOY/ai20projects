Realistic computer-generated (CG) face stimuli are now widely available for studying the perception of emotions and other social cues from the face. Computer animation models are a popular choice in studying facial expressions, as they allow generating research stimuli in ways that would not be possible with human actors (e.g., Jack et al., 2012). Some CG emotional facial expressions can also trigger physiological responses that are as intense as those of real human faces (e.g., Joyal, 2014). At the same time, most CG face models are still easily recognizable as human-made, especially when they are trying to express emotional or other social cues, which indicates that their neural processing likely diverges from real faces in the early perceptual phases. Whether CG faces are viable stimuli for research purposes hinges on whether they can tap into the same social and emotional neural processes as real human faces. Neural processing of social cues from real and CG faces has still received scarce research attention, even though it has important implications for research and human computer interaction. In the present investigation, we aim to uncover whether neural processing of real and carefully matched CG faces diverges, focusing on two social cues: gaze and emotion.
Face perception is sustained by a distributed and interconnected neural system (Fox et al., 2009; Haxby et al., 2000; Ishai et al., 2005; Rossion et al., 2012; Vuilleumier and Pourtois, 2007). The lateral fusiform gyrus (FG), often termed as the fusiform face area (FFA), is often considered the major entry node into this network (e.g., Ishai, 2008). According to Haxby's model (Haxby et al., 2000; see also Iidaka, 2014), the inferior occipital gyrus (IOG; or the occipital face area, OFA) encodes low-level visual features of faces and provides input to the FFA for encoding invariant facial features (e.g., identity) and to the pSTS for encoding variant aspects of faces (e.g., gaze direction and facial expressions). Furthermore, Haxby's model posits that several other extended regions function in concert with these core regions to extract meaning from faces. In particular, the amygdala (AMG) can interact with core systems with respect to processing emotional and threat-related information (e.g., Mattavelli et al., 2014).
Given its focal role in integrating facial information and encoding invariant facial features, the FFA is a likely candidate for detecting quantitative differences between real and CG faces. Previous electroencephalography (EEG) studies have demonstrated weaker face-specific N170 responses over occipital-parietal areas for faces with decreasing realism (with the exception of neonatal stylizations; Schindler et al., 2017) and for robot as compared with human faces (Dubal et al., 2011), which provides indirect evidence for realism-related encoding in the FFA. In contrast to previous EEG studies, fMRI studies have provided inconsistent findings on the involvement of the FFA in distinguishing real and artificial faces: James et al. (2015) observed greater responses to real as compared with cartoon faces, whereas Tong et al. (2000) found no evidence for such differences. Assuming that near-human artificial entities can elicit aversive and even eerie feelings in their observers, as suggested by the uncanny valley hypothesis (Mori, 1970/2012), it is conceivable that CG faces would also evoke greater threat-related responses in the AMG than real faces. Hence, our first research question (Q1) was whether real and CG faces, irrespective of facial expression, are processed differently in the face perception network (the FFA and AMG in particular).
Given that the FFA and AMG are both involved in encoding emotions from human faces (e.g., Fusar-Poli et al., 2009), it is also reasonable to ask whether emotional facial expressions of real as compared with CG faces would evoke greater responses in these regions. Previous fMRI studies using robot faces as research stimuli have found mixed results for the FFA. Gobbini et al. (2011) showed that emotional facial expressions posed by a robot and a real human evoke similar responses in core face perception regions (the OFA, FFA, and STS), whereas Chaminade et al (2010) observed greater responses to a robot's facial expressions in the FFA and occipital regions. Several fMRI studies have already employed CG facial expressions of emotion in lieu of real ones; as one example, Said (2010) investigated categorical and non-categorical responses in the pSTS using CG faces displaying morphs between anger and fear. To the best of our knowledge, to date only one fMRI study has yet explicitly compared real and CG facial expressions with each other. In this study, Moser et al. (2007) tentatively showed (p < 0.05, uncorrected) that CG as compared with real facial expressions elicit weaker responses in the FG, STS, and OMPFC but, importantly, similar activations in the AMG. However, their results were pooled across all basic emotions, which begs the question of whether different AMG responses to real and CG faces could be evoked by specific facial expressions. In the present study, we focused specifically on anger and fear, given that previous evidence indicates that the AMG is more sensitive to fear (Costafreda et al., 2008; Fusar-Poli et al., 2009) and possibly to anger (Mattavelli et al., 2014) than to other basic emotions. In our second research question (Q2), we hence asked whether emotional facial expressions of real as compared with CG faces would evoke different responses in the FFA and AMG.
Besides facial expression, gaze direction is arguably one of the most important social signals conveyed by the face. Previously, single-unit recordings in the macaque monkey have identified AMG cells that are sensitive to direct gaze (Brothers and Ring, 1993). Faces with direct as compared with averted gaze have consistently evoked greater AMG responses in PET (Calder et al., 2002; Kawashima et al., 1999; however, see also Calder et al., 2002; Wicker et al., 1998) and in fMRI neuroimaging studies with human participants (Burra et al., 2013; however, see Engell and Haxby, 2007). One possibility is that the AMG, via a subcortical face processing route, mediates affective responses to being looked at by another person (de Gelder and Rouw, 2001; Johnson, 2005; Nummenmaa and Calder, 2009; Senju and Johnson, 2009; Tamietto and de Gelder, 2010). Interestingly, in an EEG study, Pönkänen et al. (2011) showed that direct as compared with averted gaze evokes greater face-sensitive responses when observing live faces but not when observing face images. This suggests that the neural processing of direct gaze could be mediated by the perceived animacy of faces. Hence, our third research question (Q3) was whether real as compared with CG faces with direct gaze would evoke different responses in the AMG.
Instead of evoking effects independently of each other, gaze direction and facial expression could also interact. In particular, according to the shared signal hypothesis, gaze direction can amplify threat signals conveyed by angry (threat from the observed person) and fearful (threat from environment) facial expressions (Adams et al., 2003). Anger is consistently recognized faster and rated as more intense when combined with direct gaze, whereas fear is recognized faster and rated as more intense when combined with averted gaze (e.g., Adams and Kleck, 2005, 2003; El Zein et al., 2015; Sander et al., 2007; however, see also Bindemann et al., 2008). In agreement with the AMG's role in threat processing, anger appears to evoke greater responses in the AMG when paired with direct rather than averted gaze (Sato et al., 2004). Recent evidence also indicates that fear evokes greater AMG responses when paired with averted rather than direct gaze, but this pattern only occurs when faces are presented briefly and it is reversed at longer display times (Adams et al., 2012; van der Zwaag et al., 2012). Several previous studies using longer display times actually demonstrated the opposite pattern to the shared signal effect (Adams et al., 2003; Hadjikhani et al., 2008; Sauer et al., 2014; Straube et al., 2010). Using CG faces, N'Diaye et al. (2009) demonstrated the shared signal effect for angry and fearful facial expressions in the AMG and other face-sensitive regions (e.g., the IOG and the right FG). Previous studies have not yet compared the shared signal effect in real and CG faces, however. Hence, in our fourth research question (Q4), we asked whether real and CG faces would evoke different shared signal responses in the AMG.
