Trading in the stock market is an attractive field for people from various domains ranging from professionals to laymen in the hope of lucrative profits. It is not so easy to earn profit from trading in the stock market because of the uncertainty involved in the stock price trend. Traders’ emotion is also one of the reasons for losses in stock market trading. The conventional wisdom is that the stock market trading is the game of GREED and FEAR. The trend and the price of stock changes so frequently that a human trader cannot react instantly every time (Chia et al., 2015, Huang et al., 2014).
We can replace the human trader with a computer program. The program will trade as per trading logic as written by the programmer, and this process of automated trading is called Algorithmic Trading (AT) (Treleaven et al., 2013, Yadav, 2015). AT eliminates human emotions, and also the time required in AT for making trading decisions, and executing the taken trading decision is far less compared to a human trader (Andriosopoulos et al., 2019, Meng and Khushi, 2019). Trading decisions can be Buy, Sell, or Hold (do nothing) stock of companies registered on the stock exchange. The stock exchange is the platform where traders can buy or sell the stock. Generally, stock exchanges are under the regulations of the Government in many countries.
The trader does the stock trading for profit, which depends on the timing of buying or selling. Rules which determine when to sell, buy, or hold the stock are called trading strategies. There are many trading strategies available, for example, Mean reversion (Miller, Muthuswamy, & Whaley, 1994). The same predefined trading strategy is not always profitable as it might be profitable in one of the trends out of uptrend, downtrend, or sideways trend but not in all trends. One of the significant research problems in stock trading is the selection of the best trading strategy from the pool of trading strategies at a specific instant of time. It is helpful to select a strategy if we have some predictive ideas about the future trend of the stock price.
The prediction of future trend or price of a stock is also a research problem. The stock price of any company in the stock market depends on diverse aspects like the financial condition of the company, the company’s management, policy of the government, competition from other companies, natural disasters, and many more (Fischer, 2018). Many of these aspects are uncertain aspects like the government can hike tax on petrol to promote the use of electric vehicles. News about a company can cause a sudden rise or fall in the price of the stock of the company. As the price of the stock depends on news, long-duration (Week, Month, or Year) prediction of price or trend of the stock is not reliable. Whereas the prediction methods can work well if the duration of prediction is small (Minute, Hour, or a Day) as chances of big impacting news related to the company are reduced (Zhu, Yang, Jiang, & Huang, 2018).
Trading strategies based on predictions are mostly static in nature (James et al., 2003, Fong et al., 2012, Hu et al., 2015). In a static strategy, once a strategy is decided and deployed for trading, it remains unchanged for the entire trading period. Static trading strategies have a high risk as the trends in the stock market are uncertain and change very frequently. An ideal trading strategy should be dynamic, and it should be capable of changing its trading decision as per the changes in the stock trend. So, instead of using any predefined strategy, we should use a self-improving trading strategy from the current stock market behaviour (Environment). Reinforcement Learning (RL) can develop such a self-improving trading strategy. In RL, the learning base on the rewards received from the environment (Sutton and Barto, 2018, Moody et al., 1998, Gao and Chan, 2000, Du et al., 2016).
In this research paper, our objective is to develop a self-improving agent-based Algorithmic trading model (a computer program), that will find a dynamic trading strategy using Reinforcement Learning (RL) from the current stock market patterns. Once the strategy found, it will be deployed for stock trading and will be updated to incorporate the changes in the stock market patterns. In this study, we have used a model-free off-policy RL algorithm called as Q-learning (Garcia-Galicia et al., 2019, Lee et al., 2007). The proposed trading model will help algorithmic traders for short term trading. We experimented with the proposed model on various individual stocks from the Indian stock market and also tested on the index stocks of the Indian and the American stock market. In this paper, we propose an innovative way to use unsupervised learning method k-Means to represent the behaviour of the stock market (environment) using a finite number of states. Grouping historical information using unsupervised learning methods to represent the behaviour of the stock market (environment) using a finite number of states for trading using Reinforcement Learning (RL) is key to our proposed work. No work is available for the Indian Equity stock market using the Q-learning method of RL with a finite number of discrete states of the environment.
The RL agent to handle the financial portfolio is proposed by Garcia-Galicia et al. (2019). The RL agent to improve the trading strategy based on the box theory proposed by Zhu et al. (2018). The author Lee et al. (2007) proposed use of multiple RL agents for trading. The authors in Park, Sim, and Choi (2020) proposed deep Q-learning based trading strategy for a financial portfolio with continuos size state space. They experimented with their proposed model on US Portfolio and Korean Portfolio data. The authors in Calabuig, Falciani, and Sánchez-Pérez (2020) proposed a Reinforcement Learning model for financial markets using Lipschitz extensions for a reward function. They produce some more states artificially for better learning of the model. The authors in Wu et al. (2020) proposed a stock trading strategy using deep reinforcement learning methods and Gated Recurrent Unit used to capture significant features from a raw financial dataset. They claimed that the proposed model outperforms the DRL trading strategy and Turtle trading strategy.
All the models which use deep reinforcement learning have continuos and infinite state space to represents the behaviour of the environment. For a human trader, the state-action mapping of such a model is a black box, whereas, in our proposed models, this is not the case as we are using finite states. The author Pendharkar and Cusatis (2018) recommended discrete RL agents to do automated trading for a personal retirement portfolio. The authors in Yun, Lee, Kang, and Seok (2020) proposed a two-stage deep learning framework for the management of the portfolio. And used cost function to address absolute and relative return. Their model outperforms ordinary deep learning models. The authors in Alimoradi and Kashan (2018) proposed a method for extracting stock trading rules using League Championship Algorithm and backward Q-learning. They claimed that their proposed model outperforms Buy-and-Hold and GNP-RA methods.
The rest of the paper organised as follows: Section 2 describes methodologies used like Q-learning. Section 3 tells details of the proposed models and also provides background for the proposed models, and Section 4 discussed the experimental results. The conclusion the paper is described lastly in Section 5.
