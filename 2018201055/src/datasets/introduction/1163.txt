Algorithms are increasingly being used in high-impact domains of decision-making, such as loans, hiring, bail, and university admissions, with wide-ranging societal implications. However, issues have arisen regarding the fairness of these algorithmic decisions. For example, the risk assessment software, COMPAS, used by judicial systems in many states, predicts a score indicating the likelihood of a defendant committing a crime if given bail. ProPublica analyzed recidivism predictions from COMPAS for 10,000 criminal defendants, looked at false positive rates and false negative rates for defendants of different races, and found the tool to be biased against black defendants [3]. Equivant (formerly called Northpointe), the company that developed the COMPAS tool, on the other hand, focused on positive predictive value, which is similar for whites and blacks [12]. That is, by some measures of fairness, the tool was found to be biased against blacks; meanwhile by other measures, it was not. Which definition of fairness should be measured?
The above scenario is not a rare case. Given the increasing pervasiveness of automated decision-making systems, there's a growing concern among both computer scientists and the public about how to ensure algorithms are fair. While several definitions of fairness have recently been proposed in the computer science literature, there's a lack of agreement among researchers about which definition is the most appropriate [17]. It is very unlikely that one definition of fairness will be sufficient. This is supported also by recent impossibility results that show some fairness definitions cannot coexist [23]. We propose that to the extent computer science researchers are interested in public responses to algorithmic definitions of fairness, it would be important to investigate these public views [26], [27], [25], [8], [34].
Substantial research has been done within fields such as moral psychology [35], [6], law [16], [32], and sociology [7] in order to understand people's perceptions of fairness. We contribute to an emerging area of research on how the general public views fairness criteria in algorithmic decision making. Prior research has investigated the influence of different factors on views of algorithmic fairness [30], perceptions of discrimination in targeted online advertising [31], perceptions of features used in algorithmic decision making [20], [19], perceptions of justice in algorithmic decision making under different explanation styles [8], and preferences about equality of false positive/negative rates and accuracy in different contexts [33]. In contrast to this work, our goal is to understand how people perceive the fairness definitions proposed in the recent computer science literature, that is, the outcomes allowed by these definitions. We believe this understanding will open an important conversation between computer science researchers and the public in how precisely to define algorithmic fairness when building AI systems.
We take three definitions of fairness from the computer science literature, and design experiments to study how laypeople perceive these definitions. Overall, within the domain of loan allocations, the results show a public preference for one of these definitions: calibrated fairness. Furthermore, the results show that providing contextual information regarding the recipients of the loan applicants – that is, sensitive information on race or gender – influences respondents' perceptions of fairness. Our results provide some evidence for public support for affirmative action within the domain of loan allocations.
By testing people's perception of different fairness definitions, we hope to spur more work on understanding definitions of fairness that would be appropriate for particular contexts. In line with recent work examining public attitudes of the ethical programming of machines [9], [5], we suggest that these public attitudes serve as useful and important input in a conversation between technologists and ethicists. Much of the work in this line of literature has used opinion polling (or more generally crowdsourcing) to examine public attitudes about fairness. As an example, Yaari and Bar-Hillel [35], Pierson [30], Plane et al. [31], Grgić-Hlača et al. [20], Grgic-Hlaca et al. [19], Binns et al. [8] used survey methodologies to examine people's attitudes about fairness and discrimination. These findings can help technologists to develop decision-making algorithms with fairness principles aligned with those of the general public, to make sure that designs are sensitive to the prevailing notions of fairness in society. Crowdsourcing can also be used to understand how preferences vary across geographies and cultures. Though far from perfect, Amazon Mechanical Turk provides one representative population to carry out large-scale online studies in order to conduct research on this domain [29], [10].
By showing the general public's attitudes toward three definitions of algorithmic fairness from the computer science literature, our research contributes to an ongoing debate within the computer science community regarding how to define algorithmic fairness. We suggest that it is important to take into account the principle of affirmative action, which seems to matter to the general public, even though the definitions themselves would not have allowed sensitive attributes like gender or race to influence loan allocations. Such discrepancies between the assumptions made by computer scientists and the fairness perceptions of the general public should be thoughtfully considered before deploying tools that may directly affect the public.
