Computer-Based Learning Environments (CBLE; Jacobson & Archodidou, 2000) are interactive learning environments that provide individualized learning opportunities. In a CBLE, the computer system provides tutoring functions such as presenting information, providing hints/feedback, and answering questions to assist students in solving problems and learning skills or concepts (Ma, Adesope, Nesbit, & Liu, 2014). Depending on students' responses, the system may adapt a response to provide a tutoring function that is most appropriate. Such an interactive learning environment is vastly different from the typical classroom environment as there are increased opportunities for students to interact with the tutor, thus enhancing students’ performance in problem-solving tasks (Hooshyar et al., 2018; Roscoe, Allen, & McNamara, 2018, Wang and Lin, 2018).
The structure of our CBLE is similar to that of an intelligent tutoring system (ITS; Anderson, Boyle, &amp; Reiser, 1985a, 1985b) and consists of three components: a tutoring model, a cognitive model, and a student model, as shown in Fig. 1. To activate the learning environment, a student interacts with a CBLE through the user interface. The tutoring model first receives input before sending the data to either the cognitive model or the student model. The cognitive model comprises of learning materials. The student model contains information such as the student's answer and records of solution steps, as well as the student's academic records. An example of how the CBLE functions is as follows. First, a student inputs steps of his or her solution to an exercise question. Second, the tutoring model processes the inputs to the cognitive model. Third, the cognitive model verifies the student's solution before transmitting the results back to the tutoring model. If the solution requires finetuning, the tutoring model will send suggestions back to the student.Download : Download high-res image (198KB)Download : Download full-size imageFig. 1. Structure of computer-based learning environment.
The similarities between ITSs and CBLEs allow researchers to draw from research focusing on ITSs as well. A recent meta-analysis of ITS studies (Ma et al., 2014) found that ITS are as effective as human tutoring on learning (g = −0.11), however, the included studies focused primarily on K-12 and university students. While ITS and CBLEs have been used in multiple domains (i.e., STEM, language, social sciences, the humanities) for young people (i.e., individuals ranging in age from 10 to 24 years; (World Health Organization, 2020), of the use of such systems and learning environments for older adults has a narrower focus on technology usage. This focus includes educating older adults (i.e. 60–75 years old) on how to use everyday technologies such as purchasing a subway ticket on a merchant machine (Struve & Wandke, 2009), using recommending systems for online shopping (Savvopoulos & Virvou, 2010), or using smart devices (Bruder, Blessing, & Wandke, 2014; Hagiya, Horiuchi, & Yazaki, 2016; Hagiya, Yazaki, Horiuchi, & Kato, 2015; Ribeiro & de Barros, 2014; Toyota, Sato, Kato, & Takagi, 2014).
Research has shown that learning to use technology is a form of lifelong learning for older adults (Huber & Watson, 2014). Technologies play critical roles in improving the life quality of older adults. However, older adults are facing challenges of learning new and evolving technologies. One way to older adults’ use of CBLE might be by teaching learners to find multiple solutions within the CBLEs. Research in mathematics and science education show that encouraging learners to apply multiple solutions to a particular problem improved their learning (Groβe, 2014). Previous studies provide empirical evidence that students in the multiple-solution group outperformed the control group on learning outcomes (Levav-Waynberg & Leikin, 2012; Rittle-Johnson & Star, 2007). CBLEs have also been used to encourage multiple problem solutions when teaching young people STEM subjects. (Hu & Taylor, 2016; Tenison & MacLellan, 2014; Waalkens, Aleven, & Taatgen, 2013; Willenham, 2009). The promising evidence supporting the use of multiple solutions to improve learning outcomes thus makes it a feasible strategy to be integrated within CBLEs for older adults seeking informal education.
Livingston (2006) defined informal education as all forms of intentional or tacit learning in which people engage either individually or collectively without direct reliance on a teacher or externally organized curriculum. According to Holland (2019), effective informal online learning environments ought to be interactive, and offer segmented titles and specify learning objects. Several studies have been conducted to examine the effectiveness of informal learning environments for older adults. Specifically, Chiu, Tasi, Yang, and Guo (2019) conducted a qualitative study that revised teaching strategies applied by experienced instructors to teach older adults how to incorporate technology into their daily lives. Findings suggest that instructors need to consider adapting their curriculum to meet the needs of older adults. In another study, Villar and Celdrán (2013) investigated the participation of older adults in formal, non-formal, and informal learning environments. Results showed that participants were less constrained and more self-directed in learning activities in informal learning.
In this study, we examine the effectiveness of an CBLE that provides informal education for older adults seeking to install a smart-home system. Specifically, the CBLE is designed to support multiple solutions at the teaching and training stages for older adults. Unlike previous CBLEs that have been implemented for formal education purposes with younger participants (Ma et al., 2014), this study seeks to extend research to include informal education using CBLE with older adults and implementing the CBLE in an ecologically valid environment.
1.1. Learning methods for CBLEs with older adultsLeung et al. (2012) surveyed participants of varying ages (i.e. young, middle-aged and older adults) on their preference of learning methods when attempting to use new technology. Older adults indicated a higher preference for learning resources that were interactive, had demonstration features, and provided practice opportunities. This finding reinforces the benefits of utilizing CBLE with older adults as CBLEs provide individualized learning environments that cater directly to the individual (Virvou & Moundridou, 2000).One way to support older adults’ learning by providing demonstrations and practice opportunities is through worked examples. Worked examples comprise of three components: a problem statement, detailed solution steps, and a solution (Wittwer & Renkl, 2010). Previous studies show that studying and practicing worked examples increases knowledge transfer among students (Atkinson, Derry, Renkl, & Wortham, 2000) and promotes problem-solving skills (Bokosmaty, Sweller, & Kalyuga, 2015). The provision of worked examples reduces the cognitive demand of learners (Pass et al. 2006), allowing one to better focus on the task at hand.Previous studies using CBLE have examined the effectiveness of using trial-and-error and worked examples to teach older adults how to use modern technology (i.e. vending machines, Struve & Wandke; and smartphones, Ribeiro & de Barros, 2014). However, these studies show that there were no significant differences between the two conditions on learning performance, suggesting that both strategies may be beneficial for older adults.
1.2. Present studyIn this study, we present an online CBLE, Smart-Home-in-a-Box (SHiB) CBLE, to teach older adults how to self-install a smart-home system in their own houses. SHiB is a ubiquitous system seeking to improve the quality of living for older adults living alone (Cook, Crandall, Thomas, & Krishnan, 2013). When learning with an CBLE, older adults may prefer to follow detailed step-by-step instructions, while young adults prefer to learn by trial-and-error. However, the lack of significant results from previous studies suggest that while individuals may have a preference for a learning method, it may not be effective for promoting learning outcomes (Ribeiro & de Barros, 2014; Struve & Wandke, 2009).Although the primary focus of the study is the effectiveness of learning with CBLE for older adults, young adults were also recruited to serve as a comparison group. The purpose of this study is twofold: (a) explore the effectiveness of using multiple solutions at the training stages for young and older adults on learning outcomes (Experiment 1), and (b) investigate the effectiveness of answer validation in the training stages (Experiment 2). Both experiments examined the teaching strategy effects on user satisfaction for the CBLE user interface. The section below provides a brief introduction to the SHiB CBLE utilized in this study.1.2.1. Design of SHiB CBLESHiB is a ubiquitous smart-home system that once installed, aims to recognize older adults’ activities and their routines to match their health needs (Fernández-Caballero, González, Navarro, & Cook, 2017). The SHiB is a kit that contains motion-aware sensors, temperature-aware sensors, relays, and an Ethernet server (Cook, et al., 2013).SHiB CBLE1 is a web-based CBLE designed to teach older adults how to install the components of the SHiB kit in different dwellings and to evaluate a participant's ability to do so. Based on the SHiB kit installation instructions, device positions have requirements. As house layouts are different, it is impractical to provide explicit instructions for all houses. Thus, the positions of the devices in the SHiB CBLE are relative to the layout of each house such that there are multiple suitable locations for the devices. SHiB CBLE judges accuracy of device placement by examining the devices' locations with the SHiB kit installation instructions. SHiB CBLE employs a multiple-solutions teaching method in the training and provides feedback for wrong answers during this phase. To assess understanding of device placement, the CBLE requires individuals to provide multiple solutions on post-tests. Submitted solutions are evaluated for position accuracy, for questions that required multiple solutions, submitted solutions were further evaluated for differences between solutions 1 and 2. Solutions to participants answers are computed in real-time and are corresponding to the submitted answers.The SHiB CBLE adopts the following recommendations discussed in Wolfson, Cavanagh, and Kraiger (2014): 1) participants can spend as much time as they want to complete the task; 2) difficulty level of tasks increases gradually to avoid unnecessary confusion; 3) each task is composed of multiple small tasks; and 4) participants are informed of task content at beginning. Placement requirements for each of the device types are available for participants. The interface of the system consists of three parts: a function button area, a layout area, and an instruction area (see Fig. 2). The three parts are designed to provide sufficient information without adding unnecessary information. The SHiB CBLE runs in a browser window developed by the researchers in Javascript. This allows researchers to collect data from participants more efficiently. Due to practical reasons, we have no control over the size of the application window and the size of the user's display. As both experiments utilized the same measures, scoring system and procedure, an overview of the measures, scoring and the procedure for both experiments are presented below.Download : Download high-res image (530KB)Download : Download full-size imageFig. 2. SHiB CBLE user interface.1.2.2. Measures and scoringIn both experiments, participants' prior knowledge of the task was assessed using a pre-test that required participants to place three room sensors in places they thought were accurate. Panticipants' score in placing the room sensors was measured as a fraction of the number of correctly placed devices over the total number of devices to be placed. If a participant placed all three sensors in the pre-test accurately, they received 100%. Learning performance following the training phase was assessed by the four posttests. The same scoring method applied to the posttests such that participants’ success in placing the sensors was also measured as a fraction of the number of correctly placed devices over the total number of devices.In each post-test, if a participant correctly placed all devices, he or she received +25%. If a participant succeeded on all the post-tests, they received 100%. If they succeeded in three out of four post-tests, they received 75%. We evaluated the degree of effectiveness of each teaching strategy as an effect-size using Hedges’ g.The SHiB CBLE recorded participants’ steps for each task and compared that with the step length for each participant to represent the total amount of time they spent on each task.User experience was evaluated at the end of the experiments via a post-experiment survey. The survey had twelve questions relating to participants' thoughts on using the CBLE. The twelve questions can be separated into two parts: the first ten questions were relevant to participants' likeliness of using the tutoring system, the ease use of the system, the functional integration of the system, and their confidence of using the system. The remaining two questions were relevant to learning via comparing multiple solutions and participants’ subjective feelings about understanding the teaching contents. Participants were asked to indicate the degree to which they agreed about statements that assessed their confidence in using the CBLE, the complexity of the CBLE, the utility of the CBLE and how well they understood the content, using a 5-point Likert-type scale ranging from1 (strongly disagree) to 5 (strongly agree).1.2.3. Demographic surveyParticipants’ backgrounds were assessed before the start of each experiment. Participants were asked to respond to a survey which included questions about their age, gender, marital status, education level, native language, geographical region, race, and computer experience.1.2.4. ProcedureFor both experiments 1 and 2, the entire procedure was administered through the SHiB CBLE. All participants were randomly assigned to one of the three teaching strategy groups. Participants received instructions before each task, which informed them on how to use the keyboard to control a red avatar in the layout to pick, move, and place devices to finish tasks. In the pre-test, participants had to place three devices. In the training session, participants in the textbook group received worked examples before training exercises started. The participants could look up the solution during the training. Participants in the exploration group had no hint. They received feedback from the CBLE after they placed all the devices. Participants in the combination group received worked examples at the beginning of exercise 1 and exercise 2. However, they had to figure out a solution for the third exercise independently. After they submitted their answers, they received feedback from the CBLE. When participants finished the training exercises, they had to complete four post-tests. In each post-test, participants received an introduction to the task. After they submitted answers, they had the choice to alter their answers according to the evaluation feedback from the CBLE or to ignore the mistakes. A post-survey was administered to the participants when they finished four posttests. The average time spent on the study for each respective experiment was 45 min.
