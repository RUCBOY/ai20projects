Modern software systems and services are commonly distributed, composed of front-end applications running on end-user devices, and microservices running in the cloud. This also holds for modern Cyber–physical Systems (CPS), such as data-driven applications for smart traffic, agriculture, or utilities. These applications rely on data from sensors and perform computationally-intensive tasks (data analytics, optimization and decision making, learning and predictions) which cannot be executed on energy constrained devices and are therefore executed in the cloud.
However, the connection with the physical world inherent to CPS requires these systems to operate and respond in real-time, whereas cloud was primarily built to provide average throughput through massive scaling. Real-time requirements impose bounds on response time, and when executing tasks in the cloud, a significant part of the end-to-end response time is due to communication latency.
The concept of edge–cloud aims to reduce this latency by moving computation to a large number of smaller clusters that are physically closer to end-user devices. Throughout the paper, we use the term edge–cloud in line with the definition of Satyanarayanan (2017), i.e., we assume that computation which would be traditionally centralized in a data-center (in the case of a regular cloud), is moved to network edges, closer to the users. This differs from the fog-computing (a related field of research), where the workload is traditionally decentralized, executing on end-user devices and a localized cloud (e.g., on an IoT gateway) is used for off-loading.
While usage of edge–cloud computing reduces communication latencies, edge–cloud alone does not guarantee bounded end-to-end response time, which becomes more determined by the computation time. The reason is that the cloud itself focuses on optimizing the average performance and cost of computation, but does not provide any guarantees on the upper bound of the computation time of individual requests. What is needed to address the requirements of modern cloud-connected CPS is an approach that can reflect the real-time requirements of modern CPSs even with the cloud in the computation loop.
Guarantees on a single request are the domain of real-time programming. But that itself is rarely a reasonable choice as it comes at a very high price of forcing developers to a low-level programming language, limited choice of libraries and the use of a relatively exotic programming model of periodic non-blocking real-time tasks.
In this paper, we advocate the use of standard cloud technologies (i.e., microservices packaged in containers running on top of Kubernetes) and modern high-level programming languages (e.g., Java, Scala, Python) for development of microservices that have real-time guarantees. We restrict ourselves to the class of applications where soft real-time requirements are enough (i.e., the guarantee on the end-to-end response is probabilistic — e.g., in 99% of cases the response comes in 100 ms and in 95% of cases the response comes in 40 ms). As it turns out this is a wide class of applications including augmented reality, real-time planning and coordination, video and audio processing, etc. Generally speaking, this class comprises any application that has a safe state and has a local control loop that keeps the application in the safe state while computation is done in the cloud. Consequently, the soft real-time guarantee pertains to qualities such as availability and optimality, but not to safety.
Also importantly, microservices of the considered class work with continuous workload (i.e., processing video or audio streams, etc.). Starting and closing the workload (stream, etc.) are explicit operations.
In this context, the article presents an approach to providing soft real-time guarantees on response time of microservices running in a container-based cloud environment (e.g., Kubernetes), with microservices developed in high-level programming languages (Java in our case).
In particular, we elaborate a method that allows us to predict the upper bound of the response time (at a given confidence level) of a microservice when sharing the same computer with other microservices. This prediction method is essential for controlling admission to the edge–cloud and for scheduling deployment of containers to computers in the edge–cloud. Combined with adaptive control of deployment and re-deployment of components, this enables providing microservices with probabilistic guarantees on end-to-end response time.
An important feature of our approach is that we aim to remove the burden of specifying the required computational resources from the developer of services that need soft real-time guarantees. To this end, we treat microservices as black boxes and do not require any apriori knowledge about the microservices from the developer. Instead, our system performs experiments on the microservices to collect the data needed for performance prediction and deployment decisions.
In our approach, we are specifically targetingprivately-controlled (non-public) edge–cloud environment, inwhich the edge–cloud operator controls not only the infrastructure but also the deployed microservices. This contrasts with public clouds, in which the provider needs to cope with unknown applications, unknown workloads, unknown clients, etc.
The paper is structured as follows. Section 2 shows a motivation example. In Section 3 we present our approach and in Section 4 its evaluation. In Section 5 we discuss limitations of our approach while Section 6 shows related work. Section 7 concludes the paper.
