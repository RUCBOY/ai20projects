Computer vision is a field of study in which researchers develop techniques to help computers understand the content of digital images such as photographs and videos (Brownlee, 2019). Computer vision algorithms and applications have been adopted in various fields, such as image recognition (Farabet et al. 2012; Tompson et al. 2014) and object detection (Girshick et al. 2014; Sermanet et al., 2013). One of the elements essential to developing autonomous driving, for instance, is computer vision (Pohlen et al. 2017).
Meanwhile, several studies have applied computer vision techniques to an examination of traffic safety. Dubey et al. (2016) and Ordonez and Berg (2014) predicted the perceived safety of streetscapes using convolution neural network (CNN) and support vector machine (SVM) classifier trained on street-level imagery in order to visualize the spatial distribution of perceived safety at a city scale. Yet, the use of current computer vision methods has a clear limitation to investigating the complicated relationship between the built environment and perceived safety (Dubey et al., 2016). This is largely because the computer vision model makes it difficult to find the safety determinants of the built environment’s characteristics. The reliance upon “black-box models,” in which the rationale for the generated outputs is inscrutable (Cabitza et al. 2017), is another factor that reduces researchers’ ability to understand the relationship between the characteristics of the built environment and perceptual outcomes. Opaque models like CNN, therefore, cannot serve as a theoretical tool for understanding why a neural network operates effectively (Alemi et al. 2016). Therefore, the black-box nature of neural networks is a barrier to their adoption in research applications where interpretability is paramount, like traffic safety (Shrikumar et al. 2017).
One of the ways to overcome the limitations of current computer vision methods is to use a semantic scene labeling method. Recently, there has been growing interest in using semantic scene labeling approach in the computer’s vision (L.-C. Chen et al., 2014; Long et al., 2015; Noh et al., 2015; Shrikumar et al., 2017). In contrast to the conventional scene recognition approach, which aims to determine the overall scene category by placing emphasis on understanding its global properties (Zhou et al. 2014), the scene labeling approach seeks to identify the individual components of a whole scene as well as the complex relationships among the semantic entities usually found in street scenes, such as pedestrians, cars, road, or sidewalks (Cordts et al., 2016).
Furthermore, applying the scene labeling approach to street-level imagery allows for an analysis of the built environment characteristics measured at the eye-level. Eye-level measurements may more accurately reflect a person’s actual perception of the attributes of a particular built environment (Jiang et al., 2015). Given the difficulty of scalably measuring the built environment at the street level, however, few studies on perceived safety have accounted for eye-level urban characteristics. Combining street-level imagery and the scene labeling approach may overcome this limitation by acquiring data on what people typically experience and perceive at the eye-level on the ground (Li et al., 2015).
The ability of scene labeling methods to analyze the urban built environment at the eye-level also provides the advantage of measuring the visibility of pedestrians and drivers, which is one of the important factors in urban traffic safety (Dadashova et al., 2016; Hills, 1980; Retting et al., 2003; Schofer et al., 1995). For instance, Meir et al. (2013) suggested that pedestrians’ traffic crashes tend to take place at intersections or curves where visibility is obstructed by the road’s curvature, fixed objects, parked vehicles, darkness, or other visibility limitations because these visibility obstructions interfere with the pedestrian’s ability to detect oncoming vehicles while also obstructing the motor vehicle drivers’ vision, preventing drivers from noticing pedestrians, who may be masked by obstructions (Aoki & Moore, 1996; Petch & Henson, 2000). Thus, improving pedestrian and driver visibility at intersections is essential both for enhancing perceptions of safety and reducing potential crash risks (Lee et al., 2016).
The present study examined the association between urban environmental attributes derived from scene labeling methods (i.e., scene segmentation and object detection algorithms) and school-aged children’s perceptions of safety, the latter of which provides information on the potential risks of pedestrian crashes (Lee et al., 2016; Meir et al. 2015). First, we sought to overcome the technical limitations of current computer vision methods by applying the scene labeling approach, which allows us to analyze complex relationships among the urban characteristics of built environments. A second aim of this study was to inform public policy for reducing the crash risk of children in school zones through the use of this novel method for deriving accurate measures of intersection characteristics at the eye-level, which offers a more precise understanding of perceived safety.
