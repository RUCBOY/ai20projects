Pharmaceutical manufacturing is undergoing rapid change in many ways. Factors such as pricing pressure and regulatory expectations on integration of quality by design principles (QbD) (Juran, 1992), are contributing to fundamentally challenging how medicines are developed and manufactured (Peterson et al., 2009; Collins, 2018). Pharmaceutical companies, federal agencies, and academic institutions are becoming more and more interested, involved, and invested in modernizing pharmaceutical manufacturing by developing and implementing innovative and emerging technologies. Continuous manufacturing – abbreviated as CM hereinafter – certainly has been an area of increased interest due to its many short and long term benefits (Lee et al., 2015; Myerson et al., 2015; Collins, 2018), which include “potential to improve agility, flexibility, and robustness” (Lee et al., 2015) and realizing six sigma quality pharmaceuticals that “patients deserve” (Yu and Kopcha, 2017). Readers are referred to Collins (2018) and Lee et al. (2015) for further discussions on evolution of pharmaceutical manufacturing and QbD, current challenges faced, and implications of CM in an highly regulated environment.
Traditionally, Design of Experiments (DoE) approach has been the bedrock for pharmaceutical process development (Burdick et al., 2017), along with other basic skills in industrial statistics such as regression, ANOVA (Analysis of Variance), and statistical process control (Peterson et al., 2019). In the DoE approach, experiments are performed sequentially, and process knowledge/understanding built accordingly. After sufficient knowledge is deemed to have been gained, empirical models are developed – generally linear – and used to map out input vs. output relationships. In part, this approach worked well for batch processes, because batch, by definition, involves many discrete steps, which allows: 1) measurements on attributes of interest after each step, 2) independence of each step, i.e., process parameter for the previous step does not impact the current step, 3) number of process parameters considered per step is (usually) small, which allows DoE resolution to be high with reasonable amount of experimentation, and 4) any non-conforming materials, i.e., the whole batch, caused by process deviation can either be discarded or reprocessed without any impact to the overall process. Design Space, Process Parameter (PP), Proven Acceptable Range (PAR), and Critical Process Parameter (CPP) as defined in ICH Q8, 2009, as well as Knowledge space, are then defined for each stage and overall manufacturing route optimized.
Integrated CM requires higher level of system understanding/approach (Lee et al., 2015) to quantify highly confounded input vs. output relationships. This is primarily because CM unit operation outputs depend on all prior unit operations’ inputs, e.g., flow rates and concentrations, in addition to its own. One may contribute the difference to how time scales are defined. In batch processes, time is an independent parameter and unit operations can be stopped at anytime, e.g., reaction time, holding time, distillation time, etc. without affecting downstream process, enabling a wide range of operation times with offline samples. In contrast, in telescoped CM unit operations, input to a unit operation is dictated by the output from previous unit operations, e.g., flow rates and concentrations, which leads to (residence) time being a dependent parameter typically with a narrower range. As such, for the case of three interconnected unit operations each with five process parameters, a systems approach requires more in-depth understanding of 15 total inputs and their interactions on output attributes of interest. For a 2-factor full factorial DoE, this would require 215 32,768 experiments, not factoring in any replications! Moreover, input vs. output relationships are most likely non-linear, making direct utilization of DoE methodology even more challenging.
Mechanistic models for physicochemical processes tend to be mathematically complex and non-linear, with mix of algebraic, differential, and/or integral equations. As more and more models are interconnected (just like CM), overall process/system model become even more complex, with process parameters being severely confounded. Therefore, it may be the case that to optimally assess the response surface associated with the overall mechanistic model, special experimental designs and analysis methods are needed, via means of Computer Experiments (Fang et al., 2006). Computer Experiments have been widely used in other industries where underlying equations are often very complex, e.g., aerospace, automotive, and medical device industries (Peterson et al., 2019), and requires a deterministic model. The developed model is then sliced-and-diced through a metamodel, or surrogate model, and this workflow “necessitate[s] different approaches from those of physical experiments.” (Fang et al., 2006)
Here, we present a case where mechanistic model was first developed for an Active Pharmaceutical Ingredient (API) CM process, and Computer Experiments methodology used to provide detailed understanding of input vs. output relationships. First, we provide an overview of the process. Then, we provide details for each step/stage, with respect to physicochemical transformations that take place. Detail is given on a stage where biphasic reaction with mass transfer necessitated phenomenological (semi-mechanistic) model development. Lastly, we provide an overview of steps taken in analyzing and slice-and-dicing system model using Computer Experiments methodology via Latin hypercube experimental design and meta-modeling, to identify or inform criticality of process parameters.
