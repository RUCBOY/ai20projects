With the development of Internet, data collection methods in many research fields have changed. In social sciences, psychology and medical sciences in particular, the development of online survey platforms (e.g., LimeSurvey, MTurk) resulted in formidable opportunities to collect data at lower cost, and to reduce possible human errors in both data collection and management (Litman, Robinson, & Abberbock, 2017). Nevertheless, from such new ways to deal with ancient methodological issues, new threats have emerged. Making research crowdsourced using online instruments resulted indeed in making it more directly accessible, including to fraudulent participation. Assumingly, the risk of collecting invalid data has risen, in particular concerning retributed participation (Barends & de Vries, 2019).
On one hand, a large corpus of literature has been provided to highlight the effect of either careless responding in online surveys (Bowling et al., 2016; Curran, 2016; Francavilla, Meade, & Young, 2019; Holtzman & Donnellan, 2017; Litman, Robinson, & Rosenzweig, 2015; Meade, Ward, Allred, Pappalardo, & Stoughton, 2017; Ward & Meade, 2018; Ward & Pond, 2015), or fraudulent participants who complete several paid surveys per day to earn money (Chandler & Paolacci, 2017; Chandler, Mueller, & Paolacci, 2014; Sharpe Wessling, Huber, & Netzer, 2017). On the other hand, very few studies have addressed the question of the presence of fraudulent data generated by form fillers, botnets or malwares such as survey bots (Buchanan & Scofield, 2018; Dupuis, Meier, & Cuneo, 2019). The aim of the present study is to keep filling this gap by providing new findings on the topic.
Whereas responses provided carelessly can result from various factors, the main reason underlying the presence of computer-generated data in online research is the possibility to earn much money easily by completing a maximum of paid surveys. Such an assumption has several implications:
-First, that the resulting fraudulent data can be assumed as purely invalid.-Second, that one impostor is likely to complete several different surveys.-Last, that one impostor is, as well, likely to complete a same survey several times using many different accounts (Handorf, Heckman, Darlow, Slifker, & Ritterband, 2018; Konstan, Rosser, Ross, Stanton, & Edwards, 2005).
Blatantly said, even a single impostor can bias study results much, in many ways (i.e., by making study results neither representative, significant nor even valid) and repeatedly. With new instruments to complete retributed research explicitly against the rules, this risk is likely to increase. Developing techniques to prevent from such an emergent threat becomes thus a priority.
Several authors consider that most of malicious programs provide randomly distributed response sets (Buchanan & Scofield, 2018; Dupuis et al. , 2019; Holden, Wheeler, & Marjanovic, 2012; Meade & Craig, 2012). Such unsuited data has a large number of detrimental effects on survey results. One of the worst for personality psychology in particular is that the correlations between items tend to be null even when total scores keep looking normally distributed, which represents a major threat to questionnaire factor structures (Kam, 2019). Assumingly, existing findings concerning random responding are thus generalizable to computer-generated response sets, subject to adequate concept disambiguation. As stated by Dupuis et al. (2019), random responding is often taken as a synonym of careless responding or insufficient effort responding, though the latters correspond to a more specific concept. Accordingly, random responding will only refer herein to the definition of providing truly randomly distributed response sets.
From the articles adhering to the present definition of random responding, several have proposed indices to detect problematic response sets under assumptions of their random distribution (Curran, 2016; DeSimone, Harms, & DeSimone, 2015; Dupuis et al., 2019; Holden et al., 2012; Niessen, Meijer & Tendeiro, 2016). According to Niessen et al. (2016), three categories exist, namely: consistency indices, outlier statistics, and external measures.
1.1. Consistency indicesConsistency indices rely on the assumption that human responding leads to coherent sets of answers, that is to say, data including correlated variables. Additionally, sufficient quality responses are expected to include correlations between groups of relevant variables, whereas null correlations are expected when responses are provided randomly. Assessing individual consistency requires to measure the correlation between two groups of items. Each index corresponds to a different item pairing technique. For instance, consistency can be measured by the correlations between antonyms, synonyms, or by opposing odd and even items related to the same dimensions (Curran, 2016; DeSimone et al., 2015; Marjanovic & Holden, 2019; Niessen et al., 2016). Every response consistency index relies on the bisection method. Most of them results from the calculation of a correlation between groups of items, but some indices result from the correlation between the factor scores measured by both halves of the questionnaires (Dupuis, Meier, Capel, & Gendre, 2015; Johnson, 2005). So does the index called response reliability (as detailed below).
1.2. Outlier statisticsOutlier statistics are meant to measure either the adequacy with or the distance from the rest of a sample. The first category corresponds to the person-total correlation, and the second category includes the Mahalanobis distance (Curran, 2016; Hong, Steedle & Cheng, 2019; Kim, McCabe, Yamasaki, Louie & King, 2018; Marjanovic & Holden, 2019; Motamedi, Little, Nielsen & Sulik, 2019; Niessen et al., 2016). The person-total correlation consists of correlating an individual's responses on each item and the average value on each item for the entire group of respondents. The Mahalanobis distance is a very different technique detecting multivariate outliers using a chi-square test to determine significantly different variable sets.
1.3. External measuresMeasures collected during the questionnaire completion that do not belong to the questionnaire itself represent important additional information. These indices could be regrouped into two categories: external items and metadata. External indices include bogus items (Curran, 2016; DeSimone et al., 2015; Kim et al., 2018; Meade & Craig, 2012) and explicit instructed response items (e.g., "Select answer 2 to show that you have read this question with care"; Kim et al., 2018; Niessen et al., 2016). Metadata includes information collected using computer-administered questionnaires, for instance, the time taken to respond to question logs or to complete the entire questionnaire (Buchanan & Scofield, 2018; DeSimone et al., 2015) and click count (Buchanan & Scofield, 2018).
1.4. Study purposeIn a recent attempt, Dupuis et al. (2019) have compared seven indices in terms of detection of computer-generated data under controlled conditions. Their conclusion was that response coherence, the Mahalanobis distance, and the person-total correlation resulted in better predictions than other indices. However, their findings were based on 5-point questionnaires and were relying on multivariate normality assumptions that are at higher risk to be violated with dichotomous data. Testing the validity of these indices under other conditions remained undone. The present article purports thus to compare the same indices using data from dichotomous-itemed questionnaires from two major Swiss health studies.
