Brain-computer interface (BCI) is a communication system that allows its users to interact with external devices using the brain signal directly, and it does not depend on the peripheral nerves and muscles [1,2]. BCI analyzes the brain signal data collected from specific tasks and converts the brain information into control commands that can be used to control computers or communication devices. It provides a new way of communication, which can help people who suffer from devastating neuromuscular injuries and neurodegenerative diseases to restore their communication ability to some extent, assist patients with epilepsy, stroke, and other diseases to biofeedback treatment, and so on. The BCI technology has attracted wide attention from many fields, such as neurology, rehabilitation engineering, psychology, computer science, engineering, and mathematics.
The basic framework of BCI is shown in Fig. 1, which includes five modules: signal acquisition, preprocessing, feature extraction, classification, and control interface. The electroencephalography (EEG) signal is one of the most common bio-potential signals used in the signal acquisition module, because it is non-implantable, non-invasive, inexpensive, and easy to use. The high temporal resolution and multichannel of the EEG signals result in that a considerable number of features will be extracted and some of these features may be redundant, irrelevant, or trivial [3,4].Download : Download high-res image (100KB)Download : Download full-size imageFig. 1. Framework of BCI.
Since the brain can be divided into various functional areas, some of the acquired EEG signals from adjacent EEG electrodes may come from the same functional area, and the features extracted from these signals may be redundant. For example, in BCI Competition III dataset IVa (http://www.bbci.de/competition/iii/), the EEG signals xC1 and xC3 are acquired from ‘C1’ and ‘C3’, where ‘C1’ and ‘C3’ are the locations of EEG electrodes and they are close to the primary motor cortex. If five motor-related frequency features FiC1 (i = 1, …, 5) and FiC3 (i = 1, …, 5) are extracted from xC1 and xC3, respectively, by the same method, then FiC1 will be similar to FiC3(i=1,…,5), and thus they are redundant.
If some features have no relationship with the classification, then they are called the irrelevant features. For example, in motor imagery BCI, the EEG signals from motor-related areas (e.g., the primary motor area, the pre-motor area, and the supplementary motor area) are very important for classification. However, the EEG signals derived from other functional areas (e.g., the auditory area) may be independent of motor imagery. Under this condition, the extracted features from these EEG signals have no relationship with the classification, and thus cannot improve the classification performance.
In addition to the redundant and irrelevant features, it is necessary to note that there may exist some trivial features, which have very little effect on improving the classification accuracy, but will cause overfitting of a classifier.
Obviously, the redundant, irrelevant, and trivial features increase the computation burden of the training process of the classifier, degrade the generalization ability of the classifier, and decrease the classification accuracy. Therefore, feature selection should be employed before the classification, as shown in Fig. 1. The task of feature selection is to select some important features from all features, with the purpose of reducing the feature dimensionality, accelerating the training process, simplifying the classifier model, and improving the classification accuracy [[5], [6], [7]].
Feature selection is a challenging problem mainly due to the following two issues: the large search space and the interference of redundant, irrelevant, and trivial features [8]. Firstly, the search space grows exponentially with the increase of the number of features. For example, the total number of possible feature subsets is (2n − 1) for n features. Secondly, in order to select some important features, it is necessary to remove the redundant, irrelevant, and trivial features, since they have side effects on the classification performance. According to whether it is independent of the subsequent classifier, feature selection can be divided into two categories [7]: filter methods and wrapper methods.
The filter methods do not depend on any classifier. They generally use the statistical measures of the training data to evaluate a feature's importance, which include distance function [9], rough set [10], mutual information [11], fuzzy set [12], statistical correlation coefficient [13], etc. However, they cannot guarantee the optimal feature subset.
The wrapper methods employ a classifier as a “black box” to evaluate the feature subsets based on the classification performance. Although they are computationally intensive and time-consuming, the wrapper methods have better performance than the filter methods. It is because the wrapper methods consider the performance of the selected features on a classifier, which is ignored by the filter methods [7]. So the wrapper methods have the potential to obtain a subset of features with higher classification performance. The wrapper methods are currently a hot topic in the field of feature selection. This paper focuses mainly on the application of the wrapper methods on the EEG signals.
In principle, the wrapper methods contain two important components: subset search, the aim of which is to generate a candidate feature subset from the original feature set, and subset evaluation, which makes use of a classifier to assess the goodness of this feature subset. In recent years, evolutionary algorithms (EAs) have become effective subset search methods. Moreover, various EAs, such as differential evolution (DE) [14,15], particle swarm optimization (PSO) [[16], [17], [18], [19], [20]], and genetic algorithm (GA) [[21], [22], [23]], have achieved better performance than traditional subset search methods. Compared with traditional subset search methods, EAs do not require domain knowledge and do not need any assumptions about the search space, such as nonlinearity and separability. Another advantage is that EAs are population-based search algorithms and have powerful search ability.
For the classification of motor imagery BCI based on EEG, proper features are crucial to obtain a high classification accuracy. Considering the characteristics of the EEG signals, the time-frequency-space three-dimensional features are extracted, which forms a feature set with a considerable number of features. Under this condition, feature selection should be carried out in a high-dimensional search space. Note, however, that EAs are easily trapped into a local optimum due to high dimensionality.
To improve the performance of EAs on a high-dimensional feature selection problem of motor imagery BCI based on EEG, the main idea of this paper is to remove unimportant features (i.e., redundant, irrelevant, and trivial features) in the iterative process of EAs. By doing this, the dimension of the search space can be reduced and the important features can be maintained simultaneously. To this end, we propose a dimensionality reduction mechanism (called DimReM) in EAs-based feature selection.
The main contribution of this paper are summarized as follows:
•The current feature selection methods aim at directly selecting some important features. However, this paper proposes an opposite point of view. DimReM first determines whether a feature is unimportant by taking advantage of the information from evolution. Afterward, the unimportant features are deleted generation by generation. As a result, the important features are maintained.•DimReM has a simple structure, and does not introduce any additional parameter and complicated operator.•DimReM is readily embedded into different EAs. Moreover, we have successfully integrated DimReM with three EAs and three classifiers.•Systematic experiments have been conducted on the EEG datasets and three datasets from other fields to verify the effectiveness of DimReM. The results verify that DimReM can find feature subsets with higher classification accuracies while smaller numbers of features.
The rest of this paper is organized as follows. Section 2 introduces the related work, including feature extraction and feature selection. In Section 3, three EAs are briefly introduced. Section 4 gives the details of the proposed DimReM. Section 5 presents the experimental results and discussions. Finally, Section 6 concludes this paper.
