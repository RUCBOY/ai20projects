The provision of proper environmental enrichment may reduce the occurrence of tail-biting and aggression in group housed pigs (Lahrmann et al., 2018). For instance, a wooden beam (Larsen et al., 2019) and a chewable rubber ball with protrusions (Telkänranta et al., 2014) have been shown to reduce the occurrence of tail-biting, and a smooth surfaced solid ball can reduce the risk of aggression (Fu et al., 2018). Recognising the engagement of pigs with enrichment and quantifying the time that the pigs spend with the enrichment can bring about two advantages: (1) the animals’ preferred objects can be determined which can improve the longer-term benefits of enrichment (Turner et al., 2006) and (2) the enrichment engagement time can serve as an quantitative indicator of positive welfare for pig production (Brown et al., 2018). It is widely known that not all objects have a long-lasting positive effect on animal welfare. Understanding the engagement of pigs with different objects is necessary to more accurately specify the type of enrichment object and also help a farmer in deciding when to change objects when the animals become bored. Therefore, recognition and quantification of the pigs’ engagement with different objects can have value to both research and farming practice.
Presently, evaluation of pigs’ engagement with enrichment has mainly been performed through human observation, which is time-consuming, laborious and hard to perform objectively and consistently both within and between observers. Addressing this challenge requires the ability to capture both spatial and temporal information on pig behavioural within the region of the enrichment object. Top view cameras have been demonstrated in the literature to be useful in capturing behavioural information on group housed pigs. However, computer vision technology has not been used to automatically recognise this behaviour. Automatic monitoring of enrichment engagement (EE) through computer vision technology has the advantage of being non-intrusive, less-subjective and uninterrupted and also has the potential to recognise simultaneously occurring behaviours along with those performed towards the enrichment materials provided to the pigs (Nasirahmadi et al., 2017).
Recently, deep learning-based computer vision approaches, especially through convolutional neural networks (CNN), has been widely used for the studies of pig behaviours. Initially, Zheng et al. (2018) recognised 5 postures of a lactating sow (i.e. standing, sitting, sternal recumbency, ventral recumbency and lateral recumbency) by using Faster R-CNN (that shares the convolutional features in a Region Proposal Network (RPN) and a Fast R-CNN (Girshick, 2015)) and obtained sows accurate location. Subsequently, Zhu et al. (2020) proposed an end-to-end refined two-stream Red-green-blue Depth (RGB-D) Faster R-CNN algorithm by fusing RGB-D image features in the feature extraction stage to recognise the above 5 sow postures. Yang et al. (2020) extracted the spatial temporal features mainly by using fully convolutional networks (FCNs) and optical flow analysis and classified these features by using hierarchical classifier to recognise sow drinking, feeding, nursing, moving, medium active and inactive behaviours. Yang et al. (2018) used Faster R-CNN to locate and identify individual pigs and extracted feeding area occupation rate to recognise feeding behaviour. Furthermore, Zhang et al. (2019) proposed a Sow Behaviour Detection Algorithm based on Deep Learning (SBDA-DL) to recognise drinking, urination and mounting of sows. In the above pig behaviour studies based on CNN, the CNN architecture was used to extract spatial features and train individual image frames.
As behaviour performed towards the enrichment mainly manifests as a continuous interaction process between pigs and objects, a computer vision algorithm that considers the spatial-temporal motion patterns in videos is necessary to recognise EE of pigs. Recent studies in the computer vision domain have shown that combining the convolutional neural network (CNN) with a long short-term memory (LSTM) framework can offer a powerful framework for extracting spatial-temporal features (Donahue et al., 2015, Srivastava et al., 2015). Long short-term memory (LSTM) is a commonly used Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997) that has been widely used for gesture recognition (Tsironi et al., 2017), online handwriting recognition (Nguyen et al., 2018) and text report classification (Banerjee et al., 2019). However, the use of LSTM for automated pig behaviour monitoring studies is still limited. Chen et al. (2020) was one of the first studies to extract the spatial-temporal features by combining the CNN (Visual Geometry Group 16 (VGG16) architecture) and LSTM in order to recognise aggressive video episodes of pigs. In that study, the motion velocity and interaction pattern of aggressive behaviours between adjacent frames changed much faster than that of non-aggressive behaviours, and thus non-aggressive pigs were considered as the background and the entire image in the video was directly used as data for training the model.
There are significant differences between pigs’ behaviour towards enrichment and pigs’ aggressive behaviour. However, the velocity and interaction pattern of EE behaviour may be similar to that of other non-EE behaviours. Therefore, in developing an algorithm for automated recognition of EE, using the entire image as data to train the model may lead to inaccurate results. As a result, in this paper we develop a HSV (Hue, Saturation, Value) colour space-based tracking algorithm (Gonzalez and Woods, 2007) of pigs performing behaviour towards the enrichment object to further remove the region that is unrelated to EE in an image in the conditions of dim illumination, crowded pigs and dirty objects. On the other hand, in order to ensure the accuracy of recognising EE, this paper applies the InceptionV3 network (Szegedy et al., 2016), which is a CNN architecture with greater depth and width than VGG16.
Hence, the aim of this study is to combine InceptionV3 and LSTM to automatically recognise episodes of EE in pigs. Furthermore, this study aims to preliminarily determine pigs preference to 3 different objects by applying the developed algorithm to calculate the duration of engagement with each object.
