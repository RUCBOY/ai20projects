Breast cancer is one of the most common cancers and the second leading cause of death in females [1]. Early detection and treatment is the best way to reduce mortality [2]. Ultrasound (US) is a useful screening modality for breast cancer detection and diagnosis and can particularly improve the sensitivity of dense breast scanned by mammography [3]. 2-D breast scan by US with a handheld probe (HHUS) is a conventional method for breast examination. However, it is difficult to scan the entire breast image with HHUS. Hence, the 3-D automated breast ultrasound (ABUS) is designed to address the issues in HHUS [4]. The ABUS utilizes a larger transducer for scanning the whole breast images and generates the 3-D breast volume for locating tumors [5]. In general, it is a time-consuming process for the physician to review and locate suspicious lesions in ABUS images. To reduce the review time and improve tumor detection, a computer-aided detection (CADe) system is introduced to assist the physician. Several CADe methods have been proposed for ABUS image detection, such as an adjusted Otsu's threshold [6], the topographic watershed [7], and the multi-scale blob detection algorithm [8]. However, these methods usually require a series of image processing algorithms that are relatively time-consuming.
Recently, deep learning technology has shown proficiency in medical image tasks, such as object classification [9], detection [10], and segmentation [11]. Convolutional neural networks (CNNs), which exploit convolutional layers to obtain features automatically, are the most common type of network for performing image tasks. Several well-known CNN architectures, such as ResNet [12] and DenseNet [13], showed that a deeper CNN with fewer parameters could achieve better performance. In medical image analysis, many 2-D CNN models have been employed for several CADe systems. Yap et al. used a patch-based LeNet, a U-Net, and transfer learning with a pretrained fully convolutional network (FCN) for lesion detection on ultrasound images [14]. Setio et al. exploited a multi-view CNN for pulmonary nodule detection in CT images [15]. Nevertheless, most of these systems only consider 2-D slices as the input and lose spatial information about the 3-D volume. Recently, Chiang et al. proposed a CADe system based on a 3-D CNN with a sliding window method for dividing the 3-D volume into volumes of interest (VOIs), a candidate determination model, and a post-processing method to localize tumor position [16]. Although data augmentation had been performed in the previous paper, the data imbalance problem still existed.
To address these issues in conventional CADe systems and take advantage of 3-D CNNs, a CADe system designed with two different 3-D CNN models for ABUS images is proposed in this study. Because the general, deeper 3-D CNN will result in large numbers of parameters and may lead to computational inefficiency, two different models, the simplified 3-D VGG-16 and the 3-D DenseNet, are employed in our CADe system. To diminish the high variance and bias in a single model, the ensemble method [17] is used. In addition, focal loss provided by Lin et al. [18] is also introduced into the proposed system for solving the data imbalance problem and raising the tumor detection rate. Furthermore, a post-processing method [16] is utilized for determining the final detected tumor location.
