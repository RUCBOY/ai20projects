The human brain is often considered the most complex system known. It has a fantastic capacity to learn and remember, to recognize patterns in space and time, solve problems of all kinds, innovate tools and machines, create beautiful art and science. Is it reasonable to believe that we, in a foreseeable future, will be able to understand all the wonders of our own brain, enough to be able to mimic it and build artificial brains and minds that correspond to or even surpass the capacity of the human origin? Can we seriously believe that we (soon, or ever) will be able to build robots that know of and can reflect upon their own existence?
Indeed, it seems that almost everybody today is talking about and has an opinion on artificial intelligence, AI. Either hoping such systems will solve all kinds of problems that humans can, or currently can’t solve, or fearing that AI will soon take over and make humans obsolete, or both. Some enthusiasts even believe AI systems eventually can become conscious and have a will of their own, an own agenda. How realistic are these hopes and fears?
Having been involved in computational neuroscience for over thirty years, I’ve often been thinking about these and related problems, and lately focusing on what is required to have consciousness and free will. I therefore gladly accepted the invitation to review the book, The Brain and AI, which I had heard about for some time, but had not found time to read yet. Now, this gave me a good opportunity to do so, and I surely didn’t regret my decision. It was like reading a thrilling science fiction story, or more accurately, taking part in an extremely interesting dialogue between two highly informed and competent experts in their respective fields, Chinese scientist Fanji Gu and German engineer Karl Schlagenhauf (from now on referred to as just Karl and Fanji).
