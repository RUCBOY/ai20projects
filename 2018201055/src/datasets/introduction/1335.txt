Computational models are well established tools for the design, development and study of energy technologies, including their constituent components and materials [29], [32]. They range from systems-level models for control and automation to ab-initio models for materials screening. Even for a given task at a particular spatio-temporal scale, there is typically a large set of approaches available, differing in, primarily, the level of detail included in the underlying model, the particular numerical techniques employed, and the selection of numerical parameters that control the truncation errors. This results in different levels of computational complexity and attendant time cost, which are strongly correlated with the accuracy of the solutions obtained.
In a given modelling task, we may classify competing computer models, depending on their accuracy and associated complexity, as either low-fidelity (lower accuracy, lower complexity) or high-fidelity. We may even introduce three or more fidelity levels and classify the various computer models available accordingly. Selecting an approach based on its fidelity will inevitably involve a trade-off. While high-fidelity models are more accurate, they are (usually) computationally expensive, difficult to implement and difficult to understand for practitioners. Low-fidelity models, on the other hand, can provide rapid solutions and are relatively straightforward to implement, the price for which is a possibly unacceptable level of accuracy.
Modelling tasks such as computer-based optimization of an energy technology or the screening of energy materials require extensive exploration of a design space. In such cases, the computer models are frequently replaced with computationally cheaper approximations, termed surrogate models, constructed via machine learning methods, projection schemes to lower dimensionality or by combining models of different fidelity (multi-fidelity approaches). Multi-fidelity approaches can also be combined with machine-learning and reduced-order modelling approaches.
Classical machine-learning approaches include Gaussian Process (GP) models, artificial neural networks, support vector machines and polynomial response surface models [15], [26], while reduced-order models are typically based on proper orthogonal decomposition or Krylov subspaces [10], [17], [28]. GP models can be extended to multi-output problems of the type considered here in a number of ways, including treating the output index as an additional input parameter [16], multi-dimensional GP priors with a linearly separable covariance [4], and dimensionality reduction with separate regression on vector components in the resulting low-dimensional linear subspace of physical or feature space [13], [34]. Both machine-learning and reduced-order model approaches typically require large training data sets based on high-fidelity models.
In most practical scenarios, however, computational resources are limited, and often insufficient for the acquisition of a large volume of high-fidelity data with acceptable accuracy (often in practice there is even a limited availability of low-fidelity data). Hence, constructing surrogate models that rely only on high-fidelity models may not be desirable, while low-fidelity models lack sufficient accuracy.
One effective way of resolving this dilemma is to use a combination of both low- and high-fidelity data. Although low-fidelity samples are noisy and biased, they normally show a strong correlation with the high-fidelity samples. It can be possible to harness this correlation to avoid full reliance on high-fidelity data. A prominent example of this type of surrogate model combines the stochastic collocation methodology with a multi-fidelity approach [20], [35]. Examples of its effectiveness include: frequency-modulated trigonometric functions [20], heat driven cavity flows [11], acoustic horn problems [35], molecular dynamics simulation [24], parametric studies of NACA airfoils [30], discrete-space evolution probability simulations [23], and irradiated particle-laden turbulence [14].
This surrogate modeling approach is also designed to work with a limited number of samples and provide an optimal sampling strategy for the high-fidelity simulations. The authors in  [20] use a greedy procedure to select amongst low-fidelity data in order to identify input samples at which to conduct a low number of high-fidelity simulations. They then use the low-fidelity data to find coefficients for an interpolation at out-of-sample points using the high-fidelity results. The reasoning behind the use of low-fidelity data in the diagnostic context of importance sampling points stems from the existence of an expression for the upper bound of the multi-fidelity emulator error, which is a function of the low-fidelity stochastic collocation model error [12]. The limitation of this approach is that it is based on an assumption of the low- and high-fidelity data sharing a similar correlation structure, which limits its application to many complex problems. The other limitation is that it requires an execution of the low-fidelity model when making predictions of high-fidelity outputs. This severely restricts its application when the low-fidelity simulations are also expensive to run.
Autoregressive models in statistics have also been used to construct multi-fidelity emulators. Kennedy and O’Hagan in their seminal work of [15] proposed this approach in its original form with the assumption of a linear relationship between different fidelity levels. The low-fidelity correlations are captured based on low-fidelity observations and transferred over to enhance the high-fidelity model. This method was improved upon in [8] using a deterministic parametric form of the mapping (from low to high fidelity) and an efficient numerical scheme to reduce the computational cost. Despite its success in several experiments, this parametric approach requires expert knowledge for model selection as well as a large dataset for model training.
To overcome the limitations of the linear assumption, a nonlinear autoregressive model (NAR) was introduced in [21] by replacing the linear transformation with a GP model. NAR has been successfully applied to a number of problems. In these enhanced multi-fidelity autoregressive approaches, in order to fully capture and propagate the uncertainty through all fidelity levels, a chain of GP models is jointly trained in a deep GP framework [6]. In recent work, the NAR model was generalised for efficient emulation of high-dimensional output (order 1 million) simulation problems [33].
Despite the success of NAR and its variants, they lack a principled way to sample parsimoniously from expensive high-fidelity simulations, leading to a potential waste of computational resources on the generation of very similar high-fidelity results. Although experimental design techniques, e.g., Latin hypercube [2] and Sobol sequences [31], can be used to improved sampling efficiency, they do not incorporate any particular knowledge of a specific problem and inevitably, therefore, lead to inefficiency.
In this paper, we first derive a general Bayesian framework for multi-fidelity simulation based on the kernel extension of a general linear model. This framework bridges the connection between the NAR and the stochastic collocation approaches. It provides a unified way of understanding multi-fidelity models and allows for the following modifications to be made in order to combine the advantages of stochastic collocation and NAR:
•We introduce a noise-free assumption for the simulation data, which naturally extends our model to high-dimensional problems.•By incorporating the NAR structure and an isotropic kernel function, predictions of the high-fidelity models no longer require extra simulations from the low-fidelity model.•By incorporating a sequential learning approach to construct the multi-fidelity model, we propose Greedy-NAR, a Bayesian NAR that can construct itself automatically without the need of a special experimental design or a priori assumptions about the underlying physics.
We apply the sampling approach to several practical examples of the type found in energy engineering and science applications, demonstrating that it reduces the resultant surrogate model estimation error when dealing with limited computational resources for the high-fidelity data acquisition.
The paper is organized as follows. We firstly define the types of problems under consideration in Section 2. We then derive the model framework based on a general linear form in Section3, followed by a discussion on the connections to stochastic collocation and NAR and our modifications for obtaining improved results. The experimental results are provided in Section 4.
