Odometry refers to an estimation of the displacement of a robot or vehicle in an incremental way, relative to its previous estimated position. It is used in mobile robot localization in combination with absolute position measurement to periodically recalibrate the vehicle's location [1], [2], [3].
Odometry is a field of mature solutions dominated by wheel shaft encoder reading [4]. Unfortunately recording of displacement with wheel encoders is inhibited by errors such as slippage of the wheels and the presence of particles on the surface in the path of the wheels [2], [3], [5], [6]. As the consequence of their non-tactile measurement method, optical mouse sensors avoid these pitfalls and have the added advantage of operating without moving parts. Computer mouse sensors are cheaper than wheel encoders, while performing odometry at a high resolution [7], [8], [9].
Although promising, the operation of optical mouse sensors as displacement sensors is not without limitations. The key parameters in its operation are the sensitivity, position and orientation of each sensor on the robot frame. Errors in these parameters will give rise to systematic errors in the calculated position and orientation of the robot. Opposite to random errors that can compensate each other as the consequence of averaging, systematic errors will accumulate with each incremental step.
The sensitivity of an optical computer mouse sensor tends to be inconsistent and depends on the distance between the sensor aperture and the floor surface and on properties of the sensor's motion trajectory such as velocity and curvature [7], [10], [11], [12]. To remedy these limitations, modifications have been proposed to the sensor optics to render it afocal, effectively mitigating the height dependency of the sensitivity [13]. To further improve the performance several researchers have proposed and successfully demonstrated the use of multiple redundant mouse sensors and combinations with other position measurement methods through sensor fusion [10], [14], [15], [16].
Under the condition that the displacements recorded by the different mouse sensors are measured strictly coincidentally, the displacement and rotation of the robot can be calculated through linear regression [17], [18]. Research towards the optimal geometrical arrangement of multiple mouse sensors on the robot frame for this conversion indicates that placement of the sensors furthest away from the geometrical center of the robot yields the most accurate calculation of position and orientation of the robot [18], [19], [20], [21], [22]. The results also demonstrate that the orientation at which the sensor is mounted in the robot frame has no effect on the accuracy of the calculation. Nonetheless, orientation and position of each sensor in the robot frame are the parameters in a regression model. Their exact values are hard to establish in a setup using optical computer mouse sensors, as the position and orientation of the small apertures of the sensors are difficult to measure and the location of the sensors in the robot frame might not coincide with their design value, due to manufacturing tolerances.
This problem can be solved through a one-time calibration measurement of the robot displacement using a separate highly accurate calibration system. The calibration measurements reveal the deviation in the robot's displacement as measured by the mouse sensors. Through its relation with the errors in the sensorsâ€™ positions, the deviation can then be traced back to improve the accuracy of the sensor positions established [18]. The method does not evaluate the sensors orientations, but assumes them to be aligned with the robot frame coordinate system.
In another approach a robot is moved over an arbitrary homogeneous path comprising of a line or an arc, recorded by the mouse sensors on the robot frame. The recorded displacements of the separate sensors are coupled by the kinematics of the robot, in which the position and orientation of the sensors in the robot frame are parameters. These parameters are estimated by minimization of the difference between the displacement derived from the kinematic model and the displacement as recorded by the sensors [23]. The method still requires an accurate figure for the distance between at least two sensors on the robot, which is difficult to obtain. The calculation of the sensor positions and orientations is a brute force computation solving all parameters for all sensors in one minimization operation, while reconstructing the path of the robot at the same time. The calibration method is validated in an additional experiment by moving the robot in a full circular path, measuring the distance and orientation from the end position as measured by the sensors to the starting position of the path. Although the results reported are satisfactory, a more analytic method could potentially obtain better results more efficiently.
The linear regression model used in these papers to relate the displacements of the optical flow sensors to the translation and rotation of the robot, is purely static. This condition presumes synchronization of the acquisition of displacement data from the sensors in a schedule of strict coincidence. Uncertainty about the times at which the displacement was recorded by the different sensors translates into uncertainty of the calculated position and orientation of the robot. Notwithstanding this prerequisite, previous papers completely omit the aspect of synchronicity in data collection from different sensors or disregard explanation on the design and implementation of a scheduled data acquisition system [17], [18], [23].
The approach presented in this paper improves on the previous calibration and measurement methods in multiple aspects. It starts with a simple, but cardinal calibration of the sensor sensitivity, using a yardstick only. In a subsequent operation, it separately calibrates the orientation and position of all optical mouse sensors, by subjecting the robot frame to pivoting motions around fixed pivot points, without any prior knowledge of the location of the sensors or the pivot points. By forcing the robot to travel over arcs of constant radius, the orientation and position of the sensors can be derived directly from their displacement as measured, using a model of the kinematics of the pivot motion. Since the pivot point is stationary in each experiment, its displacement as calculated with the calibrated values of the parameters of the odometry system directly reflects the measurement error of the position and orientation of the robot and can serve as a validation for the calibration.
This approach is innovative in the following three aspects. Firstly, it requires only one measurement to set the length of a straight path to calibrate the sensor sensitivities. Orientations and positions of the sensors can be derived subsequently from the displacements measured by the optical flow sensors on the robot in the pivoting experiments. Hence the operation described is highly suitable to be implemented as a self-calibration procedure.
Secondly, validation of the calibration through calculation of the displacement of the pivot point requires no additional measurements and defines an unambiguous measure to compare the performance of different robots and navigation systems.
Thirdly, the developed micro controller based setup implements strict synchronization of the data acquisition by multiple optical flow sensors over a data bus (I2C) without requiring additional hardware. To the best of our knowledge, no other sources elaborating the synchronization of the data collection by multiple optical mouse sensors are available in the open literature.
The paper is organized as follows: after an introduction to odometry in Section 2, Section 3 describes the electronic design of an odometry with optical mouse sensors that secures proper synchronization of the data acquisition from all mouse sensors. Section 4 explains the design of the calibration procedure. Section 5 describes the execution of the experiments performed as a part of the calibration procedure. The experimental results are validated with calculations of the robot path in Section 6. The results and improvements demonstrated are listed in Section 7.
