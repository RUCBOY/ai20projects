1.1. Theoretical backgroundIn psychology, the construct of Collaborative Problem Solving (CPS) is defined as solving problems in collaboration with others. CPS is a conjoint construct that is comprised of the two components of problem solving on the one hand and social collaboration on the other. We assume that problem solving accounts for the cognitive component, involving the ability to transform a current problem state into a desired goal state (Mayer & Wittrock, 2006, pp. 287–303), whereas social collaboration accounts for the skill that allows a person to interact in synchrony with other participants (Griffin & Care, 2015). Their combination defines the interdependent skill of problem solving to move toward a common goal with other people (Fiore et al., 2010, Griffin, 2014). More specifically, PISA 2015 defined CPS as follows, after carefully assembling existing CPS definitions (for more information, see OECD, 2013):“Collaborative problem solving competency is the capacity of an individual to effectively engage in a process whereby two or more agents attempt to solve a problem by sharing the understanding and effort required to come to a solution and pooling their knowledge, skills and efforts to reach that solution” (Organisation for Economic Co-operation and Development (OECD), 2013, Organisation for Economic Co-operation and Development (OECD), 2017).The construct of CPS can be applied to various real-world settings, for example, the sharing of knowledge with colleagues in the workplace to overcome the individual boundaries of expertise. It can go beyond the workplace to the planning of tasks we complete with our families and friends in our private lives. Especially in our increasingly globalized and hyperconnected world, through digitalization we apply CPS in diverse aspects of 21st century life. Due the increasing significance of CPS, educational and political initiatives, including PISA 2015 and ATC21S, are assessing CPS to ensure that students demonstrate proficiency in CPS skills at the end of compulsory education. Other programs at the national levels are currently discussing the integration of CPS (e.g., in the US National Assessment of Educational Progress; NAEP; Fiore et al., 2017), so the relevance of CPS assessments in education is expected to remain high in the foreseeable future.However, even though the construct of CPS is receiving increasing educational attention, there is a general debate on the ideal methodology for the assessment of CPS due to a lack of empirical evidence in academic research (von Davier & Halpin, 2013). Traditional assessments, such as situational judgment tests in which individuals react to hypothetical role-relevant scenarios (Mumford et al., 2006, Patterson et al., 2016) or paper-based questionnaires (e.g., Aguado et al., 2014, Wang et al., 2009), however, are increasingly being replaced by computer-based assessment approaches, especially in large-scale settings such as applied in ATC21S (Griffin & Care, 2015) and PISA 2015 (OECD, 2017). These can simulate complex and dynamic CPS situations in virtual tasks, similar to the situations people face in real life.To assess the collaboration aspect of CPS, virtual CPS tasks require participants to collaborate with either computer-simulated agents (the human-to-agent technology: H-A) or real humans (human-to-human technology: H-H). Both approaches have advantages and disadvantages in the assessment of CPS (e.g., O'Neil, Chuang, & Chung, 2003). H-A approaches, as applied in PISA 2015, can offer standardized assessment conditions, which are especially crucial for student comparisons on the individual level. However, such conditions are often criticized for being limited in the extent to which they can allow natural collaboration to unfold because they limit conversational interactions between team partners (Graesser, Kuo, & Liao, 2017). In comparison, H-H approaches, such as applied in ATC21S, assess CPS during collaborations between humans and therefore provide better representations of natural collaboration. However, they lack controllability, which was crucial for the PISA 2015 CPS assessment, which aimed to compare students' CPS skills across countries. Also, H-H logfiles with natural speech information are very complex to analyze (Liu, Von Davier, Hao, Kyllonen, & Zapata-Rivera, 2015) and would take too long to be implemented in large-scale assessments (Care, Scoular, & Griffin, 2016).At this point in the science, a very limited body of research has explored differences between H-A and H-H assessment approaches (e.g., Rosen and Tager, 2013, Rosen, 2015). In academic research, this concern has been addressed in theoretical reviews (Graesser et al., 2017) but not empirically. Despite the advantages of the H-A approach, which offers assessment standardization and controllability of effects, the extent to which the H-A methodology in the PISA 2015 CPS assessment was able to capture the real dynamics of H-H interactions given the a priori constraints of the H-A approach has yet to be determined. Therefore, the extent to which the CPS skills assessed in PISA 2015 represent the way students would interact with human partners needs to be identified. The validity of the PISA 2015 H-A approach is of critical interest, considering the large impact of the PISA 2015 CPS results across the globe on educational systems and policies as well as the research opportunities it provides for academic research.We conducted the current study to validate the PISA 2015 CPS assessment by investigating the effects of replacing computer agents with real students in classroom tests (human-to-human; H-H). For this purpose, we obtained the otherwise confidential PISA 2015 CPS tasks so that we could generate additional data for this study and extend the main PISA 2015 trial. The interface in the H-H tasks remained nearly identical to the original PISA 2015 CPS assessment. We adopted the predefined chat design from the original PISA H-A tasks, so the H-H condition was constrained. More specifically, students selected from a predefined set of possible answers with one agent being replaced with a real student in the tasks. Therefore, there were always two humans interacting by selecting from a fixed set of chat options. This H-H condition was indeed constrained by these chat options, but less constrained than the H-A condition. Students were also informed about which types of partners they were collaborating with (computer-agents or computer-agents and a real classmate) in order to emphasize a likely effect of the collaboration partners’ nature on the main test takers. We identified the dimensionality of the underlying CPS construct and compared the agent effects (H-H versus H-A) on CPS performance accuracy and behavioral actions. If no substantial differences in collaborative activities occur, and if no variation in CPS scores is identified between the H-A and H-H formats, such outcomes would support the validity of the PISA 2015 CPS assessment as an authentic representation of collaborative behavior by the computer agents.
1.2. The PISA 2015 CPS assessmentPISA 2015 assessed CPS by employing different computer-based tasks that required active social collaboration with simulated agents during the solving of real-life problem scenarios in digital tasks (Organisation for Economic Co-operation and Development (OECD), 2013, Organisation for Economic Co-operation and Development (OECD), 2017). The students’ social collaboration was based on selecting predefined messages from lists of possible messages and sending them through a chat window in which the computer agent and students exchanged information in order to solve the required problem in the task space. Actions were performed in the action space of the task in order to solve problems that required non-chat actions, such clicking, dragging and dropping, or moving the elements on the screen in the task. Each correctly selected message or action that was chosen reflected a specific CPS skill for which students received credit (1 point; in a few cases, 2 points) or no credit (0) otherwise.Overall, 12 distinguishable CPS skills were assessed in the PISA 2015 tasks (the PISA 12-cell matrix; OECD, 2013), and each CPS skill was conceptualized on the basis of four individual problem solving processes, i.e., (A) exploring and understanding, (B) representing and formulating, (C) planning and executing, and (D) monitoring and reflecting, which was crossed with three newly conceptualized social collaboration dimensions, i.e., (1) establishing and maintaining a shared understanding, (2) taking appropriate action to solve the problem, and (3) establishing and maintaining team organisation. Table 1 displays the PISA 12-cell matrix that represents the CPS framework (OECD, 2013).Table 1. The 12-cell matrix illustrating the 12 CPS skills in the PISA 2015 assessment.(1) Establishing and maintaining shared understanding(2) Taking appropriate action to solve the problem(3) Establishing and maintaining team organisation(A) Exploring and Understanding(A1) Discovering perspectives and abilities of team members(A2) Discovering the type of collaborative interaction to solve the problem, along with goals(A3) Understanding roles to solve problem(B) Representing and Formulating(B1) Building a shared representation and negotiating the meaning of the problem (common ground)(B2) Identifying and describing tasks to be completed(B3) Describe roles and team organisation (communication protocol/rules of engagement)(C) Planning and Executing(C1) Communicating with team members about the actions to be/being performed(C2) Enacting plans(C3) Following rules of engagement, (e.g., prompting other team members to perform their tasks.)(D) Monitoring and Reflecting(D1) Monitoring and repairing the shared understanding(D2) Monitoring results of actions and evaluating success in solving the problem(D3) Monitoring, providing feedback and adapting the team organisation and rolesNote. Drawn from the OECD CPS Draft Report in PISA 2015 (2013).To provide an example, Fig. 1 illustrates the original PISA 2015 CPS task called “Xandar,” which was assessed in the main PISA 2015 assessment (OECD, 2017). In Xandar, students were required to compete in a contest along with their collaboration partners Alice and Zach in which they had to answer questions about the geography, people, and economy of the fictional country called Xandar (OECD, 2017). In general, Xandar assessed students’ decision-making, coordination, and consensus-building collaboration skills through correctly selected predefined messages and actions (OECD, 2017). More specifically, in the first part of Xandar as illustrated in Fig. 1, students were required to communicate with team members about the actions to be/being performed in this specific problem scenario (CPS skill “C1” in the PISA 12-cell matrix; OECD, 2013). Therefore, the third message “Maybe we should talk about strategy first” was scored as the correct message.Download : Download high-res image (100KB)Download : Download full-size imageFig. 1. Illustration of the PISA 2015 CPS task Xandar as retrieved from the official OECD report (OECD, 2017). Screenshot 1 illustrates a typical predefined message selection scenario for communicating with the computer agents. Message 3 is scored as the correct message representing the CPS Skill C1: communicate with team members about the actions to be/being performed in this specific problem scenario.
1.3. H-A versus H-H assessmentsPISA 2015 aimed to compare differences in educational systems, evaluate their impact on students' CPS proficiencies (OECD, 2017), and eventually draw implications for current educational policies. For this, PISA 2015 applied the H-A method to create dynamic CPS situations while standardizing assessment conditions across participating countries in a controlled manner. Computer agents can be validly used as conversation partners in students' collaborative learning (e.g., Biswas, Jeong, Kinnebrew, Sulcer, & Roscoe, 2010) and CPS assessment (e.g., Rosen, 2015) and possibly offer the advantage of assessing a wider spectrum of CPS skills (Rosen, 2015). In addition, the H-A technology allows CPS assessments to control for external effects, such as group composition effects (e.g., Wildman et al., 2012), personality effects (Herborn, Mustafic, & Greiff, 2018), or the partner's CPS activity and proficiency. In addition to controlling for these external effects, PISA 2015 could assess range of CPS skills and abilities with agents in CPS scenarios that were both varied and standardized across the students who participated in the PISA 2015 CPS assessment. In turn, this enabled the comparison of PISA 2015 CPS results between cultures and languages to a great extent.The H-A method enabled researchers to standardize the agents’ responses regardless of the messages selected by the students or the CPS level. Each scenario had a fixed sequence of assessment episodes that all students received; each assessment episode had the same starting point and converged on the same end point after interactions between the student and agent. Students with strong CPS skills who selected a response that offered zero points early in the test could still score well overall due to later standardized conditions in the task; and likewise, students with low CPS skills who chose a response that offered points early in the test would still be identified as low performers on the basis of their subsequent (poor) responses.However, assessing CPS with standardized H-A technologies also has some key drawbacks. As applied in the ATC21S project (Griffin & Care, 2015), H-H technologies assess CPS during collaboration between humans. Therefore, H-H assessment approaches provide more natural human collaboration situations that are closer to the kinds of CPS situations students encounter in real life. Unexpected responses or actions, which might not be assessed by standardized algorithms, would therefore not be captured in H-A approaches (Graesser et al., 2017, Rosen, 2015). For example, Graesser et al. (2017) stated that conversational interactions can be limited when comparing them with free chat sessions between collaboration partners. Students were allowed to type individual messages during their collaborations in ATC21S. Therefore, communication and actions that were not coded in a standardized algorithm could be captured, and logstream data enabled researchers to conduct both qualitative and quantitative analyses (Care et al., 2016). This raises the question of the extent to which the H-A methodology used in the PISA 2015 CPS assessment was able to capture the real dynamics of H-H interactions given the a priori constraints of the H-A approach.
1.4. The present studyWe conducted this study to investigate whether the original PISA 2015 CPS tasks were able to reflect the extent to which students' collaborations with computer agents represented the way students would interact with human partners. In other words, our long-term goal was to determine whether agents can replace humans as collaboration partners in CPS assessments. This study does not fully achieve this long-term goal but does take an initial step in addressing the issue. In particular, some of the original PISA 2015 CPS tasks were reformatted and redesigned into a constrained H-H format by replacing one of the agents with a classmate in each task to allow real human interaction to take place. One of the computer agents was replaced by a classmate, a peer of equal status to the student. It is important to note that the computer-agents replaced by classmates were not in the role of the experts, but rather, the role within the group was defined by the students’ CPS skills preforming the computer-agent. The predefined chat communication was adopted and extended in the new H-H tasks. More specifically, the original PISA 2015 H-A approach, as illustrated in Fig. 1, was fully adopted, and only the type of collaboration partners was changed (computer-agents or computer-agents and a real classmate). Students in the role of the collaboration partners also received predefined messages to choose from. Fig. 2 illustrates a reformatted PISA task in H-H format and provides an example chat turn of an assessment episode.Download : Download high-res image (961KB)Download : Download full-size imageFig. 2. Illustration of the PISA 2015 CPS task the Visit as retrieved from the official OECD report on released field trial cognitive items (OECD, 2017). Screenshot 1 illustrates a typical predefined message selection scenario for communicating with the computer agents. The computer-agent George replies with the standardized message “I kind of like the idea of the market. It would be cool to go there”. Screenshot 2 illustrates the chat turn in H-H format. The student replacing George also received predefined messages to choose from.Using the example of Fig. 2, students of equal status to the main test taker were in the role of the collaboration partners and replaced George in the task “the Visit”. These students acted as George within the group, and also received predefined messages to select from and to reply in the group chat. Among the predefined messages is George's original message “I kind of like the idea of the market. It would be cool to go there” that the agent George sent to the chat in the H-A format (Fig. 1). Based on the CPS proficiency levels as published in the PISA 2015 CPS report (OECD, 2013), George's original message was rated as medium collaboration proficiency. In addition, the two further messages “I like all ideas” (low collaboration proficiency) and “Let's think, whether the market or the car factory is the better idea” (high collaboration proficiency) were also offered to the students replacing George, so that they also had three messages to select from.In a first step, this study investigated the factorial validity of both approaches in assessing CPS using several consecutive confirmatory factor analyses. The reformatting allowed stipulating the following research questions for this study.Research Question 1: Are there differences in factorial validity when assessing students’ CPS performance using computer agents versus classmates?In a second step, this study further compared the validity by investigating into the effects in CPS performance accuracy and behavioral actions between type of format (H-A versus H-H) by looking at the accuracy as well as the number of students’ interactions with the problem in each task individually. We examined the differences in the correctness scores and number of actions made by students assessed using only computer agents with that of students assessed using a classmate.Research Question 2: Are there differences in CPS performance accuracy and behavioral actions when assessing students’ CPS performance using computer agents versus classmates?
