Mental healthcare has been transformed by the development of Information and Communication Technologies (ICT), which are now completely integrated in public health strategies. For example, Canada has promoted “E-Mental Health” [1], defined as a way to help “people living with a mental health problem or illness to feel more knowledgeable and better express their needs” using ICTs [1,2]. One technological field is recently gaining a lot of attention: Artificial Intelligence (AI). Although promising, AI raises numerous ethical risks [3]. This paper investigates how to better identify and address ethical issues in AI, with a focus on mental health and suicide prevention.
1.1. Artificial intelligence“Artificial Intelligence” designates a wide field of study. It is the domain interested in the study and design of intelligent machines (McCarthy, 1998). Its goal is to build machines “capable of performing tasks that we define as requiring intelligence, such as reasoning, learning, planning, problem-solving, and perception” ([4], p. 15). It is difficult to accurately define and terms can vary. A recent international report suggested it may be better to use the term: Autonomous Intelligent System (AIS) [3]. Today, the most widespread AI technique is Machine Learning (Demiaux et al., 2017), or “computational methods using experience to improve performance or to make accurate predictions” [5].
1.2. Artificial intelligence, mental health and suicide preventionAI appears to be promising for mental health. It may help diagnose patients more accurately [6], improve clinical decision making [7,8], and detect suicide risk more accurately than with traditional techniques [9]. Governments (e.g. US Veterans Affairs), private companies (e.g. Facebook) are now using AI in the hope of saving more lives [4,10]. AI technologies can help analyze large volumes of texts, for instance from micro-blogs [11] and Electronic Health Records [9]. This is timely considering that the WHO has recently set ambitious goals to improve the way suicide rates are monitored, suicidal individuals are detected, and psychological help is delivered [12,13].
1.3. Important ethical challengesDespite its promise of being helpful, AI raises numerous ethical issues, including the risks of: discrimination [14], non-respect of privacy [15,16], and its general lack of transparency [17,18]. These risks have been studied and guidelines have been proposed to prevent them [19,3,20]. However, they often are not covered in current legal systems [21]. Furthermore, guidelines often lack enforcement and reinforcing mechanisms, making them inoperative [22].The use of AI en mental health and suicide prevention also raises major ethical concerns. For instance, AI-based detection systems may make classification errors [9,23] and it is difficult to obtain informed consent when using largescale datasets to prevent suicide [24,25]. AI is considered in its “infancy” and thus, it has been suggested that its use should require additional steps “before integration in healthcare settings” ([9], p. 15). Tucker suggested to conduct “a careful analysis of the ethical issues (to) help to ensure that individual rights are fully considered and properly integrated into new clinical care prevention programs” [26]. Since ethical questions associated with the use of AI in suicide prevention “come without defined and predicted outcomes, companies need to put in place standardized and accountable processes to reach ethically justifiable and consistent answers” [15]. However, today there are no existing guidelines on how to ethically use AI in suicide prevention [9] nor in mental health in general. This lack of guidelines may explain why publications in suicide prevention rarely mentioned encountering ethical issues while using Big Data or AI [10]. The present study attempts to address this need.
1.4. Addressing the ethical challenges in AIToday, at least 83 declarations of principles and lists of standards have been published, to improve the ethical regulation of AI [27]. These declarations are a good step forward, but do not always specify how to apply them in practice for specific fields [28]. For example, the EU released guidelines for a “trustworthy” AI that include recommendations and checklists. It is a benchmark to guide a responsible development of AI [29]. Similarly, the Treasury Board of Canada has created an algorithmic impact survey to help administrators assess and mitigate the level of risk of AI-based services [30,31]. Other checklists exist in research: “Geneth: A General Ethical Dilemma Analyzer” [32] is a general ethical dilemma analyzer using Machine Learning. DELICATE, “A Checklist for Trusted Learning Analytics” for developers [33], asks questions about 8 key actions: determination, explain, legitimate, involve, consent, anonymize, technical, external.Checklists are one type of ethics tool, amongst others. Frameworks also exist, such as the Human-Centered AI Canvas [34] and the ethical OS toolkit [35]. All these tools contribute to a better recognition of the importance of ethics in AI and could solve what has been called today’s “moral overload” [3].In order to address contemporary ethical concerns in the use of AI in mental health and suicide prevention, this paper introduces and presents the creation of an ethical tool: the Canada Protocol, a checklist intended for everyone planning on using AI in these fields. Although the Canada Protocol was designed for the use in suicide prevention and mental health, much of its content pertains to other applied uses of AI to help with health problems people experience.
