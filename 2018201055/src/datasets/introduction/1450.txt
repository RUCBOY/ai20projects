Over the past forty years, numerous approaches have been evolved in DDBS literature to functionally manage the ever-growing data. Nevertheless, several issues for improving DDBS design quality are still open-ended challenges. Particularly speaking, the relational distributed database which is still growing in popularity. Consequently, the continuous interest is also still emphasized towards finding the well-designed approaches to keep the sustainability of DDBS performance. On the other hand, the most critical contributor in performance is that how much amount of data is being transmitted over the network when distributed queries are under processing. This dominant contributor has widely known as Transmission Costs (TC) for which most of the previous DDBS works had come to find a solution of influential impact. Moreover, it has been noted that almost the majority of earlier works have never been recorded to come up with a clear definition for this contributor by which performance is set to be graded (Amer et al., 2018a, b).
In the meantime, the wealthy existence of design approaches leads to more confusion when it comes to select certain approach to design DDBS. Fortunately, to lessen this burden, a consensus on the principles and concepts which ground these approaches still exist (Amer et al., 2018a, b). Additionally, the ever-progressing researches in DDBS and distributed computing domains are still in the development now and then to tackle the DDBS design challenges. Some of these approaches are subtly optimized or well extended to incorporate other techniques straightforwardly to sustain DDBS throughput (Raouf et al., 2018; Luong et al., 2018; Wiese et al., 2016; Sewisy et al., 2017; Amer, 2018; Abdalla and Artoli, 2019). The findings of these approaches have been reinforced by placing them directly under the test on either synthesized data in a simulated environment or on real datasets. In this work, therefore, we seek to present a new approach with the major purpose of significantly decreasing TC. The proposed approach was essentially designed to involve and develop: (1) an aggregated similarity between queries to fragment data using the single-linkage Agglomerative Hierarchical Clustering (AHC) process, (2) a greedy-based data allocation model to allocate the resulted fragments, and (3) integrate the site clustering procedure which was drawn in (Amer et al., 2017). It is worth indicating that the choice of the single-linkage has been on purpose as our work seeks to join each cluster pair based on the maximum similarity value between the cluster pair. In other words, the closest distance is adopted to define the similarity between each cluster pair. Data allocation, on the other hand, was made using a greedy-based algorithm which is highly contingent on the dynamic programming (Knapsack problem, in this work). That is, data allocation was being treated as an optimization problem. Moreover, a theoretical comparison and empirical evaluation for the proposed work of this paper is made with state-of-art approaches. In fact, the evaluation results have been promising in terms of the sustainability of DDBS performance and TC reduction.
The main contributions of this paper are listed as follows: (1) leveraging data fragmentation algorithm based on an aggregated similarity. The similarity measure is anticipated to reduce the number of iterations needed to perform AHC and then find solution space of smaller size comparing with the state-of-art. Moreover, the proposed approach does not need the query frequency matrix to perform fragmentation, nor affinity matrix or even attribute usage matrix which makes our work the best option to design DDBS at the initial stage of design; (2) drawing the site clustering process to produce minimum number of highly balanced clusters. The clustering process of queries, on the other extreme, struggles to find the balanced cluster as each cluster must have at least (√N) query to help distributing the workload over network sites/clusters while distributed queries are being handled; (3) suggesting a greedy-based data allocation algorithm to contribute in minimizing TC. That is, dealing with data allocation as an optimization problem and finding a model to address this problem. Finally, along this paper, for the sake of readability and simplicity, the proposed approach is given an acronym named “ASGOP”. This acronym, on the other hand, is found by taking the first letters of each word of paper's title as follows; An Aggregated “A”; Similarity-based “S”; Greedy-Oriented “GO” and Approach “P”.
The remaining of this paper is structured as follows; in section (2), a deeply-made investigation for the earlier works which are closely related is presented. Section (3) holds the proposed methodology including data fragmentation procedure, the proposed aggregated similarity measure, AHC review in a brief, fragmentation evaluator, site clustering process and data allocation cost model foundations and algorithms. Section (4) elegantly provides experimental setup of ASGOP, datasets descriptions, the running example to draw the proposed work mechanism and performance evaluation. An experimental implementation along with the discussion of findings are drawn in section (5). Finally, in section (6), the conclusions and future work directions are given.
