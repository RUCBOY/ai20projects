We review research on the impact and value of graph technologies for K-12 science and mathematics learning. We characterize the ways graphing is assessed and the investigative features these technologies support. Graph technologies are widely available in precollege classes where they support a variety of investigative features, such as generating hypotheses or predictions (Mokros & Tinker, 1987; Songer & Linn, 1991), analyzing or interpreting data from multiple sources (Kastberg & Leatham, 2005; Tortosa, 2012), and reflecting on results (McElhaney & Linn, 2011).
Diverse and sophisticated graphing tools allow designers to strengthen graph understanding as part of teaching mathematics or science (Ainsworth, 1999; diSessa, 2004; Greeno & Hall, 1997). Graphing technologies can illustrate relationships between changes in temperature and motion using Microcomputer Based Laboratories (MBL); results from changing variables in simulations of phenomena such as climate change, population growth, or tectonic plate movements; and impacts of changing parameters governing functions in mathematics. Some studies use technology to support graph construction. Others emphasize comprehending features of a graph or labeling graphs (Yeh & McTigue, 2009). Furthermore, students can be challenged to invent graph representations (diSessa, 2004) or create a graph that depicts a narrative such as a hike (Vitale, Lai, & Linn, 2015). Students can simultaneously explore how airbags deploy and how position and motion graphs work (McElhaney & Linn, 2011) or how the parameters of a function impact the graph shape (Berg & Boote, 2017; Berg & Phillips, 1994).
Activities that involve creating and interpreting graphs are central to the U.S. Next Generation Science Standards (NGSS; NGSS Lead States, 2013) and Common Core Mathematics Standards (CCMS; Common Core State Standards Initiative, 2010). These standards focus on integrated understanding and emphasize the use of authentic data across K-12 instruction. The NGSS argue that preparing informed citizens and professionals requires attention to interpretation and design of graphs depicting contemporary dilemmas. For example, several performance expectations in the NGSS directly address graphing such as “5-PS1-2. Measure and graph quantities to provide evidence that regardless of the type of change that occurs when heating, cooling, or mixing substances, the total weight of matter is conserved.” (NGSS Lead States, 2013, p. 43; See LaDue, Libarkin, & Thomas, 2015, for other NGSS connections to graphs in the high-school context). Likewise, the CCMS addresses graphing with expectations such as “5.G: Graph points on the coordinate plane to solve real-world and mathematical problems.” (Common Core State Standards Initiative, p.38). Achieving the NGSS and CCMS requires instruction that incorporates graphs across mathematics and science along with valid and reliable assessments of student graph proficiency (NGSS Lead States, 2013; Wang et al., 2012). Our review investigates how existing research literature is meeting these challenges using meta-analysis techniques for design and comparison studies focused on graph technologies.
Graphs are vital for learning, technical occupations, and public discourse (Arsenault, Smith, & Beauchamp, 2006; Krohn, 1991). They take advantage of the human capacity to visualize large amounts of data in ways that reveal patterns, uncertainty, and critical events (Friel, Curcio, & Bright, 2001). Graph shapes, allow people to infer underlying processes and interactions within systems from individual data points (Shah & Hoeffner, 2002) and to predict future trends (Ellington, 2006; Wang et al., 2012). For example, an analysis of the points on a temperature/time graph can help determine how an object changes temperature over time, and also support predictions that extrapolate the temperature change beyond data within the graph (Linn, Layman, & Nachmias, 1987). Yet, interpreting graphs is challenging for most people as shown in international comparisons (OECD, 2006) and previous reviews in mathematics education (Cheung & Slavin, 2013; Leinhardt, Zaslavsky, & Stein, 1990; Rakes, Valentine, MaGatha, & Ronau, 2010) and science education (Glazer, 2011; Nakhleh, 1994; Shah & Hoeffner, 2002). We build on these reviews to analyze the impact of graphing technologies and the role of investigative features that could add value to graphing technologies. Specifically, we identify and analyze strengths and gaps in use of investigative features that can amplify the impact of graphing technology and improve instruction and understanding in both science and mathematics.
1.1. Characterizing graphing instructionTo characterize graphing instruction, we focus on the investigative features described in the research literature to categorize student science and mathematics activities using graphs. We link these investigative features to the broader categorizations of the NGSS science and engineering practices. The NGSS performance expectations include eight science and engineering practices (SEPs): 1. Asking questions and defining problems, 2. Developing and using models, 3. Planning and carrying out investigations, 4. Analyzing and interpreting data, 5. Using mathematics and computational thinking, 6. Constructing explanations and designing solutions, 7. Engaging in argument from evidence, and 8. Obtaining, evaluating, and communicating information (NGSS Lead States, 2013). The practices include investigative features such as “questioning and generating hypotheses, experimenting, designing, and planning, predicting, modelling/visualizing, observing and data collection, analyzing data, interpreting and explaining, developing/evaluating/arguing, reaching conclusions, and communicating findings” (Authors, 2014; See also; National Research Council, 2012). These practices and their accompanying investigative features are important for understanding the impact of graph technologies, the ways these technologies support student learning, and the gaps that graph technologies could fill in student learning.
1.2. Measuring graph proficiencyTo fully capture the value of graph proficiency, we need comprehensive assessments. Our use of the term ‘graph proficiency’ is meant to capture the broad range of roles for graphs articulated in the research literature, e.g., graphicacy, meta-representation, experimentation. Several reviews of the nature of graph-based items in standardized tests reveal that graph proficiency is rarely measured, and that, when it is measured, items often focus on comprehension of graph features such as student ability to locate a point on a graph or to determine whether the graph labels are accurate (Miller & Linn, 2013; Yeh & McTigue, 2009). Choice of assessment may reflect the nature of instruction and could impact the interpretation of learning outcomes. For example, as explained by Leinhardt et al. (1990), “Construction is quite different from interpretation [comprehension]. Whereas interpretation relies on and requires reaction to a given piece of data (e.g. a graph, an equation, or a data set), construction requires generating new parts that are not given.” (p. 12).Some studies assess graphicacy, defined as “proficiency in understanding quantitative phenomena that are presented in a graphical way” (Wainer, 1992, p. 16). Graphicacy refers to the ability to read and interpret graphs (Friel & Bright, 1996). Others study graph sense, “the ability to recognize components of graphs, speak the language of graphs, understand relationships between tables and graphs, respond to questions about graphs, recognize better graphs, and interpret contextual awareness of graphs” (Delmas, Garfield, & Ooms, 2005, p. 2). diSessa (2004) refers to meta-representational competence as the ability to choose an appropriate external representation for data or to use novel external representations productively. Many studies assess student experimentation by looking at how they interpret graphs or generate trials in a simulation (Roschelle et al., 2010).To determine how well outcome measures align with graph proficiency (Pellegrino, Wilson, Koenig, & Beatty, 2014), we analyze the use of three main categories of assessments of graph proficiency: Construction of a graph; critique of a graph, and comprehension of a graph (Lai et al., 2016; Yeh & McTigue, 2009). Measures of graph proficiency vary not only in form (construction, comprehension, and critique), but also in format (multiple-choice, open-response recall, and open-response explanation), and disciplinary focus (mathematics and science). We review all graph technology studies featuring questions about graphs and analyze how form, format, and discipline contribute to our research questions.
1.3. Research questionsWe investigate three research questions. For research questions one and two, we report on the role of graph assessment form (Construction; Comprehension; Critique; Lai et al., 2016) and format (multiple choice and open response) to enrich our analysis in determining how such assessment factors may influence our findings for these two research questions. Our research questions are:1.What is the overall impact of instruction supported by graph technology on K-12 students' learning? We answer this question by conducting a meta-analysis of design studies that analyze graphing instruction using pre/post data measuring graph proficiency.2.Does the impact of technology-based graphing instruction differ from the impact of non-technology-based instruction? We meta-analyze studies that have either pre/post and post-test only results that compare instruction with and without digital technology.3.What investigative features characterize the use of K-12 graphing technologies? We answer this question through a binary scoring of studies based on the presence of a particular investigative feature, such as collecting data, drawing conclusions, reflecting, etc. (See Method for all features).
