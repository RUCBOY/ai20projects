On top of devastating health effects, epidemic impacted hugely on world economy. In the Ebola outbreak between 2014–2016 where more than 28,000 and cases were suspected and 10,000 deaths in West Africa [1], $2.2 billion was lost [2]. On the other hand, SARS took over 648 lives from China including Hong Kong and 700 lives worldwide between 2002 and 2003 [3]. Its losses on global economy are up to a huge $100 billion, 1% and 0.5% dips of GDPs in Chinese and Asian domestic markets respectively [3]. Although the current coronavirus (codename: NCP or COVID-19) epidemic is not over yet, its economy impact is anticipated by economists from IHS Market to be far worse than that of SARS outbreak in 2003 [4]. The impact is so profound that will lead to factories shut down, enterprises bankruptcy especially those in tourism, retail and F&B industries, and suspensions or withdrawals in long-term investment, if the outbreak cannot be contained in time. Since the first case in December 2019, the suspected cases and deaths around the world skyrocketed to over 76395 confirmed cases and 2348 deaths, mostly in China, by the time of writing this article.
An early intervention measure in public health to thwart the outbreak of COVID-19 is absolutely imperative. According to a latest mathematical model that was reported in research article by The Lancet [5], the growth of the epidemic spreading rate will ease down if the transmission rate of the new contagious disease can be lowered by 0.25. Knowing that the early ending the virus epidemic or even the reduction in the transmission rate between human to human, all governments especially China where Wuhan is the epicentre are taking up all the necessary preventive measures and all the national efforts to halt the spread. How much input is really necessary? It is fundamentally challenging in making a sound decision. When an epidemic has just started, available data is scarce, information is limited, and uncertainty is high. Unlike common Influenza, there is neither precedent case exactly the same nor sufficient knowledge. Many decision makers could only take references from SARS which is by far the most similar virus to COVID-19. However, it is difficult as the characteristics of the virus are not fully known, its details and about how it spreads are gradually unfolded from day to day. Given the limited information on hand about the new virus, and the ever evolving of the epidemic situations both geographically and temporally, it boils down to grand data analytics challenge this analysis question: how much resources shall be enough to slow down the transmission? This is a composite problem that requires cooperation from multi-prong measures such as medical provision, suspension of schools, factories and office, minimizing human gathering, limiting travel, strict city surveillance and enforced quarantines and isolations in large scales. There is no easy single equation that could tell the amount of resources in terms of monetary values, manpower and other intangible usage of infrastructure; at the same time there exist too many uncertain variables from both societal factors and the new development of the virus itself. For example, the effective incubation period of the new virus was found to be longer than a week, only some time later after the outbreak. Time is an essence in stopping the epidemic so to reduce its damages as soon as possible. However, uncertainties are the largest obstacle to obtain an accurate model for forecasting the future behaviours of the epidemic should intervention apply. In general, there is a choice of using deterministic or stochastic modelling for data scientists; the former technique based solely on past events which are already known for sure, e.g. if we know the height and weight of a person, we know his body mass index. Should any updates on the two dependent variables, the BMI will be changed to a new value which remains the same for sure no matter how many times the calculation is repeated. The latter is called probabilistic or stochastic model — instead of generating a single and absolute result, a stochastic model outputs a collection of possible outcomes which may happen under some probabilities and conditions.
1.1. BackgroundDeterministic model is useful when the conditions of the experiment are assumed rigid. It is useful to obtain direct forecasting result from a relatively simple and stable situation in which its variables are unlikely to deviate in the future. Otherwise, for a non-deterministic model, which is sometimes referred as probabilistic or stochastic, the conditions of a future situation under which the experiment will be observed, are simulated to some probabilistic behaviour of the future observable outcome. For an example of epidemic, we want to determine how many lives could be saved from people who are infected by a new virus as a composite result of multi-prong efforts that are put into the medical resources, logistics, infrastructure, spread prevention, and others; at the same time, other contributing factors also matter, such as the percentage of high-risk patients who are residing in that particular city, the population and its mobility, as well as the severity and efficacy of the virus itself and its vaccine respectively. Real-time tools like CDC data reporting and national big data centres are available with which any latest case that occurs can be recorded. However, behind all these records are sequences of factors associated with high uncertainty. For example, the disease transmission rate depends on uncertain variables ranging from macro-scale of weather and economy of the city in a particular season, to the individual’s personal hygiene and the social interaction of commuters as a whole. They are dynamic in nature that change quickly from time to time, person to person, culture to culture and place to place. The phenomena can hardly converge to a deterministic model. Rather, a probabilistic model can capture more accurately the behaviours of the phenomena. So for epidemic forecast, a deterministic model such as trending is often used to the physical considerations to predict an almost accurate outcome, whereas in a non-deterministic model we use those considerations to predict more of a probable outcome that is probability distribution oriented.In order to capture and model such dynamic epidemic recovery behaviours, stochastic methods ingest a collection of input variables that have complex dependencies on multiple risk factors. The epidemic recovery can be viewed in abstract as a bipolar force between the number of populations who has contracted the disease and the number of patients who are cured from the disease. Each group of the newly infested and eventually cured (or unfortunately deceased) individuals are depending on complex societal and physiological factors as well as preventive measures and contagious control. Each of these factors have their underlying and dependent factors carrying uncertain levels of risks. A popular probabilistic approach for modelling​ the complex conditions is known as Monte Carlo (MC) simulation which provides a means of estimating the outcome of complex functions by simulating multiple random paths of the underlying risk factors. Rather than deterministic analytic computation, MC uses random number generation to generate random samples of input trials to explore the behaviour of a complex epidemic situation for decision support. MC is particularly suitable for modelling epidemic especially new and unknown disease like COVID-19 because the data about the epidemic collected on hand in the early stage are bound to change. In MC, data distributions are entered as input, since precise values are either unknown or uncertain. Output of MC is also in a form of distribution specifying a range of possible values (or outcome) each of which has its own probability at which it may occur. Compared to deterministic approach where precise numbers are loaded as input and precise number is computed to be output, MC simulates a broad spectrum of possible outcomes for subsequent expert evaluation in a decision-making process.
1.2. Monte Carlo Simulation for epidemicsRecently as epidemic is drawing global concern and costing hugely on public health and world economy, the use of MC in epidemic modelling forecast has become popular. It offers decision makers an extra dimension of probability information so called risk factors for analysing the possibilities and their associated risk as a whole. Decades ago, there has been a growing research interest in using MC for quantitatively modelling epidemic behaviours. Since 1957, Bailey et al. was among the pioneers in formulating mathematical theory of epidemics. Subsequently in millennium, Andersson and Britton [6] adopted MC simulation techniques to study the behaviour of stochastic epidemic models, observing their statistical characteristics. In 2003, House et al. attempted to estimate how big the final size of an epidemic is likely to be, by using MC to simulate the course of a stochastic epidemic. As a result, the probability mass function of the final number of infections is calculated by drawing random samples over small homogeneous and large heterogeneous populations. Yashima and Sasaki in 2013 extended the MC epidemic model from over a population to a particular commute network model, for studying the epidemic spread of an infectious disease within a metropolitan area — Tokoyo train station. MC is used to simulate the spread of infectious disease by considering the commuters flow dynamics, the population sizes and other factors, the proceeding size of the epidemic and the timing of the epidemic peak. It is claimed that the MC model is able to serve as a pre-warning system forecasting the incoming spread of infection prior to its actual arrival. Narrowing from the MC model which can capture the temporal–spatial dynamics of the epidemic spread, a more specific MC model is constructed by Fitzgerald et al. [7] in 2017 for simulating queuing behaviour of an emergency department. The model incorporates queuing theory and buffer occupancy which mimic the demand and nursing resource in the emergency department respectively. It was found that adding a separate fast track helps relieving the burden on handling of patient and cutting down the overall median wait times during an emergency virus outbreak and the operation hours are at peak. Mielczarek and Zabawa [8] adopted a similar MC model to investigate how erratic the population is, hence the changes in the number of infested patients affect the fluctuations in emergency medical services, assuming there are epidemiological changes such as call-for-services, urgent admission to hospital and ICU usages. Based on some empirical data obtained from EMS centre at Lower Silesia Region in Poland, the EMS events and changes in demographic information are simulated as random variables. Due to the randomness of the changes (in population sizes as people migrate out, and infested cases increase) in both demand and supply of an EMS, the less-structured model cannot be easily examined by deterministic analytic means. However, MC model allows decision makers to predict by studying the probabilities of possible outcomes on how the changes impact the effectiveness of the Polish EMS system. There are similar works which tap on the stochastic nature of MC model for finding the most effective escape route during emergency evacuation [9] and modelling emergency responses [10].
1.3. Motivation and contributionsOverall, the above-mentioned related works have several features in common: their studies are centred on using a probabilistic approach to model complex real-life phenomena, where a deterministic model may fall short in precisely finding the right parameters to cater for every detail. The MC model is parsimonious that means the model can achieve a satisfactory level of explanation or insights by requiring as few predictor variables as possible. The model which uses minimum predictor variables and offers good explanation is selected by some goodness of fit as BIC model selection criterion. The input or predictor variables are often dynamic in nature whose values change over some spatial–temporal distribution. Finally, the situation in question, which is simulated by MC model, is not only complex but a prior in nature. Just like the new COVID-19 pandemic, nobody can tell when or whether it will end in the near future, as it depends on too many dynamic variables. While the challenges of establishing an effective MC model is acknowledged for modelling a completely new epidemic behaviour, the model reported in [10] inspires us to design the MC model by decomposing it into several sub-problems. Therefore, we proposed a new MC model called composite MC or CMC in short which accepts predictor variables from multi-prong data sources that have either correlations or some kind of dependencies from one another. This helps provide the decision maker a fuller view hence richer information that contribute to an optimal decision (so called the best out of the worst decision). The challenge here is to ensure that the input variables though they may come from random distribution, their underlying inference patterns must contribute to the final outcome in question. The methodology is focused on finding the best possible algorithmic approaches for generating stochastic future outcomes, when a novel epidemic just emerged with very limited available information on hand.In computer science, this represents a typical problem of machine learning over incomplete/limited data in early epidemic. Considering multi-prong data sources widen the spectrum of possibly related input data, thereby enhancing the performance of Monte Carlo simulation. However, naive MC by default does not have any function to decide on the importance of input variables. It is known that what matters for the underlying inference engine of MC is the historical data distribution which tells none or little information about the input variables prior to the running of MC simulation. To this end, we propose a pre-processor, in the form of optimized neural network namely BFGS-Polynomial Neural Network is used at the front of MC simulator. BFGS-PNN serves as both a filter for selecting important variables and forecaster which generates future time-series as parts of the input variables to the MC model. Traditionally all the input variables to the MC are distributions that are drawn from the past data which are usually random, uniform or some customized distribution of sophisticated shape. In our proposed model here, a hybrid input that is composed of both deterministic type and non-deterministic type of variables. Deterministic variables come from the forecasted time-series which are the outputs of the BFGS-PNN. Non-deterministic variables are the usual random samples that are drawn from data distributions. In the case of COVID-19, the future forecasts of the time-series are the predictions of the number of confirmed infection cases and the number of cured cases. Observing from the historical records, nevertheless, these two variables display very erratic trends, one of them contains extreme outliers. They are difficult to be closely modelled by any probability density function; intuitively imposing any standard data distribution will not be helpful to delivering accurate outcomes from the MC model. Therefore in our proposal, it is needed to use a polynomial style of self-evolving neural network that was found to be one of the most suited machine learning algorithm in our prior work [11], to render a most likely future curve that is unique to that particular data variable.The composition of the multiple data sources is of those relevant to the development (rise-and-decline) of the COVID-19 epidemic. Specifically, a case of how much daily monetary budget that is required to struggle against the infection spread is to be modelled by MC. The data sources of these factors are publicly available from the Chinese government websites. More details follow in Section 2 below. The rationale behind using a composite model is that what appears to be an important figure, e.g. the number of suspected cases are directly and indirectly related to a number of sub-problems which of each carries different levels of uncertainty: how a person gets infested, depends on 1) the intensity of travel (within a community, suburb, inter-city, or oversea) 2) preventive measures 3) trail tracking of the suspected and quarantining them 4) medical resources (isolation beds) available, and 5) eventual cured or dead. Some of these data sources are in opposing influences to one another. For example, the tracking and quarantine measures gets tighten up, the number of infested drops, and vice-versa. In theory, more relevant data are available, the better the performance and more accurate outcomes of the MC can provide. MC plays an important role here as the simulation is founded on probabilistic basis, the situation and its factors are nothing but uncertainty. Given the available data is scare as the epidemic is new, any deterministic model is prone to high-errors under such high uncertainty about the future.The contribution of this work has been twofold. Firstly, a composite MC model, called CMCM is proposed which advocates using non-deterministic data distributions along with future predictions from a deterministic model. The deterministic model in use should be one that is selected from a collection of machine learning models that is capable to minimize the prediction error with its model parameters appropriately optimized. The advantage of using both fits into the MC model is the flexibility that embraces some input variables which are solely comprised of historical data, e.g. trends of people infested. And those that underlying elements which contribute to the high uncertainty, e.g. the chances of people gather, are best represented in probabilistic distribution as non-deterministic variables to the MC model. By this approach, a better-quality MC model can be established, the outcomes from the MC model become more trustworthy. Secondly, the sensitivity chart obtained from the MC simulation is used as corrective feedback to rules that are generated from a fuzzy rule induction (FRI) model. It is known that FRI outputs decision rules with probabilities/certainty for each individual rule. A rule consists of a series of testing nodes without any priority weights. By referencing the feedbacks from the sensitivity chart, decision makers can relate the priority of the variables which are the tests along each sequence of decision rules. Combining the twofold advantages, even under the conditions of high uncertainty, decision makers are benefited with a better-quality MC model which embraces considerations of composite input variables, and fuzzy decision rules with tests ranked in priority. This tool offers a comprehensive decision support at its best effort under high uncertainty.The remaining paper is structured as follow. Section 2 describes the proposed methodology called GROOMS+CMCM, followed by introduction of two key soft computing algorithms — BFGS-PNN and FRI which is adopted for forecasting some particular future trends as inputs to the MC model and generating fuzzy decision rules respectively. Section 3 presents some preliminary results from the proposed model followed by discussion. Section 4 concludes this paper.
