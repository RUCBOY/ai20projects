Attention is an important topic in the computer vision field. In the past 20 years, saliency map estimation [52] and saliency object estimation [51] are two intensively-studied problems concerning visual attention, with the goal of estimating saliency regions or saliency objects in an image that draw the attention of the human (outside the image) who is looking at the image. In this paper, we study the attention of the human (inside a video), we call it Inside-video human attention.
Saliency-based visual attention has wide applications in video tracking [17], image retrieval [30], and scene rendering [14], while the main advantage of inside-video human attention estimation lies in its significance for human-robot interaction, which has promising applications in various facets of society like the elderly care [47], education [25], and military [13]. In a typical human-robot interaction scenario in daily life, a robot is installed with a camera capturing a video, inside which a human is performing daily activities. In this kind of scenarios, inferring human attention from the robot’s view equals to inferring the attention of a human inside a video (Inside-video human attention). Elderly care is a potential and valuable application of human-robot interaction. As we know, it is laborious for the elderly people to perform some simple activities such as open the refrigerator, lift a cup, and move a bottle. To enable the robot to assist the human, it is necessary for the robot to infer human attentional objects. For example, a human is going to take an apple from a refrigerator, when the human is approaching the refrigerator, the robot could infer that human attentional object is the refrigerator, so that the robot could assist the human to open the refrigerator door in advance.
To infer human attention, the foremost thing is to make clear what the human attention is. Originally, attention is a concept in philosophy. Nowadays, it is well known as a concept in psychology. One dominant definition in psychology is that attention is the process of attending to objects [42]. This definition indicates that the attention is based on objects. Actually, some studies [7], [8], [37] in psychophysics and biology fields as well as some inter-discipline studies in neuro image filed [63] and brain image field [34] also claim the object-based attention. Especially, Chou [8] provides the evidence of object-based attention. These studies provide the strong theory support for defining human attention as objects. Another widely accepted definition in psychology is that attention is something that happens in the mind - a mental “inside” which is linked with the perceivable “outside” [43]. This definition indicates that attention is related with the high-level mental information in human mind. When a human is doing a task, the task is a kind of high-level information in the mind, guiding human attention. For example, the juicer tends to draw human attention in the task of “make juice”, while the coffee machine tends to draw human attention in the task of “make coffee”.
Based on these studies, we define human attention as the attentional objects that coincide with the task a human is doing. With a task in the mind, a human finishes the task by doing several sub-tasks in certain temporal order. For example, when a human is doing the task of “take the water from the drinking fountain”, the human firstly finds the cup, then goes to the drinking fountain, and finally takes the water. To finish each sub-task, a human behaves purposely to operate on or approach to the attentional objects. For example, when the human is doing the sub-task of “finding the cup”, the human uses the hand to catch the attentional object cup. When the human is doing the sub-task of “going to the drinking fountain”, the human walks to approach to the attentional object drinking fountain. For inside-video human attention estimation, we have a basic assumption that attentional objects locate inside videos/images.
The intuitive method to infer human attention is to estimate where a human is gazing at. It is true in some easy situations that human gaze significantly signals the attentional objects. As shown in Fig. 1(a), human gaze conveys sufficient cues to infer the attentional objects. However, human gaze does not absolutely indicate attentional objects, since in many cases a human is not necessarily gazing at attentional objects all the time. For example, when a human is walking to a drinking fountain to take the water, though the human’s attentional object is the drinking fountain, the human could gaze at other objects in this procedure. In addition, human gaze estimation usually heavily depends on human facial information [35], [45], but the facial information is often unavailable when a human moves naturally in uncontrolled scenes. As shown in Fig. 1(b), human faces are not observable, so it is difficult to estimate the human gaze. On the contrary, in these situations, human pose is available and significantly indicates the attentional objects. In most cases, the object, a human’s hand is reaching to or a human’s body is approaching to, is most likely to be the attentional object. However, human pose can not accurately signals attentional objects in all situations. For example, during the transition of two sub-tasks, the attentional objects are hardly signaled by human pose. One main reason is that “what a human thinks” goes ahead of “what a human does”, so at the shifting time point from one sub-task to another sub-task, the attentional object might have changed while human pose still signals the attentional object in the previous sub-task. In some more complex and challenging situations, attentional objects can not be revealed even if both human gaze and pose are available. As shown in Fig. 1(c), even assuming that the human pose and human gaze are known, we still can not correctly infer the attentional objects because the human is facing a large number of objects and every object is possible to be the attentional object. In these cases, to correctly infer human attention, we need to resort to invisible high-level task information. For example, when a human is facing a juicer, a pot, and a stove at the same time, if the task was making juice, the attentional object is most likely to be the juicer, if the task was cooking soup, the attentional object is most likely to be the pot, and if the task was making pizza, the attentional object is most likely to be the stove.Download : Download high-res image (650KB)Download : Download full-size imageFig. 1. The attentional objects (denoted as red bounding boxes) in three typical situations. (a) Easy situation where human gaze or human pose significantly indicates the attentional objects. (b) Moderate situation where human gaze is not available but human pose conveys the sufficient information for inferring the attentional objects. (c) Hard situation where the attentional objects can not be estimated only depending on human pose and human gaze because both cues indicate multiple possible attentional objects (denoted as green bounding boxes). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
Based on these observations, we propose a deep neural network model that fuses both visible low-level human pose cue and invisible high-level task encoding cue. The low-level human pose conveys the rich information of human body key joints, and the high-level task encoding cue is organized as a graph which encodes a task as several sub-tasks. By integrating the low-level human pose cue with the high-level task encoding cue, our model exhibits impressive robustness and effectiveness.
To validate our model, we conduct the intensive experiments for the comparison with other methods and for the ablation study of our method. We collect a new VR dataset and re-annotate a public real dataset. The experiments on both datasets validate the effectiveness of our method.
Our contributions are three-fold: (1) We propose and define a problem of inferring inside-video human attention that is different from the traditional human (outside images) attention. (2) We propose a model that integrates the low-level visible human pose cue with the high-level invisible task encoding information. (3) We collect and annotate a large-scale dataset in Virtual-Reality scenes and re-annotate a public real dataset. To our best knowledge, our newly collected dataset is the first VR dataset for inferring the task-driven inside-video human attention, and the dataset will be publicly released.
