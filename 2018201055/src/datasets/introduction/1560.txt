Machine learning has great potential for the evaluation of constructed responses, which is supported by findings that the inter-rater reliability between human ratings and machine predicted ratings is comparable to that shown between two trained human coders (Zhai, Yin, Pellegrino, Haudek, & Shi, 2020; Liu et al., 2014a; Nehm, Ha, & Mayfield, 2012; Shermis, 2015; Zehner, Saelzer, & Goldhammer, 2016). The outcomes of machine learning-based assessments provide immediate feedback to teachers and students, and thus machine learning is increasingly used in web-based inquiry, game-based assessment, simulation assessment, and adaptive learning (Zhai, 2019; Zhai, Haudek, Shi, Nehm, & Urban-Lurain, 2020). More importantly, machine scoring shows little repetitive scoring bias (Clauser, Kane, & Swanson, 2002). That is, compared to human scorers, the machine can precisely assign the same score to the same response on multiple occasions, while human scorers might not always be able to accomplish this. Given these advantages of machine scoring, however, we noticed that prior studiesprimarily focused on examining how precisely the machine scores reflect the construct of interest, the latent trait of examinees that a test is intended to assess (e.g., domain knowledge and cognitive ability), which accounts for the variance of the examinees’ performance in tests (Cronbach & Meehl, 1955). No data to date show evidence of how machine scoring reflects the construct-irrelevant variance (CIV, also called error variance), which may lead to score misinterpretation (Gallagher, Bennett, Cahalan, & Rock, 2002; Messick, 1984). CIV can arise due to psychological or situational factors such as rater severity or contextualized features of the assessment. Without examining the CIV in automatically scored assessments, we cannot understand whether using the results from machine scoring will increase or decrease the test validity, as compared to the results of human scoring.
This study fills the gap by comparing machine and human scoring of constructed responses with rich contextualized information in a teacher pedagogical content knowledge (PCK) assessment. We focus on two sub-constructs of PCK: the teacher's ability to a) analyze student thinking, and b) respond to student thinking with appropriate pedagogical moves. Both are regarded as fundamental in teachers’ competence to transform content knowledge into a form of knowledge that is pedagogically useful for teachers (Magnusson, Krajcik, & Borko, 1999; Shulman, 1986). Effective science teaching should be based on a thorough understanding of student understanding, which requires teachers to responsively adapt or adjust teaching strategies in order to improve student thinking. The study focuses on contextualized constructed-response assessment because this type of assessment is deemed more likely to yield CIV (Haladyna & Downing, 2004; Zaichkowsky, 1985). Contextualized open-ended items are popularly used in science assessments as they are more authentic and, consequently, examinees might be more engaged in the test and perform in a way that reflects their competency (Zhai et al., 2019). However, those rich contexts or scenarios and the approach of rating the responses might result in scores that do not entirely reflect the construct of interest, thus compromising the interpretation and use of the test scores. A prior study suggests three sources of CIV might be involved in the contextualized constructed-response assessment (Zhai, Haudek, Stuhlsatz, & Wilson, 2020): variability of the scenario, rater severity, and rater sensitivity of scenarios. In this study, we investigate whether those three sources of CIV are also present when response scores are assigned by a machine learning algorithm. We employ both human scorers and an automated scoring system to score teachers’ responses to a set of video-based assessments targeting two sub-constructs of PCK: Analyzing Student Thinking and Analyzing Responsive Teaching. We asked the following research questions:
(a)How much CIV is drawn by the variability of scenarios, the rater severity, as well as the judging sensitivity of the scenarios in the PCK assessment? What is the difference of the variance between the two sub-constructs of PCK?(b)To what degree does the machine scoring approach differ from that of the human scorers with regards to the extent of CIV?
In the following sections, we first review the assessment toward science teachers’ PCK. Then, we introduce the machine scoring and human scoring of constructed responses. Finally, we present the empirical research and findings from the measure of a video-based PCK assessment.
