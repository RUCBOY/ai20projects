Failure to grasp what others know and believe about objects and events in the environment can give rise to a host of communicative and interactive issues, including difficulties in predicting behavior (Dennett, 1989; Kovács et al., 2010; Wimmer and Perner, 1983), communicating verbally (Clark and Marshall, 1981; Krauss and Fussell, 1991; Nickerson, 1999), and deciding on one's own future states and actions (FeldmanHall and Shenhav, 2019). We have suggested elsewhere that perceptual belief tracking might be particularly problematic in the context of human-robot interactions, where the perceptual capabilities of robots are diverse and often difficult to infer (Thellman and Ziemke, n.d., submitted), which limits understandability (Hellström and Bensch, 2018; Ziemke, 2020) or explainability (de Graaf et al., 2018). This problem, which we call the perceptual belief problem in human-robot interaction (HRI), has so far not been studied empirically to any significant extent. In the reported study we therefore investigated people's ability to track the perceptual beliefs of robots in a series of four experiments. In the first pair of experiments, we focused on advancing our understanding of the problem by asking whether it is in fact more difficult to infer the perceptual beliefs of a robot with non-human vision as compared with human vision (Experiment 1: Visual Belief Tracking) and hearing (Experiment 2: Auditory Belief Tracking). In the second pair of experiments we explored different types of solutions to the problem by investigating if people are able to learn or “tune in” to the non-human perception of a robot by observing it interact with the environment (Experiment 3: Endogenous Adaptation) and whether the process of inferring a robot's perceptual beliefs can be facilitated by providing people with verbal information about its perceptual capability (Experiment 4: Exogenous Facilitation).
The presented work is part of our more general, long-term endeavor to explore the characteristics and limitations of people's ability to predict robot behavior based on intentional folk-psychological constructs (Thellman et al., 2017; Thellman and Ziemke, 2019; Ziemke, 2020). People rely heavily on ascribing intentional states (e.g., beliefs and desires) to predict and explain the behavior of themselves and others in social interactions (Dennett, 1989; Fodor, 1987; Heider, 1958; Malle, 2006). Dennett (1989) referred to this as “taking the intentional stance.” The intentional stance may play an important role also in interactions with artificial systems (Dennett, 1989; McCarthy, 1979), including robots (Marchesi et al., 2019; Thellman et al., 2017; Thill and Ziemke, 2017; Wiese et al., 2017). Recent efforts have been made toward studying mental state ascription to robots empirically. However, most research in this area has focused on understanding what kinds of mental states people ascribe to robots (e.g., de Graaf and Malle, 2019; Fiala et al., 2014; Gray et al., 2007; Malle, 2019; Petrovych et al., 2018; Thellman et al., 2017; Waytz et al., 2014; Zhao et al., 2016). That means, there is so far hardly any research addressing the call for research made by Lee et al. (2005), to “study how people's mental models affect how they actually interact with robots.” This is problematic because efforts to design and improve human-robot interactions need to be informed by knowledge about the effects that people's (mis-) understandings of robots have on human-robot interactions. The present study addresses this by studying assumptions about robots that have direct consequences for interactions with robots. Some beliefs about robots—particularly beliefs about their beliefs (Dennett, 1989)—give rise to predictions about how they are going to behave in the future. For example, the belief that a robot knows (or does not know) about objects, such as oneself, moving in its environment, may yield the prediction that the robot will (or will not) act to avoid those objects in a situation where there is risk for collision. Despite a growing interest in the role of mental state attribution in people's mental models of robots, and the importance of perceptual belief tracking in the context of social interaction, no research has so far targeted people's ability to predict the behavior of robots based on assumptions about how they perceive the environment. The aim of the study presented here is to contribute toward filling this gap of research.
