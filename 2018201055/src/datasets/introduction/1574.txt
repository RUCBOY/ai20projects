We first glimpsed the potential of computers to model the world eight decades ago, in 1936, when Alan Turing devised a hypothetical computing machine while studying the foundations of mathematics [1]. Today, we use computers to describe the turbulence [2] and dynamics [3,4] of air, water and more complex fluids [5], to understand the electronic states of molecules and the kinetics of chemical reactions, for the discovery and understanding of advanced materials, to predict weather and future climate change [6], refugee migration [7], drug design and personalised medical treatments, the creation of virtual organs [8] and, we anticipate, virtual humans too.
In many of these applications, the computer is programmed to solve partial differential equations that are bereft of analytical solutions; and, of course, they can also be used to describe discrete systems, such as lattice gases and other models of fluids, gene regulatory networks with small numbers of molecules [9], agent-based simulations in ecology [10], economics [11] and epidemiology, population dynamics [12], and so on.
Extraordinary progress has been made in recent decades towards the next performance barrier which should be transcended in 2021 with the first exascale computers capable of at least one exaflop [13]. Digital computation is also poised to escape the confines of Moore’s law [14] with the advent of quantum computing [15], leading to feats of modelling and simulation far beyond those achievable by classical computing, or so we are led to believe. A 1000-qubit device would theoretically handle more simultaneous calculations than there are particles in the known universe. It is important not to get carried away, however; today, the biggest quantum computers boast only a few tens of qubits.
Recently, new approaches predicated on machine learning (ML) and artificial intelligence (AI)—terms which are often used interchangeably and synonymously in association with “big data”— have become prominent in tackling a range of complex problems and are sometimes regarded as unbounded in terms of the scope of their domains of application. All this has created the widespread expectation among the general public that we can effortlessly use computers to create virtual worlds across a range of domains, from cosmic associations of galaxies stretching over one hundred million light-years to the mesoscale that is most directly accessible to our senses, and from the molecular machines in our cells down to structures within the heart of an atom and inside the particles that comprise its nucleus.
To simulate different levels of reality, oceans of electrons within myriad microchips, organised within vast numbers of cores that reside inside thousands of nodes in supercomputers are manipulated by billions of tiny switches that flicker on and off billions of times every second. Will this burgeoning virtual environment ever become rich and textured enough to create a faithful replica of our own universe? Some go much further, and even speculate that the cosmos itself arises from interactions between energy and information [16]. Could the universe itself be a quantum computer [17]?
From a consideration of the limits of what is computable, which we discuss further below, we conclude that, although we have come a long way, digital computers – whether classical or quantum – are more restricted in their potential than many realise. Their power resides in their ability to produce vast quantities of numerical data, which lends them an aura of invincibility, yet our article will draw attention to cases where such numerical output can be wrong. Indeed, as we move into the exascale and quantum eras and start to discern what lies beyond them for modelling, there is considerable potential for surpassing the limitations of digital descriptions by falling back on a form of computation which dates back millennia: analogue computing.
