Conventional supervised learning-based image classification systems have achieved promising results due to the rapid development of deep learning technologies [1] and large-scale datasets of common categories. With the growth of digital technologies and daily increasing new items, the challenge now is to make pre-trained models can generalize to new categories without collecting new training examples with structured annotations. As a promising solution, Zero-Shot Learning (ZSL) [2], [3] can recognize unseen objects by transferring learned knowledge model from seen classes. Previous ZSL research has achieved promising results on the old setting that assume test images are from unseen classes only. A new challenging Generalized ZSL (GZSL) [4] has become the emerging problem in this research field. In particular, GZSL considers that the test image can come from both seen and unseen categories. Conventional ZSL approaches are proved to suffer from the prediction bias towards seen categories [5]. For example, a zebra can be correctly predicted by comparing its similarity to other unseen categories of dogs and cats. However, when considering training categories of horses, and cows together with unseen classes, most of zebra images will be misclassified as horses.
Most of existing GZSL work ascribe such a bias to the overlap between learned unseen distribution and that of seen classes [6]. In this paper, we investigate another important issue that has become the bottleneck issue to improve the performance of GZSL models. Since no training examples are available in ZSL, explicit prior knowledge is necessary to estimate the distribution of unseen images. There are mainly four popular knowledge representations in current work. The first is based on textual embeddings, e.g. Word2Vec [7], that are learned from large scale text dataset in unsupervised learning frameworks; the second is exploring the class relationship with ontology [8]; the third is to associate unseen categories by similes in seen classes [9]; and the last is semantic attributes, which are manually defined and annotated by domain experts. Most of theoretical studies [10] adopt semantic attributes because of that each dimension of the attribute embedding has an explicit meaning. Using attributes can help qualitatively analysis of the ZSL model. More generally, such attribute-based predictions can benefit the model interpretation in deep learning research by large audience.
Currently, latent embedding has become one of the dominant frameworks for conventional ZSL problems. The learned latent space aims to mitigate the visual-semantic gap and make the representation more discriminative [5]. Such approaches have achieved state-of-the-art performance since the latent space can effectively preserve correlated visual-semantic information and remove the redundancy. However, the latent space fails to preserve the original meaning of each dimension in the attribute space, which makes the model difficult to interpret and understand. Guo et al. [11] select a subset of the attributes for learning different class distribution, variance, and entropy. Some recent work also attempts to synthesize samples of unseen classes from the attributes using Generative Adversarial Networks (GANs) [12], and then train a supervised model for all classes. However, these methods suffer from the same problem as the traditional supervised models, i.e. when a new unseen category is added, the model needs to be trained from scratch.
This paper proposes a new idea by correcting the attributes according to the visual contexts. Our key challenge and unique contribution is to preserve the original meanings of attributes. As the problems shown in Fig. 1, we propose a General Plug-in Attribute Correction (GPAC) algorithm to make the attributes more discriminative by two constraints: (1) different class attributes should be maximumly distinguishable, especially similar ones between seen and unseen classes in GZSL problems; (2) do not change the meaning of attributes and know how much and when not to make the correction. It is worth noting that GPAC is a plug-in module rather than a new ZSL framework. The corrected attributes can be well complementary to existing ZSL approaches. This paper adopts autoencoder-based [13] and label embedding-based [14] frameworks as examples. Furthermore, we provide an iterative optimization toolbox to effectively fit the GPAC module into ZSL models. Extensive evaluations are carried on five popular benchmarks, and the results show that GPAC can not only preserve the realistic meanings of the original attributes, but also significantly improve conventional ZSL models to state-of-the-art level in GZSL tasks. The contributions of our method are summarized as follows:1)To our best knowledge, this is the first work that can explicitly correct attributes according to the visual contexts while preserving the original meaning of each attribute dimension;2)To our best knowledge, GPAC is also the first plug-in module that aims to facilitate existing approaches and makes conventional ZSL models eligible or even state-of-the-art in GZSL tasks;3)On five popular benchmarks, extensive quantitative and qualitative results manifest that the corrected attributes can better reflect the visual contexts without losing the integrability in attributes, which provides a good practice for future models when incorporating prior human knowledge.Download : Download high-res image (202KB)Download : Download full-size imageFig. 1. Designing and annotating attributes require appropriate guidelines and may not be accurate and discriminative.
The remaining part of this paper is organized as follows, Sec. 2 introduces the related works about ZSL and GZSL. Sec. 3 shows the detailed description of our method, which is followed by our experimental results and their analysis in Sec. 4. Sec. 5 makes a conclusion on this method.
