Differential privacy (DP) is recognized as the state-of-the-art approach to formally capture privacy concerns (see for examples Blum et al. [1] and Wang et al. [2]). Introduced by Dwork et al. [3], DP is used to quantify and bound privacy loss in the context of statistical disclosure control (see Elliot [4]). It is based on the elegant idea that it is possible to compare the potential output of a mechanism in the presence and absence of any given individual. Informally, differential privacy exists if the probability distribution on the published results of an analysis is essentially the same independent of whether or not a given individual is included in the data set. Although individuals may incur harm if the results of a differentially private mechanism are released, differential privacy guarantees that the probability of harm is not significantly increased by a choice to participate in the data set. Many variants of DP have been studied in the literature over the years since its introduction. Thorough surveys of differential privacy and its applications can be found in Dwork [5], Dwork and Smith [6], and Dwork [7].
The premise of DP is to bound the log-likelihood ratio of events according to two distributions: with and without some arbitrary player. The original notion of ϵ-DP bounds by ϵ this ratio across all events and all players. Thus, it considers all possible events, including small-probability ones. It also assumes symmetry between players and no difference across player types. The extended (ϵ,δ)-DP mechanism discounts small-probability events by including only events with probability of at least δ. The notion of privacy-aware choice accounts for the second caveat (the assumption of symmetry and equivalence across player types) by modeling preferences (see Cummings [8] for an overview and Gradwohl [9] for an example of such a model) or utilities (see Ghosh and Roth [10], Nissim et al. [11], and Xiao [12]) in a way that describes their behavior in strategic settings. There are also studies on privacy-aware players in a signaling model (see Daughety and Reinganum [13]), and studies that take an empirical approach in order to examine how people make privacy-related choices in practice (e.g., Goldfarb and Tucker [14]). In this paper we limit the discussion to the aforementioned original notion of DP. We hope to account for the more advanced variants in future work.
In order to study the behavioral implications of DP, we view the definition of differential privacy as follows: suppose a regulator imposes a mechanism to be chosen from some set of potential mechanisms, which are similarly desirable in terms of their original purpose (“material benefit”). The regulator may be interested in how the mechanisms rank in terms of their DP guarantees. The guarantee is meant to ensure that should a participant remove her data from the data set, the likelihood of a given output does not become significantly higher or lower. The mechanism chosen will be that which provides a stronger DP guarantee – i.e., a lower probability that removing data from the data set will change the output. In other words, employing DP as a measure implicitly provides a model of how to order mechanisms in terms of privacy loss. A mechanism with a stronger DP guarantee (i.e., a lower likelihood of privacy loss) is preferred over one with a weaker guarantee.
The goal of this paper is to identify properties that characterize the behavioral implications of differential privacy. Specifically, we study the properties of the order induced by differential privacy guarantees and identify a set of five axioms that characterizes this order as the unique possible one. The approach we take is known as the axiomatic approach, and it is prevalent in information theory, decision theory, and game theory.2 To our mind, the construction of our differential privacy preference model is important as a basis for rigorous discussion about the appropriateness of differential privacy in different settings. Instead of evaluating the general compatibility of DP to a setting, one may evaluate whether each property that characterizes it is compatible or not. As such, this application should only be limited to the original notion of DP and not to its later variants. In a companion paper ([19]), we take the opposite approach and aim to identify a preference order characterized by “natural” properties that any preference order which captures privacy concerns must comply with.
The paper is organized as follows: in Section 2, we formalize two key concepts in our model – the set of privacy-jeopardizing mechanisms and the differential privacy order. In Section 3 we gather useful definitions and notations, and in Section 4 we list five ordinal axioms that hold for the differential privacy order. In Section 5 we provide the characterization theorem showing that these axioms uniquely characterize the differential privacy order, and prove it as our main result. In Section 6 we prove independence of the axioms. In Section 7 we summarize the findings and conclude.
