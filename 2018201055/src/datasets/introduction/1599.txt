Artificial intelligence and machine learning is now considered as one of the biggest computer science revolution in modern technology impacting our lives. Hence, how to improve performance in machine learning become an important topic in current technology.
In this paper, we aim to develop a low-power numerical execution environment for machine learning. Therefore, we establish the fixed-point instruction architecture, which contains fixed-point conversion and fixed-point binary operations. With this design of instructions, we can run fixed-point instructions within CUDA [1] or OpenCL [2], [3], [4], [5], [6] kernels. In our experiment, we focus on the NNEF DNN inference model running on OpenCL framework. OpenCL kernels are compiled with SPIR-V compiler to enhance portability and then, compile to cuda backend.
NNEF(Neural network exchange format) is proposed by Khronos Group. The goal of NNEF is to create a unified network description format to transfer trained networks and facilitate deployment of networks from frameworks to inference engines. SPIR-V(Standard Portable Intermediate Representation - V) is the open standard intermediate representation which supports OpenCL and Vulkan. SPIR-V can be an intermediate representation in compiler flow across multiple backends from vendors. With SPIR-V, developers can use common frontend compiler, and therefore, improving kernel portability and reliability in heterogenous computing.
Our design of fixed-point instructions is based on integer instructions, which can be considered as the fixed-point data type with the binary point position at zero. We also designed the fixed-point operation with two factors, bit width and binary point positions. There are 8/16/32 bit widths in our design. The programmers or vendors may have concerns with the bit width, which will affect the data precision and the power consumption. The binary point position will mainly affect the data precision. Programmers may need more bits in the fractional bit in regard to certain triangular math functions.
In our proposed flow, we first compile the OpenCL kernel by clang, which is the front-end compiler of LLVM. Then, the bit code will be compiled to SPIR-V intermediate representation by LLVM-SPIRV. Next, we compile from bit code to the PTX assembly with the llc compiler. Finally, we can run the OpenCL kernel code with fixed-point instructions on GPGPU-Sim.
The experiment results demonstrate that our design of fixed-point instructions is capable of saving energy and improving performance with limited precision loss. By using our fixed-point instructions, a kernel may achieve 14% energy consumption reduction.
This paper makes the following contributions:
•We provide possible OpenCL extension with fixed-point feature and linguistics.•This work presents the design of fixed-point instructions in GPGPU-Sim.•This work proposes the enabled flow for fixed-point simulations.•This work demonstrates the use case of fixed-point instructions in a DNN with NNEF, which has data precision loss tolerance.•This work demonstrates that with the proposed design of fixed-point instructions, the total energy consumption can be reduced by approximately 14%.
The remainder of this article is organized as follows. Section 2 introduces the motivation and background of the fixed-point data type and GPGPU-Sim simulator. Section 3 describes our design of fixed-point representation and the support of the fixed-point instruction set on the GPGPU-Sim simulator. In Section 3.4, we provide our power evaluation methodology. Section 4 shows the experimental result of the DNN framework with an OpenCL fixed-point type comparing to a floating-point type. Finally, Section 6 concludes our results.
