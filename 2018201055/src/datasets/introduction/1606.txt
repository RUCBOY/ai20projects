When hiring, the manager of a firm may consider hiring multiple candidates at once, at least probationally. If she is uncertain about the candidates’ productivity and she can only observe their performance on the job, if hired, then the hiring problem becomes a multi-armed bandit problem where the arms are the different teams of potential employees. These arms are not independent on two grounds: (a) Learning about the productivity of a candidate might provide valuable information about others with similar qualifications and experience; and (b) even if productivity is independent across candidates, the productivity of two different teams with common members will not be. Thus, the well-known Gittins-index solution for bandits with independent arms (Gittins and Jones, 1974) does not apply.
The general problem of sequentially choosing subsets of some set when their distribution of value is unknown can be formulated as a dynamic-programming problem. However, except in very special cases, such problem is unsolvable—either analytically or numerically—when the set from which we choose is “large” (say, has four or more elements).1 When real economic agents face problems with this structure, we posit that they employ heuristics or rules of thumb. Francetich and Kreps (2019) analyzes simple heuristics that only employ accumulated data. The present paper examines more-sophisticated heuristics that employ the decision maker’s prior assessment of the problem and incorporate Bayesian updating.
Heuristics for multi-armed bandit problems have been extensively analyzed in a literature that spans computer science and operations research (CS-OR) under the rubric of bandit learning. Various categories of bandit problems with non-independent arms are investigated, including linear bandits, Gaussian bandits, and smooth bandits. (For an introduction to this literature focused on Thompson Sampling, see Russo et al., 2018)) The heuristics range from relatively simple to sophisticated and employ both Bayesian and classical inference methods. The paper closest to ours is Sauré and Zeevi (2013), in which the arms of the bandit are subsets of a set of products that a retailer can display to each of a finite set of sequentially-arriving customers.
Our paper departs from the bandit-learning literature in the criterion employed to assess the heuristics. The typical criterion in CS-OR is the minimization of expected undiscounted asymptotic regret. Roughly speaking, the decision-maker’s regret in any period is defined as the difference between what she would receive were she clairvoyant—namely, if she knew the distribution—and what she actually receives by employing a specific heuristic. She seeks to minimize the expected value of the undiscounted sum of her period-by-period regret. This criterion biases her search among heuristics towards those that will learn the truth (or, at least, enough of the truth so that her within-period regret is eventually zero), and only if that is assured does she consider the speed and cost of the learning process.2 Instead, we employ the criterion of maximizing the expected sum of discounted rewards, which is more standard in the economics literature. As is well known for independent-arm bandit problems, as long as future rewards are discounted, there is always positive probability that the decision maker optimally settles for an objectively suboptimal arm. This means that the optimal strategy under discounted rewards gives infinite asymptotic expected regret.
The optimal strategy is practically unobtainable in our problem, but the fact that it fares poorly (relative to alternatives) under the expected asymptotic regret criterion suggests that, for a decision maker who discounts rewards, prescriptions of the bandit-learning literature must be carefully considered. The direct message of this paper is that this is so: Under discounted rewards, bandit-learning heuristics can perform badly compared to heuristics that are based on the considerations of the classic exploitation–exploration tradeoff.
Specifically, we formulate a stylized model of this type of problem and examine six Bayes-rule based heuristics, two of which—Thompson Sampling and Upper Confidence Bounds—are generally “winners” in the bandit-learning literature. A few theoretical results about the long-run behavior of these heuristics are given, which explain why Thompson Sampling and Upper Confidence Bounds do well for infinitely-patient decision makers. However, when shorter-run costs are taken into account due to discounting, we see in simulations that these long-run-excellent heuristics can fall short in comparison to heuristics that take more seriously the exploitation–exploration trade-off.
As in the companion paper, we do not claim that we have identified the ultimate list of heuristics. We motivate our heuristics either by their simplicity, desirable asymptotic properties, or performance in simulations. This leads us to the broader, less-direct message of this research: While simulations are more problem-specific than theorems, we propose that carefully examined simulation results can provide valuable insights in settings where more-formal analysis—including the identification of payoff bounds, the standard approach in CS-OR—is precluded by complexity.3
The rest of the paper is organized as follows. Section 2 presents the formulation of the problem. Section 3 recounts standard results on the optimal solution to the decision-maker’s problem. In Section 4, we describe the six Bayes-rule based heuristics. Section 5 presents asymptotic results, while Section 6 turns to simulations. Section 7 concludes. Proofs are relegated to the appendix. A second, online appendix with supplementary material including R-language and additional data from simulations is available on the website of the first author.4
