Pattern recognition methods are a collection of fundamental tools for solving problems faced by artificial intelligence, employing the popular interest that has been growing in recent times. As a rule, we divide them into two groups [1]. The first of them – supervised learning – is responsible for the description of new, yet unknown cases, on the basis of a set of patterns already known by labels [2]. In the case of the second group – unsupervised learning – the whole analysis takes place on a set of objects without any prior description.
The most popular problem of supervised learning is the classification, which, unlike the regression estimating the continuous value, assigns new objects to the set of discrete classes. The algorithms for solving this task have been developing intensively since the beginning of this field and have already formed a large group of methods, ranging from simple solutions such as the Naive Bayes Classifier [3] or k-Nearest Neighbors [4], through decision trees [5] and forests [6], Support Vector Machines, up to neural networks, with particular emphasis on the most-recently-fashionable deep convolutional networks [7].
1.1. Imbalanced dataThe assumption of the most of classification algorithms is the equal occurrence of each of the considered classes [8]. This becomes problematic in the case when imbalance ratio, called also prior probability, is disturbed and one of the problem classes occurs much frequently than the others [9]. Data of this type are called imbalanced data. Due to the fact that the dominant majority of real decision problems, i.e. medical diagnostics, spam or fraud detection, presents imbalanced data, where, what should be emphasized, a less numerous class is the key from the perspective of the problem. Therefore, it became necessary to develop appropriate methods for counteracting the tendency of classifiers to favor the majority class [10].Most of the proposed solutions to the problem of imbalanced data may be assigned to one of three groups. The first, theoretically the simplest, are the mechanisms built directly in the process of classifier training, modifying their model to align the impact of all classes of the problem, for example by the use of appropriate loss function. The second and the most common approach is the appropriate preprocessing of training data to align the presence of problem class patterns in it. Above simple random oversampling and – undersamplig one should distinguish here the smote [11] – algorithm generating synthetic samples, along with its numerous variants, and adasyn [12], extending it to include the distribution of the majority class in the synthesis. The last, but not least, approach are hybrid methods [13], using group of diversified classifiers in the construction of the decision system [14] connected by the prior-sensitive decision rule [15].
1.2. Data streams and concept driftImportant in the context of real problems of classification is also the aspect of knowledge historicity. A classification that is completely correct at a given point in time may lose its validity in the future and eventually turn out to be wrong. Therefore, it is naïve to assume that once-trained model, used for a long time will induct an error – which once estimated – will not increase over time, and the classifier itself, will not be outdated.In many problems, we do not deal with static data set, and in addition to attributes, objects are characterized by their location in time. Such cases are called data streams and we process them, in principle, in one of two ways. In the first of them – online processing – each incoming object is analyzed separately, one by one and in this mode it drives the updating of the classification model. However, it is a very computationally intensive approach, and so-called batch processing is used much more frequently. The principle of batch processing is that incoming patterns are accumulated in so-called chunks and processed not pattern by pattern, but group by group.Among the solutions to the stream classification problem, the most popular are approaches that allow for partial model fitting, i.e. modifying the existing model with information extracted from upcoming data, like winnow [16] or vfdt [17], and the ensemble approach, especially popular in batch processing [18]. Employing the incremental learning methods requires implementation of forgetting mechanisms, either as built-in capabilities of algorithm [19] or as dataset weighting or windowing [20].In such ensembles, successive members of the committee are built on the basis of subsequent chunks, making it possible to weigh the influence of the member decision on the final prediction according to their quality determined on the latest data, and to trim the committee in order to eliminate obsolete models [21].The already mentioned aspect of knowledge historicity introduces an additional complication in the problem of data stream classification. Outdating of models with passing time is the result of the phenomenon called concept drift [22]. Among the concept drifts you may distinguish between sudden drift, where the change between class distributions occurs rapidly at precise point, as well as incremental or gradual drift, where the concepts of classes are changing smoothly [22], [18]. Solutions to this problem are focused either on the drift detection, signaling the need to rebuild the model, or on the classifier ensemble. It is also important to mention the propositions how to react to detected drifts, like dwm [23], stagger [24], or gt2fc [25]. Appearances of concept drift in data streams have become a challenge for plethora of practical solutions, such as computer systems security [26], [27], medical diagnosis [28] or fraud detection [29].
1.3. ContributionsThe following work is intended to achieve the following goals:•Proposal of a strategy for interpreting the support obtained on a batch of data by the probabilistic base classifier in a manner that takes into account the prior probability.•Experimental evaluation of the proposed approach on the example of a collection of benchmark data streams with various level of class imbalance, compared to the method without modification.
