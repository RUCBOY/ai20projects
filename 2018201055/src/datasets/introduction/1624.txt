Modern computer systems require a flexible architecture to handle sudden and unpredictable variations in application workloads caused by an increasing number of connected devices. This flexibility is generally achieved by breaking down the system components into smaller, loosely coupled, and separably scalable units. Consequently, the increase in size and complexity of the aforementioned systems makes it difficult for system administrators to keep track of the large number of alerts generated by Intrusion Detection Systems (IDSs) and take proper countermeasures to prevent the attacker from inflicting further damage to the system.
To overcome the need for human intervention, various Intrusion Response Systems (IRSs) have been developed (e.g., [1], [2], [3], [4], [5]). They are designed to continuously monitor IDS alerts and generate appropriate actions to defend the system from a potential attack. They are classified according to their level of automation, ranging from simple static attack–response mappings (e.g., [6], [7]), to more sophisticated, automated, and stateful IRSs. The latter require an accurate model of the system, and are based on a partial representation of its state, which is leveraged to produce a possibly optimal defense policy (e.g., [8], [9], [10]).
However, using a stateful model-based planning technique to generate a defense policy could be time prohibitive for large-scale systems, because the size of the state space grows exponentially with the size of the defended system [11]. In order to deal with this issue, several optimal [9], [12], [13] and sub-optimal [14], [15] approaches have been proposed, based either on model-based planning or on model-free learning algorithms. The former is a class of algorithms that is based on a model that fully describes the system dynamics. Algorithms of the latter class, instead, aim at automatically building a model of the system through the formulation of a Multi-Armed Bandit problem [16], where an agent has to maximize its revenue in presence of alternative and possibly unknown choices, while trying to acquire additional experience that it can leverage for future decisions. Reinforcement Learning (RL) problems [16] are typical instances of the Multi-Armed Bandit problem.
An additional complexity is introduced by the fact that the behavior of distributed systems usually evolves over time. This is due, for instance, to changes in their configuration, to software updates, and to the addition or removal of users. This evolution introduces gaps with respect to the initial model (when a model-based approach is used), or with respect to the model that has been learned (when a model-free approach is used), which must be filled by continuously updating its parameters. Unfortunately, when using planning algorithms, every change to the model requires the re-execution of the entire planning process, which could be unfeasible for large-scale systems.
To deal with this problem, a model-free technique for self-defense and self-adaptation has been recently developed [17]. It leverages a RL tabular method, named Q-Learning [16], to solve the decision-making problem by building an approximation of the optimal action–value function Q∗, i.e., the function that helps in choosing the best action in a given state to ensure that the system reaches a safe state. Q-Learning has been shown to be effective in reacting to abrupt system changes, thus allowing the IRS to evolve with the protected system. However, for large-scale systems, the major drawbacks of tabular methods are: (i) the high demand of computer memory to store the state–action values for the huge state space, and (ii) the lack of a capability to make decisions based on generalizations of past experiences, thus making it impossible for the IRS to know how to behave in new unseen states. These two limitations are intertwined, because the only way to use tabular methods on large-scale systems is to use some heuristic to reduce the exploration of the state space, which in turn reduces the effectiveness of the agent when an unseen state is visited.
Deep Reinforcement Learning (DRL) [18] can be used to mitigate the aforementioned limitations because, instead of storing the full action–value table, it only needs to store the parameters of the underlying neural networks, and a single forward pass is sufficient to find the best action to take in a particular state.
However, model-free algorithms including DRL could require a long training to converge to the optimal action–value function. This could hinder their ability to directly work on production systems, because of the sub-optimality of the policies generated by agents that have not yet acquired enough knowledge of the system. Indeed, an agent with near-zero knowledge could possibly run wrong or dangerous actions, simply to learn what is their outcome. Furthermore, the actual length of every learning episode depends on the execution time of the component actions. This further exacerbates the learning problem, since every episode could require minutes, or even hours to complete.
In our previous work [19] we studied the applicability of DRL as an alternative to traditional planning and learning algorithms for intrusion response control on stationary systems. The work described herein is an extension of the previous one and presents a hybrid model-free approach for the near-optimal control of the intrusion response process on non-stationary systems. The proposed approach is composed of four phases. In the first phase, we design the model of the system that we want to protect, as we would do if we intended to use an algorithm based on planning. In the second phase, we build a software simulator of the system based on the system model. The simulator is then leveraged in the third phase, where a RL agent is attached to it, and runs to automatically learn the originally designed system model. Finally, in the fourth phase, the RL agent is detached from the simulator and attached to the real system, where it can learn possible gaps with respect to the model and automatically follow the system evolution. This entire process is based on the concept of transfer learning [20], [21], that is, the ability to transfer the learned model from one domain to a different one, however related. The key advantage of such transfer learning is that it is aimed at decreasing the training time, because the agent begins its operation on a new system with an already solid knowledge of similar systems.
This approach allows us to potentially overcome the two aforementioned issues. Indeed, when the RL agent is attached to the system in the fourth phase, it already has a good knowledge of similar systems. Therefore, the likelihood that it executes wrong or dangerous actions is reduced. Furthermore, the use of a simulator greatly reduces the training time, since the execution time of the actions can be zeroed.
A large microservice-based system is used as our proposed application, where we model and compare the performance of two RL agents, namely Q-Learning and Deep Q-Learning, in terms of steps to convergence, cumulative reward, and execution time, both on stationary and non-stationary systems.
Our simulation results show that Deep Q-Learning is orders of magnitude faster than Q-Learning in terms of training time and number of episodes needed to converge to the optimal cumulative reward on a stationary system. However, in the case of small size systems, Q-Learning outperforms Deep Q-Learning when a structural change of the neural networks is required to address a system change.
The rest of this paper is organized as follows. In Section 2, we provide the background on the underlying mathematical framework and on the system model. We present an analysis of the experimental results in Section 3 and discuss the limitations of the proposed approach in Section 4. We describe the related work in Section 5. Finally, we present conclusions and provide some hints for future work in Section 6.
