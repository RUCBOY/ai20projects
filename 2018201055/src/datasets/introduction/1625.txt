In recent years, convolutional neural networks (CNNs) have produced great changes in the field of computer vision [1]. CNN-based models have been applied in image classification [2], [3], [4], multi-target detection [5], [6], semantic segmentation [7], [8], [9], and behavior recognition tasks [10], [11], achieving the most advanced results. These applications also promote the intellectualization of robot control, allowing robots to understand the scene and automatically complete specific tasks through computer vision algorithms. The success of CNNs is attributed to its own complex structure and millions of parameters which allow the network to model complex high-level patterns, giving the network more discriminatory power.
When mapping and running a network on a computer platform, a large number of parameters will result in high memory requirements. Taking classification networks as an example, the size of the trained AlexNet [2] is approximately 230M, and that of the VGG16 [3] is approximately 527M. Therefore, convolutional neural networks require large amounts of hardware resources, hindering their deployment on embedded computing devices with limited memory. Especially in robots and intelligent image sensors, the computing power and memory of the equipped hardware are small, which leads to the inefficient use of existing models and poor real-time performance.
This dilemma has led to some compression of neural networks. The work of Denil et al. shows that there is much redundancy among the weights of a neural network and proves that a small number of weights are enough to reconstruct a whole network [12]. In this paper, we proposed a new network architecture that aimed to reduce and limit the memory overhead of convolutional neural networks. We focus on convolutional layer compression by extending HashedNet [13] to the convolutional layer and grouping the weighting matrix of the convolutional layer with the Hash function, this makes the weights of the same group share the same value. Furthermore, a Hash index matrix H is proposed to represent the hash function that reflects the shared state of the weight matrix. Then, a relaxation regularization of H is introduced into the fine-tuned loss function. Through alternate optimization, the best sharing weight in the optimal sharing state is obtained as desired.
The proposed method is verified on two real-world deep learning benchmark datasets. The image classification experiments on three network models, including AlexNet, VGG16 and ResNet50 [14], showed that the proposed method could greatly reduce the size of the neural network model, while having little effect on the prediction accuracy. The experiments also showed that the method had a good effect on the acceleration of the convolution network. The compressed networks can be deployed on the embedded systems of the robots, and improve the speed of the robot’s perception for surrounding objects.
