In the past few decades, there has been a rapid growth of interest in automated learning from data across various scientific fields including statistics [52], engineering [30], computer science [23], [39], mathematics, and many more. An overview of machine learning problems in a wide range of contexts (statistical learning theory, pattern recognition, system identification, deep learning, and so on) can be found in [1], [6], [19], [20]. One of the main paradigms is to learn an unknown target function from a given collection of input–output pairs (supervised learning), which can be rephrased as the problem of finding an approximation of a multi-dimensional function. For example, in [34], [35], the authors demonstrated a connection between approximation theory and regularization with feedforward multilayer networks. In general, learning a smooth function from data is ill-posed unless a priori information about either the data structure or the generating function is provided [15], [33], [50].
One of the well-known methods to make the learning problem well-posed is to exploit additional properties of the target function [18]. For example, if the target function depends only on a few active coordinates associated with a suitable random matrix, the function can be recovered from a small number of samples [15]. On the other hand, many well-known learning methods consider the target function in a particular function class (such as radial basis functions, multivariate polynomials, projection pursuit, feed-forward neural networks, and tensor product methods) and add a penalty (such as Tikhonov regularization or sparse constraints) to the associated parameter estimation problem. For example, an adaptive high-dimensional polynomial interpolation technique is presented in [7] and an optimal least square method is proposed in [9].
Recently, sparse models combined with data-driven methods have been investigated intensively for learning nonlinear partial differential equations, nonlinear dynamical systems, and graph-based networks. The model selection problem for dynamical systems from time series dates back to [11] where the authors investigate the concepts from dynamical system theory to recover the underlying structure from data. In [55], the authors construct a sampling matrix from the data matrix and its power to recover the ordinary differential equations and find an optimal Kronecker product representation for the governing equations. Furthermore, based on the observation that many governing equations have a sparse representation with respect to high-dimensional polynomial spaces, the authors in [4] developed the SINDy algorithm which uses that sampling matrix and a sequential least-square thresholding algorithm to recover the governing equations of some unknown dynamical systems. The convergence of the SINDy algorithm is provided in [57]. A group-sparse model was proposed in [44] to learn governing equations from a family of dynamical systems with bifurcation parameters. By exploiting the cyclic structure of many nonlinear differential equations, the authors in [46] proposed an approach to identify the active basis terms using fewer random samples (in some cases on the order of a few snapshots). For the noisy case, in [43] the authors use the integral formulation of the differential equation to reduce the effect of noise and identify the model from a smoother basis set. To learn a nonlinear partial differential equation from spatio-temporal dataset, the authors in [42] proposed a LASSO-based approach using a dictionary of partial derivatives. In [40], the authors developed an adaptive ridge-regression version of [4] for learning nonlinear PDE, while in [36] a hidden physics model based on Gaussian processes was presented. On the other hand, the data are often contaminated by noise, contain outliers, have missing values, or have a limited amount of samples. When the given data are limited, there are several works addressing learning problems ranging from sampling strategies in high-dimensional dynamics using random initial conditions [45], to a weighted ℓ1-minimization on the lower set [8], [37], model predictive control using SINDy [24], and sample complexity reduction to linear time-invariant systems [14]. In [47], the authors proposed a method to approximate an unknown function from noise measurements via sequential approximation. Geometric methods, such as [28], can be used to approximate functions in high-dimensions when the data concentrate on lower-dimensional sets.
Regarding supervised learning analysis, the input data are assumed to be independent and identically distributed (i.i.d.). However, this assumption does not hold in many applications such as speech recognition, medical diagnosis, signal processing, computational biology, and financial prediction. Alternatively, for non-i.i.d. processes satisfying certain mixing conditions, various reconstruction results have been addressed in different contexts. The convergence rates of several machine learning algorithms have been studied for non-i.i.d. data. Examples include weighted average algorithm [12], least squares support vector machines (LS-SVMs) [21], and one-vs-all multiclass plug-in classifiers [13]. In [53], the authors discussed several mixing conditions for weakly dependent observations which guarantee the consistency and asymptotic normality for the nonlinear least squares estimator. Minimum complexity regression estimators for m-dependent observations and strongly mixing observations were proposed in [32] using certain Bernstein-type inequalities for weakly dependent observations. In [41], a conditionally i.i.d. model for pattern recognition was proposed, where the inputs are conditionally independent given the output labels. In [49], the authors proved that if the data-generating process satisfies a certain law of large number, the support vector machines are consistent. In [22], a Bernstein-type inequality for geometrically C-mixing processes is established and applied to deduce an oracle inequality for generic regularized empirical risk minimization algorithms. Using a strong central limit theorem for chaotic data and compressed sensing results, the authors in [51] proved a reconstruction guarantee for sparse reconstruction of governing equations for three-dimensional chaotic systems with outliers. The common technique in the mentioned works is the application of either a central limit theorem or a suitable concentration inequality for the given data.
In this work, we study the problem of learning nonlinear functions from identically distributed (but not necessarily independent) data that are corrupted by outliers and/or contaminated by noise. By expressing the target function in the multivariate polynomial space, the learning problem is recast as a sparse robust linear regression problem where we incorporate both the unknown coefficients and the corruptions in a basis pursuit framework. The main contribution of our paper is to provide a reconstruction guarantee for the associated ℓ1-optimization problem where the (augmented) sampling matrix is formed from the data matrix, its powers, and the identity matrix. Although the data may not be i.i.d., we prove that the sampling matrix satisfies the null space property, provided that the data are compact and satisfies a suitable concentration inequality. Consequently, the basis pursuit problem will be guaranteed to have a unique solution and be stable with respect to noise. Numerically, we use the well-known Douglas–Rachford algorithm to solve the corresponding optimization problem. In general, the algorithm using monomial bases may be numerically unstable when the degree of the polynomial is large. However, our simulations indicate that the proposed method works well in various situations.
The paper is organized as follows: In Section 2, we explain the problem setting. In Section 3, we first recall the theory from compressive sensing, then present the theoretical reconstruction guarantees. In Section 4, we state the recovery results for various types of data including i.i.d. data, exponentially strongly α-mixing data, geometrically C-mixing data, and uniformly ergodic Markov chain. The numerical implementations and results are described in Section 5. We discuss the conclusion and future works in Section 6.
