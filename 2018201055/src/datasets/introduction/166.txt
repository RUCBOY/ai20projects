A complex-valued Hopfield neural network (CHNN) with a multistate activation function is a multistate extension of Hopfield neural network [1], [2]. It has been used as a model of associative memory and applied to storage of multilevel data, such as gray-scale images [3], [4], [5], [6], [7], [8], [9], [10], [11]. The CHNN has been extended using hypercomplex numbers [12], [13], [14]. Hyperbolic numbers form a hypercomplex number system of dimension 2, like complex numbers. A couple of models of hyperbolic-valued Hopfield neural networks (HHNNs) have been proposed [15], [16]. Quaternions have most often been used to extend CHNNs, many models of quaternion-valued Hopfield neural networks (QHNNs) have been proposed as listed below:
1.A QHNN with a split activation function [17], [18], [19].2.A QHNN with a phasor-represented activation function [20], [21], [22].3.A QHNN with a continuous activation function [23], [24], [25], [26].4.A QHNN with a disc activation function [27].
A phasor-represented activation function was also introduced to a bicomplex-valued Hopfield neural network (BHNN) [28]. Moreover, octonion-valued Hopfield neural networks were proposed [29], [30], [31].
Several hypercomplex-valued Hopfield neural networks employ multistate activation functions and have been used as alternatives of CHNN [32], [33]. A directional multistate activation function was introduced to an HHNN [35], [36]. Since the weight parameters need a lot of resources, they should be reduced. For reduction in the weight parameters, twin-multistate activation functions were introduced to a QHNN, a BHNN, and a split quaternion-valued Hopfield neural network (SQHNN) [37], [38], [39]. Table 1 summarizes the numbers of weight parameters, which are represented as relative values to that of a CHNN. The listed models have the storage capacities in proportion to the numbers of weight parameters.Table 1. Numbers of weight parameters.ModelWeight parametersCHNN1HHNN1QHNN0.5BHNN0.5SQHNN0.5
A multistate Hopfield neural network converges to a fixed point in asynchronous mode, whereas it converges to a fixed point or is trapped at a cycle of length two in synchronous mode. In particular, a CHNN with projection rule does not converge in synchronous mode [34]. It is a disadvantage of multistate Hopfield models that parallel processing is not available in recall process. For few models, the stability conditions are known in synchronous mode. A rotor Hopfield neural network (RHNN) with projection rule converges in synchronous mode. An RHNN is an alternative of CHNN using vector-valued neurons and matrix-valued weights [40], [41]. The disadvantage is that double weight parameters of CHNN are required. In this work, we propose a real-weight CHNN (RWCHNN). An RWCHNN restricts the weight parameters of a CHNN to real numbers and requires only half weight parameters of a CHNN. We provide the stability conditions for RWCHNNs in synchronous mode and prove that the projection rule satisfies the stability conditions. Thus, the RWCHNN is the first model that converges in synchronous mode and has half weight parameters of a CHNN. The RWCHNN with projection rule has half storage capacity of CHNN. We conduct computer simulations to evaluate the update counts unless convergence. The computer simulations conclude that the update counts until convergence of RWCHNN in synchronous mode is much small than those of the others in asynchronous mode.
