The climate system is commonly modelled as an object subdivided into components, such as the atmosphere, the oceans, the land surface, the ice sheets, and the biosphere, and submitted to a number of external influences, including fluctuations in incoming solar radiation, volcanic eruptions, and human activities.1 The dynamic range of processes involved in the climate system and the importance of living processes produce a level and a quality of complexity that are challenging to simulate. Even a General Circulation Model (GCM), involving several million lines of computer code, can only approximately represent climate dynamics based on idealisations under forcing scenarios and specific boundary conditions.
Hence, it is generally agreed that projections of future climate change should not rely on a single model. For this purpose, it has become common to rely on multiple simulations generated by Multi-Model Ensembles (MMEs). The status of these MMEs constitutes a subject of increasing philosophical attention, notably in the work of Parker (e.g. Parker, 2006, Parker, 2010a, Parker, 2010b, Parker, 2013, Parker, 2018, Betz, 2009, Frigg et al., 2013, Frigg et al., 2015, Katzav et al., 2012, Lenhard and Winsberg, 2010, Winsberg, 2012, Winsberg, 2018).
Specifically, MMEs provide a means to estimate the uncertainty induced by choices of representation of the specific processes at work in the climate system. This form of uncertainty is termed “structural uncertainty” in the standard scientific literature (Tebaldi and Knutti, 2007, Meinshausen et al., 2009, Knutti et al., 2010, Flato et al., 2013, p. 754–755).2 Parker makes it clear that structural uncertainty is uncertainty about what would constitute an adequate model structure but cannot be uncertainty about what would constitute a perfect model structure, for it is explored in practice via state-of-the-art models that are imperfect. “After all, adequacy (not perfection) is all that is really needed, and it is plausible that the structures of today's models are adequate for some predictive purposes of interest” (Parker 2010b, p. 991).
Created in 1995, the Coupled Model Intercomparison Project (CMIP) is nowadays the reference framework in which GCMs are gathered into MMEs. For example, twenty-three GCMs, developed in Australia, Canada, China, France, Germany, Japan, Korea, Norway, Russia, the United Kingdom and the United States of America, composed the MME of CMIP5 built in 2013. CMIP was originally developed to enable scientists to compare model outputs in a consistent fashion, and thus identifying robust outputs, shared biases, the origins of disagreements, and which specific processes require more understanding in order to improve the models. It was later used for uncertainty quantification.
However, it is worth questioning whether MMEs such as the ones built in CMIP are well adapted to quantify climate uncertainties. According to a common critique, MMEs are “ensembles of opportunity” (Knutti et al., 2010, Meehl et al., 2007, p. 754; Tebaldi and Knutti 2007) in that their members are not designed in the first place to sample the range of uncertainty, but are rather the state-of-the-art models available at the time, provided by the modelling centres willing to participate (Parker, 2010a, Parker, 2010b, Parker, 2011, Parker, 2013, Katzav and Parker, 2015). Indeed, any modelling centre may in principle apply for archiving its own GCM outputs in the CMIP database, as long as the model complies with the imposed standards of CMIP.
What this criticism tells us is that, when scientists historically first saw the opportunity to take advantage of the plurality of GCMs developed all over the world, it was too late to build MMEs with adequate properties for quantifying an uncertainty range. We analyse the situation as an epistemological change: When building GCMs, scientists design and tune them to get the representations of the climate that are expected to provide the most accurate projections. From this perspective, an ensemble is “a collection of best guesses” (Parker 2013), i.e. a set of models that are roughly equally good and equally bad. However, covering the full range of uncertainty requires more than a collection of best guesses. The GCMs must jointly contribute to forming a representative sample of climate possibilities, and this sample must possess adequate properties to this end. Given the arguments that MMEs are ensembles of opportunity, we will first make clear what these adequate properties are commonly supposed to be: they include systematicity, comprehensiveness, and model independence (2).
The “ensemble of opportunity” criticism implicitly contains a positive message: MMEs could, in principle at least, be better designed. In other words, if we could coordinate the worldwide development of GCMs, then we might be able to design the members of MMEs to provide, for the same computing cost, a more reliable quantification of uncertainties about future climate.3 This suggestion allows us to address what Parker (2018) introduces as one of the remaining philosophically interesting questions related to model ensembles: “How can ensemble studies be designed so that they probe uncertainty in desired ways?”
In order to allow for the construction of better ensembles, we first need a characterisation of what GCMs are, and how MMEs should be designed. This is what our contribution aims to offer.
Beforehand, we make explicit why, following Parker (e.g. 2010b, 2013), MME optimisation is particularly hard to conceptualise (3). We then introduce two views on how an MME should be constructed and assessed, depending on the object we assume an ensemble is a sampling of. Hence these views come with different interpretations of GCMs constituting MMEs. These interpretations are not complete or mutually exclusive. They rather highlight different aspects of GCMs.
The first interpretation is suggested by the definition of structural uncertainty: Models are combinations of modules and parameterisations. An ensemble is here obtained by “plugging and playing” with interchangeable modules and parameterisations (4).
The second interpretation is suggested by practices underlying climate modelling as a social and epistemic process: Models are aggregations of expert judgements that result from a history of epistemic decisions made by scientists about the choices of representation. An ensemble is here a sampling of expert judgements from modelling teams (5).
Modules and parameterisations are mathematical structures and are therefore of a different nature than expert judgements. Hence, as we will show, the two interpretations involve distinct domains from philosophy of science and social epistemology. Nevertheless, because they illuminate different aspects of GCMs, both interpretations allow us to highlight distinct problems related to MMEs and therefore are complementary to each other in our exploration of ways to design better MMEs (6).
More precisely, we will argue that the first interpretation helps in properly formalising the adequate properties of MMEs (6.1) while remaining silent on the way we should define the space of model structures (6.2). We will further argue that, unlike the first interpretation (6.3), the second interpretation accounts for the fact that confidence in model projections is generated by the social and historical processes underlying model assessment (6.4) and also for the influence of non-epistemic values in choices of representation (6.5). We will finally suggest some consequences of adopting both interpretations for designing better MMEs (7).
