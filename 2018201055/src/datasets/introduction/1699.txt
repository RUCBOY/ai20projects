Next generation e-science is producing colossal amounts of data, now frequently termed as “big data”, on the order of terabyte at present, and petabyte or even exabyte and zettabyte in the predictable future. These scientific applications typically feature data- and network-intensive workflows comprised of computing tasks with intricate inter-task dependencies. The overall performance of executing such large-scale scientific workflows in network environments depends on how those tasks are assigned and executed on the selected computer nodes or virtual machines (VMs) based on different scheduling schemes.
It is estimated by Gartner that the worldwide public cloud services market will grow 18% in 2017 to $246.8 billion, up from $209.2 billion in 2016. Typically, a cloud environment is a collection of data centers that are installed globally with thousands of computers or servers using VMs and network storage technologies. Each cloud environment requires massive cooling systems to maintain the required operational temperatures. Servers and cooling systems combined use up to 80% of all the electricity within a data center [1]. Thus, an entire cloud with more than one data center consumes a huge amount of energy, typically in the range of 10 to 20 Megawatts during normal operation [2]. More recently, the estimated global data center electricity consumption in 2018 was 198 TWh, or almost 1% of total global electricity consumption [3], [4]. The average cost of operation of servers has increased by a factor of four in just a decade [5] and following the current trend, it is expected to double in the next five years. Apart from the operational cost, the global information and communication technology (ICT) industry accounts for approximately 2% of the global carbon dioxide (CO2) emissions due to PCs, servers, supercomputers, local area networks (LANs), office telecommunications and printers, cooling solutions, fixed and mobile telephony [6]. Therefore, optimizing energy consumption and reducing carbon footprint of data centers have been considered an increasingly important challenge. Any optimization in operational cost, environment footprint, and system reliability will indisputably lead to not only economic but also environmental benefits.
A lot of work has already been done to optimize the execution of large-scale scientific workflow applications in the recent years. Some applications have a demand for high reliability, large throughput, and low completion time; while others run under a certain monetary budget or energy consumption constraint, for which the highest possible level of QoS may not always be desirable. In this paper, we tackle the problem of developing a scheduling algorithm in a cloud environment to optimize energy consumption, execution time, and throughput of computation-intensive workflows, more specifically, to minimize energy consumption and execution time while maximizing throughput using a meta-heuristic genetic algorithm, named Energy Aware, Time, and Throughput Optimization (EATTO), based on the Bat algorithm [7]. The key contributions of this work lie in: (i) modeling and scheduling of a complex DAG-structured scientific workflow in the cloud; (ii) proposing a task allocation scheme for a multi-objective optimization problem (MOP) which takes latency, throughput, and energy into consideration, and finds the best possible compromise for all three of them; (iii) conducting an extensive set of performance evaluations and comparisons to illustrate effectiveness and superiority of the proposed algorithm.
