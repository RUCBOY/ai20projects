With the rapid development of mobile Internet and artificial intelligence technology, the more and more communication has turned to human-machine communication. Also, the demand for an AI-based machine to recognize the user’s emotions and give the corresponding feedback is becoming stronger. People expect the interactive machines to have the ability of observation, understanding, and abundant emotion similar to human beings, thus putting forward the new requirements for human-computer interaction [1]. However, the existing human-computer interaction mode of many service robots is mechanical and monotonous, relying only on the keywords matching and background search, which is not intelligent enough and lacks the understanding of the semantic context [2], [3]. Therefore, we need to add emotional elements and intentional elements, and use affective computing technology to achieve emotional interaction. Emotional interaction has become the main trend in the human-computer interaction in the advanced information age. Besides, emotional interaction makes the human-computer interaction more intelligent. Namely, it makes the human-machine interaction as natural, cordial, vivid, emotional and temperature as human-human interaction is, thus realizing a deep human-computer interaction mode and understanding. Also, the emotion recognition plays an important role in the human-computer emotional interaction. Emotion recognition enables machines to perceive human emotional states and produce the ability of empathy. In the United States and Europe, many powerful laboratories have established special research groups to research and develop emotional systems and received sponsorship and support from some leading companies in that field. For instance, the famous Emotional Computing Team of the MIT Media Laboratory developed an emotional computing system, which collects data by using the biosensors and a camera capable of recording facial expressions; the collected data is then processed by the so-called “Emotional Assistant” adjustment program to recognize the human emotions [4]. Further, the Softbank company in Japan launched an emotional escort robot named Pepper, which can identify user emotions by analyzing facial expressions [5]. The Inner-scope, a neuroscience company, can predict whether the movie will make a splash by observing the highlights that make the audiences’ brains highly active [6]. In [7], the authors propose a novel smart cushion system for detecting the user’s stress state. In [8], the authors propose a novel emotional cognitive system, which can analyze and predict postpartum depression based on prenatal data. In [9], the authors propose a creative gaming system to help users improvise. However, the current research on emotion recognition mostly focuses on single-modal recognition such as expression recognition, speech recognition, limb recognition, and physiological signal recognition. Nevertheless, the lack of the single-modal emotional information and vulnerability to various external factors lead to lower accuracy of emotion recognition (i.e., the facial expression is easily occluded, and speech is vulnerable to the interference from the surrounding-noise).
Emotion denotes a subjective attitude of humans nervous system toward the external relations. Brain first sends the instructions for the corresponding feedback which influences the human facial expression, frequency and speed of voice, and body language expressions, and also influences the human organs such as heart, arms, legs, brain, etc [10]. Therefore, considering a certain complementarity among different modal emotion data, researchers have started to use the facial expression, blink, gestures and some other psychophysical signals in the emotion recognition research. For example, in [11], the authors use three physiology signals, namely EDA, PPG and EMG, to identify human emotions together. Multimodal information fusion for data-driven emotion recognition has been attracting the attention of researchers in the affective computing filed. Compared to the single-mode emotion recognition, multimodal information fusion for data-driven emotion recognition has higher accuracy. The multimodal emotion data fusion and recognition was firstly proposed by Bigun and Duc in 1997 [12]. They fused the facial and voice data and put forward a statistical method based on the Bayesian theory. In recent years, the technology of artificial intelligence and muti-sensor data fusion [13], [14] has been developing rapidly. Therefore, great progress has been achieved in research on the multimodal emotion data fusion and recognition. The multimodal emotion recognition has abundant and wide prospect of application. Besides, it helps to provide some useful functions to the empty-nest elderly and children [15]. By capturing human emotion, the psychological comfort for the empty-nest elderly and children can be achieved, helping to solve their psychological problems and undertake the load of psychologists. Through dialogue, a machine equipped with the mature artificial intelligence considers the patient’s emotion and helps to alleviate disease.
With the aim to help those who are interested in the emotion recognition to know the multimodal emotion recognition comprehensively, we need to present a comprehensive and systematic survey. Although there existed a few review papers about multimodal emotion recognition, for example, the survey paper [1] analyzed the major trends and system-level factors correlated to the effects of multimodal emotion recognition. And paper [16] reviewed muti-sensor fusion. The review [17] mainly discussed the development history of affective computing, multimodal emotion dataset, methods for multimodal features extraction(such as visual, voice and textual features), multimodal fusion technology, and applicable API. However, the physiological signal (such as an EEG modality) was not considered. Due to the great improvement in the development of deep learning and some other AI technologies, many findings of the multimodal emotion recognition have been obtained in the last two years. The aforementioned papers don’t cover the multimodal emotion recognition of AI technology fully. Also, we consider that the physiological change is take control of by the automatic nervous system and endocrine system, and almost not controlled by the subjective ideas, so the emotion recognition based on the physiological signal is objective [18]. Thus, with the change in the humans subtle physiological state (such as EEG and electrodermal activity), the specific fluctuations in human emotions can be observed and the corresponding emotional change can be recognized. For instance, when people become nervous under pressure or excited because of evil motive, the sympathetic nerve will cause the relevant somatic reactions, such as heartbeat acceleration, blood pressure increase, breath acceleration, body temperature rise, and even muscle or skin tremble [19], [20], [21]. Compared with the emotion recognition based on face recognition and movement, the recognition based on the EEG data or other physiological has higher credibility because it is natural and cannot be disguised or changed artificially. Besides, due to great achievements in the field of dry electrodes and wearable technology., the emotion analysis based on the EEG data obtained in a real environment (not limited to the laboratory environment) is more available [22], [23].
Compared with the related literature [1], [16], [17], this paper mainly focuses on the data-driven multimodal emotion data fusion and recognition with AI technology. Considering the real-time emotion health monitoring system, the progress in key technologies related to the dataset, feature extraction, features fusion and classifying in the multimodal emotion recognition field is analyzed and summarized. This paper aims to comprehensively explain the data-driven multimodal emotion information fusion and help clearly understand the scientific problems and future research direction in that field.
