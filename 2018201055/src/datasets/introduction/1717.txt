Deep neural networks have achieved great success in various applications in recent years. Especially the appearance of Imagenet [1] and convolutional neural networks [2], [3] has promoted the development of computer vision. Sometimes we can obtain abundant images easily in the real-world environment while the high-quality labeled data are scarce. Moreover, consuming time increases with a larger size of training data but the accuracy of the model is not keeping pace with it. In other words, a part of images are redundant for the performance of the CNN model, leading to worthless work on the expensive annotation task. It is thus important to find some effective ways to choose samples as less as possible from the unlabeled pool to maximize the accuracy. Active learning is a common method to address this problem.
The purpose of active learning is reducing the labeling cost[4]. It iteratively queries some labels from the unlabeled pool based on different measures, and tries to train an effective model with fewer queries. Some literatures have demonstrated that informativeness and representativeness are effective criteria in active learning[5] and proposed a lot of active learning algorithms [6], [7], [8] based on different selection methods on the classification task.
Although many active learning algorithms have been proposed these years, they directly select samples from the unlabeled pool, leading to two potential issues: 1) Instances redundancy among the same batch. Similar images may be selected at the same time, resulting in information redundancy among samples in the feature space. From the perspective of CNN models, these similar images may have the same information in the feature space and have no contributions to the current CNN model. 2) Too many hard samples in the selected batch. Commonly, hard examples which cannot be classified specifically can enhance the robustness of CNN models, but sometimes they also cause overfitting, especially when the number of training data is insufficient at the first few iterations of active learning.
Self-paced learning (SPL) [9] is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. It controls the number of hard samples by weighting losses on all labeled samples. Different from that, we propose a novel criterion calculated in the feature space to estimate the complexity of each sample.
Inspired by insights of the previous work as well as the recent successful algorithms, i.e. pre-clustering [10], core-set approach [6], self-paced curriculum learning [11] and multi-criteria fusion [8], [12], we propose a novel AL framework named “self-paced multi-criteria active learning” (SPMCAL) for CNN models to accelerate the performance in the image classification application. Unlike [10], [13], [14], we use the clustering method to form a center points dataset in the feature space rather than assigning the same label to the same group samples. This center points dataset contains several representative hard samples and more easy samples distributed in the feature space uniformly. Inspired by self-paced curriculum learning [11], we treat the increasing number of training samples at each iteration as the process of self-paced learning from simplicity to complexity. Along with the process of the algorithm, we increase the number of selected center data points and form a scale-changed pre-clustering center points dataset in order to cover more hard samples.
Fig. 1 shows the framework of our proposed active learning algorithm. Firstly, we utilize the idea of the core-set problem [6] and form a scale-changed center points dataset dynamically. In this step, we cluster a center points set to reduce information redundancy among unlabeled samples in the feature space. This process also avoids the situation of too many hard samples existing in this new dataset, ensuring the effectiveness and stability during the training process. Secondly, we consider multi-criteria (discrepancy and uncertainty) of each sample in the center dataset. In this step, we propose a novel criterion discrepancy based on the distance between the center points and the labeled pool, aiming at keeping differences between unlabeled data and labeled samples in the feature space. Thirdly, we dynamically balance discrepancy with uncertainty at each iteration to calculate the final scores of unlabeled samples.Download : Download high-res image (460KB)Download : Download full-size imageFig. 1. Self-paced multi-criteria active learning framework.
Another problem we faced is the poor feature representation ability of the CNN model when labeled samples are insufficient. Some semi-supervised methods utilize consistency regularization and powerful augmentation skills to train the model in a self-training manner [15], [16], [17], [18]. They improved the performance by utilizing high-confident samples with pseudo-labels, and the loss functions of these methods are consist of two parts which are calculated on labeled data and unlabeled data separately. However, these methods take a long time to train because they gradually increase the number of training samples. Another domain that aims at learning features of samples in a data-efficient way is contrastive learning [19]. The progress in contrastive learning has also promoted the performance of unsupervised learning [20], [21], [22], [23], [24]. They try to learn visual features through estimating different augmentations whether belonging to the same sample and have achieved a great success. Inspired by these, we design a classification loss function which is combined with similarity to boost the performance of the model in active learning and the results of our experiments also show the effectiveness of this loss function.
The main contributions of this work are summarized as follows:
1.We combine clustering and CNN models in the feature space and apply them on active learning to simulate steps of self-paced learning.2.We propose a novel criterion discrepancy which can effectively estimate information redundancy between center points and the labeled pool in the feature space.3.We design a loss function for active learning and experiments demonstrate the effectiveness of it.
The rest of this paper is organized as follows. Section 2 introduces some related work. In Section 3, our approach is discussed in detail. Section 4 presents the experiments and several advanced studies. The conclusion is delineated in Section 5.
