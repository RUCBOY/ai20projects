As one of the most incredibly influential parts of Artificial Intelligence (AI), Deep Neural Network (DNN) is designed to mimic the network of neurons that makes up a human brain so that computers will understand things and make decisions in a human-like manner. It has been widely applied in peopleâs life, such as unmanned vehicles, autonomous driving, monitoring security [1], [2], [3], and so on. An important and challenging application of DNN is performing real-time object detection to help a camera locate instances of semantic objects of a certain class [4], [5], [6], [7], and recognize each physical shape [8] as humans do. Some of the detection decisions are made locally, and the others are shared among distributed vehicles or uploaded to the cloud via broadband 5G channel for coordination and supervision since the detection results are very critical for the safety of autonomous driving. However, recent studies have shown that deep learning models are highly susceptible to adversarial perturbations [9]. The adversarial images can cause serious traffic accidents. For example, if the adversarial stop sign is not recognized correctly, the autonomous driving vehicle will not stop. So, it is worthy studying the attack technologies deeply for enhancing the robustness and security of neural networks and autonomous driving. Szegedy et al. [10] first propose the vulnerability of deep learning models in the field of image classification, that is, adding carefully created perturbations can cause the image classifier to misclassify input images with extremely high confidence, while the same perturbation can fool multiple image classifiers. Then, FGSM [11], C&W [12], and Decision-Based Attack [13] are proposed to attack DNNs based image classifiers. To save the time cost in training adversarial examples, Moosavi-Dezfooli et al. [14] and Mopuri et al. [15] propose universal adversarial perturbations against image classifiers. Meanwhile, more and more methods are presented to attack other tasks, for example, Chen et al. [16], Xie et al. [17], and Wei et al. [18] design attacks on object detections.
Object detection is a fundamental task of AI in computer vision and thus has broad applications like traffic sign recognition, face retrieval, and so on. In addition, the object detection algorithm has made great breakthroughs in recent years. The most popular algorithms can be divided into two categories. One is proposal-based algorithms (R-CNN [19], Fast R-CNN [20], Faster R-CNN [21], etc.). They are two-stage models and need to generate candidate box first, then classify and return the candidate box. The other is regression-based methods such as YOLO [22], YOLO9000 [23], YOLOv3 [24] and SSD [25], which use a convolutional neural network to directly predict the category and location of different objects. Proposal-based models are accurate but slow, regression-based models are faster but the accuracy is lower.
The operation of DNN of AI depends on the training process and it has vulnerability as mentioned above. The object detection models are also vulnerable to adversarial examples. At present, some attack methods against object detection models have been proposed. Xie et al. [17] propose optimization-based Dense Adversary Generation (DAG) and successfully attack Faster R-CNN. Wei et al. [18] propose Unified and Efficient Adversary (UEA) to attack both proposal-based models and regression-based models. Xin et al. [26] propose a DPATCH which is added to the top left corner of the input images to deceive object detectors. However, the adversarial examples generated by DAG and UEA are image-dependent, so it will spend a lot of time training adversarial examples, and DPATH is so obvious for defenders to find out that it is a modified image.
To address these issues, in this paper, we propose to use the transferability of universal perturbations generated against image classification tasks to attack both proposal-based and regression-based object detection models. We leverage generic representations learned by VGG16 [27], VGG19 and Densenet169 [28] and a generator trained on ImageNet dataset to construct transferable universal adversarial perturbations. Then add the universal perturbations by resizing and pile-up to attack object detectors.
This paper is an extended version of a previous conference paper [29]. In our previous work, we used ResNet architecture generator to train universal perturbations against the VGG16 model and used them to attack YOLOv3 and Faster R-CNN detectors in order to verify the transferability of the universal perturbations, and we used 40 images to evaluate the attack effect. Compared to the conference version, we change the quantity and architecture of generators, the structure and depth of the target models used in the training process, and the test dataset, etc. In summary, the main contributions (novel contributions highlighted in bold) of this work can be summarized as follows:
•We discover the cross-task, cross-model, and cross-dataset transferability of universal perturbations and use them to attack object detection systems. Universal perturbations trained against classification models can be used to attack object detection models.•We succeed in implementing cross-task, cross-model, and cross-dataset black-box attacks using the transferability of universal perturbations.•We use two different methods, resizing and pile-up, to solve the problem of perturbations and target images size mismatch, successfully attack both proposal-based and regression-based object detectors.•We explore the attack effect of the universal perturbations generated by different architecture generators like ResNet and recursive U-Net.•We explore the attack effect of universal perturbations generated against different structure target models like VGG model and Densenet model and target models with the same structure but not the same depth like VGG16 and VGG19.•We significantly increase the number of target images to test the attack effect of the universal perturbations and replace a more appropriate measurement.
The rest of our paper is organized as follows: we introduce some related works for object detection models and adversarial attack methods in Section 2. In Section 3, we propose our approaches to generate universal perturbations and attack object detectors. After that, the setup and results of our experiments are presented in Section 4. Finally, we draw the conclusions of our entire work.
