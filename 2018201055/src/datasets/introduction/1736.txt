Convolutional neural networks have achieved remarkable success in the field of computer vision [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]. However, the promising performance of CNNs is often accompanied by high computational cost and memory storage which is caused by over-parameterization. To address these problems, many methods have been proposed to compress CNNs such as low-rank decomposition [11], parameter quantization [12], [13], knowledge distillation [14], [15], and network pruning [16], [17], [18]. A lot of research efforts have focused on network pruning and practically demonstrated its effectiveness in compressing and accelerating CNNs.
A typical pipeline of network pruning consists of three steps: (1) pre-train a large model, (2) prune the large model based on a certain criterion and 3) finetune the pruned model [19]. Generally, there are two main types in network pruning including non-structured pruning and structured pruning. Non-structured pruning directly prunes the weights of each layer in an independent way to achieve the sparsity of the whole network. It requires specialized hardware or software to speed up the sparse CNNs. On the contrary, structured pruning removes structures including kernels, filters, and layers without requiring specialized hardware or software.
Our work focuses on pruning the filters from convolutional neural networks which is a branch of structured pruning. The filters of convolutional layers typically account for fewer parameters than the fully connected layers, but they account for most of the floating points done by the network [20]. Therefore, filter pruning is crucial to reducing the computation of the model. Existing works on filter pruning follow a very similar standard. First of all, the filters are ranked according to a certain pruning criterion such as mean activation [21], l1-norm [17] and the percentage of zeros in the filters [22]. The scoring criterion determines the importance of each filter and accordingly only top-m ranked filters can be retained. The final pruned model is then finetuned to regain the performance. For example, Li et al. [17] use the l1 norm to rank the importance of filters with the intention of pruning unimportant filters and exploring the sensitivity of layers for filter pruning. Liu et al. [23] impose the L1 regularization on the scaling factors in batch normalization (BN) layers to identify insignificant channels with small scaling factors. Luo et al. [24] propose entropy as a measure to evaluate the importance of each filter. A filter with a higher entropy can be considered more important. Hu et al. [22] calculate the average percentage of zeros in the corresponding feature maps of filters and then prune the zero activation neurons. Molchanov et al. [21] use a Taylor expansion as the pruning criterion to approximate changes in cost functions induced by pruning filters. Luo et al. [25] prune the filters based on the statistics information computed from its next layer instead of the current layer. He et al. [18] propose a LASSO regression as the channel selection strategy and least square reconstruction to prune filters. Li et al. [26] propose a Runtime Neural Pruning framework (RNP) which can dynamically prune the network at the runtime. They model the pruning process as a Markov decision process and use the reinforcement learning for training. Different from the aforementioned criteria, we randomly prune filters of convolutional layers to show the state-of-art results. Fig. 1 illustrates the general process of our work. Fig. 2 shows the mechanism of filter pruning employed in our multiobjective pruning.Download : Download high-res image (40KB)Download : Download full-size imageFig. 1. A general procedure of our pruning work.Download : Download high-res image (54KB)Download : Download full-size imageFig. 2. The green line of the kernel denotes the pruned filters. After the convolution operation, the input feature maps xi of the ith convolutional layer will be transformed into the output feature maps xi+1 of which the green one is pruned.
These methods above intend to reduce the computational costs while maximizing the accuracy simultaneously. However, they do not look at the pruning from the multiobjective angle. For the work of [23], its objective includes the accuracy loss and a penalty term on the sparsity of the network. For the work of [26], the objective consists of the classification loss and the penalty term representing the tradeoff between the speed and accuracy. Instead, we intend to model the pruning as the multiobjective problem. For one thing, we aim to maximize the classification accuracy of convolutional neural networks while pruning. For another, we need to find the best pruning rate in order to keep the whole network compact without hurting its performance. Therefore, motivated by [27], our work combines random pruning with evolutionary multiobjective algorithms (EMOAs) which are utilized to gain a tradeoff between the classification accuracy and the pruning rate.
Evolutionary multiobjective algorithms (EMOAs) [28], [29] have attracted a lot of research attention in the domain of data mining during the last two decades. They are now widely applied to various real-world applications including image processing [30], [31] and the tuning of machine learning problems [32], [33]. In reality, many real-life tasks have multiple conflicting measures of performance or objectives that need to be optimized simultaneously to achieve a tradeoff. EMOAs have been proposed to address multiobjective optimization problems. They have evolved from traditional aggregating approaches to the elitist Pareto-based approaches and recently to the indicator-based approaches. Among Pareto-based approaches, the most representative elitist EMOAs are strength Pareto evolutionary algorithm (SPEA) [34] and SPEA2 [35], Pareto archived evolutionary algorithm strategy (PAES) [36], Pareto envelope-based selection algorithm (PESA) [37] and PESA-II [38], and nondominated sorting genetic algorithm-II (NSGA-II) [39]. For the indicator-based algorithms, EMOAs adopt a selection mechanism and use an indicator to guide the search, especially for performing solution selection. The S metric selection evolutionary multiobjective optimization algorithm (SMS-EMOA) [40] has been proposed and seems to scale better when dealing with many objectives which is exactly the main advantage of indicator-based EMOAs.
The main contributions of our work are summarized as follows:
(1)We implement random filter pruning on ResNet-50 for remote sensing scene classification without the loss of the classification accuracy.(2)We model the pruning process as the multiobjective optimization problem (MOP) in our work with the intension of keeping pruned ResNet-50 compact while maximizing the remote sensing scene classification performance.(3)Evolutionary multiobjective algorithms are employed to address the multiobjective optimization problem residing in our pruning work.
The rest of the paper is organized as below: in Section 2, we introduce our multiobjective ResNet pruning model; in Section 3, we describe the optimization using EMOAs; in Section 4, we show the experimental results; in Section 5, we draw conclusions.
