The ability to apply Machine Learning (ML) techniques to data collected in organizations holds great promise for application in leadership and management research more generally (e.g., Chaffin et al., 2017; Wenzel & Van Quaquebeke, 2018). ML is a subfield of computer science, referring to algorithms with ability to learn from patterns in data to make predictions of outcomes without constant supervision and reprogramming by a human (e.g., Mikalef, Pappas, Krogstie, & Giannakos, 2018). This type of approach can be highly effective when large amounts of data are available to analyze and is often – especially in disciplines outside of Computer Science and in practice – referred to as “Big Data Analytics” (e.g., Mikalef et al., 2018; Oswald, Behrend, Putka, & Sinar, 2020). Enormous amounts of data can be fed into ML models, which, if trained accordingly, can result in prediction models, that are produced quickly and free from some types of human bias (e.g., George, Osinga, Lavie, & Scott, 2016).
The use of ML in organizations has received increased interest (in research and practice) which can be partly attributed to the fact that “sophisticated technologies for collecting and storing data allow for an exponential increase in organizational data that can be collected” (Oswald et al., 2020, p. 506). Data gathered from these sources require more powerful processing and offer opportunities for exploration of research questions that in the past were not easily accessible. For instance, ML techniques have been widely applied to disease prognosis and prediction, such as predicting cancer susceptibility, recurrence and survival (e.g., Kourou, Exarchos, Exarchos, Karamouzis, & Fotiadis, 2015). Such predictive models can be transferred to an organizational setting where ML has been used to predict, for example, employee turnover (e.g., de Oliveira, Zylka, Gloor, & Joshi, 2019), return to work after sick leave (e.g., Na & Kim, 2019), physiological markers of stress (e.g., Bacciu, Colombo, Morelli, & Plans, 2018; Reddy, Thota, & Dharun, 2018), and employee performance (Kirimi & Moturi, 2016). For leadership scholars who are interested in understanding relationships between leadership and follower, team, or organizational outcomes, the application of ML to data collected in the field represents an opportunity to examine exactly such relationships and build powerful leadership models.
Leadership researchers primarily seek to understand leadership phenomena and processes to be able to predict the occurrence of leader and follower behaviors and/or explain the effects that leaders have and describe its causal underpinnings. Thus, leadership scholars typically attempt to answer two key questions that are of high theoretical and practical relevance: first, how can we best predict the behaviors, decisions, and performance of individuals, groups, or firms based on leader characteristics or behaviors? Second, identify the causes of those outcomes – for example, what interventions in leadership behavior would cause positive improvements in team performance?
The application of ML can help to address both questions related to prediction and causality in leadership research. However, while advanced analytical techniques such as ML have developed at pace to deal effectively with large datasets (e.g., Oswald & Putka, 2015), they are usually designed to solve prediction problems (Kleinberg, Ludwig, Mullainathan, & Obermeyer, 2015). They do so by learning from large quantities of data how to make predictions on “out of sample” data with high accuracy. Moreover, while ML approaches can do well in addressing prediction problems, they are not often applied (correctly) to answer causal questions.
This paper will discuss the potential of using ML in research (and practice) to inform and extend leadership research and theory. We will discuss the application of ML in leadership research questions, addressing both prediction and causality questions through different research designs. As opportunities for using ML in organizations often go in tandem with the availability of Big Data (e.g., Oswald et al., 2020) our discussion will pay specific attention to the application of ML to Big Data in organizations. ML can, however, be applied to data that do not fall within common definitions of “Big Data” (see discussion of definition further below).
Researchers have recently begun to introduce ML tools designed to address questions of causal inference (Athey & Imbens, 2016; Wager & Athey, 2018). With these methods, it is possible to delve deeper into causal inference by combining ML techniques with experimental methods. Thus, a key objective of this paper is to propose steps to establish causal inference between variables in the realm of leadership research using randomized controlled experimental designs - the gold standard for causal inference (Antonakis, Bendahan, Jacquart, & Lalive, 2010; Hauser, Linos, & Rogers, 2017) - and incorporate novel statistical techniques (Pearl & Mackenzie, 2018) to gain a deeper understanding of the inferences that can be drawn from the application and outcomes of experimental methods. We propose that combining the application of predictive algorithms and experimental designs to draw causal inferences through a recently developed technique to isolate heterogeneous treatment effects (Athey & Imbens, 2016; Wager & Athey, 2018) can help advance leadership theory, methodological approaches and create research with strong policy and practical implications (Antonakis, 2017).
By reviewing the application of ML (specifically, but not exclusively applied to Big Data) in the leadership domain, focusing on issues of prediction and causality, we seek to make three contributions to the literature. First, we disentangle how ML techniques can be used to inform predictive and causal models of leadership effects, respectively, and clarify why both types of model are important for leadership research. In discussing both predictive and causal models we provide guidance on how ML techniques can be used to extend our understanding of leadership effects on outcome variables such as follower performance and affect. Second, whereas previous research in management has focused on the use of ML (often in conjunction with Big Data) to inform predictive models, we highlight the potential for future research to extend these approaches to causal models and discuss the state-of-the-art techniques to apply ML to experimental designs. With these new methods, ML techniques can be used to delve deeper into causal mechanisms when combined with experimental methods. Thus, in addition to understanding if a leadership intervention will improve, for example, team performance or employee well-being, we highlight how ML tools can identify for which leaders and team members the intervention is likely to work particularly well (or badly). Third, in addition to discussing the theoretical implications of using ML to understand causal models, we provide practical guidance for how this can be done. In doing so, we seek to provide impetus for future research in this area and encourage researchers to consider these techniques to answer leadership questions for the range of study designs being used.
The remainder of this review is organized as follows. We begin by introducing concepts of ML and Big Data and discussing their application in management and leadership research. We then discuss how ML techniques can be used to explore leadership processes in both predictive and causal models. Finally, we identify key areas for future research that can help to produce a reliable and systematic body of evidence to serve as a platform for leadership theory development and trustworthy policy recommendations.
Machine Learning and Big DataBoth ML and Big Data are terms that tend to be used rather casually with differing definitions across academic disciplines and the business community. In order to understand their definitions (and perhaps misconceptualizations) and current applications in research and practice better, we briefly review the origins of these terms below.ML has become a catch-all term for algorithms that predict anything. Often credited to Arthur Samuel (Samuel, 1959), the term ML first appeared much later (Koza, Bennett, Andre, & Keane, 1996) and refers to a computer program that learns patterns from data without being explicitly programmed. In his 1959 paper, Samuel suggested that a computer can be programmed so that it will learn to play a better game of checkers better than can be played by the person who wrote the program (Samuel, 1959). Today, ML algorithms sift through vast numbers of variables, looking for combinations that reliably predict outcomes. In some ways, this process resembles using the use of traditional regression models,2 but ML is adept at handling enormous numbers of predictors and combining them in nonlinear and highly interactive and complex ways (Obermeyer & Emanuel, 2016). This functionality allows the application to new kinds of data, whose sheer volume or complexity would previously have made analyzing them difficult or impossible. In this paper we use the term ML synonymously with “Predictive Modeling” as defined by Donoho, 2015, Donoho, 2017: the application of algorithms trained on data with a focus on accurately predicting a specific outcome. The concept harkens back to the statistician Leo Breiman's call for more focus on predictive algorithms in addition to the traditional focus among statisticians on generative algorithms, where the attention instead lies in identifying the proper stochastic model for a process and examining the parameters resulting from fitting that model with the data (Breiman, 2001; Donoho, 2015, Donoho, 2017). Donoho (2017) credits to Breiman (2001) that “the relatively recent discipline of ML, often sitting within computer science departments… [is] the epicenter of the Predictive Modeling culture.” (p. 751). Popular use of the term ML is in this vein, a class of algorithms focused on estimating an outcome, with less focus on interpretation of the parameterization of the model itself.ML algorithms are often grouped into categories such as supervised algorithms, unsupervised algorithms, and reinforcement learning algorithms (e.g., Hastie, Tibshirani, & Friedman, 2009). Simply put, supervised algorithms are those with a clearly defined dependent variable (i.e., an outcome, or target variable) which is described as a function of independent variables (i.e., covariates, predictive variables, or features) given a specified probability and loss functions (Hastie et al., 2009). Examples of supervised models include the linear regression model, classification tree, and artificial neural network. Unsupervised models, by contrast, lack a unique target variable on which to focus the optimization of the model training procedure. Instead, unsupervised models seek to describe all variables in a dataset by uncovering multivariate relationships. Examples of unsupervised models include Principal Components Analysis, the association rules algorithms (e.g., the a-priori algorithm), and clustering algorithms including k-means, hierarchical clustering, and Gaussian mixture models. Reinforcement learning is often considered a separate branch of ML, where an agent learns the behavior of a system through trial and error (Kaelbling, Littman, & Moore, 1996). Compared to supervised and unsupervised algorithms which work on static datasets, reinforcement learning relies on the dynamic ability to posit a new case and retrieve feedback, learning the outcome associated with the case. Reinforcement learning is an area of artificial intelligence at the forefront of current research (Dabney et al., 2020).In this paper, our discussion of ML focuses on supervised ML for two reasons. First, while the literature on unsupervised ML and causality has seen some recent advancements (see An, Xiao, Yuan, Yang, & Alterovitz, 2019), uncovering causality in supervised ML has garnered more attention and, as a result, a more comprehensive toolkit is readily available (which we will be introducing in this paper). Second, the field of leadership research has traditionally studied well-defined outcome variables (such as specific leaders' or followers' behaviors or traits), which is the focus of supervised ML.Like ML, Big Data has been defined in various ways but, in contrast to ML, there is no single well-accepted definition. The extensive use of digital technologies and the wide range of data-reliant applications have made the term “Big Data” pervasive across a range of disciplines including sociology, medicine, biology, economics, management, and information science (De Mauro, Greco, & Grimaldi, 2016). However, the popularity of this phenomenon has not been accompanied by the development of an accepted definition. Donoho, 2015, Donoho, 2017 historical charting of the term helps understand the development of various conceptualizations of Big Data. Most studies that discuss Big Data treat the term as a “catch-all, amorphous phrase that assumes that all Big Data share a set of general traits” (Kitchin & McArdle, 2016, pg., 2016). For instance, it is common for researchers to suggest that Big Data possesses three traits (Laney, 2001): volume (consisting of enormous quantities of data), velocity (created in real-time), and variety (being structured, semi-structured and unstructured) – the “three Vs.” However, analysis of 26 datasets revealed that Big Data do not all share the same characteristics and that there are multiple forms of Big Data (Kitchin & McArdle, 2016). Many authors use a wide range of defining characteristics which exceed the much cited three Vs (e.g., Shaffer, 2017). Kitchin and McArdle (2016) argue that to be considered “Big Data” datasets should possess the majority of the seven traits set out in Kitchin, 2013, Kitchin, 2014 typology of Big Data (volume, velocity, variety, exhaustivity, resolution and indexicality, relationality, extensionality and scalability), of which velocity and exhaustivity are the most important. For instance, rather than data being occasionally sampled (either on a one-off basis or with a temporal gap between samples), Big Data are typically produced much more continually. Exhaustive data sets refer to those where an entire system (such as an organization) is captured, rather than being sampled (Mayer-Schonberger & Cukier, 2013).Within the field of organizational behavior and industrial/organizational psychology, new technologies have greatly expanded the type and amount of data that is accessible to researchers and practitioners (Guzzo, Fink, Tonidandel, King, & Landis, 2015; Oswald et al., 2020). For instance, the use of wearable devices to capture, simultaneously and rapidly, large quantities of data can be used to examine research questions that were previously difficult to address in the field. As highlighted by Wenzel and Van Quaquebeke (2018) in a review of Big Data in organizational and management research: “the act of gathering, analyzing, and interpreting Big Data is, by and large, unfamiliar territory” (p. 201) for management and leadership researchers. In these fields the lack of a clear and widely applicable definition is often acknowledged and has resulted in rather pragmatic approaches (with many authors adopting definitions encompassing three or four characteristic traits). Wenzel and Van Quaquebeke (2018), for example, define Big Data as “observational records that may be exceptionally numerous, highly heterogeneous, and/or generated at high rate and systematically captured, aggregated, and analyzed to useful ends” (p. 550) and also point to key drivers of Big Data: instrumentation (technological instruments that emit a range of data modalities), interaction (temporal interactions resulting in ordered records), and interconnection (communication, collaboration and creation of content). In their focal article on “Big Data recommendations for Industrial and Organizational Psychology,” Guzzo et al. (2015) define Big Data “by more than just the volume, variety, and velocity of electronic records, however. It also encompasses new sets of tools and techniques for statistical analysis” (p. 493).Acknowledging this definitional challenge, Oswald et al. (2020) propose to “remain practical and problem-focused as a way to accumulate practical intelligence on Big Data questions from a more bottom-up approach” (p. 506). In the realm of strategic Human Resource Management, Minbaeva (2017) advocates that evidence based, strategic decision-making in organizations requires “smart data” rather than Big Data, with smart data referring to “data that is organized, structured, and continuously updated” (p. 112). Notwithstanding these definitional issues, we will discuss various types of data that organizations collect as examples and would like to emphasize that for the purpose of our paper, it is the application of “learning” algorithms (i.e., ML) in leadership research — not Big Data by itself - that is fundamental.Examples of continuously collected data in organizations include selection and assessment data (e.g., psychometric testing data, selection interview records), salary, performance data, promotion records, absence data, employee turnover data, or employees' and managers' free-text responses in annual performance appraisals. Indeed, a “defining characteristic of contemporary organizations is the rapid pace at which massive amounts of information are collected and stored” (McAbee, Landis, & Burke, 2017, p. 278). With increased application of sophisticated technology in Human Resource Management processes, data that are being collected continuously have grown dramatically in terms of volume and complexity, which explains why such data are typically labelled Big Data. For example, electronic performance monitoring - i.e., the use of technological means to observe, record, and analyze information that directly or indirectly relates to employee job performance (Bhave, 2014) - is now common within many organizations (Ravid, Tomczak, White, & Behrend, 2020). Examples include call monitoring, wearable sensors, e-mail and internet usage monitoring. Such information can be analyzed by ML tools to predict, for instance, employee personality, job attitudes, health, and performance (e.g., Kosinski, Bachrach, Kohli, Stillwell, & Graepel, 2014; Kozlowski, Chao, Chang, & Fernandez, 2020). Furthermore, Hauser and Luca (2015) argue that “data audits” in organizational research should be accompanied by increasingly readily available external data sources. Examples include collecting information from social networking websites (e.g., Facebook, Twitter) to evaluate/screen applicants during the selection process for employee recruitment (e.g., Brown & Vaughn, 2011; Roulin & Bangerter, 2013) or for research purposes to predict personality profiles (Kosinski, Stillwell, & Graepel, 2013). For leadership scholars who are interested in understanding relationships between, for example, leader and follower variables, the application of ML to Big Data has created major opportunities to examine exactly such relationships in organizations in the field. In the section below we will discuss how ML, especially applied to Big Data, can inform the development of predictive leadership models.
Machine Learning, Big Data, and predictive models in leadership researchThe ability to apply ML in field research, coupled with the availability of “Big Data” in organizations, has the potential to expand our understanding of leadership processes and models. The reason for this is simple: as leadership researchers we are often interested in prediction; we want to know, for example, which leadership characteristics or behaviors will predict future, for instance, employee performance (e.g., Cavazotte, Moreno, & Hickmann, 2012) or well-being (Inceoglu, Thomas, Chu, Plans, & Gerbasi, 2018). We are interested in predicting who is likely to be promoted to a leadership position and become an effective leader (e.g., Reichard et al., 2011). The application of ML is a powerful tool that can aid leadership scholars with optimizing such prediction models. For example, we know that certain leadership styles correlate with outcomes such as followers' job satisfaction and performance (e.g., Lee, Lyubovnikova, Tian, & Knight, 2020; Piccolo et al., 2012). ML can contribute to leadership research by identifying more, disparate, sets of variables that are predictive of effective leadership and have a positive effect on followers. For instance, a study by Spisak, van der Laken, and Doornenbal (2019) used ML to examine multiple personality and contextual predictors of leader effectiveness across a range of analytical methods, testing competing leadership theories at a level of complexity that was previously not conceived as being possible. Such tests allow for “naïve” data exploration without needing to specify a theoretical model beforehand. In fact, many contemporary applications of ML do not have a priori expectations, theories, or hypotheses about the underlying relations, but rather find patterns in the data to build predictive models (McAbee et al., 2017). This data-driven approach has similarities to inductive reasoning – representing a departure from typical approaches in leadership research which are theory-led and deductive, using quantitative data, or emphasize theory building and are inductive, using qualitative data. Observation-driven, exploratory approaches may identify patterns and highlight boundary conditions, thereby generating novel research questions (Woo, O'Boyle, & Spector, 2017). Such approaches can also be abductive: starting from an initial idea or “hunch” which is used to interpret empirical findings and generate plausible explanations (e.g., Van Maanen, Sørensen, & Mitchell, 2007). These insights (inductively or abductively derived) can provide new theoretical directions for future research in leadership, for example, by considering specific contextual variables which are receiving increasing attention and can be complex to model (e.g., Oc, 2018).While ML algorithms excel at identifying patterns in data, they can suffer from “overfitting” – learning patterns within a given dataset so well that the algorithm will not make accurate predictions on another dataset stemming from the same process (Cawley & Talbot, 2010). To guard against overfitting, ML models are often iteratively trained and tested on separate datasets using methods such as bootstrapping, cross validation, or the jackknife method (Efron & Gong, 1983). These iterative steps aim to ensure that the resulting ML model is robust and, as much as possible, generalizable to new data which will be fed into the model in the future (Hastie et al., 2009). This iterative process makes ML techniques better equipped to handle and predict outcome(s) on new “unseen” data, allowing them to be employed across different contexts, datasets, and decision problems. While many ML techniques do require human-led decisions, one critical insight from the ML literature is that these models use an empirically automated way to minimize out-of-sample error, using the data itself to create a model and test it iteratively (Kleinberg et al., 2015). That is, while bootstrapping, cross validation, and jackknife work differently, the goal is the same: to estimate the variability of the predictions from the ML models on “out-of-sample” data. The bootstrap resamples observations with replacement, the jackknife holds out a single observation from training, and cross validation breaks the data into “folds” (i.e., non-overlapping subsets of the data), where the model is trained on some folds and predictions are made on the remaining folds. Although different mechanically, these methods ensure that the model is not overfitted on just one dataset while performing poorly when new data are added, but quite the opposite: the “train–test” processes ensure that ML models are optimized to do well in “out of sample” data (Hastie et al., 2009; Kleinberg et al., 2015).Clearly ML can extend our understanding of leadership processes, for example by allowing us to test variables that predict leadership emergence, or which leader characteristics or behaviors predict follower outcomes. However, as with many analytical approaches, the use of ML is not without limitations, posing, for instance, risks of biased sampling or deceiving data quality (see review by Wenzel & Van Quaquebeke, 2018). Further, the application of ML to Big Data does not solve the fundamental problem of drawing causal inferences from observational data sets. This is, of course, an old issue in the social sciences, from psychology to economics to management: the ability “to explain behavior - that is, to accurately describe its causal underpinnings - and to predict behavior - that is, to accurately forecast behaviors that have not yet been observed” (Yarkoni & Westfall, 2017, p. 1).While the goal of some leadership studies is prediction, for many it is causation. For instance, a common leadership study involves the examination a leadership variable (e.g., a leader's behavior of style), a distal outcome (e.g., team performance) and a more proximate mediating construct, such as follower motivation (Fischer, Dietz, & Antonakis, 2017). This type of study design attempts to explain the link between the leadership variable and outcome through some underlying mechanism that explains the causal relationship, following an input-process-output logic (see Fischer, Dietz, & Antonakis, 2017). One critical problem remains in a world of data science, ML and Big Data: the research designs used in the majority of such studies typically suffer from endogeneity issues (i.e., the predictor variable is correlated with the error term of the outcome, and/or mediator variable), and do not allow us to determine the causal relationships of the variables of interest, or whether the causal effects even exist at all (e.g., Antonakis et al., 2010; Hughes, Lee, Tian, Newman, & Legood, 2018). An endogenous predictor is related to the measured outcome/mediator in multiple ways, as a meaningful antecedent, but also in some unanticipated way(s) (e.g., common method bias, reciprocal effects, relationship with a common cause). Despite ever more data (access) and sophisticated statistical techniques, endogeneity problems are not solved with the application of ML. ML may be good at predicting outcomes, but the identified predictors are not typically causes (and even in those cases where they are, standard “out of the box” ML methods are not apt to identify them as such, requiring other ways to demonstrate causality). Thus, the usual caveats about not confusing correlation with causation still apply; in fact, they become even more important as researchers begin including potentially thousands or even millions of variables in statistical models (Obermeyer & Emanuel, 2016). Big Data may involve ongoing observations of discrete events with temporal ordering, which can facilitate more nuanced examinations of direction, magnitude, frequency, speed, and points of change associated with a phenomenon (Wenzel & Van Quaquebeke, 2018). However, although time series data can help support causal claims, which require that X precedes Y temporally and that X and Y are correlated beyond chance, many temporally ordered observations do not inherently demonstrate causality (Antonakis et al., 2010). This is because the third condition that is required to demonstrate causality is often not met: ruling out any other causes that could explain the relationship between X and Y (Kenny, 1979).As with all organizational and management research, it is therefore imperative to clarify the objectives of one's research (what is the problem we want to address?) and to explicitly distinguish between issues of prediction and causality before deciding whether ML is an appropriate tool for examining this research question. The ability to predict an outcome or phenomenon using ML (without establishing causal relationships between variables) can of course be a valuable objective. As an illustrative example, consider team performance as an outcome variable of interest. Viewed as a prediction problem alone, an organization might ask – do we need to set aside a big pot of performance bonuses this year? We do not need to focus on the causes of good team performance in this case; instead we want to know what the outcome will be in a year's time and how much money to set aside. With enough historical data to train on, ML can help us get an accurate answer. However, viewed as a causal problem, one could ask: does a certain leadership behavior make it more likely that the team will perform well? This is a causal question – for example, whether a certain leadership behavior causes team performance to change. If so, identifying these behaviors can help inform recruitment and leadership development processes in the organization (therefore addressing a different issue).More recently, advances in econometrics and statistics have introduced the use of ML tools for estimating causal effects within subsets of the data (Athey & Imbens, 2016; Athey, Tibshirani, & Wager, 2019; Wager & Athey, 2018). With these new methods, ML techniques can be used to delve deeper into causal inference by combining them with experimental methods. So, in addition to understanding if a leadership intervention will improve team performance, these causal ML tools can identify for which leader and team characteristics the intervention works particularly effectively.
Machine Learning, Big Data and causal models in leadership researchRandomized controlled experiments are the gold standard for establishing causal inference (Hauser et al., 2017; Lonati, Quiroga, Zehnder, & Antonakis, 2018). Rigorously designed experiments, with randomly assigned treatment and control groups can establish that a treatment has a causal impact on an outcome through the direct manipulation of that treatment; for example, finding that a leader's charismatic speech caused an increase in workers' task output by about 17%, compared to workers who listened to a standard motivational speech (Antonakis, d'Adda, Weber, & Zehnder, 2014). However, while ML tools are often used to inform predictive models based on observational (a.k.a., non-experimental) data, until recently they have been seldom used to improve the ability of lab or field experiments to understand causal effects and draw inferences. Here we first discuss the foundation of experimental design before diving into the potential that ML offers experimentalists interested in examining causal relationships.Causal inference has a simple premise – to understand whether a variable X has a causal effect on variable Y. We typically consider the role of an “intervention” (or “treatment”) that manipulates X and we are interested in any change in Y as a result of manipulating X exogenously (for detailed information and background on experiments in management and leadership research, see, for example, Antonakis et al., 2010, Hauser et al., 2017; Hughes et al., 2018; here we summarize some key points). Rubin and collaborators first postulated and explored the “potential outcomes” framework (Rubin, 1974; Rosenbaum, 1984a, Rosenbaum, 1984b; Rosenbaum and Rubin, 1983a, Rosenbaum and Rubin, 1983b; Holland and Rubin, 1983, Holland and Rubin, 1987), providing a mathematical foundation for causal inference. Rubin noted that different outcomes exist for an individual observation – one outcome if the intervention is applied and one if withheld – however, in reality, only one of these potential outcomes can ever be observed; either the outcome for the observation when it was treated, or the outcome when it was not treated. While it will never be possible to observe both potential outcomes for the same individual observation (nobody can be “treated” and “not treated” at the same time), we can compare groups of individuals where one did receive the intervention (the “treatment” group) and one did not (“control” group). If the groups are similar enough before the treatment is applied, any difference in the outcome can then be attributed to the treatment. One intuitive method for creating similar groups is to randomly assign the treatment – i.e., individuals do not choose whether to receive the intervention. As a result of randomizing, the control and treatment groups are, in expectation, comparable (or exchangeable) in all possible dimensions (at least in expectation across both observable characteristics, such as gender, race or age, and unobservable factors, such as attitudes, behaviors or motivations). Thus, after the experiment is completed, it is then possible to compare the outcome of interest between the two groups and attribute the difference in outcome to the treatment, thereby establishing causality. The difference observed in the outcome between the groups is known as the average treatment effect (Angrist & Pischke, 2008; Glennerster & Takavarasha, 2013; Rubin, 1974). Today, this causal framework underpins much scientific discovery where randomized control trials and propensity score methods are deployed, having built on and displaced previous methods attributed to Fisher (1935), Kempthorne (1952), Cochran and Chambers (1965), and Cox (1958; see Holland, 1986). This rigorous definition of causality supplants previous methods such as those suggested by Granger (1969) that temporal considerations can indicate causality. While some approaches to understanding causality can use temporal separation, Granger's approach is not appropriate in all situations (Holland, 1988).When randomization is not possible, other frameworks have been proposed for drawing causal inferences (for an overview, see Angrist & Pischke, 2008), such as regression discontinuity, instrumental variables, special instances of time series, leveraging graphical models (see Pearl, 2019), and propensity score matching (Imbens & Rubin, 2015; Rubin, 1974). These methods often rely on data that are akin to randomization, or where quasi-randomization can be inferred, even if the setting was not designed or implemented as a randomized experiment (see Antonakis et al., 2010 for an overview). For example, if a training course were offered to potential future leaders based on performance ratings, these approaches might focus on leaders just above and below the cut-off for admittance to the course. For simplicity, in this paper we focus on the experimental results of randomized control trials but most of our discussion of the methods proposed below can be extended to quasi-experimental methods as well.Even when randomized control trials are used, leadership researchers are left with a conundrum. Experimental methods focus on the average treatment effect – that is, the impact of a treatment on average in a study population. Of further interest, researchers might want to know for whom the treatment works particularly well (or badly) – since there may be a distribution of smaller and larger effects around the average treatment effect in a population. For example, say a leadership researcher wants to understand the impact of bonuses on performance. Consider that a hypothetical study finds that the introduction of a one-time annual bonus improves performance by 10% on average in the treatment group, relative to the control group (or using another method mentioned above). A researcher may then also explore “heterogeneous treatment effects”: treatment effects within specific groups of interest. For example, do bonuses improve performance more for women or men? Is performance boosted by bonuses more for experienced employees compared with less experienced employees? Traditionally, researchers would need to hypothesize in advance that an intervention is going to be useful for a specific subpopulation (guided by theory or prior empirical literature) to test for heterogeneous treatment effects. Alternatively, some do not pre-specify the analysis and engage in “data mining”, a process that is frowned upon as the researcher could exploit “researcher's degrees of freedom” (Simmons, Nelson, & Simonsohn, 2011) – searching the data for any statistically significant results, which may result in spurious and non-replicable findings (Simmons et al., 2011). Even with prior theorizing and pre-specification, it is plausible that researchers may not hypothesize all relevant subgroups that would benefit from the population – perhaps in part because some groups are very specific (e.g. the bonuses may especially boost the performance of female hires with an engineering background) and would be missed, even though they may be practically and/or theoretically relevant.Athey and Imbens (2016) and Wager and Athey (2018) proposed a novel solution to this problem using ML (which we collectively refer to as the Causal Forests method henceforth). Because ML methods excel at finding patterns by searching through data, guarded against overfitting by the methods described above (such as cross validation, bootstrapping or the jackknife), they are appropriate for empirically guided data exploration – and they enable researchers to do so while not exploiting researchers' degrees of freedom (Simmons et al., 2011), as much of the model testing and validation process is automated. In short, this method estimates heterogeneous treatment effects by casting a wide net of potentially relevant predictor variables to locate subpopulations that differ in the extent to which they respond to the treatment. This enables new kinds of causal insights: for example, what would have been the causal effect of the intervention for an individual (based on certain covariates) had they been in the treatment group, not in the control group? For which individuals (based on certain covariates) was there no effect, or even a negative effect? Answering these questions can help in the application of future treatments into the field, deploying interventions where they will be most effective and least detrimental.The Causal Forests method offers an empirical ML-enabled way to answer these questions. This approach usually starts with the application of a randomized experiment and then uses ML to discover and estimate treatment effect heterogeneity within relevant subpopulations. Specifically, an experiment would randomly assign an intervention to a treatment group while the control would group not receive the intervention. For example, a leadership development training could be tested as an intervention for managers to improve employee-manager relationships (we will discuss this example below further). Once the experiment has concluded, a ML method referred to as “Causal Random Forests” (Athey et al., 2019; Wager & Athey, 2018) can then be applied to identify those for whom the treatment worked most effectively (or least). This technique works by identifying comparable groups of individuals in both the control and treatment groups. The algorithm splits the data into partitions based on covariates, aiming to maximize the causal treatment effect between the treatment and control groups within a partition (i.e., the difference in outcomes between employees similar on covariates but who happen to be in either the control or treatment group). Intuitively, this method applies the same iterative process described above to answer the question: which variables (i.e., covariates) are most indicative of large treatment effects in the population? The result of applying the algorithm is that every individual in the study population receives an estimated “individualized” treatment effect – a measure of how large (or small) the treatment effect would have been had that individual been treated. This method allows researchers to identify heterogeneity in treatment effects, answering the question: for whom - based on ML - identified characteristics — did the treatment work particularly well (or badly)?While this paper focuses on the Causal Forests method, it is worth noting that determining causality through ML is a growing area of interest, with other approaches having been studied as well. For example, the Transformed Outcome Trees (Beygelzimer & Langford, 2009; Sigovitch, 2007; Weisberg & Pontes, 2015), Fit Based Trees (Zeileis, Hothorn, & Hornik, 2008), and Squared t-statistic trees (Su, Tsai, Wang, Nickerson, & Li, 2009) all share the goal to make causal inferences but with differing technical approaches.One reason we focus on the Causal Forests method is because it is a natural extension of Rubin's original causal model which offers desirable characteristics for causality, which we reviewed above. However, two other well-known other approaches that have found widespread application include Granger Causality (Granger, 1969) and probabilistic graphical models (Koller & Friedman, 2009). Granger Causality is a specific approach used with time-series data that tests for similarity in time-lagged variation. In this context, causality means that a change in a variable temporally precedes the change in another variable. While still widely applied as a causal method, Granger's formulation does not exclude the possibility of a third confounding variable that may cause both variables to change (Antonakis et al., 2010), making it less stringent than the Rubin causal model. Nonetheless, Granger Causality has found widespread appeal, and is sometimes referred to as a “predictive causality” (Diebold, 1998) because this time-series technique can be applied with great success to problems where time-lagged predictable variation is common, such as in neuroscience (Chockanathan, DSouza, Abidin, Schifitto, & Wismüller, 2019). Probabilistic graphical models, on the other hand, operate on a statistical basis; learning and drawing inferences from observational (i.e., non-experimental) datasets in which relationship between variables are statistically likely or unlikely to be causal linkages (Dawid, 2010). The Graphical Causal Bayesian Model, for example, uses a Bayesian score (Sucar, 2015, p. 243) to calculate the reliability of a causal relation between two variables. However, these models do not proceed from the potential outcomes framework and do not rely on randomized assignment to infer causality; instead, they offer a probabilistic pathway to causality (for a broader discussion of probabilistic causality and related concepts, see Hausman, 2010). While these approaches are useful, both Granger Causality and probabilistic graphical models suffer from a shortcoming common to other discussions of causality in statistics – outside of ML – that an unobserved confounding variable may be the ultimate cause for the observed change. Or, that these models are not counterfactual – where two groups are compared directly but for the applied intervention. This issue cannot be resolved without a proper, randomized control group – i.e., using the counterfactual logic of what would have happened in the absence of treatment, which prompted Pearl (2000) to introduce the language of the “do” operator. The “do” operator is a formalization in Pearl's Causal Calculus framework to explicitly convey the random (exogenous) variation of a variable of interest (in contrast to an endogenous variation often found in non-experimental data). Therefore, while the above approaches are often useful, we believe that Rubin's causal framework offers the most compelling basis to extend into machine learning – which is the proposition of the Causal Forests method.Another reason to focus on the Causal Forests method is its effectiveness in determining causal relationships. Athey and Imbens (2016) quantitatively evaluated many well-known algorithms that aim to uncover causal effects using simulated datasets. The Causal Tree (Athey & Imbens, 2016) performs best at recovering the heterogeneous causal effects in point estimate and coverage. Wager and Athey (2018) further describe other research in the area include applying transformations to the outcome variable and applying the LASSO algorithm (Tian, Alizadeh, Gentles, & Tibshirani, 2014; Tibshirani, 1996, Tibshirani, 2011), using the Random Forest algorithm to separately model outcomes for treated and control groups (Foster, Taylor, & Ruberg, 2011), using LASSO for estimating interaction effects (Imai & Ratkovic, 2013), with Bayesian additive regression trees (Green & Kern, 2012), and exploring linear outcomes under interactions with the treatment (Taddy, Gardner, Chen, & Draper, 2016). Other methods for exploring causality with ML include using targeted learning (Van der Laan & Rose, 2011), explicitly using experimental design principles (Rosenblum & van der Laan, 2011), adjusting confidence intervals to account for adaptive estimation (Wager & Walther, 2015), and brute force methods such as exhaustive search (e.g., Chisholm & Tadepalli, 2002). When directly compared, the Causal Forests method outperforms other methods in simulation comparisons (Athey et al., 2019; Athey, Imbens, Pham, & Wager, 2017; Athey, Imbens, & Wager, 2016; Wager & Athey, 2018).
Step-by-step guidesIn the following, we offer a short and practical guide on to how the Causal Forests can be used. The focus of this guide is to introduce this method for practical application in leadership research. However, for completeness, we also cover the basic principles of a typical “ML prediction” approach and a typical “experimental” approach before explaining how the two can be combined in the Causal Forests method.1.Machine learning approachThe traditional supervised ML approach aims to predict outcomes – to understand which variables predict an outcome variable. We will briefly outline how a researcher might go about testing and validating a ML algorithm for this prediction exercise. We accompany each step with an illustrative (hypothetical) example that is relevant to leadership scholars. For example, a leadership researcher may want to know what variables predict employees' well-being.1.Define the problem. Commonly leadership researchers are interested in predicting individual, team or organizational outcome variables; such as employee, or team performance, well-being, attitudes or turnover. In addition to conventional measures of these outcomes (e.g., observational data in the form of questionnaires), technological devices enable the use of Big Data such as physiological indicators of well-being (Henning & van de Ven, 2017) or geospatial and verbal tracking data (Pentland, 2012).2.Define a study population, of those individuals on whom we want to make predictions. For example, we focus on employees in an organization.33.Define the outcome variable. In our example, we focus on the employees' well-being, which we assume is measured on a scale from 1 to 5 through an employment engagement survey on an annual basis.44.Choose ML algorithms. The choice of algorithm depends on multiple factors such as the goal of prediction, the volume and nature of the data, and the outcome variable. Further, multiple algorithms can be compared for performance. For example, our outcome variable (employee well-being) could be interpreted as a continuous variable or an ordinal categorical variable. Appropriate models should be selected for comparison given the goals of the analysis and outcome variable, such as Ordinary Least Squares (OLS) for continuous or ordered probit in case of ordinal categorical. Further, if we operate under the assumption that we have a large number of potentially relevant predictors (i.e., covariates) in our dataset, an algorithm with coefficient-shrinkage, such as LASSO (Tibshirani, 1996, Tibshirani, 2011) or Elastic Net (Zou & Hastie, 2005), may be an appropriate choice. Random Forests (Breiman, 2001), on the other hand, may be preferred if the number of predictors is particularly large, or we want an algorithm that requires little tuning and works well out-of-the-box, or the outcome variable is categorical in nature. While some algorithms such as artificial neural networks or “deep learning” networks work best with large datasets, others such as Gaussian processes or some types of Bayesian analyses are challenged computationally in the face of large amounts of data, working best with smaller datasets. As with any statistical model, choose an algorithm that best fits the problem. For an excellent introduction to a variety of models and their application in the social sciences, see Athey and Imbens (2019). Today, there exist many implementations of ML algorithms in for all popular statistical software programs; for first-time users, the “caret” package in R, the Python-based libraries such as scikit-learn or Keras (https://keras.io), are a good place to start, bringing together a large variety of ML algorithms with ample of documentation, tutorials and online help available.5.Divide the data into “folds” (or use another “train–test” or validation technique). In cross validation — a popular validation technique — folds are k non-overlapping, randomly partitioned subsets of the data (in our example, employees with all available covariates). A common choice is k = 10, so that the data are split into 10 equally sized folds. To prepare the data for the training and testing procedure, the dataset must contain a column for the outcome variable (e.g., well-being) and J additional columns (covariates, or “features” in ML) that could be potential predictors of the outcome variable.5 There can be a large number of J predictors, possibly even more than there are M rows or observations.6 Some software packages, such as Python scikit-learn, will perform this cross-validation procedure for the user automatically.6.Train and test the algorithm on the data. Each of the k folds is then used, in turn, for testing purposes with the others to train the model, which reduces the risk that the model is overfit. First, model parameters are estimated where the kth fold (for example, the 1st fold) is held as the test data. The model is trained on all other folds (in our example, folds 2 through 10) combined together. Next, the model is tested on the kth fold (which was held for test purposes) to see how well it does “out of sample.” Then the algorithm moves on to the next fold (e.g., 2nd fold), which is held as the test data, where parameters are again estimated using the remaining k–1 folds (e.g., 1, 3, 4, 5, …, 10), and so forth.7.Define a measure of “success”. Unlike typical regression models based on frequentist statistics, ML algorithms do not usually evaluate success by returning p-values. This is in part because, given a large enough dataset, almost anything might be statistically significant (as defined by frequentist statistics) and no valuable insight would be gained. Instead, some ML algorithms, such as Random Forests, return lists of “variable importance” (Archer & Kimes, 2008) that aim to give the researcher an insight into which predictors play a particularly important role in predicting the outcome variable.Once the algorithm has been tested the researcher is left with a list of “important” variables that are predictive of the outcome variable. For example, the researcher might find that a good employee-manager relationship, regular working hours and level of seniority are predictive of employee well-being at the firm.2.Experimental approachThe experimental approach aims to test the causal effect of an intervention – to explain what changes an outcome. Here we briefly describe how a researcher might go about testing an intervention using an experiment. We continue with the example of employee well-being introduced above. Based on the (hypothetical) finding above that a good employee-manager relationship is an important predictor of employee well-being, a researcher might hypothesize that developing certain leadership qualities could improve the relationship between employee and manager and thereby improve employee well-being A leadership scholar might ask, for example, whether a novel leadership development training can improve a manager's ability to connect with their employees and, as a result, increase employee well-being. (This guide is a shortened version of Hauser and Luca (2015) and Hauser et al. (2017), which discuss each step in more detail, especially within a field organizational context.)1.Define a study population, among whom the intervention will be tested. For example, we focus on managers (i.e., leaders) and employees (i.e., followers) in an organization. Note that the study population here is different from the ML example above, since the intervention will be delivered to managers, but the outcome measured at the employee level. For simplicity, we assume that every follower in our sample has exactly one leader and leaders supervise exactly one follower, not an entire team.72.Define the outcome variable. In our example, we are interested in employees' (i.e., followers') well-being (e.g., measured through a regular employee engagement survey on a scale from 1 to 5).3.Design an intervention. The intervention (i.e., treatment) is typically of primary interest to scholars, and the aim of the study is to identify the causal effect of the intervention on an outcome variable of interest. In our example, we assume that the intervention is a novel leadership development program that aims to make managers more empathetic to employees' concerns (for an overview of theories of behavior change using experimental treatments, see also Hauser, Gino, & Norton, 2018 and Rogers & Frey, 2014).4.Divide the study population in two, randomly assigning half of the participants (in our example, randomly selected managers) into the treatment group and the other half into the control group. The treatment group receives the intervention, the control group does not.5.Define the hypothesis to be tested. We might hypothesize that the leadership development training (our intervention) among leaders has a causal effect on followers' well-being (our outcome variable).6.Define a measure of “success”. In frequentist statistics, which are still predominant in the social sciences, if the p-value of the coefficient in front of the intervention dummy (treatment = 1, control = 0) in a standard OLS regression is below 5%, researchers typically declare the result to be statistically significant and the intervention to have been successful.Thus, once the experiment has been run, we would simply run an ANOVA analysis or an ordinary least square (OLS) regression predicting employee well-being based on whether the manager has randomly been assigned to receive the intervention (leadership development training). The coefficient associated with the intervention dummy in the OLS regression is our measure of success. For example, the researcher might find that the p-value is below 5% (and the direction of the coefficient is in the predicted direction) and therefore conclude that the treatment on average had a causal effect on employee well-being (i.e., managers attending the training display more empathic behavior towards their employees, which has a positive impact on the employees' well-being).3.The combined experimental-Machine Learning approachThe Causal Forests method is a combination of the aforementioned ML and experimental approach. Where ML focuses on prediction and the experimental approach isolates the causal pathway, the combination of both enables researchers to answer additional questions. For example, for whom (based on available covariates) did the treatment work particularly well? Assuming highly granular data on participants, the interpretation of the estimates from this method might no longer be described as an “average treatment effects” (at the study population level). Instead, this method edges closer to what one might describe as “individual treatment effects” (which has been described in the medical literature as “personalized medicine” or “precision medicine”: Ghahramani, 2015; Mesko, 2017) and in marketing as “personalized advertising” (Matz, Kosinski, Nave, & Stillwell, 2017; Matz, Segalin, Stillwell, Müller, & Bos, 2019). Real “individualization” is, of course, not technically possible: as Rubin (1974) observed, no individual can ever be in two states at the same time (receiving the treatment and not receiving the treatment). However, given enough information (covariates) about every individual and a large enough sample size, the causal effect of a treatment on similar individuals can be estimated.The general procedure of this method is as follows. First, the researcher runs an experiment with randomized control and treatment groups, as described above. As before, the experiment returns a (causal) average treatment effect of the intervention on an outcome variable. Then a similar ML algorithm is applied, using many potential predictors – but not to predict the outcome variable as before but to predict the response to the treatment (the difference between the control and treatment groups' outcome variable). For each individual, an estimate can be calculated – the extent to which an individual (identified by a large number of covariates) would respond to the treatment, compared to other individuals with similar covariates in the control group. The data can thus be partitioned into individuals who have (or would have, had they been in the treatment group) experienced large treatment effects, and those who have (or would have) experienced small, no or even negative treatment effects.We explain this with the below step-by-step guide, continuing the example of employee well-being as the outcome of interest. Using the Causal Forests method, we are not only interested in understanding the average treatment effect of a leadership development training intervention but also the heterogenous treatment effects (based on leaders' and/or employees' covariates) for whom the intervention had the largest causal effects and which covariates were important in differentiating the heterogeneous treatment effects.1.Define a study population, among whom the intervention will be tested. We again focus on managers (i.e., leaders) and employees (i.e., followers) in an organization.2.Define the outcome variable. We continue with the focus on employees' (i.e., followers') well-being, measured on a continuous scale from 1 to 5.3.Design an intervention. We assume the same intervention as before: a novel leadership development program that aims to make managers more empathetic to employees' concerns.4.Divide the study population in two, randomly assigning half of the participants (here, managers) into the treatment group which receives the intervention and the other half into the control group which does not receive the intervention.5.Run the experiment. At the end of the experiment, each employee-manager pair is associated with a column that contains the treatment status of the manager (1 = treatment, 0 = group) and the outcome variable of the employee (a continuous measure between 1 and 5), assessed at the end of the experiment.6.Apply the Causal Forests algorithm. In principle, this algorithm follows a similar procedure as discussed above. Instead of predicting well-being directly, however, it estimates the difference in the potential outcomes were the individual assigned to the control and treatment groups. Based on the Random Forest algorithm (Breiman, 2001), the Causal Forest algorithm searches through the covariates, looking for the variables and splits that maximize the difference in the outcome between the treated and control groups. The process is iterated many times using different subsets of the data, resulting in a forest of decision trees. The algorithm is available for free in the R package “grf” (HYPERLINK “https://grf-labs.github.io/grf/” https://grf-labs.github.io/grf/; https://CRAN.R-project.org/package=grf; Athey et al., 2019). In addition to estimating the treatment effect for each individual, the algorithm estimates the variance of the treatment effect, allowing the researcher to evaluate if each individual treatment effect is non-zero. For our example, while the training might lead to higher average employee well-being, the Causal Forests method can identify a subset of the employees where the intervention substantially increased well-being (for example, due to low a priori well-being), and a subset where the well-being is not impacted (say, those who already report the highest well-being before the treatment was applied). In addition to estimating the individual treatment effects, the Causal Forest algorithm provides a variable importance measure – a numerical value for each covariate, representing how important each variable is in differentiating the treatment effects. For example, the Causal Forest might indicate that the most important variables in determining treatment effects are due to manager experience, and personality traits of the employees such as introversion. These importance metrics can provide leadership researchers with insights into future areas of research.7.Define a measure of “success”. The outcome of this method is a quantifiable measure of treatment heterogeneity at an individual level – or, put differently, the extent to which an individual would respond to the treatment, as estimated by their covariates. In this setting, there is no well-defined concept of “success” as the information is the heterogeneity of the treatment success across individuals. (Although, arguably, general measures of model fit should be considered in evaluating whether the covariates are important and practically useful in the exercise.) The interpretation of these estimates is of interest, however. One way to look at the results is to split the data into partitions, by the magnitude to which the treatment would have improved (or worsened) the outcome for different individuals, relative to control.The result of the Causal Forests method is an insight into the heterogeneity of causal treatment effects. While heterogeneity analyses have been done many times for specific subsamples in the past (for example, for men or women only; for low-income households; or for experienced or inexperienced managers), this approach gives the researcher a comprehensive and systematic approach to identify subsets of data that she would likely not be able to access otherwise or test for (or if she did, one might fear that she was “data mining” or “p-hacking” the data, which is not an issue when using the Causal Forests approach).Furthermore, this approach not only provides the researcher with an empirical overview at the causal relationship of the treatment for specific subsets of the data, but also with a clear prediction for whom this causal relationship might hold in the future, bringing causal explanations together with prediction. For example, the researcher would not only be able to conclude that the intervention — the leadership development training — has on average had an effect on improve followers' well-being but she might also be able to show that it works best for inexperienced leaders who lead teams with introverts (an area of the organization where this intervention could be rolled out to with likely success), whereas the intervention does little to help experienced leaders or a team with many extroverts. Finally, this approach also enables the researcher to find pockets of the organization where the intervention could backfire – for example, if the team is composed of all male followers led by a female manager – providing both practically and theoretically relevant insights.
