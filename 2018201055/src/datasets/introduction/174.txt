A complex-valued Hopfield neural network (CHNN) employs a multistate activation function and is a most successful extension of Hopfield neural network [1], [2], [3], [4]. A multistate Hopfield neural network makes it easier to store real data. Multistate neural associative memories have attracted attention from many researchers, and further extensions have been attempted. Novel learning algorithms and architectures have been proposed for improvement in basic properties, such as storage capacity and noise tolerance. Isokawa et al. proposed a quaternion-valued Hopfield neural network (QHNN) employing a split activation function [5], [6], [7]. It was extended to a QHNN employing a phase-angle activation function [8], [9], [10], [11]. Moreover, commutative quaternions were introduced and a commutative quaternion-valued Hopfield neural network (CQHNN) was proposed [12]. Castro and Valle proposed a continuous-valued QHNN [13], [14], [15]. Hyperbolic numbers were also introduced to extend the CHNNs [16], [17]. Hopfield neural networks have also been extended using matrices and Lie algebra [18], [19], [20], [21].
The CHNNs employing a multistate activation function can process multilevel information and have been applied to the storage of image data [22], [23], [24], [25], [26], [27], [28]. Many alternatives of CHNN have been proposed using Clifford algebra, such as quaternions and hyperbolic numbers. A hyperbolic-valued Hopfield neural network (HHNN) improves the noise tolerance of CHNN [29], [30]. Twin-multistate activation functions were introduced to a QHNN and a CQHNN [31], [32], [33]. A rotor Hopfield neural network (RHNN) is an alternative of CHNN using matrices and provides excellent noise tolerance [34], [35], [36]. In this work, we propose a new alternative of CHNN, referred to as a synthetic Hopfield neural network (SHNN). An SHNN synthesizes a CHNN and an HHNN, and is the first combination model of different hypercomplex-valued Hopfield neural networks. Since the operations of complex and hyperbolic numbers, such as addition and multiplication, are different, a CHNN and an HHNN are synthesized by representation using matrices. Then, an SHNN can be regarded as a restriction of RHNN.
Hopfield neural networks require many weight parameters. Let N be the dimension of training data. A Hopfield neural network has the weight parameters of O(N2), where O(Â·) is Landau symbol. In general, the model with more weight parameters provides better noise tolerance, whereas it needs more resources; the noise tolerance and reduction in the number of weight parameters are trade-off. The number of weight parameters should be determined taking the available memory resources into consideration. If many options of weight parameters are provided, users can select the model suitable for the given memory resources. Thus, the noise tolerance should be evaluated under the same number of weight parameters. A CHNN and an HHNN have same number of weight parameters. A QHNN and a CQHNN have half weight parameters. An RHNN has double weight parameters. An SHNN is the first model with 1.5 times of weight parameters of CHNN. Table 1 summarizes the properties of alternatives of CHNN, including the numbers of weight parameters. The weight parameters are counted as real parameters. Since an SHNN has fewer weight parameters than an RHNN, it is acceptable that an SRHNN under performs an RHNN. However, an SHNN must outperform the others. A QHNN and a CQHNN have 0.5N neurons, since a neuron of QHNN and CQHNN has two multistate components and the others have N neurons.Table 1. This list summarizes the properties. N is the dimension of training data. The weight parameters are counted as real parameters. Storage capacities of projection rules are listed. Global minima show the number of global minima corresponding to a training pattern.ModelWeight parametersStorage capacityGlobal minimaQHNN, CQHNN0.5N2+O(N)0.5N2KCHNNN2+O(N)NKHHNNN2+O(N)N4SHNN1.5N2+O(N)N2RHNN2N2+O(N)2N2
A projection rule is a practical learning algorithm since it is a one-shot learning algorithm with large storage capacity. The training patterns correspond to the global minima of energy function by the projection rule. In this work, the projection rule is also provided for the SHNNs. Although an SHNN has 1.5 times of weight parameters of CHNN, their storage capacities are same since the projection rule for an SHNN is a composition of those for a CHNN and an HHNN. The storage capacities are listed in Table 1.
Rotational invariance is an inherent property of CHNN. Let K be the resolution factor. Rotational invariance causes K rotated patterns corresponding to a training pattern. The rotated patterns will be explained in Section 7. The rotated patterns are global minima of energy function and reduce the noise tolerance. The mixture patterns of rotated patterns are local minima of energy functions. The mixture patterns increases explosively with an increase in rotated patterns. Thus, the noise tolerance rapidly deteriorates with an increase in rotated patterns. Each alternative of CHNN has inherent invariance, which causes many global minima. Table 1 also lists the numbers of global minima corresponding to a training pattern. All the global minima of a CHNN and an RHNN with one training pattern have been theoretically determined in [37]. Those of the other models have also been determined by theory and computer simulations [29], [31], [33]. As there are more global minima, the noise tolerance becomes worse. From the numbers of weight parameters and global minima listed in Table 1, an SHNN is expected to have worse noise tolerance than an RHNN and have better noise tolerance than the others. The noise tolerance of SHNNs will be evaluated by computer simulations.
The rest of this paper is organized as follows. Section 2 introduces 2-D Clifford algebras, the algebras of complex and hyperbolic numbers, and defines the CHNNs and HHNNs. Section 3 defines the RHNNs, where the CHNNs and HHNNs are also regarded as RHNNs. The SHNNs are proposed in Section 4, and the projection rule is described in Section 5. Computer simulations are conducted in Section 6. Section 7 discusses the inherent invariance as a major factor to deteriorate the noise tolerance. Finally, Section 8 concludes this paper.
