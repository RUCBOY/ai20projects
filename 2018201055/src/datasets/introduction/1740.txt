The Nearest Neighbor (NN) problem, and its variations such as k-Nearest-Neighbors, Approximate Nearest Neighbor and All-Nearest-Neighbors problems, are fundamental in Computation Geometry and many other related research areas. Among the family of NN problems, the Approximate Nearest Neighbor problem, ϵ-NN for short, draws the most attention. Many proximity problems in computation geometry can be reduced to ϵ-NN, such as approximate diameter, approximate furthest neighbor, and so on [14]. ϵ-NN is also important in many other applicational areas, such as databases [29], data mining [18], information retrieval [23] and machine learning [24]. Besides, the ϵ-NN problem is more popular than the exact NN problem since approximation makes the problem easier, and the approximate result of nearest neighbor works almost as good as the exact one in most applications.
Due to its importance, many algorithms have been designed to solve the ϵ-NN problem since 1990s, most of which focused on the case under Euclidean Space Rd. Given a set P of points extracted from Rd and a query point q∈Rd, the ϵ-NN problem is to find a point p∈P such that D(p,q)≤(1+ϵ)D(p⁎,q) where D(⋅,⋅) is the Euclidean distance function and p⁎ has the minimal distance to q in P. The algorithms to solve ϵ-NN can be categorized into four classes as follows.
The first class of the algorithms tries to directly solve the problem, and the authors usually build data structures that support solving ϵ-NN efficiently. Arya and Mount [5] give a such algorithm with 1/ϵO(d)⋅n space, 1/ϵO(d)⋅nlog⁡n preprocessing time and 1/ϵO(d)⋅log⁡n query time. The query time here means the time to find the result of ϵ-NN using the pre-built data structure. Another work [6] gives an algorithm requiring O(dn) space, O(dnlog⁡n) preprocessing time and (d/ϵ)O(d)⋅log⁡n query time. Kleinberg proposes two algorithms in [19]. The first algorithm is deterministic and achieves O(dlog2⁡d(d+log⁡n)) query time, using a data structure that requires O((nlog⁡d)2d) space and O((nlog⁡d)2d) preprocessing time. The second algorithm is a randomized version of the first one. By a preprocessing procedure that takes O(d2log2⁡d⋅nlog2⁡n) time, it reduces the storage requirement to O(dn⋅log3⁡n), but raises the query time up to O(n+dlog3⁡n).
The second class of the algorithms considers the specific situation of ϵ=dO(1). One such algorithm is given in [8]. It can answer O(d)-NN in O(2dlog⁡n) time with O(d8dnlog⁡n) preprocessing time and O(d2dn) space. Chan [10] improves this result by giving an algorithm that can answer O(d3/2)-NN in O(d2log⁡n) query time with O(d2nlog⁡n) preprocessing time and O(dnlog⁡n) space.
The third interesting class of work considers the ϵ-NN problem from another point of view. They exploit some kind of intrinsic dimension of the input point set P other than using the Euclidean dimension. An exemplar work is given in [21]. The paper gives an algorithm whose query time is bounded by 2O(dim(P))log⁡Δ+(1/ϵ)O(dim(P)), where dim(P) is the intrinsic dimension of the input point set P, and Δ is the diameter of P.
Besides these algorithms mentioned above, Indyk and Motwani [17] initiate the work on the fourth class of algorithms. The key idea is to define an Approximate Near Neighbor problem, denoted as (c,r)-NN, and reduce ϵ-NN to it. The definition of (c,r)-NN problem will be given in the next section. With (c,r)-NN defined, the problem of solving ϵ-NN is divided into two parts. One is to solve (c,r)-NN, and the other is to reduce ϵ-NN to (c,r)-NN. This paper focuses on the latter part. Some works about the two parts of problem are introduced below.
Algorithms to solve (c,r)-NN  The existing algorithms for (c,r)-NN mainly consider the specific situation of d-dimensional Euclidean space with L1 and L2 distance metrics, which are the well-known Manhattan distance and Euclidean distance, respectively. Table 1 summarizes the complexities of the existing algorithms for (c,r)-NN under Euclidean space and L1 distance. The query time here means the time to find the result of (c,r)-NN using the pre-built data structure. These papers also give solutions under L2 distance, but these results are omitted here due to space limitation.Table 1. Solutions to (c,r)-NN under Euclidean space and L1 distance.SourcePreprocessing TimeQuery TimeSpace[17] (ϵ=c−1)O(n⋅1ϵd)O(1)O(n⋅1ϵd)[22] (ϵ=c−1)O(nd3ϵ2(nlog⁡d)O(1ϵ2))O(dϵ2polylog(dn)⋅log⁡1f)O(d3ϵ2(nlog⁡d)O(1ϵ2))[27]O(n(cc−1)2log⁡n)O(dno(1))O(n(cc−1)2)[3], [4], [12]O(dn1+12c−1log⁡n)O(dn12c−1)O(dn+n1+12c−1)[1]O(dn1+o(1)log⁡n)O(n2c−1c2)O(dn1+o(1))
The listed solutions in Table 1 can be divided into three groups. The first group includes the one given in [17], which is deterministic, and the other groups are randomized. The second group includes the one given in [22], which is based on a random projection method proposed in [19]. One distinguished characteristic of the method is that the preprocessing algorithm is also randomized. The last group includes a long line of research work based on Locality Sensitive Hashing (LSH), which is first proposed in [17]. These works are summarized into three terms in Table 1, which can be viewed as the space-time trade-off under LSH framework.
Finally, comparing the five results in Table 1, it can be seen that the query time grows and the space requirement drops from the first one to the last. The five results form a general space-time trade-off about the solution to (c,r)-NN. The details of these algorithms will not be explained too much since solving (c,r)-NN is not the main focus of this paper.
Reducing ϵ-NN to (c,r)-NN  So far there are three different algorithms for such a reduction. All these algorithms require a preprocessing phase to build a data structure, and then invoke the algorithm for (c,r)-NN in query time to answer ϵ-NN based on the pre-built data structure. Thus the query time complexity of the three algorithms is the number of invocations of (c,r)-NN algorithm, which has a different meaning with the query time of the former listed algorithms.
The first algorithm given in [17] builds a data structure using O(n⋅polylog(n)) space, and the query time is O(log2⁡n). The second one proposed in [15] reduces the query time to O(log⁡nϵ), where the preprocessing time and space complexities are both O(d⋅nlog⁡nϵlog⁡nϵ). The two algorithms are both deterministic. The third algorithm is a randomized one [16]. It invokes the (c,r)-NN algorithm in preprocessing phase to build the data structure, and thus the preprocessing and space complexities includes the complexities of (c,r)-NN algorithm, which is too complex to be cited here. The query time of the algorithm proposed in [16] is O(logO(1)⁡n), where the O(1) order of exponent comes from the constant success probability requirement of the randomized algorithms. The detailed analysis can be found in [2] and [16].
The most notable observation about the three algorithms is that the query time of them are all greater than O(log⁡n). It will be seen that the algorithm proposed in this paper achieves O(log⁡n) query time. The distinction and contribution of the algorithm proposed in this paper are introduced below.
The proposed method  In this paper a new algorithm for reducing ϵ-NN to (c,r)-NN is proposed. Comparing with the former works [15], [16], [17], the algorithm in this paper has the following characteristics:
(1) It achieves O(log⁡n) query time, where the query time is the number of invocations of the (c,r)-NN algorithm. It is superior to all the three existing works. This is the most distinguished contribution of this paper.
(2) Its preprocessing time complexity is O((dϵ)d⋅nlog⁡n), and the space complexity is O((dϵ)d⋅n). Our method has better complexity than the other three works in terms of n, so that it is much suitable to big data with low or fixed dimensionality. This situation is plausible in many applications like road-networks and so on.
(3) In terms of the parameterized complexity with d as a constant, our result is the closest to the well recognized optimal complexity claimed in [6], which requires O(nlog⁡n) preprocessing time, O(n) space and O(log⁡n) query time.
Note that there is an O((d/ϵ)d) factor in our preprocessing and space complexity. This factor originates from a lemma cited from [28]. It is worthy to point out that the O((d/ϵ)d) upper bound is actually very loose. In Section 5 the progress on reducing this factor is discussed, and more possibilities to further reduce this factor are revealed. In our future work we will continue working on this direction and make the result in this paper more close to optimal. In this sense, the work in this paper is more promising than all the three existing works.
The rest of the paper is organized as follows. We first give the formal definition of the problem to be solved and introduce the mathematical preparations in Section 2. Then the algorithms including the preprocessing and query algorithm are proposed in Section 3. The complexities of the algorithms are analyzed in Section 4. How to reduce the O(d/ϵ)d factor is discussed in Section 5. Finally we conclude this paper in Section 6.
