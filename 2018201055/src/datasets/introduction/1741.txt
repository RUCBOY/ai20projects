There is a growing interest in studying generative adversarial networks (GANs) in the deep learning community [1], [2]. Specifically, GANs have been widely applied to various domains such as computer vision [3], natural language processing [4], speech synthesis [5] and time series synthesis [6]. Compared with other deep generative models (e.g., variational autoencoders (VAEs)), GANs are favored for effectively handling sharp estimated density functions, efficiently generating desired samples and eliminating deterministic bias. Due to these properties GANs have successfully contributed to plausible image generation [3], image to image translation [7], image super-resolution [8], image completion [9] etc.
However, three main challenges currently in research into GANs are: (1) Mode collapse – the model cannot learn the distribution of the full dataset well, which leads to poor generalization ability; (2) Difficult to train – it is non-trivial for the discriminator and generator in a GAN to achieve Nash equilibrium [10] during training; (3) Hard to evaluate – the evaluation of GANs can be considered as an effort to measure the dissimilarity between the real distribution pr and the generated distribution pg. Unfortunately, the accurate estimation of pr is intractable. Thus, it is challenging to have a good estimation of the correspondence between pr and pg. Challenges (1) and (2) are more concerned with computational aspects where much research has been carried out to mitigate these issues [11], [12], [13]. Challenge (3) is similarly fundamental, however limited literature is available and most of the current metrics only focus on measuring the dissimilarity between training and generated images. A more meaningful GAN evaluation metric that is consistent with human perceptions is paramount in helping researchers to further refine and design better GANs.
Although some evaluation metrics, e.g., Inception Score (IS), Kernel Maximum Mean Discrepancy (MMD) and Fréchet Inception Distance (FID), have already been proposed [10], [12], [14], they have a number of limitations. Firstly, these metrics do not agree with human perceptual judgments and human rankings of GAN models. A small artifact in images can have a large effect on the decision made by a machine learning system [15], whilst the intrinsic image content does not change. In this aspect, we consider human perception to be more robust to adversarial images samples when compared to a machine learning system. Secondly, these metrics require large sample sizes for evaluation [12], [16] and acquiring large-scale samples for evaluation sometimes is not realistic in real-world applications since generating them may be time-consuming. Finally, the existing metrics are not able to rank individual GAN-generated images by their quality i.e., metrics are generated on a collection of images rather than on a single image basis. The within-GAN variances are crucial because they can provide an insight into the variability of that GAN.
The current literature demonstrates that a CNN is able to predict neural responses in the inferior temporal cortex in an image recognition task [17], [18] via invasive BCI techniques [19]. The ways in which a CNN can be used to predict a neural response with a non-invasive BCI aspect is still an open question. Fig. 1 illustrates a schematic of different mesoscopic and macroscopic neural measurement techniques using invasive and non-invasive approaches. In this schematic, only EEG (Electroencephalography) is non-invasively measured from the human scalp [20]. Other types of measurements of neural dynamics such as ECoG and LFP are invasive, which requires electrodes to be implanted. Compared to invasively measured neural dynamics, EEG pros are that it is a simple measurement, a non-painful experience during recording, easier to get ethics approval for and more easily generalized to real-world applications. However, EEG suffers challenges such as low signal quality (i.e., low SNR), low spatial resolution (interesting neural activities can span all of the scalp and are thus difficult to localise), all of which makes predicting EEG responses challenging.Download : Download high-res image (412KB)Download : Download full-size imageFig. 1. Schematic of different types of recorded neural signals (illustrated in (a)) via invasive and non-invasive measurements (illustrated in (b)). Figure from [21].
With the success achieved by deep neural networks (DNNs) in areas including computer vision and natural language processing, the operation and functionality of DNNs and their connection with the human brain has been extensively studied and investigated in the literature [18], [22], [23], [24], [25], [26], [27], [28], [29]. In this research area, the convolutional neural network (CNN) is widely studied and compared with the visual system in the human brain because both are hierarchical systems and the processing steps are similar. For example in an object recognition task, both CNNs and humans recognize an object by progressively extracting higher-level representations of the visual input through a hierarchy where successive layers operate on the inputs of the proceeding layers e.g., certain patterns of basic shapes, edges and colors as input can be determined at higher levels of the hierarchy to be a particular complex object composed of the inputs. Work reported in [18] outlines a CNN approach to delving even more deeply into understanding the development and organization of sensory cortical processing. It has recently been demonstrated that a CNN is able to reflect the spatio-temporal neural dynamics in the human brain's visual processing area [22], [25], [26]. Despite much work carried out to reveal the similarity between CNNs and brain systems, research on interactions between CNNs and neural dynamics is limited.
In [17] the authors demonstrate that a CNN matched with neural data recorded from the inferior temporal cortex of a human subject [30] has high performance in an object recognition task. Given the evidence above that a CNN is able to predict neural responses in the brain, we explore the use of CNNs to predict P300 [31], [32] amplitudes in this paper. This type of model can then produce (synthetic) EEG feedback for different types of GANs.
By applying advanced statistical and machine learning techniques to non-invasive EEG, better source localization and reconstruction becomes possible. Our previous work [33], [34] demonstrated the effectiveness of using spatial filtering approaches for reconstructing P300 source ERP signals. Remaining low SNR issues can be further remedied by averaging EEG trials. Based on this evidence, we explore the use of DNNs to predict a metric we call Neuroscore [35], when neural information is available via EEG.
In this work, we describe a metric called Neuroscore to evaluate the performance of GANs, which is derived from a neurophysiological response recorded via non-invasive electroencephalography (EEG). We demonstrate and validate a neural-AI interface (as seen in Fig. 2), which uses neural responses as supervisory information to train a CNN. The trained CNN model is then able to predict Neuroscore for images without requiring the corresponding neural responses. We test this framework via three models: Shallow convolutional neural network, Mobilenet V2 [36] and Inception V3 [37].Download : Download high-res image (328KB)Download : Download full-size imageFig. 2. Schematic of a neuro-AI interface. Stimuli (image stimuli used in this work) are simultaneously presented to an AI system and to participants. Participants’ neural responses are transferred to the AI system as supervised information for assisting the AI system to make decision.
In outline, Neuroscore is calculated via measurement of the P300, an event-related potential (ERP) present in EEG, via a rapid serial visual presentation (RSVP) paradigm. The P300 and RSVP paradigm are mature techniques in the brain-computer interface (BCI) community and have been applied in a wide variety of tasks such as image search [38], information retrieval [39], and others. The unique benefit of Neuroscore is that it more directly reflects human perceptual judgment of images, which is intuitively more reliable compared to conventional metrics in the literature [14]. In summary, our contributions are two-fold:
•We combine human perception research with GANs and deep learning research. This represents a new avenue of investigation in the development of better GANs technologies.•We propose a type of neuro-AI interface and training strategy to generalize the use of Neuroscore, which can be directly used for GAN evaluations without recording EEG. This enables our Neuroscore to be more widely applied to real-world scenarios, with a new measure we name synthetic-Neuroscore.
