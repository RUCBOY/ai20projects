Graphs are important data structures to conveniently model a wide range of real-world scenarios such as social networks, the Web, road networks, etc. With the increasing scales of graphs, graph processing has attracted considerable research interest [8], [20], [37], [44], [46], [47], [48], [52]. A triangle, i.e., a subgraph of three vertices pairwise connected, is an important concept in graph structure analysis. Triangle Counting (TC), which obtains the number of triangles in a graph, is a fundamental tool in graph processing to compute important graph metrics such as clustering coefficient and transitivity ratio [51], and has been widely used in real-world applications [4], [6], [20], [36], [45], [50], [55], [58], [63], [64], [70]. In common graph algorithms [18] such as BFS and PageRank [59], [60], a vertex only needs to access its own neighbors. However, TC algorithms are unique because a vertex usually needs to access the neighbors of its own neighbors [18]. Thus, the graphs used by state of the art TC algorithms usually need different processing (Section 2.2).
Given the tight coupling of different parts in a graph, graph processing in a shared-memory machine with all data fit in memory, i.e., shared-memory graph processing, is usually more efficient than in a cluster from the perspectives of efficiency, ease of programming and speed [20], [30], [59], [60]. Current commodity servers can be equipped with hundreds of GBs or even TBs memory and tens of cores, enabling them to efficiently process graphs with billions of edges in memory [20], [48], [52], [59]. This allows companies such as Twitter to process graphs in a single server [30].
The combination of the huge sizes of graphs and the limited memory capacity of a single server restrains the applicability of shared-memory graph processing [12]. The rapid growth of graph size and the relatively slow growth of memory capacity exacerbate the problem. On the other hand, the long-standing and ever-widening gap between the computing power of CPU and the bandwidth of memory, i.e., the memory wall problem, grows with more cores integrated in a single CPU. Therefore, trading the spare computing power of CPU for memory space, e.g., reducing data sizes by compression, is a reasonable solution to the aforementioned problem and can be increasingly reasonable with the gap becoming wider. To the best of our knowledge, no prior work tries to address the index compression problem of the popular graph representation CSR (Compressed Sparse Row, detailed in Section 2.1) in the large scale graph processing scenario, potentially missing an important opportunity to improve shared-memory graph processing.
Existing graph compression schemes only consider the sequential settings and are not suitable for share-memory processing [10], [11], [12], [13], [42], [67], try to compress specific graph representations such as trees [21], [28] and can only support limited graph algorithms, aim at accelerators such as FPGAs but CPUs [37], [46] or can only gain reasonable results on graphs of specific scale range [8]. Ligra+ is a shared-memory graph processing system working on compressed CSR-represented graphs of any scale and efficiently supports a wide range of graph algorithms [60]. Nevertheless, Ligra+ does not include any TC algorithms and only compresses the adjacency list (adjlist), i.e., leaves index data uncompressed (detailed in Section 2.1). Our evaluations show that index data also need to be compressed due to its up to around 80% percentage, as detailed in Section 2.4.
To mainly compress the index structures of CSR-represented graphs, we propose a chunk-based index compression scheme, called Chunked Index Compression for Parallel In-Memory graph processing (CIC-PIM, detailed in Section 3.1). The key idea of CIC-PIM is to exploit the ubiquitous power-law and sparseness features (Section 2.4) of large scale graphs, and divide index structures into chunks of appropriate size, then compress the chunks with our novel lightweight fixed-length byte-aligned encoding.
Evaluations show that, CIC-PIM achieves space savings of more than 60% on index data and more than 50% on entire graphs while still improving speed compared with state of the art approaches. After CIC-PIM compression, graphs of up to 2-fold (the size ratio of uncompressed graphs to CIC-PIM-compressed graphs) larger than those uncompressed can be processed with all data fit in memory, resulting in speedups or fast in-memory processing unattainable previously. The CIC-PIM advantages stem from its unique compression-friendliness after chunking, real random-access-supported and cache efficient design of the compressed index structures, and lightweight decoding routine.
The contributions of the paper include:

•Mainly to compress the index structures of graphs, we design a scheme CIC-PIM, which achieves significant space savings while still improving speed.•We conduct in-depth analysis to demonstrate the efficacy of the techniques in CIC-PIM.•We perform extensive evaluations driven by nine real-world graphs to evaluate the efficacy of CIC-PIM. Results indicate notable improvements over state of the art approaches.
The rest of the paper is organized as follows. Section 2 presents the background information which motivates our studies. The CIC-PIM scheme and the related issues are described in Section 3. Section 4 shows and analyzes experimental results. In Section 5, existing solutions and related works are reviewed. Finally, Section 6 concludes the paper with remarks on future work.
Table 1. Notation and Concept.Notation/ConceptDescriptionTCTriangle Counting or Triangle Counting algorithm(s)common graph algorithmsAlgorithms in which a vertex only needs to access its own neighbors, in contrast to TC (Section 1 Par. 1)adjlistAdjacency list, i.e., the neighbor list of a vertexpartially-compressed graphA graph with adjacency lists compressed but index data uncompressed, i.e., Ligra+-compressed graph (Section 2.1 Par. 2)fully-compressed graphA graph with adjacency lists and index data both compressed, i.e., CIC-PIM-compressed graph (Section 2.1 Par. 2)adjlist arrayThe array containing the adjlists of all vertices (Section 2.1 Par. 2)offset arrayThe array containing the beginning offsets (into the adjlist array) of adjlists of all vertices (Section 2.1)degree arrayThe array containing the degrees of all vertices (Section 2.1)index data/structuresThe degree array and the offset array (Section 2.1 Par. 2)orientationThe efficient heuristics used in almost all state of the art TC algorithms (Section 2.2)oriented graphThe TC-algorithm-used graphs on which orientation has been executed. (Section 2.2)unoriented graphIn contrast to oriented graphs, including directed and undirected graphs used by common graph algorithms (Section 2.2)MTCThe fastest shared-memory TC algorithm working on uncompressed graphs (Section 2.2 Par. 4)CMTCThe proposed MTC-based algorithm working on Compressed graphs (Section 3.3 and Fig. 8)
