From responding physiologically to concrete and physical stimuli to elaborating opinions and viewing future actions and emotions, the brain is responsible for all these amazing actions, which means that there must be signs indicating their existence. For this reason, the BCI potential has attracted the attention of many researchers for a large set of applications [1]. A typical example would be the use of BCI as a new type of controller [2], [3]: to move a wheelchair [4] or for “brain typing”[5].
There are also a few studies that focus on the visual information extracted from EEG, for example the approach for automatic image annotation, using a CNN with an EEGNet architecture [6], or the image reconstruction methods [7], [8]. Other recent studies [9], [10], successfully use neural networks (CNN and Recurrent NN) for EEG classification tasks and confirm their viability for this kind of problems. The authors of another article [11], used Support Vector Machine (SVM), k-Nearest Neighbour (k-NN), Multi-Layer Perceptron Artificial Neural Network (MLP-ANN) and Logistic Regression (LR) in their research work to extract the meaningful EEG signal patterns from a large volume of poor quality data having artifacts noises. For EEG decoding and visualization, deep learning with convolutional neural networks has been used [12]. In this paper, we also investigate a deep learning approach in combination with EEG input, using CNNs and LSTMs, for the task of image classification.
Previous works prove that EEG can be successfully used in several applications and that it is possible to extract meaningful semantic data using BCI. In this paper, we focus on the problem of predicting from EEG data the visual classes seen by human subjects and aim to answer the following questions:
1.Can we accurately predict visual classes from noninvasive EEG signals alone? In literature some articles indicate that invasive systems can be used for similar tasks [13], [14]. However, very few showed that the relatively weak EEG signals are relevant for high-level visual classification. A set of recent papers [7], [15], [16] achieved 83% on multi-class visual recognition from EEG signals alone but in their case the training and testing samples were collected in the same continuous recording session. In this paper we study the more realistic scenario when training and testing data are collected at completely different times. This case is much more difficult, but we show ways in which EEG data can be effectively used for image classification.2.Is visual recognition based only on features extracted from the brain areas traditionally related to vision or is it the result of a more complex process that also involves other areas of the cortex, responsible for higher level non-vision thought? Consistent with previous research [15], [17], our experiments suggest the interesting case that vision might go well beyond simple appearance based processing.3.Can we improve image classification if we use information extracted from EEG signals in conjunction with standard classic computer vision features? We show that EEG, even when they are weaker than visual features extracted directly from the images, are in fact useful for prediction as complementary signal. By capturing different kinds of information, not learned by deep neural networks directly in the image domain, EEG brings additional discrimination power that significantly boosts the classification accuracy.
There are many studies that attempt to decode EEG data using brain-computer interfaces (BCI) for a multitude of tasks and applications. Only a very small fraction focus on vision [7], [8], [15], [17], [18], [19], out of which most pose the problem as a recognition task [17], [18], [19]. One closely relates to our work [15], by proposing a deep learning approach in order to predict the class of the image seen by a human subject from the corresponding noninvasive EEG. We will refer to this article as state-of-the-art.
More specifically, the authors address the problem of visual classification using recurrent neural networks and achieve an average accuracy of about 83% on the test set [15]. They collected data for each class, per subject, in a burst of 25 seconds, then used the first 20 seconds for training, the next 2.5 seconds for validation and the last 2.5 seconds for testing. While authors of this article [15] obtained high accuracy, they trained and tested on data taken in the same burst. As a consequence their approach suffers from overfitting as shown in another study [20] and confirmed in our experiments (3.2 Classification task: Comparisons of models and subjects, Table 5), as their model learns noisy signals that are specific to that particular burst and are less related to the actual semantic image class. We train and test on data taken at different times of the day which is important when making predictions based on EEG signals for real world applications. We take a different approach in data processing and use Gabor filtering in order to remove the high frequency signal that is prone to overfitting when used in combination with powerful deep networks (3.2 Classification task: Comparisons of models and subjects, Table 5). By our novel processing of the system combined with our electrode signal selection mechanism (3.1 Ablation study: Selecting relevant signal, Fig. 4) we are able to achieve a competitive performance in the realistic scenario when the test data is taken at a different time than the train data.
We collected a novel image-EEG dataset by using an affordable, industry-level BCI with 14 electrodes and images from six different classes, including objects and different scenes. We collected the training and test data in distinct sessions, separated by a few hours at least. We wanted to better mimic the real-world conditions and to eliminate all possible interference between the training and test data sets. Instead of choosing from the EEG bands (Epsilon, Delta, Theta, Alpha, Beta, Gamma or Lambda) the ones most relevant for the experiments, we project the entire spectrum on the space of Gabor wavelets, across a relatively large range of frequency bands. As we show in experiments, this approach is efficient and robust to overfitting (3.2 Classification task: Comparisons of models and subjects, Table 5).
Another related paper augments visual features extracted from images with EEG signals [16]. The authors learn a joint encoding of the visual and EEG information with a Siamese network. However, they used the same dataset as [15] - thus suffering from the same limitation in terms of using training and testing data from the same recording session. Different from their work, we show that EEG data can be effectively used to boost visual recognition even in the case when the training and testing sessions are distinct and relatively far apart in time (2.3 Visual features augmentation, Fig. 9).
Thus, the research problem we are facing is relatively new. The main challenge, as seen in our ablation tests (3.1 Ablation study: Selecting relevant signal, Fig. 4), is that EEG signals are weak and generally noisy. They are the aggregate result of firings from billions of neurons, each having specific and often local tasks, over relatively large brain areas. On top of that, the process of capturing qualitative data is made even more difficult by the experimental setup, in which the subject is asked to wear a noise-sensitive and often uncomfortable BCI device for a relatively long amount of time, while remaining focused. Despite the obvious difficulties and challenges posed by the problem, this paper makes several contributions, at the intersection of computer vision and brain computer interfaces:
•We propose an unprecedented approach to classify visual classes from EEG data by capturing signals at different frequencies with Gabor filters and obtain an average classification accuracy of 66.76% over all classes and subjects, and a peak accuracy of 96% on specific classes.•We investigate the viability of using EEG data alongside state-of-the-art deep neural networks and show a significant boost in recognition from 91% to over 97%.•We investigate the relevance of each electrode input for classification and experimentally confirm that the most relevant EEG signals come from brain areas that are involved in higher cognitive reasoning, not from areas dedicated to early visual processing (e.g. V1).•We acquired a novel dataset with over 4 hours of EEG recording from 6 different subjects and 6 different visual classes, including 3 object classes and different 3 outdoor scenes, with distinct training and testing recording sessions, which we will make publicly available.
