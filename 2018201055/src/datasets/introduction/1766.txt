Judicial systems rely on the forensic science disciplines to provide scientific evidence that can be used in the court of law [1]. One of these disciplines is forensic firearm examination. The main role of firearm examiners is to provide evidence about the source of cartridge cases and bullets that are recovered after a shooting incident. These cartridge cases and bullets contain marks with features – striations and impressions – that originate from components of the firearm with which they were fired. Those features can be compared with the features in reference shots fired with a submitted firearm. The results of such a comparison are used to provide a judgment about the question whether the shots were fired with the submitted firearm or with a different firearm. When there is no submitted firearm, the features of different cartridge cases from the crime scene can also be compared to judge whether those were fired with the same firearm or with different firearms.
Comparing features is traditionally done by examiners, acting as the main instrument of analysis and interpretation [[2], [3], [4]]. Even though courts often treat the testimonies of examiners as impartial [3,5], they are often criticized for their lack of scientific rigor [1,[6], [7], [8]]. In particular, forensic disciplines that rely heavily on feature comparison, such as firearm examination, would benefit from the development of a research culture where the goal is to develop a well-established scientific foundation, and where judgments are substantiated by empirical research in addition to training and experience [8,9]. Such research should focus on the validity and reliability of methods and their application [8]. Several avenues of research have been proposed. These include the development of more objective computer-based methods [8]; the quantification of the variability of features coming from the same or from different sources [1]; the shift from the false idea that judgments could be based on uniqueness of features to establishing their evidential strength and to report judgments in probabilistic terms [8]; the determination of the validity of judgments [1,8], preferably by double-blind proficiency tests in casework [9,10]; the implementation of context information management to minimize the risks of cognitive bias [1,3,6,8,9,[11], [12], [13], [14]], including e.g., (linear) sequential unmasking, where the evidential material is not examined simultaneously with the reference material, but before examination of and comparison with the reference material [3,15,16]; the management of case information [[17], [18], [19]] and blind peer review [3,6,7,[20], [21], [22], [23]].
In firearm examination most scientific effort has been on the determination of the validity of judgments and on the development of more objective computer-based methods. Multiple studies have been set up with the aim to show that examiners are able to correctly judge whether a cartridge case or bullet was fired with a specific firearm or not [e.g., 24,[25], [26], [27], [28], [29], [30], [31]]. Overall, these studies report low error rates when comparing the judgments of examiners with the ground truth, which is the known correct answer (same-source or different-source judgment) based on the study design. Although these results seem promising, it is unsure how these results of experiments relate to the validity of judgments that can be expected during actual casework. There are several limitations of these studies, such as dependencies between judgments due to study designs, the use of closed sets with reference specimens for each questioned sample, and the relevance of the specimens used when compared to normal casework. Blind proficiency tests performed in the normal case flow seem to overcome these issues [10,32].
More objective computer-based methods have been developed [e.g., 33,[34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47]]. These studies usually rely on 3D surface topography measurements of the striation or impression patterns in fired cartridge cases or bullets. The measurements are then compared to each other by computer algorithms, resulting in a comparison score that gives some degree of similarity. Depending on the applied interpretation paradigm these scores are then used directly to decide (implicitly assuming some prior odds and cost/benefit of wrong/right decisions) about the source of the questioned cartridge cases or bullets and to assess an error rate [e.g., 36,38,39,42], or to determine the evidential strength [e.g., 33,[48], [49], [50]]. As a measure of the evidential strength, the likelihood of the comparison score is assessed for mutually exclusive propositions, for example H1: the two cartridge cases were fired with the same firearm, and H2: the two cartridge cases were fired with different firearms. The ratio of these likelihoods provides the evidential strength, the likelihood ratio (LR) [51]. An LR above 1 represents support for H1 over H2, and a LR below 1 means support for H2 over H1. These two interpretation paradigms correspond to two currently applied reporting formats for examiner judgments. In one of these, categorical same-source decisions are made when the features are in “sufficient agreement” [52], according to the scientifically-flawed individualization principle, and in the other the likelihoods of the features are assessed given two propositions resulting in an opinion on the strength of the evidence [[53], [54], [55], [56]].
Independent of the applied interpretation paradigm, both the examiners and the computer-based methods take into account the degree of similarity of the features in e.g., two cartridge cases when providing information about their source. Ceteris paribus, a higher degree of similarity will provide stronger support for the proposition that cartridge cases are from the same source. Because the examiners and the computer-based method consider similar features and apply (subjective) comparison algorithms based on the same metric of degree of similarity, it seems reasonable to expect that the outcomes of the two are coherent, in the sense that the judged degrees of support of examiners are positively correlated to the comparison scores from the computer-based method. Neither the examiner judgments nor the outcomes of a computer-based method can be considered as a golden standard. The validity of examiner judgments needs to be further determined [1,8], while the computer-based methods are still in an experimental stage.
The aims of this study are to assess the validity and reliability of source judgments by examiners and the validity of a computer-based method, to determine the relation between examiners’ judgments and the outcomes of a computer-based method, and to determine how calibrated the judged degrees of support of firearm examiners are. To do this we focus on one of the marks that is present in cartridge cases fired with Glock pistols, the firing pin aperture shear mark. The features of this mark are striations on the primer cup of the cartridge case that are caused by the margins of the firing pin aperture of the breechface when the barrel unlocks from the slide.
