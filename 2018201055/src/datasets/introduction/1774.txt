With the rapid development of Semantic Web technologies, various knowledge graphs are published on the Web using Resource Description Framework (RDF), such as Wikidata [1] and DBpedia [2]. Knowledge graphs make RDF links among different entities available to construct a large heterogeneous graph, which facilitates to support semantic search [3], question answering [4] and other intelligent services. Meanwhile, public accessibility of visual resource collections has attracted much attention towards different Computer Vision [5] (CV) research purposes, including visual question answering [6], image classification [7], object and relationship detection [8], etc. We have witnessed promising results by encoding entity and relation information of textual knowledge graphs for CV tasks. Whereas, most knowledge graph construction work in the Semantic Web and Natural Language Processing (NLP) [9], [10], [11]communities still focus on organizing and discovering textual knowledge only in a structured representation. There remains a relatively large scope of utilizing visual resources for knowledge graph (KG) research. A visual database is commonly defined as a rich source of image or video data and feeds sufficient visual information about entities to KGs. Obviously, making link prediction and entity alignment in a wider scope can empower models to make better performance when considering textual and visual features simultaneously.
In order to highlight the advantages of Semantic Web to the academic and industry community, a number of KGs have been constructed over the last years, such as Wikidata [1] and DBpedia [2]. These datasets make the semantic relationships and exploration of different entities possible. However, there are too few visual sources within these textual KGs. Visual question answering and image classification performance are consolidated by several methods [6], [12], [13], [14]. They have been developed for connecting textual facts and visual resources, but the underlying RDF links [15] from different entities and images to objects in the same image are still very limited. Hence, a scarcity of existing data resources is not capable of bridging the gap between visual resources and textual knowledge graphs.
As mentioned above, general knowledge graphs focus on the textual facts. At present, there is a lack of the complete multi-modal knowledge graph in academic community, which will hinder the future research of multi-modal fusion. To extend the capacity of knowledge graphs, we provide a comprehensive multi-modal dataset (called Richpedia) in this paper, as shown in Fig. 1.Download : Download high-res image (548KB)Download : Download full-size imageFig. 1. Graphical illustration of multi-modal knowledge graph. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)
In summary, our Richpedia data resource generally makes the following contributions:
•To our best knowledge, we are the first to infuse comprehensive visual-relational resources into general knowledge graphs. Hence, we establish a big and high-quality multi-modal knowledge graph dataset, which offers a wider data scope to the researchers from The Semantic Web and Computer Vision.•We propose a novel framework to construct the multi-modal knowledge graph. The process starts by collecting entities and images from Wikidata, Wikipedia, and Search Engine respectively. Images are then filtered by a distinctive retrieval model. Finally, RDF links are assigned between image entities based on the hyperlinks and descriptions in Wikipedia.•We publish the Richpedia as an open resource, and provide a faceted query endpoint using Apache Jena Fuseki.1 Researchers can retrieve and leverage data that distributed over general KGs and image resources to answer more richer visual queries and make multi-relational link predictions.
The rest of this paper is organized as follows. Section 2 describes the construction details of the proposed dataset. Section 3 describes the overview of the Richpedia ontology. The statistics and evaluation are reported in Section 4. Section 5 composes related work and finally, Section 6 concludes the paper and identifies topics for further work.
