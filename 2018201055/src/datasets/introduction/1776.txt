iNaturalist,1 eButterfly,2 BirdLab, HerpMapper,3 iSpot4 and SPIPOLL5 are few from many Citizen science (CS) platforms dedicated to biodiversity observation. These platforms allow collecting biodiversity data from numerous participants, and contribute to raise public awareness and increase knowledge on biodiversity issues. For example: eButterfly collect butterfly pictures from many volunteers, to provide data about butterfly abundance, in order to determine how climate change may be impacting butterfly distribution. BirdLab allows participants to collect bird pictures and play a collaborative game in order to understand the behaviors and the feeding strategy of birds. HerpMapper aims to gather and share information about reptile and amphibian observations across the planet. iSpot and iNaturalist iSpot aim to collect data of any creature in nature across large temporal and geographic scales. In these platforms, people upload their observations of wildlife and help each other to identify it. Each user can change its identifications many times forming an identification history. In addition to other available tools, iNaturalist offers an automated species identification computer vision tool. Similarly to previous CS, SPIPOLL collect data of flowering plants and insect pollinating it from many participants, in order to study changes in pollinator assemblages across space and time. In the SPIPOLL, after taking picture of pollinators on flowers and sharing it on the website, users are asked to identify each photographed insect, using an interactive identification key containing more than 600 insect names. Users can change their identifications, following the advices from the comments or the suggestions of other participants. After that, experts validate or correct the identifications. Data quality represents a critical aspect for the success of any CS project (Bonney et al., 2009). Improving data quality represents a big challenge in order to increase researchers' confidence in the gathered data from a big numbers of participants with varying levels of expertise. In particular, it is difficult for a limited number of co-opted experts to validate the gathered data manually. In addition, studies (Deguines et al., 2018) have shown that users increase their expertise while participating and that the speed of learning depends on the insect, as difficulty of identification varies among insects. As a consequence, it might be useful to detect potential miss-identification and to focus the limited expert workforce on them. To do so, we trained several ML algorithms to predict if the identification is correct or not in order to validate the obtained data. These algorithms are trained using a set of extracted features from the set of users' observations. The following of this paper is organized as follows: Section 2 provides an overview of the related work in the area of answer predictions. Section 3 presents the general structure of the SPIPOLL website. Section 4 introduces the details of our prediction model. Section 5 describes the experimental setup and obtained results. Finally, we provide some concluding remarks in Section 6.
