Science networks host applications that process large amounts of data derived from a diverse set of complex experiments. Oftentimes, these applications require large and frequent data transfers with explicit network performance requirements such as high-speed data delivery. Consequently, these transfers become very sensitive to performance degradation events. Even the slightest amount of packet loss can significantly increase the overall data transfer time [1], which in the context of complex experiments can be interpreted in delays in data availability. In order to ensure timely data availability, science networks feature dedicated systems (e.g., PerfSONAR [2]) that are able to detect and report on performance degradation events (e.g., latency increase, throughput degradation). However, these systems report transfer degradation events after they occur and to this day no scientific method that predicts different negative performance events is available for science networks.
Solutions that predict different aspects of TCP performance such as throughput or packet loss prediction, are largely centered around two approaches: formula-based and history-based predictions. Formula-based methods predict performance aspects using mathematical expressions that relate the predicted variable to path and end host properties such as Round Trip Time (RTT) or the receiver’s window size. In most cases, measurements for the aforementioned properties are gathered using different active or passive network measurement tools. A common shortcoming of formula-based approaches is that they are greatly affected by the continuously evolving TCP implementations, making maintenance of up to date formula-based models a cumbersome process. On the other hand, history-based approaches produce a time series forecast of the desired attribute (e.g., packet loss) based on measurements derived from previous file transfers, collected either passively (e.g. through monitoring a link) or actively (e.g. by conducting file transfers of different size). Although for certain aspects of TCP performance, history-based approaches tend to be more accurate than formula-based predictions [3], existing solutions focus mostly on predicting network throughput.
The goal of this work is to develop an accurate light weight machine learning tool to predict end-to-end packet loss, manifested in the number of retransmitted packets in science flows of arbitrary size. We believe that understanding the nature of packet retransmissions would allow both scientists and network operators to mitigate packet loss through different host or flow reconfiguration techniques. We investigate the hypothesis that packet retransmissions are due to a combination of factors related to the selected “path” along with end host network configuration. We argue that the accuracy of formula-based solutions can be augmented by a tool that takes into account measurements of path and host attributes from previous data transfers.
Towards our goal for developing a robust analytical framework for retransmission prediction we focus our efforts on answering the following questions: 1. Which path properties or combination of path properties are needed in order to generate an accurate prediction? Do different combinations of input parameters demonstrate different levels of accuracy? 2. Do we need to take into account end host (i.e. client/server) network configuration parameters in our prediction? 3. Can we generate accurate predictions for data transfers of arbitrary sizes? and finally, 4. How robust is a history-based solution shifts in data transfer behavior? (e.g., when path properties change significantly or when the end hosts are reconfigured).
The contributions of this paper are:

•A light weight machine learning tool based on Random Forest Regression that is able to predict packet retransmissions in science flows of arbitrary size. Our predictor takes into account a combination of path properties (including RTT) and host parameters (such as TCP maximum congestion window) in order to predict the number of retransmitted packets in each network flow.•A thorough evaluation of our proposed solution made with real world flow traces that represent data transfers between different scientific facilities and/or end users across the world. We evaluate our predictor using flow data that follow multiple network paths that exhibit fundamentally different behavior in terms of TCP related parameters (e.g., Round Trip Time, available throughput, etc.). We measure the accuracy of our tool under different subsets of input parameters in order to make a recommendation on the most suitable combination of path and host related properties. To generate predictions for different sizes, we purposely train and test on datasets with transfers that range between a few hundred megabytes to many gigabytes.
The paper is organized as follows: Section 2 describes related work and Section 3 describes the individual datasets used for our analysis as well as the collection and aggregation tools used. Our solution and architecture is presented in detail in Section 4. Important evaluation aspects and obtained results are presented in Section 5. Finally, we conclude with important observations and suggestions for future work in Section 7.
