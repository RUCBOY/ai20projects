Nonnegative matrix factorization (NMF) aims to decompose a nonnegative matrix into two nonnegative factor matrices, the basis matrix and the encoding matrix (also called the coefficient matrix), whose product is as close to the original matrix as possible [1]. The nonnegative constraints on the two factor matrices lead NMF to a parts-based representation approach, since they only allow additive operation [2]. Many studies have proved that the part-based object representation is consistent with human brain perception mechanism [3], [4]. Intuitively, the nonnegative representation coefficients conform to the concept of combining parts to compose a whole. Therefore, NMF has increasingly become one of the most popular methods for feature learning and widely applied in various fields, such as document clustering [5], [6], [7], [8], [9], [10], [11], image processing [12], [13], [14], [15], [16], [17], [18], face recognition [19], [20], [21], and source separation [22], [23].
Because of the good characteristics of NMF, some improved methods of the original NMF have been proposed with different constraints on the two factor matrices. Depending on whether supervised information, such as class label and pairwise constraint, is used, existing NMF-based methods can be divided into three classes: supervised [24], [25], [26], [27], [28], [29], semi-supervised [13], [30], [31], [32], [33] and unsupervised [8], [9], [10], [11], [12], [34], [35], [36], [37], [38], [39]. In the supervised scenario, the discriminant NMF (DNMF) [25] is one of the most representative methods. It adds the Fisherâ€™s criterion to the object function of NMF as the regularization term. With the intra-class scatter and the inter-class scatter formulated on the coding matrix, DNMF can learn a more discriminative representation of data. Different from [25], [26] which focus on imposing discriminant constraints on the encoding matrix, Discriminative and orthogonal subspace constraints based NMF (DOSNMF) [27] defines two graphs on the basis matrix, the penalty graph and the intrinsic graph, and then incorporates them into the loss function of NMF. The former graph connects marginal samples belonging to different classes and characterizes the inter-class distinctness. The latter one connects each sample with its neighbors belonging to the same class and captures the intra-class compactness. These two graphs have been shown to be an effective way to find a more discriminative representation [40]. Semi-supervised NMF-based methods learn from a combination of both abundant unlabeled samples and a small number of labeled samples to obtain the two factor matrices. As a representative semi-supervised algorithm, constrained NMF (CNMF) [30] regards the label information as additional hard constraints to merge the samples from the same class into an encoding vector. Thus, the obtained representation has more discriminating power. Pairwise constraint-based NMF method (CPSNMF) [32] is another form of the semi-supervised one. The must-link and cannot-link constraints are used to obtain data weight. With data weight, the intrinsic graph is constructed to characterize the geometry information of the data distribution, which is applied to the NMF objective function as the regularization term. Semi-supervised robust structured NMF (SRSNMF) [13] leverages the block-diagonal structure and the L2,p-norm loss function to learn a robust discriminative representation.
Compared with supervised and semi-supervised ones, unsupervised NMF methods have attracted much attention, since it is fairly expensive to obtain label information. Furthermore, the application of label information will relatively narrow the overall insight in the solution area [14]. The original NMF is the simplest unsupervised method. Although NMF has the good physiological and psychological interpretation of data, its factor matrices are not sparse enough and performance needs to be further improved. To increase the sparse of the component matrices, local NMF (LNMF) [19] and sparseness constraint NMF (SCNMF) [41] merge the localization constraint and the sparseness constraint to learn the nonnegative representation, respectively. Orthogonal NMF (ONMF) [34] adds the orthogonality constraint on one nonnegative factor to NMF, which performs well in clustering tasks. Furthermore, three improved ONMF methods [35], [36], [42] are proposed to enhance the clustering performance from different views. Gong et al. [43] investigated a new NMF updating strategy to serve the clustering task. To save storage space and improve computing performance, Chen et al. [44] made use of the left and right semi-tensor product to decompose a nonnegative matrix into two factor matrices. Above-mentioned algorithms ignore the intrinsic geometric structure of the data space. In fact, it has been shown that learning performance can be improved significantly by respecting the local geometric structure of data [45], [46]. Cai et al. [47] constructed a nearest neighbor graph to formulate the local structure of data and thus proposed an unsupervised graph regularized NMF (GNMF). In this way, GNMF can find a compact representation via regarding the geometric structure as the manifold regularization term. Thereafter, manifold regularization that represents the local structure is extensively applied in different NMF approaches. Zhang et al. [48] presented a manifold regularized matrix factorization method (MRMF) by adopting both manifold constraint on the basis matrix and orthogonally constraint on the encoding matrix to the objective function of NMF. Wang et al. [49] developed a multiple graph regularized NMF (MGNMF) trying to replace the manifold regularization in GNMF with the mixed graph constructed by a linear combination of a few graphs. Liu et al. [50] proposed a general subspace constrained NMF (GSCNMF) by incorporating subspace learning method, such as principal component analysis (PCA), linear discriminative analysis (LDA), and LLP [51], into the KL divergence cost function of NMF. Gao et al. [52] presented a novel local centroid structure NMF by introducing multiple local centroids to capture the manifold structure. Different from other graph regularized NMF methods predefining the graph, an adaptive manifold regularized matrix factorization algorithm (AMRMF) [53] and an NMF algorithm with adaptive neighbors (NMFAN) [54] apply the structured graph optimization [55] to the objective function of NMF. The two methods simultaneously perform graph learning and factor matrix decomposition in each iteration. This indicates that the intrinsic graph is dynamically constructed in the process of solving the two factor matrices. Following this line, Yi et al. [56] developed an NMF algorithm with locality constrained adaptive graph (NMF-LCAG) to integrate graph construction to the processes of matrix decomposition.
It is noteworthy that the above-mentioned methods are sensitive to noise and outliers. Actually, many data in real-world application usually contain the potential noise. The performance of NMF and its variants degrades when the data include noise and outliers. It is proved that L2,1-norm based loss function is robust to noise and outliers [57]. Kong et al. [58] presented L2,1-norm based NMF (L21NMF) by replacing the least square loss function of NMF with L2,1-norm based loss function to address the noise problem. Both robust manifold NMF (RMNMF) [59] and manifold NMF with L2,1-norm (MNMFL21) [60] introduce manifold regularization into the objective function of L2,1NMF. These two methods are essentially the L2,1-norm versions of GNMF. Li et al. [61] developed a graph regularized nonnegative low-rank matrix factorization algorithm (GNLMF), which combines NMF and manifold structure into a low-rank recovery framework to enhance its robustness.
As we have described above, many studies have been devoted to improve the original NMF from different perspectives. However, a critical problem in most existing NMF methods is that the global geometrical structure of data is ignored. It is shown that the global geometrical structure is important for feature learning and successfully applied in various fields [62], [63], [64], [65], [66], [45]. Generally, data structures of various real-world applications are complex, and a single representation (either global or local) is not sufficient to discover the underlying real structure [67], [68]. For example, although face images from one person exist on a single manifold, face images from different persons lie on different manifolds. Current manifold learning methods might be unsuitable for the scenario of different manifolds, since their model base on the characterization of locality [66], [45], [67], [68]. Thus, it is necessary to characterize the global geometric structure of the data space for recognizing images from different manifolds [69], [70], [71].
To addressing this issue, in this paper we present a novel unsupervised method, called robust structured nonnegative matrix factorization (RSNMF). Particularly, our model characterizes the global and local structure of the data space into the process of NMF and aims to find a latent representation space based on both the whole and parts. In such a new low-dimensional representation space, both the global and local consistencies of data are taken into account. To achieve this, we propose an LDA-like criterion by minimizing the local scatter and maximizing the global scatter. This makes the learned representation have stronger discriminating power. Furthermore, RSNMF imposes joint L2,1-norm minimization on both the loss function of NMF and the regularization of the basis matrix. The former is used to improve the robustness of the proposed method; the latter is used to select features across the samples with joint sparsity. We also develop an optimization scheme to alternately solve the low-dimensional representation based on the iterative updating rules of the two factor matrices. Finally, we provide the convergence proof of our optimization strategy.
The main contributions of this paper are summarized as follows.
1)We present a novel unsupervised NMF framework to explicitly exploit both the global and local geometric structures of the raw data and incorporate them as the additional regularization terms. Therefore, the proposed method is suitable for both single manifold and multiple manifold scenarios and can be applied to various fields.2)The proposed method uses the variance of the data to model the global structure and a nearest neighbor graph to model the local manifold structure. It can take into account the global and local consistencies in the process of matrix decomposition. Thus, the multiplicative updating rules for solving our RSNMF are very efficient. In such a way, our method has more discriminating power than state-of-the-art NMF methods.3)Our method imposes joint L2,1-norm minimization on both the predictive function of NMF and the regularization of the basis matrix. Thus, we use L2,1-norm not only to formulate a general loss function for NMF, but also to improve the robustness of the proposed algorithm. In addition, we add the L2,1-norm regularization of the basis matrix in our objective function to select features across all samples with joint sparsity.4)The proposed unsupervised framework can leverage the power of both L2,1-norm NMF and structure regularization. We formulate this framework as an optimal problem and subsequently develop an efficient iterative scheme to optimize it with the theoretical and experimental analyses. The new model is a general framework in which a few methods, such as L21NMF, GNMF, RMNMF, and MNMFL21, become its special cases. Besides, supervised information can be used to characterize the geometric structures in our method. This naturally leads to other extensions of RSNMF (e.g., semi-supervised RSNMF).
The remainder of this paper is organized as follows. In Section 2, we briefly reviews some related works. In Section 3, we describe the proposed RSNMF in detail and give a convergence proof of our method. Extensive experimental results on seven image data sets are presented in Section 4. Finally, we briefly provide some conclusions in Section 5.
