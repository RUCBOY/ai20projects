The utilization of radiation detection devices has had a wide range of applications, and a successful technological evolution [1]. The task of isotope identification through gamma-ray spectroscopy is a mature field that has been partially automated by incorporating full-spectrum identification algorithms, which typically include a defined library of isotopes. In spectroscopy, the goal is to find a pattern, full energy peaks in most cases, and differential count rates by analyzing the distribution of counts over a spectrum. [2] presents 12 common steps in spectral analysis which can be summarized by visualizing the information, identifying dominant features in the spectrum, aligning identified peaks with known energies, studying the vicinity adjacent to dominant peaks, identifying energies associated with other peaks (if any), choose candidate isotope for matching to selected peak(s), and iterate until a match is found. [2] also states that “existing algorithms may capably identify a peak in a spectrum, but they are restricted thereafter to comparing that peak energy to a look-up table for possible isotope matches”. Value matching is even more difficult when there is a blending of neighboring peaks.
In addition, users are not always experts; hence, they are not always able to conclusively identify the source(s). Additional challenges to perform spectroscopy outside laboratories, where the environment is not well controlled, such as detection-time limitations, shielding, unknown backgrounds, calibration, maintenance, and timely decision-making are also commonly present [3]. Machine learning (ML) and deep learning (DL) can give the detection system the capability of adapting to different radiation fields without the need for lookup tables and human experts by leveraging pattern recognition to identify characteristics or deviation in spectra information [4].
While various neural network models have reported good accuracy in gamma spectroscopy tasks, there is a trade-off between accuracy and explainability that is commonly left out [5]. If accuracy is the only metric of performance, then black-box models will be trustworthy; however, for sensitive or high-stakes applications, understanding the reasoning behind classifications or actions becomes important [6] because of the possible risk imposed to society (e.g., [7], [8], [9], [10], [11], [12]). Explainable AI (XAI) is a very active area of research [13], [14], [15] that seeks to validate learning algorithms, especially when the explanations match domain knowledge. With a prudent attitude toward research on unintended consequences, this study explores how and why a prediction is being made in a way that is comprehensive or intuitive for a domain expert to evaluate the quality of the model.
The remainder of this paper is organized as follows. Section 2 introduces the software utilized to generate the data for this study, and the concepts of convolutional neural networks and saliency. Section 3 presents the need for explainability by showing that the creation steps of a convolutional neural network tasked to classify radionuclides provides a level of intuition, but no reason behind the classification. Section 4, presents the model’s classification explanation associated with the prediction (i.e, the network’s regions of interest) in a form of saliency vectors. Section 5 shows a region of interest (ROI) comparison between a shallow and convolution network and their differences between model accuracy and its ability to show a physical characteristic associated with the classification. Moreover, suggestions on further research and challenges are presented. Finally, Section 6 provides a summary and conclusions.
