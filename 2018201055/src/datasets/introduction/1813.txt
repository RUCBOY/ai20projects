Blind image deconvolution, a.k.a. blind deblurring, is a fundamental problem in image processing, computational imaging, and computer vision. It has earned intensive attention in the past decade since the seminal work of Fergus et al. [1] for camera shake removal.
Among multiple algorithmic ingredients of blind deblurring, it is universally acknowledged that the unnatural image priors [27] are playing the dominant role in the success of reasonable blur kernel estimation. To bring out the proposed approach in a smooth way, representative image priors specifically proposed for blind deblurring are summarized in this section.
According to the literature review, blind deblurring can be divided into three categories of methodologies in general, i.e., Maximum-a-Posterior (MAP), Variational Bayesian (VB), and Representation Learning (RL). In the past several years, the RL particularly the convolutional neural networks (CNN), has been successfully applied to kinds of imaging and vision tasks. It is discovered, however, that the generalization capability of existing CNN-based blind approaches [6], [7], [8], [9], [10], [11], [12], [13], [14], [68], [69], [70], [73] is far from enough to deal with blurs in different imaging scenarios, e.g., natural, manmade, low-illumination, text, or people. In comparison, the gradient-based algorithms formulated in the MAP framework such as [3], [15] are demonstrating greater and greater potentialities in the practical deblurring tasks. Thus, this paper still follows this route and tries to further exploit the potential of gradient-based models. In specific, a simple, robust yet discriminative prior is introduced for blind deblurring. To be noted that, our discussions in this paper are limited to the spatially-invariant image deblurring. One consideration is that the performance of previous advanced non-uniform blind methods such as [16], [17] are not competitive at all to those uniform ones, as demonstrated by Lai et al. [2]. Another consideration is that an image prior for uniform blind deconvolution can be naively extended to the spatially-variant problems [3].
1.1. Existing image priors in blind deblurringFor the sake of description clarity, a short review on image priors proposed and used in blind deconvolution is made in this part. About twenty years ago, a total variation (TV)-based blind deblurring approach was presented by Chan and Wong [18], being viewed as the first modern and influential algorithm to the blind task. However, it is interesting to note that little significant progress was made since then till the exciting work of Fergus et al. [1] about fifteen years ago. It utilizes mixture of Gaussians as the natural image prior for camera shake removal rather than the TV model. The inference is made in the VB framework and afterwards simplified in [19] for better and faster blur removal. As a matter of fact, an empirical observation in [20] indicates that within the VB framework the naive Gaussian prior is even qualified for the blind task [20] to a certain degree. Nonetheless, when exactly the same image priors as the ones in [1], [19] are plugged into the MAP framework (e.g., Gaussian, mixture of Gaussians, fields of experts [21], failure results are usually produced [1], [20] such as the pair of delta blur kernel and original blurred image. Note that, another naive MAP approach [22] regularized by the framelet-based natural image prior is also reported in the risk of similar failure cases.An intuitive failure cause of those naive MAP methods is that, the harnessed natural image priors favor a blurred image to its sharp counterpart. To ensure the success of MAP-based blind deblurring, kinds of tricks have to be exploited in practice. For instance, the seminal TV-based method [18] is recently explored once again in [23], which provides an in-depth analysis on its real working principle. After ten years of publication of [18], the first creative trick for boosting it, proposed in [24], is to add shock filtering into TV minimization for better prediction of salient edges as clues to kernel estimation. Inspired by [24], the shock filtering is also applied in [25], [26] with higher quality blind deblurring results produced. Actually, as phrased in [27] the natural priors combining with shock filtering essentially plays a role of implicit unnatural sparse representation.Undergone several years of exploration since the exciting work of Fergus et al. [1] in 2006, unnatural image models have been predominating the blind deblurring literature until now. On this line, the first daring try is harnessing the normalized sparsity measure [4] with the idea that the image prior should favor a sharp image to its blurry one. Nevertheless, the method can not produce state-of-the-art performance on this or that benchmark dataset, let alone blurry images in the wild [2]. The normalized sparsity is mathematically an approximation of the L0-norm in essence, indicating that the salient edges are more important than the faint textures for the success of blind image deconvolution. Inspired by the normalized sparsity measure, a simple yet effective L0-norm-based blind deblurring approach is subsequently presented in [27].In literature, two unnatural image models [28], [29] using the Lp-norm are tried, too, wherein the parameter p is set as 0.3 in [28] and a non-increasing sequence 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.4, …, 0.4 along iteration in [29]. The TV-based method [18] is recently investigated in [30] again which proposes a nonconvex logarithmic TV prior with improved deblurring performance on some benchmark datasets. Moreover, an unnatural iteration-wise hyper-Laplacian image prior [31] is learned in the MAP framework leading to better results than existing gradient-based approaches. Although the learned prior in [31] is claimed to be discriminative, its generalization performance is questionable to blurry images in the practical diverse imaging scenarios.In fact, unnatural image priors are not only requested in the MAP framework but also advocated in the VB case in spite of its more robustness in posterior inference. In distinction to [1], [19], recent empirical and theoretical findings both prove that the Jeffreys’ prior could achieve more accurate blur kernel estimation [32], [33], and this non-informative prior is essentially similar to the logarithmic TV prior [30] to a large degree. In addition, the prior is even shown optimal to a certain degree in terms of deblurring quality [33]. Another work by the authors of the present paper has proposed to determine priors for blind image deblurring as a self-learning problem [34]. The learned model resembles the non-informative Jeffreys’ prior in a sense, whose negative-logarithm is obviously another approximation to the L0-based model.Instead of approximating the L0-norm with diverse strategies, three pure L0-based image priors [27], [35], [36] are proposed in 2013 for blind deblurring. However, they are found not generalized well to large-scale blurs especially in specific imaging scenarios, e.g., face, text, or low-illumination images. In [37], a bi-L0-L2-norm-based regularization term imposed on both sharp image and blur kernel is proposed for higher precision of kernel estimation. In [38], a L0-norm-based joint intensity and gradient prior is presented for the text image deblurring. Furthermore, an exemplar-driven approach with L0-norm-based gradient regularization is proposed in [39] for the facial image deblurring.An alternative strategy to formulate blind deblurring is to explore the patch-based priors [40], [41]. In [40], a novel idea of internal patch recurrence is exploited for the blind problem. While, in [41] the idea of modeling via external patch querying is proposed for edge-based blur kernel estimation. However, it generally takes a higher computational cost to query a large external dataset [3]. Recently, a novel patch-based approach is presented in [42] harnessing the normalized color-line priors, and shows better performance than [40], [41]. To be noted that, the text deblurring method [38] has been also extended to a patch-based scheme for handling natural image deblurring [43]. Its core idea is essentially to exploit the structural sparsity prior, implemented via a couple of rank penalty terms on similar patches over both the intensity and gradient domains.Furthermore, several other methods are proposed to improve the robustness of blind deblurring to noise [44], outlier [45], [46], and other possible degradations, e.g., light streak [47]. One may refer to [2], [48], [63] for a more comprehensive survey on image priors and other technical components in blind deblurring.According to above discussions, numerous blind deblurring algorithms have been reported in the past decade, achieving better and better performance on one or another synthetic dataset. However, as empirically concluded in [2], the performance of existing methods on the benchmark datasets is generally inferior to that on the real-world blurred images. In other words, existing blind deblurring algorithms are far from being practical in terms of the restoration quality.Actually, a real breakthrough for blind deblurring is just made recently in [3], which combines the L0-regularized sparsity on both domains of image gradient and dark channel. The experimental results demonstrate its superior performance to all the representative methods in the past decade as studied in [2]. Note that, although the L0-based dark channel prior (DCP) is discriminative as desired, the combinatorial L0-regularized model [3] is not necessarily so. That is to say, the composite prior does not necessarily prefer a sharp image to its blurred counterpart. We note that the work [3] can be thought of as a smart generalization over [38] which is not a pure gradient-based approach, either. More recently, a convolutional neural network (CNN) based classifier is learned in [71] to distinguish whether an input image is sharp or not, which is directly plugged into the MAP framework of [3] as a replacement of the L0-based DCP. In addition, an L0-based bright channel prior (BCP) is proposed in [15]. The BCP is then combined with the L0-regularized sparsity on both image gradient and dark channel of [3], expecting to achievie more robust blur kernel estimation. Note that, the two successors of [3], i.e., [15], [71], are natually with higher computational burden than [3]. Besides, the composite priors in [15], [71] are not necessarily discriminative as a whole, either.In this paper, the main motivation is to exploit the full potential of gradient-based methods, attempting to explore a simple, robust yet discriminative image prior for blind deblurring. Specifically, our contributions in this paper are three-fold: Above all, a pure gradient-based heavy-tailed prior is proposed indicating that the success of blind image deblurring requires dual principles of discriminativeness (DPD). On the one hand, since this paper formulates blind deblurring into a model-based minimization problem, the image prior should be able to discriminate between a sharp image and its blurry counterpart. On the other hand, a really good image prior for blind deblurring should be also capable of discriminating between strong edges and faint textures, so as to reduce the possible interfering effects on blur kernel estimation caused by the textures. Such an idea is actually a consensus among existing blind deblurring literature [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37]. What makes us particularly surprised is that, the above DPD required by blind deblurring can be easily implemented with the well-known gradient-based hyper-Laplacian priors [58]. To the best of our knowledge, it has been the first time that the concept of DPD is clarified and formulated for the blind deblurring problem and to be demonstrated very necessary for a blind deblurring method both understandable and applicable. In specific, inspired by the first daring try for discriminative blind deblurring in [4], our image prior essentially falls into a spatially variant hyper-Laplacian model. Secondly, a plug-and-play algorithm is derived to alternatively estimate the intermediate sharp image and the nonparametric blur kernel. With the numerical scheme, intermediate image estimation is then simplified to a simple image filtering problem. Finally, a great many experiments are performed accompanied with comparisons with state-of-the-art methods on both synthetic benchmark datasets and real blurry images in various scenarios, e.g., natural, manmade, low-illumination, text, or people. Experimental results validate well the effectiveness and robustness of the proposed approach, which is proved to be a promising new candidate solution for blind image deconvolution.The rest of the paper is organized as follows. In Section 2, a pure gradient-based discriminative image model is provided as a novel candidate solution to blind image deblurring. To test its performance, Section 3 formulates blind deblurring into an unconstrained optimization problem and solves it utilizing a plug-and-play numerical scheme. In Section 4, a series of comprehensive analysis is performed to empirically clarify the motivation of this paper in an intuitive perspective. Section 5 provides numerous experimental results so as to demonstrate the effectiveness and robustness of the proposed solution along with comparisons against the state-of-the-art methods. This paper is finally concluded in Section 6. We should note that, the present journal paper is an extensively extended version of our previous conference paper published in [72]. The difference mainly lies in the following three aspects: (1) A fairly more comprehensive overview is provided on blind image deblurring algorithms, following which the dual principles of discriminativeness are claimed for the first time for blind deblurring; (2) A fairly more detailed analysis is provided on the motivation of proposing dual principles of discriminativeness in this paper, where Section 4 is a completely new part; (3) A fairly more convincing experimental comparison has been made among the proposed approach and existing blind methods, particularly those very recent deep learning-based ones [68], [69], [70], [14], [73].
