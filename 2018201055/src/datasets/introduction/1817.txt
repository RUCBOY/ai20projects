Video segmentation is a challenging and fundamental problem that usually aims to separate the foreground and the background pixels in all frames of a given video. It has been an active area of research in computer vision over the past years, and potential applications includes video editing [1], media diagnosis [2], and autonomous driving [3].
Recently, due to developments in deep learning, image segmentation based on multiscale analysis [4] and synthesizing-based data augmentation [5] has been used to provide acceptable outputs. Contexts in spatial, temporal, and channel domains are important factors to enhance the effectiveness of existing approaches. Examples of relationships in the domains of the DAVIS16 dataset [6] are illustrated in Fig. 1. The top and middle rows show that there are many highly related regions (represented by dotted boxes with the same colors) through a temporal sequence or in a single image, and these temporal and spatial contexts enhance the robustness of the inference. In the bottom row, the feature maps in different channels are illustrated. We find that the high-value regions (in red dotted boxes) in different channels are related to different parts of the object, for instance, the foot and head of a person, and that the pairwise relations between the different parts provide additional semantic cues to refine the segmentation result. However, how to simultaneously capture long-range dependencies in spatial, temporal, and channel spaces remains an important issue in video segmentation.Download : Download high-res image (309KB)Download : Download full-size imageFig. 1. Illustrations of relations in temporal, spatial, and channel domains in DAVIS16 dataset. In the top row, frames in a temporal sequence have many highly related regions. In the middle row, regions in different positions with the same semantic concept have a similar appearance. In the bottom row, high-value regions in different channels are related to different parts of the object. .
To model the relation in a specific domain, nonlocal neural networks [7] learn long-range dependencies in the spatial domain by using the affinity between pixels. We need an approach to flexibly extend this mechanism to different spaces and to design a new method to properly combine context features from multiple spaces to enhance the discrimination capacity in pixelwise classification tasks such as video segmentation.
In this paper, we present a novel framework, called the triple attention network (TriANet), which is illustrated in Fig. 2. The temporal attention map is learned by using the representations from past frames and the current frame and captures temporal dependencies between memory information and current observations. The channel attention map and spatial attention map are obtained by means of the current image, as the spatial and channel dependencies are dynamic and have nothing to do with the historical information. Then, the feature maps in each domain are updated to be context features that contain enough semantic information for video segmentation.Download : Download high-res image (290KB)Download : Download full-size imageFig. 2. The structure of our approach. Temporal, spatial, and channel contexts are exploited by using separate self-attention networks.
The contributions of this paper are as follows:
•We present a new triple attention network with a self-attention mechanism to enhance the discriminant ability of feature maps for video segmentation.•We simultaneously exploit the temporal, spatial, and channel context knowledge by using relative lightweight networks to improve the segmentation results.
Experimental results on the Shining3D dental, DAVIS16, and DAVIS17 datasets show that our method yields satisfactory results when compared with state-of-the-art video segmentation methods.
