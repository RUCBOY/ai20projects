Video segmentation usually aims to separate, or segment, some objects in the foreground from the video background. This topic has been an active area of research in pattern recognition over the past decade, with a wide range of applications. Examples of applications include video editing [1], video summarization [2], scene understanding [3], and autonomous driving [4].
Recently, methods such as one-shot learning [5] and meta-learning [6] have been applied to video segmentation. However, progress in image segmentation has been limited by temporal contexts such as feature alignment, mask propagation, and motion exploitation [7]. This is because these temporal inferences cannot be compared with spatial inferences, which leads to ambiguity in determining where to segment an image.
We illustrate the video segmentation problem in Fig. 1. Here, the leftmost image is the input, the second and third images are the corresponding spatial (DeepLabV3+ [8]) and temporal (MaskTrack [9]) inference results, and the fourth image is the ground truth. Careful inspection of the second and third images shows that segmentation from spatial and temporal methods is very good, but not perfect. Spatial and temporal inference have their own advantages in different situations.Download : Download high-res image (253KB)Download : Download full-size imageFig. 1. Illustrations of spatial and temporal inference in the DAVIS16 data set. The first column is the input, the second and third columns are spatial and temporal inference results, and the fourth column is the corresponding ground truth.
Objects are segmented even less well than those in Fig. 1 when the video has a cluttered background or dim lighting. Such harder samples use boundary knowledge to guide the refinement. However, harder samples can only be distinguished with the aid of human resources or a time-consuming training process in machine learning [10]. Video segmentation requires an unsupervised active learning approach to identify the representative samples. To this end, we determine the uncertainty of samples by measuring the confidence value produced by spatial and temporal inference. We use this uncertainty to select the most informative samples for active learning.
In this paper, we develop a probability-based approach that uses a temporal inference such as optical flow to model the distribution in mask propagation for sample selection.
Our approach is demonstrated in Fig. 2. Image segmentation and motion inference are learned simultaneously in multitask learning. This approach is used to select an image mask, which is transferred to the next frame in a probability framework. Finally, samples with maximum uncertainty are selected. This process improves the accuracy of active learning.Download : Download high-res image (262KB)Download : Download full-size imageFig. 2. Example of how our approach works. Image segmentation and motion inference are estimated jointly, and the result is refined by means of the spatial-temporal comparison. Each segment is produced with an uncertainty value that informs segment selection over time via active learning..
The contributions of this paper are the following:
•We introduce multitask learning using a recurrent network to improve the accuracy of motion inference in multiscale analysis.•We present a novel way to use temporal context to infer the mask in the next frame via a probability framework, and we compare this result to the spatial result according to the confidence interval.•We use uncertainty in the probability mask transfer to improve sample selection for unsupervised active learning.•We have collected and annotated an oral video sequence from the Shining3D dental data set so that the approach proposed in this paper can be compared to other state-of-the-art approaches on video segmentation. We have published this data set so that anyone working on this topic may freely obtain this resource.
We show in the Results section that, in experiments with the DAVIS16 data set and the Shining3D dental data set, our approach yields results that are competitive with those of state-of-the-art video segmentation approaches.
