High-capacity deep learning models [1], [2], [3] usually have millions of parameters or even more, and most of them are built to perform supervised learning on adequately labeled training data. The data acquisition as well as the cost of manual annotations limits the potential applications of deep models in the data-scarce regime. In addition, the training and test data are typically assumed to be sampled from a set of pre-defined categories. Generalizing the models trained under this assumption to unseen categories with few annotated examples is highly challenging. Humans have a strong ability to learn from limited data, while the data-driven deep models are still far from such level of generalization capability. Few-Shot Learning (FSL) aims to learn new visual concepts with very limited examples and generalize well to their variants. To reduce the dependence on large amount of training data, the mainstream FSL methods are based on meta-learning [4], [5], [6], [7], [8], [9], [10], [11]. This is a task-level learning paradigm. To obtain a generic model across different tasks, a large number of similar few-shot tasks involving different classes are constructed in the learning process. A meta-learner learns to adapt different tasks. This paradigm has shown the effectiveness of generalizing deep neural networks to unseen tasks with only few labeled examples. To maintain the model generalization, low-complexity neural networks are typically used for meta-learning. However, a limited number of instances per class may not sufficiently express a visual concept. Combining FSL and semi-supervised learning is an attractive choice, since this will enable meta-learning to benefit from the available unlabeled data in exploring the true data distribution without extra category information. There are a number of relevant methods proposed recently [12], [13], [14]. In these methods, a pseudo label is assigned to each unlabeled instance according to the prediction in a specific episode. We argue that the iterations between pseudo-labeling and re-training are more likely to cause the issue of error propagation. This motivates us to construct more reliable training targets (not necessarily pseudo labels) for unlabeled instances and further augment the training data, thereby improving Semi-Supervised Few-Shot Learning (SSFSL).
In this paper, we propose a Behavior Regularized Prototypical Network (BR-ProtoNet) for SSFSL. We focus on regularizing the model’s behavior to facilitate metric learning. Due to the episodic training pattern, we map the label space of each episode to its corresponding in the whole training set, and thus aggregating the class label prediction of each unlabeled instance in different episodes. The aggregated prediction can be expected to be more reliable than the pseudo label based on the prediction in a specific episode. We require the model to maintain consistent predictions on unlabeled instances during the learning process. On the other hand, a suitable embedding space should have the property of proximity preservation. Toward this end, we construct new instances by applying adversarial perturbation on the labeled instances and prevent the model from abruptly changing its outputs on them. Further, we constrain the model to have a linear behavior along the interpolation paths between different instances, based on the similarities to the end points. As a result, the underlying data structure in the embedding space can match with that in the original data space. This is critical for metric learning. We incorporate the regularization of these three aspects into a meta-learning process. An overview of the proposed SSFSL model is shown in Fig. 1.Download : Download high-res image (487KB)Download : Download full-size imageFig. 1. An illustration of the proposed behavior-based regularization approach for SSFSL. BR-ProtoNet consists of an embedding network f and a class prototype-based classifier. To exploit unlabeled data, we adopt a pair of transformations R∧ and R∨ to match the label spaces between each episode and the whole training set. Consequently, the aggregated prediction A over training episodes can be computed and used as reliable training targets. We further construct two types of new instances via adversarial perturbation ρadv and interpolation, such that the model’s behavior is regularized over the neighborhoods of the data points and interpolation paths among them. The regularization of these aspects encourages the learnt embedding space to possess the property of proximity preservation. The loss functions Lcon, Lpert and Leval will be described in Section 4.
Contributions. The main contributions of this work are summarized as follows: (1) We propose BR-ProtoNet to enable metric learning to benefit from unlabeled data in semi-supervised scenarios. (2) Training episodes are formed by subsampling classes as well as instances, and they thus associate with different label spaces where variables represent the selected classes. We transform the predicted class probability distributions from the label space of an episode to that of the whole training set, such that the transformed results can be aggregated over different episodes to provide more reliable category information. (3) We construct effective and complementary constraints to regularize the model’s behavior, such that the embedding space is encouraged to possess the property of proximity preservation. (4) BR-ProtoNet improves the previous state-of-the-art results on multiple standard SSFSL benchmarks, especially for the challenging cases where there are distractors in both training and test data. Meanwhile, it can be easily trained end-to-end over a sequence of few-shot tasks without iterations between pseudo-labeling and re-training.
