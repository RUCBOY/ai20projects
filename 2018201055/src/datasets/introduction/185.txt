With the advancement of Web techniques, sequential recommendation has become numerous and multi-lateral in various applications, such as music playlist recommendation, product purchase recommendation, and ads click prediction. A user’s behavior in such applications can be modeled as a sequence of actions in time order, and his/her subsequent actions are influenced by his/her previous actions. Thereby, it is of great importance to model the complex sequential interactions between users and items for generating personalized recommendations.
Markov Chain (MC) based models [1], [2], which assume user’s next behavior is conditioned on the previous one (or previous few), are the early approaches for sequential recommendation. With such a strong assumption, MC-based methods may have difficulties to capture the intricate dynamics of complex scenes. In recent years, neural models such as Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) have been intensively studied and made extraordinary impacts on sequential recommendation. RNN models long-term dependencies through recurrent architecture [3], [4], while CNN can implicitly capture local features with convolutional operations across the input sequence [5], [6]. However, RNN-based and CNN-based methods tend to compress a user’s historical records into a fixed hidden state vector, which fails to explicitly capture item-item interactions, regardless of the distance between two items in the sequence.
More recently, Vaswani et al. [7] proposed a Self-Attention Network (SAN) for sequence-to-sequence (seq2seq) modeling. The seq2seq model, called “Transformer”, is composed of stacked self-attention network without using either recurrence or convolution, which has more flexibility in sequence length than RNN/CNN and is more task/data-driven when modeling dependencies. Besides, computing attention over an entire sequence only requires matrix multiplication, which can be fully parallelized compared to the sequential computation of RNN. Moreover, the self-attention mechanism has been applied to sequential recommendation and achieved satisfactory results, some examples are SASRec [8] and CSAN [9]. However, existing self-attention based methods model dependencies without considering the positions of sequences, resulting in the loss of temporal order information. So they are inadequate in characterizing and distinguishing user’s dynamic and evolving long-term preference and short-term demand, which are both crucial for sequential recommendation [10].
To this end, we propose a novel multi-layer Long- and Short-term Self-Attention network (LSSA) for sequential recommendation, which takes into account the user’s long-term taste and current interest simultaneously. The self-attention mechanism can not only keep the contextual sequential information, but also explicitly invoke item-item interactions across the user’s entire historical sequence regardless of their distances. In LSSA, we use a term “long set” to denote all historical actions prior to a user’s current interaction behavior, and use a term “short set” to represent the user’s current interaction actions. As such, the different roles that each action may play in predicting the user’s future behavior can be further distinguished. Specifically, we first embed all unique items into a low-dimensional latent space. Next, the self-attentive representation built on top of the “short set” is regarded as the user’s short-term interest. After that, we use another self-attention layer to capture the long-term preference from the historical and current actions (i.e., the “long set”). Then, a joint learning function is applied to train the user’s long- and short-term preferences separately. Finally, we integrate the long- and short-term representations together to form the user’s final hybrid representation for prediction. In addition, due to the short-term preference of the user, the training method of dividing the sub-sequences as input further improves the recommended performance to some extent, compared to the training method of inputting an entire sequence at a time. Extensive experiments show that LSSA outperforms the current state-of-the-art models (e.g., SASRec and TransRec) on three datasets by margins ranging from 8.00% to 18.23% (and 12.25% on average) in terms of HR, NDCG, and MAP.
In summary, the main contributions of this paper are listed as follows:
•We propose a novel framework, long- and short-term self-attention network (LSSA), for sequential recommendation. The proposed model applies self-attention network on sub-sequences that are partitioned by timespan and takes into account both the user’s long-term and short-term interests in the current session.•We introduce a joint learning mechanism, which combines the self-attention information on long-term and short-term item sets, to capture the user’s complex and dynamic preferences.•We conduct extensive experiments on three real-world datasets. The results demonstrate the superior performance of the proposed model LSSA comparing with other state-of-the-art baselines in terms of HR, NDCG and MAP.
The rest of this paper is organized as follows. We begin by reviewing some related work of this study in Section 2. Then in Section 3, we introduces basic definitions and the formal problem statement. We present our proposed model LSSA for sequential recommendation in Section 4. Experiments results and analysis are shown in Section 5. Finally, we conclude the paper in Section 6.
