Nowadays, deep neural networks (DNNs) have achieved successes in many applications, such as pattern recognition and computer vision [1]. Yet, designing a high performance neural network is still challenging, in that it is impossible to explore the whole architecture space and it usually needs a lot of expertise and learning time [2]. Hence, how to automatically generate well-behaved DNNs has become an interesting and critical problem.
In recent years, many methods have been proposed for automatic deep architecture learning [3], [4], [5], [6], [7], [8], [9], [10], [11]. Zoph et al. proposed the network architecture search (NAS) framework with reinforcement learning algorithms [3]. In NAS, a controller (usually represented by an RNN) enumerates a set of deep architectures, and then they are trained from scratch for a fixed number of epochs and evaluated on a validation set. Subsequently, the controller is updated based on the validation accuracies of the trained deep models. Following NAS, a series of approaches have been proposed to automatically construct deep architectures, such as the work presented in [2], [4], [5], [6], [12]. Other than reinforcement learning, evolutionary algorithms and gradient-based methods are widely applied for deep architecture learning [7], [8], [13], [14], [15]. Despite their promising results, these methods usually train hundreds or thousands of models in a single trail, which is computationally expensive with respect to both computing resources and time [3], [5], [7], [10], [16]. For instance, Zoph et al. utilized 1800 GPU-days with a reinforcement learning method [3], and Real et al. took 3150 GPU-days with an evolutionary algorithm [7]. A core problem is that they train each candidate model in a basically same fashion regardless of the model performance, namely, all candidate models are trained with very similar settings and same epochs, which may result in high computational demands.
In this paper, we propose a DNA computing inspired method called DNAND for automatic neural architecture design. DNA molecules can be regarded as a set of sequences over the alphabet Ω={A,G,C,T}, where “A, G, C, T” refer to the 4 types of nitrogenous bases: adenine, guanine, cytosine and thymine, respectively. The nitrogenous bases of two separate DNA strands are bound together to form double-stranded DNA according to the base pairing rules (A¯=T, T¯=A, C¯=G, G¯=C). A double-stranded DNA usually has the head and tail parts that are bare single-stranded. The exposed head and tail strands are used to bind with other DNA strands to form longer ones through pairing bases. In DNA computing, the double-stranded DNA chain is antiparallel: one single strand with forward direction from 5’ to 3’ is called the encoding strand, and the other single strand with reversed direction from 3’ to 5’ is the complementary strand. For example, Fig. 1 shows a simple double-stranded DNA, where 5’-ACCTT-3’ with head ‘TT’ is the encoding strand, and 3’-TGGAA-5’ with tail ‘GG’ is the complementary strand. To avoid confusion, we call double-stranded DNA directly as DNA strand for short in the following while the head and tail are specifically introduced if necessary. During the DNA computing process, huge amounts of DNA molecules, which are encoded with necessary information, react with each other in a biochemical way, and each final long DNA strand has a certain probability to represent a feasible solution for an optimization problem [17], [18], [19], [20]. The core of DNA computing is that it uses numerous DNA molecules’ reactions to solve large-scale computational problems. Thus, it is quite natural to employ DNA computing to find feasible solutions for the automatic design of DNNs, as it is essentially a large scale search problem.Download : Download high-res image (115KB)Download : Download full-size imageFig. 1. An antiparallel double-stranded DNA chain. The single strand ‘ACCTT’ with the forward direction (5’–3’) on the left is the encoding strand, and the other single strand ‘TGGAA’ with the reversed direction (3’–5’) is the complementary strand. Their exposed head and tail strands are used to connect with other DNA strands to form longer ones.
In the proposed DNAND algorithm, we use DNA strands to represent the basic building blocks of a neural network, and call them block strands. The block strands are composed of layer fragments, which represent the convolutional layers. In concreteness, we use Ω={A,G,C,T} to encode the structural information of each convolutional layer. Hence, a layer fragment can be represented by a DNA sequence such as “AAGCG…”. Fig. 2 illustrates the DNA encoding of a network block, which consists of 6 layers. The overall deep neural networks are represented by long DNA strands called architecture strands, which are obtained by the biochemical reactions among block strands in terms of the base pairing rules. After gaining the architecture strands, we translate them into real DNNs and train them using a GPU server. In addition, to alleviate the high computational demands problem and accelerate the model training, we propose the killing strategy to discontinue the training of some models in advance if their validation performances are“bad”. The experimental results demonstrate that the killing strategy can save huge computational resources with few influences on the effect of architecture search. We have conducted experiments on 4 image classification data sets, i.e., the MNIST, CIFAR-10, SVHN and ImageNet data sets, obtaining best top-1 accuracies of 99.73%, 97.12%, 98.02% and 74.10%, respectively. We also conducted experiments on the COCO data set and achieved a detection result of 22.4 mAP.Download : Download high-res image (434KB)Download : Download full-size imageFig. 2. DNA encoding scheme of the network block. (a) A block strand that can be translated to block architecture as shown in (b). This block strand is composed of 6 layer fragments (−⑤). (b) Network block corresponding to the block strand on the left.
The main contributions of this work can be summarized as follows:
•Inspired by the idea of DNA computing, we propose a DNA computing inspired NAS method, named DNAND. As far as we know, this is the first work using DNA computing algorithms for NAS.•We propose the killing strategy, which can greatly reduce the computational cost, while have few influences on the result of NAS.•We have conducted extensive experiments on the typical MNIST, CIFAR-10, SVHN, ImageNet and COCO data sets for object recognition and detection tasks. The experimental results demonstrate the effectiveness of DNAND.
