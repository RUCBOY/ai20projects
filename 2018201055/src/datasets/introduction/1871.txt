Learning is made up of four components: knowing, planning, practicing and reflecting (Johnson, 2000). Planning is considered as a substantial part of learning. Farrell (2002) argued that an effective lesson plan can achieve lesson objectives which are stated appropriately and clearly. Maconie (2006) stated that, as in many disciplines, it is vital to focus on the task and maintain this in achieving the educational goals. It is already known that there are a great number of different lesson plan stages. Hunter (1984) advocated the seven-stage lesson plan, and Johnson (2000), on the other hand, the four-stage lesson plan. Farrell (2002), however, explained the general components of a lesson plan in five stages: general appearance, encouragement, learning/participation, closure and follow-up. As the lesson plan is essential in the learning process, so is the time allocated to the components in this plan because the typical student's attention span is admittedly not very long. Attention period is defined as the time spent by a person focusing on a task (Beger, 2018). In their literature scan, Wilson and Korn (2007) expressed that the students' attention began to decline after 10–15 min. Lloyd (1968) also stated that understanding peaked in the first 5 min, decreased after 10 min and rose again between the 45th and 50th minutes (as cited in: Wilson & Korn, 2007). Frost (1965) found out that attention was higher in the first 10 min, and that the attention of 10% of the students declined in about 15 min (as cited in: Wilson & Korn, 2007). There are various estimates regarding the continuous attention span for various age groups (Bradbury, 2016; Schaefer & Millman, 1994). Cornish and Dukette (2009) maintained that continuous attention span is up to 8 min, and that this duration varies between 3 and 5 min in children and maximum 20 min in an adult. There have been several studies conducted on the measurement of attention span. There are various studies such as the relationship between the amount of note taken down by the students and attention span (Hartley & Cameron, 1967; Maddox & Hoole, 1975; McKeachie, 1986; McKeachie & Gibbs, 1999), the relationship between the amount of information remaining in the memory of the students at the end of the lecture and the duration of the lecture (McLeish, 1968 cited: Wilson & Korn, 2007), and the relationship between the attention span and the number of heart beats per minute (Bligh, 2000).
Today, the studies reveal that, born and grown up in the digital age, the individuals in the new generation called Generation Z started to use computers at a very young age and have spent a considerable amount of time with digital tools (Oblinger & Oblinger, 2005). It can also be argued that the attention span of the Generation Z, whose expectations differ according to other generations, is also likely to vary. Therefore, in this study, we examined how the emotions of the students changed during a lecture. This research was carried out by means of computerized face analysis technique unlike the previous studies.
Although it was considered, prior to the 1960s, that the facial expressions varied according to the cultures, many studies have revealed that people from different cultures interpret facial expressions in the same way. It was suggested that the facial expressions of disgust, fear, joy, surprise, sadness and anger are universal (Ekman, Rolls, Perrett, & Ellis, 1992). The images of these facial expressions are illustrated in Fig. 1 (De la Torre and Cohn, 2011; Lee, 1994).Download : Download high-res image (369KB)Download : Download full-size imageFig. 1. Basic facial expression phenotypes 1: Disgust 2: Fear 3: Happiness 4: Surprise 5: Sadness 6: Anger (De la Torre and Cohn, 2011).
There are different approaches in determining facial expressions. One of these approaches is called facial electromyography (EMG) and is performed by placing facial electrodes on different parts of the face, which allow to measure the electrical current from muscle tissues through the skin. In another approach, facial movements can be examined individually or in combination via video recordings. Facial Action Coding System (FACS), developed by Ekman and Friesen for this purpose (1976), enables the identification of visually distinguishable facial movements. In this method, 46 action units are defined for each individual muscle activity of the face. Today, it is possible to perform facial expression analysis with the help of computers as a result of increased processing capacity of computers, having faster computers and more data. As can be seen in Fig. 2, facial expressions result from the change of facial parts such as mouth, nose, eyes and eyebrows that make up facial expressions. Computer-assisted automatic facial expression analysis involves finding the face in the image taken from the camera, establishing the attributes of the facial expressions from the face image and interpreting them (Bayrakdar, Akgün, & Yücedağ, 2016).Download : Download high-res image (241KB)Download : Download full-size imageFig. 2. Facial analysis points (Kim et al., 2015).
There are several applications of automatic facial expression recognition in the literature. Kambayashi, Diago, Kitaoka, and Hagiwara (2010) monitored the facial expressions of the drivers to detect fatigue and stress that are thought to cause traffic accidents, and Hachisuka (2013) determined a driver's drowsiness based on facial expressions. Sezgin, Davies, and Robinson (2009) examined the drivers' response to audial and visual stimuli by using speech recognition and facial expression detection methods, and found the performance of the detection based on speech recognition to be very poor in noisy environments, while the performance of the detection based on video was reasonably accurate regardless of ambient noise level. Chickerur and Joshi (2015) studied images from three-dimensional models, suggesting that it would not be possible to obtain a reliable result from the images taken by traditional two-dimensional cameras due to lighting conditions and people's postures. In their study, Krithika and Lakshmi Priya (2016) developed a program to determine the emotions of the students in the e-learning environment by monitoring the head, lip and eye movements. Yang, Alsadoon, Prasad, Singh, and Elchouemi (2018) argued that the automatic facial expression recognition system they developed could monitor students' emotions in distance education applications and allow teachers to develop learning strategies according to the students' emotions. Sharmila and Kalaivani (2018) stated that written and verbal feedback from the students may not give correct information to the lecturer. In order to make the lecture more interactive, they analyzed the video in which they recorded the students' facial expressions during the lecture by using the Support Vector Mechanism method. However, Tang, Xu, Luo, Zhao, and Zou (2015) used K-nearest neighbor (KNN) and achieved an accuracy of 79%. In a study, real-time automatic participation of students from facial expressions was defined. Facial expressions of the students were collected and analyzed during a cognitive training task (Whitehill, Serpell, Lin, Foster, & Movellan, 2014). In some studies, the definition of facial expression recognition in distance learning courses was studied (Cheng Lin et al., 2013; Kim, 2017). Sahla and Kumar (2016) propose a system using deep convolutional neural network technique for students emotions in the classroom. In some studies, different methods of recognizing facial expressions were studied (Liang, 2019; Mao et al., 2019). In another study, a theoretical basis for learning the facial expressions of the students in the classroom from the video and the automatic evaluation of the teacher learning in the classroom was formed (Pan et al., 2018). In the Boonroungrut, Oo, and One (2019) study, cloud-based Facial emotion analysis was conducted to investigate students' feelings in the classroom. For this purpose, the basic chinese course lasted for 5 weeks and included 29 international students. Students' mood changes were examined. In a secondary school in China, it was reported that a similar system was used to monitor students' moods and provide feedback to the teacher (VanderKlippe, 2018). In another study, images were taken via webcam to determine the emotion and alertness of the students. In the study, a system was proposed to help the students to increase the productivity of the students and to provide appropriate interaction and feedback (Happy, Dasgupta, Patnaik, & Routray, 2013). Ayvaz, Gürüler, and Devrim (2017) tried to classify some physiological values in facial images with classification algorithms such as CART, RF, kNN, SVM by performing facial expression analysis of the participants in the e-learning session conducted via Skype software with Facial Emotions Recognition System they developed. At the end of the classification process, they calculated the Happiness, Fear, Sadness, Anger, Surprise and Disgust emotions which were accepted as universal. They have determined the SVM algorithm as the most appropriate method for this process. Khalfallah and Slama (2015) examined anger, sadness, surprise and happiness emotion rates in a virtual laboratory environment. Petrovica and Ekenel (2016), on the other hand, examined the studies on this subject.
Recent developments in neurology have shown that there are connections between emotions and cognitive, and audial functions, meaning that there is a relationship between learning and emotion (Immordino-Yang & Damasio, 2007). Studies indicate that students' emotions are vital during the lecture (Krithika & Lakshmi Priya, 2016).
Wlodkowski(1999) stated that emotions have a negative or positive effect on motivation and therefore emotions should be taken into consideration during learning. Linnenbrink-Garcia and Pekrun (2011) stated that in recent years, the effect of student emotions on learning and class participation has been studied. Cleveland-Innes & Campbell, 2012 state that emotions can help or prevent learning based on previous studies. Emotion can limit learning as a distraction, but if managed, it can also play a supporting role in thinking, making decisions, encouraging and directing (Cleveland-Innes & Campbell, 2012). Different emotions are known to cause different outcomes. While some positive emotions are positively associated with intrinsic motivation, effort, self-regulation and more sophisticated learning strategies (Pekrun, Goetz, Frenzel, Barchfeld, & Perry, 2011), negative emotions such as anger are associated with anxiety and boredom, reduced effort, poor performance, increasing external regulation, and reducing self-regulation strategies (Artino, 2009; Daniels et al, 2009; Pekrun et al, 2009). In a study, it was found that anger and anxiety occurred in children when children perceived their teachers as the direct control object. It was determined that these emotions increased external motivation, which reduced academic participation and encouraged limited participation (Assor et al., 2005). In further studies, anger was defined as the strongest individual predictor of student achievement (Kim, Park, & Cozart, 2014), and It can stimulate the learning experience or give people joy to blind their learning difficulties (Dirkx, 2008). Students who adopt a performance approach experience sadness, anxiety and anger when the goals are not achieved (Pekrun, Elliot, & Maier, 2009). Another feeling is surprise. Surprise is a short-lived feeling. Therefore, it has less effect on increasing persistence in education than emotions of confusion, frustration and boredom (D'Mello & Graesser, 2011). Another feeling is fear. This feeling in the face of uncertainty occurs in students in the online environment (Zembylas, 2008).In the classroom, it was determined that the lack of communication between teachers and students increased emotions such as anger, anxiety, boredom (Mazer, McKenna-Buchanan, Quinlan, & Titsworth, 2014). It was determined that effective teacher communication provided positive emotions such as enjoyment, hope, and pride (Titsworth, McKenna, Mazer, & Quinlan, 2013). In another study, negative feelings such as uch as anger, anxiety, shame, hopelessness, and boredom were related to students' academic achievement, learning organization, and learning strategies (Pekrun, Goetz, Frenzel, Barchfield, & Perry, 2011). In this study, the facial expressions of the students were examined in terms of disgust, sadness, happiness, fear, contempt, anger and confusion by using Microsoft Emotion Recognition API, and the changes in their expressions during the lecture were determined to answer the following questions:
1.How do the students' emotions change during a lecture?2.Do students' emotions differ significantly during a lecture according toa.Their departmentsb.Their gendersc.The class timed.The location of the computer in the classroome.The lecture typef.The session information?3.Is there a significant difference in the emotions of the students according to the lecture stages?
Such a study performed with the use of computer-assisted automatic facial expression recognition system is of importance in terms of being based on quantitative data and eliminating the deficiency determined in the literature. With the help of some concrete data acquired in the study, it will be possible to have an idea about the changes in the emotions of the students on the basis of time during the lecture, to give feedback to the educator about how long after the lecture starts their attention decreases due to the change in their emotions, and to contribute to the increase in the quality of education.
