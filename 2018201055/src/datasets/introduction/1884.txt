A software router, which is built on a hardware platform based on a commercial off-the-shelf (COTS) computer, becomes feasible because of recent advances in multi-core CPUs and fast networking technologies for COTS computers. For notation simplicity, hereinafter, a COTS computer is simply referred to as a computer. Fast IP packet forwarding has been enabled by storing data structures for IP forwarding on a fast Static Random Access Memory (SRAM) device [1], [2]. Compact data structures have become a research issue due to the need keep up with the increasing number of IP prefixes in the Internet. Most of the studies have addressed compact trie-based data structures like multi-bit trie data structures [3] by replacing consecutive elements in a trie with a single element. Such efforts enable it to store the increasing number of IP prefixes, e.g., 7 × 105 prefixes [4] on the latest SRAM device.
Recently, studies on high-speed algorithms and compact data structures are being revisited due to the emergence of a new Internet architecture called Named Data Networking (NDN) [5], wherein rich data including large video data and small sensor data are delivered by a single architecture. Fast NDN packet forwarding is not trivial compared with IP packet forwarding in terms of memory space because name-based forwarding needs a larger forwarding table than IP one in order to store about 2.1 × 108 [6] name prefixes and per-packet caching requires additional memory spaces to store packets. Therefore, NDN data structures need to be stored on a slow Dynamic Random Access Memory (DRAM) device rather than a SRAM device even if compact trie-based data structures are used to store the forwarding table [7]. This implies that hiding the latency to access NDN data structures on the DRAM device is the key to fast NDN packet forwarding, rather than compacting such data structures.
According to this implication, in our previous paper [8], we first identified DRAM access latency of NDN data structures as a true bottleneck on a current state-of-the-art NDN software implementation [9] through an analysis conducted at the level of the CPU instruction pipeline. We proposed a prefetch algorithm of NDN data structures to hide such latency. The key idea is to handle multiple consecutive packets in a batch so that data prefetch of such data structures from a DRAM device overlaps with the computation of other packets. We developed the prefetch algorithm for two consecutive packets and experimentally demonstrated that the prefetch algorithm successfully hides most DRAM access latency so that the NDN packet forwarding rate linearly increases as the number of CPU cores increases.
In this paper, we extend our previous studies [8], [10] in terms of algorithm design and performance analysis to evaluate the prefetch algorithm clearly and in depth. The main contributions of this paper can be summarized as follows:
•The design rationale of the prefetch algorithm is strengthened by carefully choosing an appropriate data structure for hiding DRAM access latency. We evaluate three representative data structures, a hash table [9], a trie [7] and a bloom filter [11], for name-based forwarding, from the perspective of ease of hiding DRAM access latency. The metric of evaluating the easiness is defined as the number of dependent DRAM accesses, which should be sequentially performed. The quantitative comparison of the three data structures reveals that a hash table incurs the smallest number of dependent DRAM accesses, and thus a hash table is the best data structure among the three for hiding DRAM access latency.•We design a prefetch algorithm for hash table-based forwarding tables by handling two consecutive packets in a batch. This algorithm hides DRAM access latency for accessing a hash entry, which is difficult to be hidden by the prefetch instruction alone when packets are handled packet-by-packet.•We design a sophisticated prefetch algorithm for Longest Prefix Matching (LPM) in the FIB, which is potentially time-consuming. The algorithm avoids computations of calculating memory addresses of FIB entries whose prefixes are shorter than the matching prefix.•We evaluate representative cache eviction and admission algorithms from the perspectives of both hiding DRAM access latency and achieving high cache hit rates. The evaluation reveals that TinyLFU [12] with FIFO eviction is an appropriate cache algorithm because this combination simultaneously achieves the small number of dependent DRAM accesses and high cache hit rates.•We experimentally show that processing two consecutive packets in a batch is sufficient for hiding DRAM access latency on modern computers. We also experimentally show that increasing the number of consecutive packets hides DRAM access latency even in the case where DRAM access latency becomes large.
The rest of this paper is organized as follows. First, we identify a true bottleneck for high speed forwarding based on the instruction level analysis in Section 3, after explaining a state-of-the-art software NDN implementation used for the analysis in Section 2. In Sections 4 and 5, we choose data structures and algorithms for name-based packet forwarding and caching among the existing algorithms from the view-point of the ease of hiding DRAM access latency. Then we design a prefetch algorithm and evaluate it by implementing a proof-of-concept prototype in Section 6. We briefly introduce related work in Section 7, and finally conclude this paper in Section 8.
