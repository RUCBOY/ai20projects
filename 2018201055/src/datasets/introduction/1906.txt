Breast cancer is a common disease for women and is considered to be the second leading cause of death worldwide [1]. According to breast cancer now [2], breast cancer is the most common cancer in the UK. Ultrasound is the complementary modality to the standard imaging method (two view mammography) in breast cancer diagnosis [3], [4]. It is the most widely used in clinical practice [5] compared to other alternatives such as tomosynthesis and magnetic resonance imaging. Due to the fact that early detection plays a main role in avoiding breast cancer deaths and increases the proportion of healing and recovery, there has been increasing interest in using ultrasound to aid in the early detection of breast cancers over the past few years [6], [7].
In breast ultrasound (BUS), radiologists are trained in interpreting the sonographic features [8]. In current practice, the clinician scans the breast and takes static images. The radiologist will assess and annotate the BUS images. Computer aided diagnosis (CAD) systems are then can be used as a “second reader” for computerised medical imaging analysis [9]. These systems are based on the assumption that the radiologist detects an abnormality and preselects a region of interest (ROI). Fig. 1 shows BUS images with manual pre-selected ROIs marked with ‘+’ and ‘x’.Download : Download high-res image (368KB)Download : Download full-size imageFig. 1. Examples of BUS images with manual pre-selected ROIs marked with ‘+’ for the upper and lower points for the lesion, and ‘x’ for the leftmost and rightmost points of the lesion. Please note that the annotations were embossed for better visualisation.
Previous work attempted to automate the process of ROIs selection [10], [11], [12], [13]. These methods were based on multi-stage image processing and/or machine learning approaches. Deep learning has gained popularity in biomedical image analysis and has achieved good results in classification [6], [14] and BUS semantic segmentation [15]. Yap et al. [7] compared the performance of lesions detection algorithms and showed that deep learning approaches are more accurate and robust across datasets. However, the limitations of their work were: (1) they detected the lesions by using segmentation approaches but not an object detection approach; and (2) they evaluated the performance based on detected point (centre of the segmented region) [7], not the overlap of the regions.
According to state-of-the-art BUS lesion detection [6], [16], a ROI is defined as a bounding box circumscribing the lesion. This paper focuses on the automatic detection of such ROIs. We propose the use of the Faster-RCNN Inception-ResNet-v2 approach [17] for BUS lesion detection. The key contributions are:
1.We automate the ROI detection using a popular deep learning approach, this is the first attempt in automation of BUS ROI detection using Faster-RCNN Inception-ResNet-v2.2.We propose two approaches to overcome the issue of lack of BUS data. First we apply a transfer learning approach and then we propose a new 3-channel artificial RGB method to improve the quality of results.3.We evaluate and compare the performance of our proposed method on two datasets – within individual datasets and composite dataset. As existing approaches do not focus on ROI bounding box detection, we compare the performance of our proposed methods with FCN-AlexNet.
