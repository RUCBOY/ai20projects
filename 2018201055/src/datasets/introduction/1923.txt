Recent advances in Convolutional Neural Networks (CNNs) have made it possible to face highly complex visual-related tasks, such as image classification [1], [2] or object recognition [3], [4]. Their biological inspiration by the visual cortex has open a plethora of possibilities for these models, solving problems hitherto reserved for the human brain. The applications of these models are expanding every day, showing important progress towards solving problems of very different nature. Besides, the use of these models in certain domains, such as autonomous cars or medicine, inevitably implies the need to ensure that the models are correctly trained and the performance is high [5]. Unfortunately, an effective training of these models entails a large amount of computational resources.
An investigation performed in 2018 by two researchers at MIT [6] showed that big and complex neural networks, such as fully-connected and convolutional feed-forward architectures, can be reduced to smaller architectures without sacrificing performance. These researchers called this The Lottery Ticket Hypothesis. In this paper, we leverage this hypothesis to assume that already trained Convolutional Neural Networks can be further optimised to obtain minimum-size models with improved performance. Previous research has already demonstrated that it is feasible to compress deep neural networks without compromising the performance [7]. Particularly, we focus on the last set of layers located after the last convolutional block. This last group, typically composed of fully connected and dropout layers, is responsible for inferring knowledge from the features processed by the convolutional blocks. The ultimate goal is to make an effective and generalisable relation between these features and the corresponding correct class for each input instance.
In this paper, we exploit a Coral Reef Optimisation based metaheuristic [8] to reconstruct from scratch the set of fully connected layers in CNNs architectures. The goal of this combination is to obtain a new model of reduced size and better performance in comparison to the original model assuming that:

•The metaheuristic is able to perform a more in-depth exploration of the search space, which helps to find better solution positions in comparison to a gradient descent based approach.•The use of smaller architectures contributes to better generalisation capacity, avoiding potential overfitting effects.•Light architectures are required in order to deploy CNN models in computationally restricted devices.
The metaheuristic implemented in this research extends the concept of Neuroevolution, a term which makes reference to the generation of Neural Networks by means of evolutionary algorithms. In this research, this search process is addressed by a modified version of the Coral Reef Optimisation (CRO) algorithm, which has important similarities with evolutionary algorithms and has proven to be powerful when facing hard optimisation problems [8]. CRO is bioinspired by the growing and reproduction of coral colonies, where there exist a fight for space in the reef. In order to avoid to set the different hyperparameters (e.g. the number of corals which reproduce), a statistically-driven version of this algorithm, called SCRO [9], creates partitions of the population according to statistics calculated from the average fitness of the whole reef. Different operators are then applied to these partitions.
The SCRO metaheuristic presents a powerful instrument to evolve CNN models, avoiding the need for fixing hyperparameters. However, the search process may face difficulties in making minor adjustments (i.e. a small increment in a particular connection weight), which can slow down the convergence. To solve this issue, two different modifications have been deployed. In the first place, we have extended SCRO to build a Hybrid SCRO (HSCRO) algorithm. The novelty of this new method resides in including a final hybridisation process where the best solution found by the metaheuristic on the last generation is optimised using a backpropagation algorithm in order make a final fine-tuning training [10]. On the other hand, the mutation operator has also been upgraded in order to perform a stratified operation, which implements both a structural and a parametric mutation.
Literature so far related to the use of a metaheuristic for the optimisation of Convolutional Neural Networks architectures presents some limitations. Most of this existing research focus on topology or hyperparameters search. We specifically focus on the last section of any CNN architecture to build a novel optimisation approach. In summary, this research presents the following contributions:

•A new Hybrid Statistically-driven Coral Reef Optimisation (HSCRO) algorithm, which is a hyperparameter-free metaheuristic including a final hybridisation process to perform a fine-tuning parameters adjustment.•A novel approach to make a complete redefinition of the fully connected layers located in the last block of Convolutional Neural Networks, including, topology, hyperparameters and parameters (connection weights).•The approach aims at generating a new lighter architecture (in terms of number of parameters) with improved performance.•An architecture-independent approach. In this research, all the experimentation has been performed targeting the VGG-16 architecture, which has been largely used in the literature, showing excellent results in varied domains. However, the approach can be exported to any other CNN architecture.
The article has been organised as follows: Section 2 describes the state-of-the-art literature, introduces important background concepts and puts all this in relation to this research. Section 3 presents in detail the approach which this research focuses on. Section 4 and Section 5 describe the experimental setup and the experiments performed, respectively. Finally 6 presents a series of conclusions and some possible lines of future work.
