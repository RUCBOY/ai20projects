In the recent decade, deep learning has achieved great success from compute vision to natural language understanding and other machine learning tasks. A typical example is the continuously improved benchmarking performance on ImageNet dataset [1], [2] with convolution neural networks. In addition, it also achieves great success with deep Q networks (DQN) for reinforced learning tasks on Atari series games [3], [4]. Similarly, on several large-scale natural language understanding tasks, deep models like transformers [5] far surpassed classical models and obtain close-to or even higher performance than human experts. This however attributes to the better representation capabilities via neural networks [6], [7]. Due to the improved understanding of neural networks, new methods are evolved to improve/stabilize the learning capabilities of deep models, such as better initialization of neural layers, better activations for representations [8], better architectures for representation learning [9], [10], more effective regularization methods like dropout [11], [12], normalization methods [13], [15], as well as richer feature extraction methods over the entire neural networks.
A typical CNN network is consisted of many blocks, each block is with one convolution layer, one normalization layer, and one activation layer. By concatenating all of these blocks together sequentially, in parallel (inception style) [8] or with some residual connections [9], the CNN network is able to convey the shallower features associated with labels from the raw data into the output of the neural network. In addition, with normalization and good activation functions like ReLU [17], neural networks are almost guaranteed to achieve state-of-the-art performance in many benchmarking tasks. Although there are many works discussing normalization techniques and activation functions separately, there are few works taking both into account in the community. In this work, instead, we are interested to learn the relation of both modules. In fact, normalization eases the learning of neural network parameters, and activation lets the network focus on the useful feature representations whereas mask out noises. However, since the two modules are independently proposed, the impact of both modules on each other is unknown and worth investigation. In this work, therefore, we try to investigate the integration of both modules, especially the batch normalization and ReLU activations [17], which are the dominating techniques applied in the community. To be concrete, we are interested in distinguishing strong signals from noises or weak signals which can be ignored or decayed. While ReLU activation simply drops the entire negative signals, LReLU discovers that the negative signals may potentially contain some useful information. Therefore, finding a better shifting threshold and only drops unnecessary signals might further improve performance. Further, taking a convolution neural network (CNN) as an example, different feature maps of it may potentially contain distinct information from each other but may also share some common information together, therefore having different local shifting threshold per feature map, as well as a global threshold across all feature maps might fully exploit information to the largest extent. Motivated by these observations, we propose a general shift-norm-activation framework, named as GSNA, along with rigorous analysis and extensive empirical studies, which validates our design both theoretically and empirically. Further, it is interesting to note that, GSNA is independent of neural architecture designs, hence they are generic and can be naturally incorporated into existing neural networks. The contribution of this work hence is summarized as follows.
•We present a shift-norm-activation framework, which integrates both normalization and activation into a single module, by filtering out signals with the found optimal threshold at both global scale and local scale. The proposed framework can be seamlessly incorporated into any neural architectures and is hence generic.•A rigorous mathematical analysis is performed on the understanding of normalization and activation. In addition, the analysis for the proposed framework are conducted on various aspects, such as computation burden, comparisons to existing methods, and its theoretical performance potential. Further, a reparameterization trick to optimize the trainable parameter initialization of GSNA is presented.•The extensively conducted experiments demonstrate the potential of the proposed framework in various computer vision benchmarking tasks. To explore its applicability, some exploratory/qualitative experiments are also conducted on other areas as well.
The remainder of this work is organized as follows. In Section 2 we review some of the most related works on activations, normalizations, as well as network architectures. In Section 3 we present the proposed generic shift-norm-activation framework, with detailed mathematical analysis on its performance potential, computation burdens, as well as the reparameterization trick for effective and efficient learning. In Section 4 we conduct various empirical studies to demonstrate the superiority of the proposed framework on some benchmarking computer vision classification tasks. To check the generality of the proposed framework, we also apply it to some other areas like natural language processing for comparison with baseline networks, and generative tasks and visualizations for qualitative analysis. We conclude this work in Section 5 with the pros and cons of the proposed framework as well as some discussions on future directions.
