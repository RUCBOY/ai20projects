One of the most important tasks of modern computer vision systems for applications such as autonomous driving, augmented reality and robotic is to recognize the 3D location and orientation of objects in RGB images. Recently, deep Convolutional Neural Networks (CNN) techniques [1], [2], [3], [4], [5] have been proved to achieve good performance on processing object detection on RGB images and have been used to solve 6D object pose estimation problem [6], [7]. The main idea of CNN-based methods is to learn the mapping function between the image and the 6D pose of the object from 3D annotated images. However, prediction of 3D information (e.g., 6D pose, 3D bounding box and 3D keypoints) from 2D RGB images is usually unstable. And the 3D annotations used during the training are always difficult to satisfy in practice. Alternatively, traditional methods do not require such a large number of 3D annotated images, but achieving high accuracy is more challenging. Traditional methods in this direction calculate the 6D object pose through the matching of local features and 2D-3D keypoints correspondences. However, they fail for objects with poor geometry or texture, because they require sufficient textures on objects to extract robust local features. Besides, the matching of local features is time-consuming and error-prone.
Following that line of thought, a natural idea is exploiting CNN to learn the 2D information (2D bounding boxes and 2D keypoints) of objects from RGB images and then estimating the 6D object pose using traditional 2D-3D space geometry algorithms. To this end, we propose a two-stage 6D object pose estimation method. The first stage takes RGB images as input, and a well-designed network detects the category, 2D bounding box and 2D keypoints of the target object. In the second stage, the 6D object pose is estimated from the keypoints and the 2D bounding box by a series of geometric reasoning methods. In our network, it consists of two tasks, namely object detection and keypoint detection. For the object detection task, its branch includes object classification and 2D bounding box regression. In the keypoint detection task, its branch will be trained by a rotation invariance loss function across different images without ground-truth keypoint annotations. Besides, a class activity map extracted from the object branch will be used as the object’s saliency region to constrain the location of learned keypoints on the object. For reasoning the 6D pose of the object, with the help of several references with real 6D poses, we can calculate the rotation matrix from the detected keypoints and keypoints of references. And the 3D translation of the object is inferred by its 2D bounding box. The overview of our proposed approach illustrates in Fig 1. Through the above ways, our framework achieves 6D object pose estimation in the situation of complex backgrounds. We evaluate our method on multiple benchmark datasets. Our method achieves accurate pose estimation. Besides, our method still performs well for OCCLUSION datasets [8] that contain multiple objects and have confusion and occlusion scenes.Download : Download high-res image (186KB)Download : Download full-size imageFig. 1. The overview of our proposed approach, where the rotation and translation of 6D object pose are calculated respectively from the predicted 2D keypoints and the detected 2D bounding box.
This paper makes three main contributions to 6D object pose estimating:
•We propose a two-stage pose estimation method that uses a more easily predicted 2D information of the object to infer the object poses. In addition, our network is trained without any 3D annotated images and 3D models.•Our experiments on multiple benchmark datasets demonstrate that our method achieves accurate pose estimation, which is superior to the most state-of-the-art keypoint-based and template-based methods, and comparable with methods using 3D annotated images.
The rest of the paper organize as follows. Section 2 presents a brief review of related work. Section 3 introduces our 6D object pose estimation framework. The experimental results, comparisons and component analysis are presented in Section 4. Section 5 concludes the paper.
