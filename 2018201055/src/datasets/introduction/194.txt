Human motion prediction (i.e., forecasting the future poses based on the past observed human actions) has attracted considerable attention recently [1], [2], [3], [4], as shown in Fig. 1. In general, predicting the following actions Ỹ={f̃T+1,f̃T+2,…,f̃T+k,…,f̃T+Δt}in Δt time steps from its historical postures F={f1,f2,…,ft,…fT} is a branch of human behavior understanding, which has shown enormous potential in computer vision, virtual reality, human-computer interaction, machine intelligence, and autonomous vehicles, especially, those applications that involves the interaction with humans [5], [6], [7]. The ability to understand human poses or behavior and to forecast the following events is one of the crucial components that embody the robot’s performance [8], [9]. This predictive ability helps robots respond promptly and even plan appropriate actions in advance. For example, autonomous vehicles traveling in an urban environment can automatically complete tasks such as navigation and obstacle avoidance by accurately forecasting the future poses of pedestrians; this can reduce the occurrence of traffic accidents [10], [3]. Furthermore, related applications such as auxiliary medical diagnoses or behavior analyses may also profit from human motion prediction [11], [12], [13]. Motion capture (MoCap) data, which includes the recorded movements of markers attached to specific joints of the human body, has become the most popular motion storage technology in the industry, due to their efficiency, flexibility, and accuracy [14], [15], [16]. Therefore, in this study, MoCap data are used to estimate the following instantaneous actions or even long-term future poses based on the previously observed sequence.Download : Download high-res image (297KB)Download : Download full-size imageFig. 1. Example results on two representative activities. In each subfigure, the first row is the historical poses, the middle is the ground truth of future motion, and the bottom represents the predicted result of the proposed model. Note that the predicted poses of the proposed model are indistinguishable from the ground truth in short-term prediction (⩽480 ms), and even for long-term prediction (⩽1000 ms), realistic and reasonable visualizations are obtained.
Human motion prediction has aroused widespread interest among researchers because of the potential benefits for many real-world applications [1], [11], [3]. However, the human motion sequence is collected by recording the 3D skeleton information of the human body with a frame by frame manner, thereby generally revealing several intricate patterns in the following aspects. First, it is difficult to model human motion sequences due to the high diversity and randomness. Second, because the movement lasts for an extended period, the indeterminacy of human behavior is high [6]. For example, pedestrians may keep walking at the same speed, accelerate, or even turn back; people standing and phoning may continue making calls or sit down. Third, the generated motion sequence should conform to dynamic constraints (i.e., continuity and consistency) to allow outstanding visualization. Fourth, the prediction algorithm must estimate the future posture in time or even real-time; otherwise, it will hinder its practical application. These factors present a significant challenge for the task of human motion prediction.
Recently, with the availability of large MoCap datasets [17], [18], researchers have attempted to resort to deep neural networks (DNNs) to efficiently model human motion sequences [6], [10]. Because human motion is essentially a sequential data, recent studies have treated motion prediction as a sequence-to-sequence (seq2seq) learning task. The typical formula is Y∗=seq2seq(F), where the F is the observed poses and Y∗ is the optimal prediction. Moreover, the symbol of seq2seq is a deep neural network, e.g., RNNs. They have introduced various variants of recurrent neural networks (RNNs) [19] to forecast future poses from historical MoCap data [1], [2], [10], [4], [5]. These models adequately analyze and use the temporal consistency of the motion sequence. Although they have achieved encouraging progress, their performance is still severely limited owing to the RNN’s defects. RNNs (or long short-term memories (LSTMs) and gated recurrent units (GRUs) with memory units) have low trainability in practical applications due to gradient vanishing and explosion. Besides, the conspicuous discontinuity between the last frame of the historical MoCap sequence and the first frame of the predicted results obtained by the RNN model is frequently observed [1], [9]. Residual connection alleviates these problems [2]; however, the chain RNN structure inevitably leads to error accumulation when temporal information is transferred step by step, which may result in abnormal visualizations. Moreover, empirical evidence suggests that recurrent models often converge to the mean pose of original future poses [2], [3], [10]; that is, the predictor can only obtain unexpected static predictions. In conclusion, the cause of the mean pose problem is that the RNN variations hardly capture the long-term dependencies of the MoCap sequence. Because the tracking of past long-distance information is lost, the mean pose becomes the model’s optimal result. For example, theoretically, the element at the i-th time step requires at least O(n) operations to interact with another at a distance of n steps, which may make it difficult for the RNN to learn contextual information that efficiently utilizes long-term temporal dependencies.
Currently, the state-of-the-art method of human motion prediction is generative adversarial networks (GANs) [10], [6], [4] for seq2seq learning of MoCap sequences. GANs[20] (particularly, Wasserstein GANs (WGANs) [21]) are one of the most promising generative models for unsupervised learning in complex distributions. Researchers have constructed various structures of GANs to ensure that the generated predictions and ground truth are theoretically balanced in terms of resolution [3], [5]. While these models may produce specious human-like poses, they are challenging to train and unstable in practical learning because the generator and discriminator have difficulties in reaching the Nash equilibrium [22], [23]. For WGANs, approximating the k-Lipschitz constraint required by the Wasserstein-1 metric (W-met) is also challenging [24], [23], [25]. Therefore, the typical GAN may only yield suboptimal results.
To address these aforementioned issues, we propose a feed-forward model, i.e., temporal convolutional generative adversarial network (TCGAN), for efficiently forecasting realistic human poses from its observation. The proposed model mainly exploits the ideas of two fundamental components: temporal convolution (TCN, 1D causal dilated convolution) for modeling long-term temporal dependencies [26], [27], [28], and adversarial regularization with spectral normalization (SN) for ensuring realistic visualization and a stable training process. With the development of seq2seq learning, TCN has proved to be a more effective strategy for capturing long-range temporal patterns. TCN has been successfully applied to various sequential learning tasks, including machine translation [29], speech synthesis [30], action segmentation [26], [31], trajectory prediction [32], and video analysis [33]. In general, TCN modules exponentially increase receptive fields in a hierarchical architecture to model the long-term dependencies of sequential data efficiently [27]. This calculation is superior to the step-by-step calculation method in chain RNNs [30]. The complexity of the interaction between elements with n time steps is O(logn) for TCNs, whereas, for RNNs, it is O(n), which is a more efficient mechanism for temporal context information. Moreover, by increasing the dilation rate of the 1D dilated convolution, the receptive field increases exponentially with the number of layers, which prevents the model from overfitting the training data. Therefore, in this work, the TCN of the hierarchical structure is used as the generator to produce high-fidelity predictions. For stability and alleviating the mode collapse problem in the training of GANs [20], [21], SN instead of the standard version is embedded in the WGANs [24]. It is not challenging to solve the k-Lipschitz constraint by normalizing the weights of each layer with SN; besides, it is simple to implement and does not require additional parameters for fine-tuning. Particularly, two global discriminators for validating the generated predictions’ dynamic characteristics are proposed (i.e., the consistency and fidelity discriminator), both of which are jointly trained in an adversarial fashion. In contrast to traditional approaches relying on RNNs, the proposed method is an fully convolutional generative model that exhibits simplicity, a higher accuracy, and stability in terms of computational complexity and number of parameters. Furthermore, we also develop a temporal attention strategy for TCNs to capture long-range correlations more effectively. For human motion prediction, temporal attention helps the model use the most relevant context and select the semantic representation adaptively from specific locations to repair corrupted frames. With such a network structure, TCGAN can efficiently generate realistic short-term human poses and even human-like long-term predictions that are consistent with the characteristics of the input sequence.
The significant contributions of this paper are as follows: (1) TCN is used to model the long-term temporal dependencies of MoCap sequences efficiently. To the best of the authors’ knowledge, this is the first research attempt that utilizes TCNs to forecast future human poses. (2) Owing to the non-challenging satisfaction with the k-Lipschitz constraint, SN is incorporated into the proposed model to achieve progressive stability and reproducibility. (3) Two discriminators are introduced, wherein the fidelity discriminator distinguishes the prediction from the corresponding ground truth, and the consistency one determines the long sequences spliced by the prediction and input sequence or the ground truth. (4) The proposed model exceeds state-of-the-art performance with a significant margin on the three large-scale action analysis benchmarks for skeleton-based motion prediction.
The remainder of this paper is organized as follows. In Section 2, the relevant research steps are briefly summarized. The proposed TCGAN framework is presented in Section 3. Section 4 presents the extensive experiments on the CMU, H3.6M and 3DPW MoCap benchmarks, which qualitatively and quantitatively demonstrates the superiority of the proposed method. Finally, Section 5 presents the conclusions.
