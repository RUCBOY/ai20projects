Human societies function best when people produce public goods that offer collective benefits that they could not otherwise obtain individually (Olson, 1965). The cooperation required to do this, however, is challenging because it creates a social dilemma (Axelrod, 1984; Dawes, 1980): the group does well if individuals cooperate, but, for each individual, there is a temptation to defect. Getting groups to cooperate, and to keep cooperating, therefore presents substantial difficulties (Hardin, 1968; Nowak, 2006).
Various strategies for individuals to minimize their own losses or to act fairly, from their own point of view, when facing a cooperation dilemma have been identified. And these actions might conceivably also sustain cooperation in groups as a whole. For instance, in a repeated game, individual strategies—such as Tit-For-Tat (Axelrod, 1984) and its variants (Hilbe et al., 2013; Nowak and Sigmund, 1993)—may lead another person to whom a person is connected to cooperate as a secondary effect. In fact, even robots can be programmed to elicit cooperation from humans (Crandall et al., 2018). Furthermore, a large body of work has explored broader, institutional approaches to overcoming cooperation dilemmas, such as reputation (Cuesta et al., 2015; Nowak and Sigmund, 2005), punishment (Fehr and Gachter, 2002; Fowler, 2005), rewards (Rand et al., 2009), population structure (Allen et al., 2017; Ohtsuki et al., 2006), tie rewiring (Rand et al., 2011), or the establishment of a central authority (Ostrom, 1990). But it is still unclear how a small fraction of individuals might guide a group of others toward the creation of public goods without a super-ordinate institutional change. And approaches that actually increase levels of cooperation in groups (from their baseline) are scant.
Here, we examine how individual autonomous agents acting locally can facilitate, and even increase, cooperation in a group of people. We focus on network interventions (Valente, 2012) on the assumption that all individuals, including those who attempt to intervene to make the situation better, are embedded in a social network and that their information and actions are limited to their local neighborhood of connections (Granovetter, 1985). We take preprogrammed autonomous agents endowed with various simple strategies (i.e., “bots” or computer programs behaving as individual actors in a social system) and introduce them into the network and have them interact as members of the group (Shirado and Christakis, 2017). We do not assume that the bots have distinctive advantages that allow them to observe and change the entire social system as a central authority might. As a result, we can (1) study the abstract principles instantiated by the bots (gaining insights into how similar actions initiated by humans might affect cooperation in groups, albeit with exquisite experimental control in the case of the bots) and (2) develop practical applications of distributed, simple artificial intelligence (AI) agents in the form of bots that might be introduced into online groups so as to modify their properties for the better (Paiva et al., 2018). In short, we explore how bots can be used to facilitate positive social outcomes.
Humans decide whether to cooperate in part based on the actions of their neighbors (Shirado et al., 2013). Thus, an intervening agent, which resembles what Axelrod has called a “reformer”(Axelrod, 1984) (here, one that it is embedded within the system as a participant itself), might, by exercising the behavioral options of cooperating or defection, not only affect its own payoffs but also have an impact on others. However, a reformer might be limited in its ability to make its own network environment favorable to cooperation (Liu et al., 2011). For instance, theory suggests that the simple strategy of continuous cooperation by a well-intentioned reformer may do little to change cooperation dynamics, especially in large groups (Olson, 1965), and any such cooperators might be solely exploited by the defectors connected to them (Axelrod, 1984).
To address the challenges faced by reformers embedded in social networks who seek to sustain or enhance group cooperation, we focus on network dynamics that allow individuals to adjust social ties and engage in a kind of social network engineering (Rand et al., 2011; Shirado et al., 2013). That is, we explore a strategy whereby each bot gives its human neighbor an option to make or break a tie with other human subjects chosen by the bot (the bots act as a kind of social assistant). To find an effective strategy for such reformer bots, we set up our experiments in two stages. In the first set of experiments, we examine various network interventions (including a variety of alternative, control strategies) using a sufficient number of bots (“Experiment 1”). We evaluate the efficacy of the intervention in a repeated set of interactions in a public goods game involving cooperation. In the second experiment, we specify the bot's intervention strategy based on the results of the first experiment and then test it with a single bot embedded in a group of human subjects (“Experiment 2”).
In Experiment 1, we recruited 896 unique human subjects through Amazon Mechanical Turk, dividing them among 56 groups, in sessions lasting an average of 24.1 min. Subjects were placed into groups of 16 individuals arranged in a network with an Erdős-Rényi random graph configuration (Erdős and Rényi, 1959) in which 30% of the possible ties were present at the outset, on average. In the sessions involving bots, each subject additionally received one connection with a bot. The subjects were therefore initially connected to an average 4.41 (SD = 1.76) other humans and 1 bot (i.e., 5.41 network neighbors on average). Subjects could identify each neighbor by a permanent, randomly generated name (see Data S1 and Transparent Methods).
Each subject played a public-goods game lasting 30 rounds with their network neighbors without knowing when the game would end. At the beginning of the game, subjects received US$1.00 as their initial endowment. In each round, all the subjects chose whether to cooperate, by reducing their own endowment US$0.05 per neighbor in order to increase the endowment of all their neighbors by US$0.10 each, or to defect, by paying no cost and providing no benefits. Subjects had to make the same choice with respect to all their connected neighbors (Allen et al., 2017; Cuesta et al., 2015; Ohtsuki et al., 2006; Rand et al., 2011, 2014; Shirado et al., 2013; Yamagishi, 1986).
After making their cooperation choice, subjects were informed of the choices made by their neighbors. Then, subjects had the opportunity to change their neighbors by making or breaking ties (“tie-rewiring” options), using a game setup that we had explored in prior experiments (Rand et al., 2011; Shirado et al., 2013). Specifically, 5% of all pairs of human subjects (i.e., 6 pairs, on average) were chosen at random in each round and were given the opportunity to rewire their ties. If a tie already existed between the two subjects, then one of the two was picked at random to be allowed to choose whether to voluntarily break the tie with the other; if a tie did not already exist between the two, a randomly selected member of the pair was given the option to form the tie (unilaterally). When making this decision, subjects were aware of whether the person to whom they might disconnect or connect had cooperated or defected in the past round. Thus, people could choose to modify a subset of their social ties (with low frequency) at each round; the network could become rewired as a result of these modifications; and all subjects' network properties (such as network degree and the fraction of cooperators among their neighbors) could change over time. Our focus was on the possible impact of network interventions with bots placed within these groups.
Within this basic setup, we introduced 16 bots into the network of 16 human subjects (except for the control sessions without any bots). Each bot had only one tie and was connected with a different subject at the beginning of the game (i.e., every subject initially had precisely one tie to a bot among their neighbors). Bots always chose cooperation in the game (except for the bots using the Tit-For-Tat strategy). And the bots never connected with each other.
To be clear, we used the artifice of single-tie bots for Experiment 1 so as to experimentally fix the total amount of intervention across sessions and treatments to 16 total ties with bots at all times and in all treatments. Sixteen single-tie bots corresponded to three human subjects in terms of average initial network degree, and the human-bot connections accounted for 11.8% of all the possible connections in a network (i.e., 16/136). Subjects were not informed that there were bots in the game (except for an extra condition making bots visible; see below).
Using the bots embedded in a network of human subjects, we tested the network-intervention strategy whereby each bot generated an additional rewiring option for its human neighbor (regardless of whether the neighbor cooperated or defected) to make or break a tie with other human subjects; the humans did not have to use the rewiring option given by the network-intervention bots. All the network-intervention strategies added the same amount of rewiring options to the social system (increasing the possible rewiring rate from 5% to 16%).
We then manipulated the criteria used by the bot reformers to pick targets. The bots intervened based on the cooperation decision that the humans had previously made. To improve the level of cooperation in a group using network engineering, an intervention would need not only to retain existing cooperators, but also to prompt defectors to change their decisions. Thus, we tested three processes, in terms of whether the approach is random, engaged, or disengaged—from the reformer's perspective—with respect to the defectors in the group (Rand et al., 2009). In the “random” process, targets are chosen at random. The target subject thus had an additional chance to form or break a tie with another subject, irrespective of the target subject's cooperation decision. In the “engaged” process, defectors are given a chance to form more cooperative ties so as to (hopefully) encourage them to switch to cooperation. The engaged process had every bot give an option to its neighbor to make a new tie with another subject who was choosing cooperation (so, for example, a defector connected to the bot was introduced to a cooperator). That is, the cooperative bots foster engagement with the defectors (i.e., the bots are conciliatory to defectors). In the “disengaged” process, the cooperative bots foster disengagement with defectors (i.e., the bots work to quarantine defectors from cooperators). The disengaged process involved the bot giving an option to its neighbor to cut an existing tie with another subject who was choosing defection.
We also conducted sessions with three types of control conditions. The first control condition did not involve any bots. In the other two control conditions, like the treatment conditions, the group of 16 human subjects had 16 single-tie bots in a network. In one control condition, the bots (again) always cooperated. In the other, they used the Tit-For-Tat strategy; they started with cooperation and then selected the same behavioral choice (i.e., cooperation or defection) as their human neighbor had selected in the previous round (Axelrod, 1984). Importantly, and in contrast to the treatments, the bots in these control conditions did not intervene in the local networks of the human subjects by giving them a rewiring option.
As noted, subjects were not informed that they were interacting with bots and that some of the tie-rewiring options were given by bots. To assess the effect of this ignorance, we also carried out experiments with an extra condition of “bot visible.” In this extra condition, subjects were informed that they were interacting with bots, which neighbors were bots, and which tie rewiring options the bots suggested them (see Data S1 and Transparent Methods). Thus, subjects could make a decision with knowledge that they had both bots and humans among their neighbors and that the bots gave them rewiring options. We examined this treatment only in the case in which the bots used the “disengaged” strategy.
In Experiment 2, which involved an additional 128 subjects in 8 sessions, we tested the minimal intervention of a single bot. This experimental setting differed from Experiment 1 only in that a group of 16 human subjects played a public-goods game with 1 bot having 5 ties, which corresponded to 1 average human subject in terms of initial network degree (in the sessions of Experiment 2, the average network degree = 5.13 at round 1). Thus, in contrast of Experiment 1, some subjects had a connection with the bot and the others did not (in every round). This single bot used a network-engineering strategy that we designed according to the results of Experiment 1 (as described below).
In sum, we evaluated the effect of bot interventions and network engineering with two experiments. Experiment 1 had 3 control conditions not involving any network-intervention bots, 3 treatment conditions (i.e., random, engaged, and disengaged criteria for how the bots treated defectors), and 1 extra condition involving manipulation of bot visibility. Experiment 2 had one condition involving a single bot. We conducted 8 sessions for each condition for a total of 64 groups with 1,024 human subjects in total.
Subjects interacted anonymously over the Internet using our publicly available software platform (available at breadboard.yale.edu). We allowed the participation only of those subjects who completed a tutorial session and passed a series of tests assessing their understanding of the game rules. We prohibited subjects from participating in more than one session. All subjects consented, and this study was approved by the Yale University Committee of the Use of Human Subjects.
