Salient object detection (SOD) mimics the human visual system to detect objects or areas that attract human attention [1]. The human visual attention mechanism is an important part of visual cognition and learning, therefore saliency detection has a wide range of real applications, such as object segmentation and recognition [2], content-related image and video compression [3], video object segmentation [4], as well as image retrieval [5]. Since Itti et al.  [6] proposed saliency detection in 1998, it has attracted the attention of countless researchers and the field has made rapid progress in terms of both new models (e.g., EGNet [7]) and datasets (e.g., SOC dataset [8], CoSOD3K [9]). However, accurate saliency detection is still a challenging task due to the complexity of real scenes.
The RGB image is a common modality widely used for saliency detection, however, as shown in Fig. 1, RGB saliency detection suffers from three major challenges, including (a) Low color difference in the foreground and background regions, (b) Complex foreground, e.g., a large number of indistinguishable significant objects in the foreground, and (c) Complex background. These challenges lead to the fact that saliency detection only relying on RGB images is unable to handle the complex scenes in the real world.Download : Download high-res image (456KB)Download : Download full-size imageFig. 1. Visual comparison of RGB image, depth map, and ground truth (GT). The depth map provides vital complementary information for resolving the challenges in RGB saliency detection.
To address these challenges, the depth map is introduced as a vital complementary modality for improving the saliency detection [10]. As shown in Fig. 1, the salient area in the depth map is close to the ground-truth, indicating that considering depth information can effectively resolve the challenges in RGB saliency detection. In addition, with the development of 3D scanning devices, depth information becomes more accessible. For instance, the stereo images [11] taken by the depth camera such as Kinect and RealSense provide distance information beyond color information. Deep learning has shown remarkable success in RGB-D saliency detection. In practice, the success of deep learning based RGB-D saliency detection is mainly determined by two key factors, including (i) How to extract features from RGB images and depth maps, and (ii) How to fuse the extracted features for the accurate saliency detection. However, these two factors are not effectively addressed in existing methods. First, features are usually extracted from the depth maps without considering the low-quality issue [11], such as heavy noise and low boundary contrast [12]. Second, most methods [7], [13] aggregate the depth and RGB features only relying on simple concatenation operations at different stages. The simple concatenation is efficient, but is unable to effectively fuse the complementary information from multi-modalities features. These two issues reduce the quality of features used for saliency detection and eventually result in unsatisfactory performance.
To resolve these issues, we propose a novel enhancement and fusion network (EF-Net) to first enhance the depth maps and then fuse the multi-modality features extracted from the RGB images and enhanced depth maps for effective saliency detection. Our motivation stems from the fact that the quality of RGB images is much better than that of depth maps. Therefore, the prior knowledge learned from RGB images can be used to resolve the low-quality issue of depth maps effectively. Specifically, we first utilize a color hint map module to learn a hint map from the RGB images. The hint map is then employed to enhance the depth map in a specially-designed depth enhancement module. Finally, we propose a layer-wise aggregation module to fuse the features extracted from the RGB images and enhanced depth maps for the accurate detection of salient objects. Our EF-Net effectively resolves the limitations in existing methods by the proposed enhancement-and-fusion framework, which provides (i) the depth enhancement module for improving the quality of depth maps with the prior knowledge given by the color hint map module, and (ii) the layer-wise aggregation module for fusing the multi-modality features. We evaluate EF-Net by performing extensive experiments on five RGB-D benchmark datasets. The experimental results demonstrate EF-Net outperforms 12 state-of-the-art (SOTA) saliency detection methods, both qualitatively and quantitatively.
The key contributions of our work are as follows:
•We propose a novel depth enhancement-and-fusion framework to resolve the low-quality issue of depth maps, which effectively improves the quality of depth maps with the prior knowledge provided by the color hint map module with RGB images.•We propose an effective layer-wise aggregation module to fuse the features extracted from RGB images and enhanced depth maps, allowing to make full use of the multi-modality data.•Our EF-Net outperforms 12 SOTA methods by a large margin in terms of F-measure and E-measure scores on five widely-used benchmark datasets. In addition, the effectiveness of key components in EF-Net is demonstrated by comprehensive ablation studies. Finally, further evaluation on RGB- Thermal (RGB-T) data demonstrates that EF-Net is an effective solution for the general multi-modality saliency detection.
Our paper is organized as follows. In Section 2, we will introduce related work. In Section 3, we will describe our EF-Net in detail. In Section 4, we will present the datasets, experimental settings, and results. Finally, we will conclude our work in Section 5.
