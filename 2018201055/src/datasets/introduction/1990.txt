Advances in educational technologies and the rise of massive open online courses (MOOCs) have generated increased interest in previously non-feasible approaches to exploring learner behavior data to provide process-oriented feedback (Sedrakyan, 2016). Examining how learners interact within virtual learning environments (i.e., with each other, instructors, the environment) provides opportunities to reveal where things are progressing well and where problems may possibly occur. Using this information, process-oriented feedback can be generated that can help teachers and learners enhance engagement and achievement (Gašević, Dawson, Rogers, & Gasevic, 2016). Such feedback is presented in the form of visualizations in several teacher- and learner-oriented dashboards (Bodily and Verbert, 2017, Dyckhoff et al., 2012, Hu et al., 2014, Mottus et al., 2015). Dashboards are instruments intended to improve decision-making by amplifying or directing cognition and capitalizing on human perceptual capabilities (Yigitbasioglu & Velcu, 2012).
Despite their popularity and the proliferation of solution providers, little is known about design aspects, such as the typology of feedback relevant in a learning context (Sedrakyan, Järvelä et al., 2016, Yigitbasioglu and Velcu, 2012). The field lacks knowledge on the type of feedback that works best for different learning goals and learners to provide targeted help.
According to Saywer (2014), most research on educational dashboards lacks both theoretical support from recent advancements in the learning sciences and an evidence-informed foundation for choosing the data that can assist in observing and assessing learning processes to identify the feedback needs of learners/teachers. As a result, instead of being useful, these instruments can be harmful. For example, most current Learning Analytics Dashboards (LADs) are based only on learner performance indicators (e.g., where a learner is doing well/poor, how much content has been completed, how much time was spent, how learners' progress compares to teacher specified and/or peer scores) that do not seem to contribute to learners' motivation and engagement (Blumenfeld, 1992, Elliot and Harackiewicz, 1996). Furthermore, recent research on the effectiveness of learning analytics tools reveals that when using performance-oriented dashboards, learner mastery orientation decreases (Lonn, Aguilar, & Teasley, 2015). This suggests that such goal orientations need to be carefully considered in the design of any intervention, as the resulting approaches and tools can affect students' interpretations of their data and subsequent academic success (Lonn et al., 2015). For instance, regarding learning goals, students can orient themselves toward either mastery-focused or performance-focused goals. Students with mastery goals are typically interested in learning as an end itself (e.g., “One of my goals in class is to learn and understand as much as I can.”) while students with performance goals are typically interested in learning as means of demonstrating their ability or competence (e.g., “I want to do better than other students in my class”; Dweck & Leggett, 1988).
Regarding the analysis approach, statistical and data-mining approaches prevail in the context of LADs. Existing LAD instruments mostly target performance visualization often in the form of outcome feedback (e.g., “How do I perform?) rather than process-oriented feedback (“How can I do better?”; e.g., by looking for inefficient processes, e.g. sequential aspects of learning, and thus neglecting the procedural aspects of learning). Thus to provide relevant feedback, it is important to know if low performance is affected by a misunderstanding of a problem, task, or concept or rather a procedural aspect of learning (e.g., not sufficient effort put in verifying a solution) thus distinguishing whether there is a need for a cognitive or behavioral type of feedback (Sedrakyan and Snoeck, 2017, Sedrakyan, 2016). Furthermore, most visual representations in the context of LADs are limited to graphs, charts, or other diagrams without providing support mechanisms to facilitate their interpretation (Park & Jo, 2015). Empirical studies show that behavioral change and improved performance were observed when supporting learners in the interpretation of visualizations (Sedrakyan, 2016).
Also, as dashboards are tools to be used by the teacher and/or the learner, acceptance of this tool by end-users is yet another factor that can interfere with the achievement of their intended goal (Davis, 1989) and thus affect learner performance. This suggests that most relevant constructs from established technology acceptance models need to be considered in the design of dashboards to allow built-in mechanisms for capturing end-user perceptions (feedback on feedback). User acceptance can be important to ensure the effectiveness and continuous refinements need of dashboard feedback (e.g., the same type of feedback may not be relevant for the same problem as learners' knowledge and expertise level changes during time), and ultimately determine its intended utility.
Common to all feedback LADs presented in the literature on dashboards is the lack of theoretical support grounded in the learning sciences (Sedrakyan et al., 2016) and research on feedback and underlying mechanisms of learning processes. Learning science (or the learning sciences) is an interdisciplinary field that works to further scientific understanding of learning as well as to engage in the design and implementation of learning innovations, and the improvement of instructional methodologies (Carr-Chellman, 2004). Research in the learning science traditionally focuses on cognitive-psychological, social-psychological, and cultural-psychological foundations of human learning, as well as on the design of learning environments often following design-based research methods (Carr-Chellman, 2004). Major contributing fields include sociocognitive science (see also the sections on Typology of Feedback and Regulation of Learning), computer science, educational psychology among others. Learning sciences study learning as it happens in real-world situations and how to better facilitate learning in designed environments – in school, online, in the workplace, at home, and in informal environments (Carr-Chellman, 2004).
In this work, we complement the engineering approach with theories in the learning sciences to design a novel artefact, namely, a process-oriented feedback model in the context of LADs. The term process-oriented feedback refers to early feedback opportunities that can be achieved during a learning process before a formal assessment of its outcome and feedback by a teacher (i.e., teacher intervention) is given (Sedrakyan, 2016). As research on feedback is closely intertwined with the concept of learning regulation, we first review the regulatory mechanisms underlying learning processes. For instance, self-regulated learning (SRL) theory explains the core aspects that can facilitate learning processes such as setting goals, planning, applying strategies, monitoring progress, and reflecting (Zimmerman, 1990). Next, we fine-tune the idea of feedback by distinguishing between learning goals (e.g., teacher specified goals) and goal orientations (e.g., mastery, performance, approach avoidance) that allows the consideration of the personal needs of learners, and efficiency and effectiveness of learning that contributes to determining timeliness aspects. We then review how feedback needs to be implemented in dashboards based on research on feedback typology as defined by cognitive, sociocognitive and behavioral theories (Bransford et al., 2000, Tomic, 1993). The design artefact presented in this article uses a conceptual model that visualizes the relationships between dashboard design and the learning science concepts to provide process-oriented feedback that support regulation of learning. It has to be noted that the goal of the work is not to propose a specific feedback design, but rather a conceptual guide for the choice of concepts for designing information systems and, thus, also helping understanding future data needs as basis for educational dashboard feedback. We provide preliminary answers to questions such as:
-What are the concepts we need to consider for the design of LAD feedback to allow the observation of learning processes with respect to potential feedback (i.e., regulation) needs for different learning goals? (Section 3)-What artefacts will enable the capture of data that will allow measurement of those concepts during a learning process? (Section 4)-How can such learning-process data be mapped to end-user (learner and/or teacher) feedback to improve the regulation of learning processes? (in particular)
The paper is structured as follows. First a review is given of earlier studies. Then, design implications are given for implementation of those results. Third, a practical case example is given which attempts to implement these ideas by also introducing analytics/visualization techniques based on empirical evidence from earlier research that successfully tested these techniques in various learning contexts. It concludes with a discussion and general scientific contributions of the work, as well as suggestions for possible future work directions.
