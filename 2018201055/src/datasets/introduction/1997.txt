Facial expression recognition (FER), as the task of classifying the expression on images or video-sequences, has become an increasingly dynamic topic in the field of computer vision in recent years. There is a wide range of applications demanding for understanding human emotion through facial expression such as human–computer interaction [47], robotics [46], and health care [29], in which facial expression recognition plays an important role.
With the tremendous breakthrough of the Convolutional Neural Networks (CNN), much significant progress has been made to deal with the image-based or video-based facial expression recognition problems [50], [57], [12]. Some works [7] try to improve the performance to recognize facial expressions via classifying with some off-the-shelf features extracted directly from the images. However, most of these previous works treat FER as a naive classification problem, which ignore the observation that the different expressions made by the same person share similar appearances with limited changes on parts of the face. Therefore, it encourages us to treat FER task as a fine-grained classification problem to investigate the detailed differences.
To gain discriminative features, we adapt a video-based metric learning framework for this task. Xu et al. [51] gains state-of-the-art performance on Person Re-identification task via a metric learning framework, which proves that such metric learning framework is efficient for capturing discriminative features. It considers the intra-class similarity and inter-class distinction, which can learn the most discriminative features for facial expression. Since Recurrent Neural Network (RNN) has shown great success by learning long-distance dependencies from sequential data with temporal representation [34], [56], we integrate a cascaded CNN-RNN structure as the backbone for video-based metric learning framework. Moreover, we propose a specific pairwise sampling strategy tailored to FER in metric training, to ensure our model focus on learning the most discriminative features for facial expression.
Meanwhile, Xu et al. [51] utilizes an attentive pooling module to assign weights to distinguish the relevant frame pairs. This module helps to capture the most contributed video frames to final classification. Similar to it, as for video-based FER, Zhao et al. [59] introduces that facial expression has the dynamic variation of expressional strength, and the image frames in one video also do not contribute equally to the final classification. The peak (vigorous intensity) expression, which shows the complete expression, can extract more useful feature than non-peak (weak intensity) expression of the same type and from the same subject. Therefore, in our video-based metric framework, an attentive pooling module is also applied to learn this time series of emotional expression to find peak frames in video-sequences for FER.
To capture more expression guided spatial contexts, the Action Units (AUs) [42], [43] based works reveal that facial expression is the result of the motions of facial muscles among the key regions (eyebrows, eyes, nose, and mouth). It is essential to guide our model to pay more attention to those subtle attributes of expression key-points such as mouth corner radian, facial wrinkles, eyebrows, etc, while ignoring the little-contributed but obvious differences, such as hair color, face shape.Recently, Non-local Network [49] proposes a self-attention approach, which takes long-range contexts as spatial information and computes the pixel-wise responses as a weighted sum of the features at all positions. Motivated by its attentive ability, we propose an action-units attention mechanism (AU attention) tailored to FER task, which concentrates the contextual information on the regions of AUs. As the features of AU areas are much more crucial, we replace the aforementioned non-local operation on the whole feature map by harvesting the contextual information only on the AU regions. Meanwhile, since it does not need to generate huge attention map to record the relationship for each pixel-pair in feature map, we reduce this dense computation in the whole feature map to a sparse fashion which only focuses on the pixels S of AU regions. The complexity of non-local operation in time and space are both O((W×H)×(W×H)), where W×H denotes the spatial dimension of input feature map. As a comparison, our sparse computation reduces the complexity from O((W×H)×(W×H)) to O(S×(W×H)), where S<W×H.
To sum up, we propose a novel Siamese Action-units Attention Network (SAANet) to establish a metric learning framework for FER, which integrates both the spatial and temporal attention modules: action-units attention module (AU attention) and attentive pooling module. For the backbone model in the siamese structure, we utilize a cascaded structure of VGG16 [40] with the Batch Normalization (BN) layers [10] and the Bidirectional Long Short Term Memory (BiLSTM) [38] as the baseline. As for the spatial attention, we propose a novel AU attention module, which learns the weighted feature representation of AU regions on faces, to enhance the spatial feature learning. We apply this module between the CNN and RNN, it can first help the model focus on the expressional regions, and then the RNN can learn the expression variation in time dimension on such regions. As for the temporal attention module, an attentive pooling matrix is utilized to further learn the expression intensity in time series following the RNN, to compute the attention vectors as contributed weights of each step in video. Moreover, to meet the video-based FER task, we utilize the Hinge loss to emphasize the intra-class similarity and increase inter-class distance within the pairwise input. Therefore, our network learns a discriminative feature representation and a similarity measurement simultaneously, thus making fully utilization of the emotion annotations.
The main contributions are as follows:
•We develop a metric learning framework with a siamese CNN-RNN baseline model to investigate detailed fine-grained distinct among facial expressions. To meet the FER task, a specific pairwise sampling strategy is proposed for such metric learning framework.•In the spatial domain, we develop a novel AU attention mechanism to aggregate the spatial contextual information on the crucial AU regions from long-range dependencies in a more efficient and effective way.•In the temporal domain, an attentive pooling module is taken to harvest the temporal correlations within the video frames.•We conduct experiments on four benchmark datasets (CK+, Oulu-CASIA, MMI, and AffectNet). The experimental results demonstrate that the proposed architecture outperforms state-of-the-art methods.
