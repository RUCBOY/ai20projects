Safety inspections on construction sites are a vital part of any company's injury prevention efforts. Despite the improvement in safety education and practices, the ever-growing desire for higher productivity is negatively impacting safety on construction sites [1]. For example, in the United States, around 20% of fatal injuries occur on construction sites [2], while construction workers make up less than 10% of the total workforce [3]. Noncompliance with proper protection and incorrect use of tools often result in environmental harm, object contact, and body part injuries. In 2017 alone, 145 fatalities were due to exposure to harmful substances or environments, and another 133 fatalities were caused by contact with objects and equipment [2]. In addition, in 2017, the U.S. Bureau of Labor Statistics (BLS) reported 8280 injuries caused by powered or non-powered hand tools, 6560 head injuries, 4850 ft injuries, and 13,530 hand injuries [4]. Ensuring that workers wear safety gear and use tools correctly makes a difference. The financial losses due to accidents on construction sites are substantial, often around billions of dollars every year. According to a previous study [5], the average total cost of a fatal accident is $4 million and that of non-fatal accident is above $42,000 (values in 2002 dollars).
Timely, effective, and accurate safety inspections are essential for evaluating and improving construction safety. Safety inspections are typically performed by manual observers who produce biweekly or monthly written reports. The frequency at which this process is carried out does not allow hazards to be recognized and eliminated swiftly. Furthermore, text-based written safety reports often do not describe safety hazards at a sufficient level of detail [6]. To bridge these two gaps in practice, efforts need to be made on automatically conducting safety inspections on massive amounts of data that is rich in details and easily comprehended by humans. Visual data from construction sites is a viable candidate for this purpose. Today, construction sites generate hundreds to thousands of photos and videos on a daily basis [7]. However, the majority of visual data is underutilized or used for progress tracking and as-built documentation purposes [6,8]. Researchers have been investigating use of computer vision methods on construction visual data. As generic computer vision methods [[9], [10], [11]] become more potent and accessible, a new opportunity arises to incorporate the massive and unmanageable amount of construction site visual data. One example is to complement today's safety inspection and documentation practices [8]. Photos are already taken on job sites on a daily basis, particularly when they are pooled from document management systems. Hence, automatically checking safety compliance from site photos increases frequency of site safety inspections. This procedure is particularly valuable if project teams are under-staffed. The effectiveness of such a procedure has already been demonstrated by recent commercial solutions. For example, SmartVid's artificial intelligence (AI) engine, which has been successfully applied to more than 1000 projects, detects hard hat, gloves, and safety vests for worker safety. Identified potential safety compliance issues are tagged and sent to safety managers for review and further comment. Corrections or manual annotations also help improve completeness and accuracy of data for future machine learning training and development purposes. When used at the enterprise level, such automated solutions help companies benchmark their projects against one another and prioritize safety training on projects. Comparing incident rates recorded via these systems against average industry numbers also helps construction companies decide on initiatives that can proactively improve safety and lower their insurance premiums.
Research on applying computer vision for safety inspection is still at an early stage [[12], [13], [14], [15], [16], [17], [18], [19]]. Many methods are not tested for robustness to occlusion, variation in object size and appearance, and differences in construction scenes [[17], [18], [19]]. Vision-based worker and equipment safety proximity checking methods often convert objects' 2D locations to 3D by camera calibration or monocular depth estimation [20]. Activity recognition from temporal data has also been investigated for safety applications. Previous researches on recognizing single worker and equipment's activity assume a few types of actions are performed during limited temporal intervals [[21], [22], [23], [24], [25], [26]]. Group and multi-agent activity recognition for construction often leverage the spatial relations between workers and equipment. Cai et al. [27] apply hand-made spatial cues, such as head pose and body orientation, to recognize groups of workers and equipment and then classify group activity. Kim et al. [28] improve individual's activity recognition by designing rule-based post-processing that leverages interacting excavators and dump trucks' object type, reconstructed 3D locations, and individual actions. Similarly, complex safety inspection tasks, such as fall protection, cannot be handled easily only using detector outputs [29]. Existing safety gear compliance checking methods, including SmartVid's AI engine, often rely on rule-based post-processing. For example, checking hardhat compliance by determining whether the hard hat box overlaps with the uppermost part of the worker box [17] will fail if the worker is not in an up-right posture. In this paper, the authors expand on this further by learning to recognize workers' interactions in static 2D images, as opposed to recognizing them using hand-made rules on detected construction objects. To learn the interactions, one needs a uniform and scalable interaction representation. Tang and Golparvar-Fard [30] present a framework to correlate construction objects with linguistic constraints in site images. This framework is extracted from textual safety rules and is associated with their visual correspondences in site images. However, Tang and Golparvar-Fard [30] only provide early examples of these correlations, and different interactions were not formalized in a consistent manner. The absence of representations that are both structured and formal impedes data annotation and learning tasks. In this article, the authors build on Tang and Golparvar-Fard [30] and formalize interactions in a uniform structure as presented in human-object interactions (HOIs).
Many safety checking tasks can be formulated as HOI recognition. In a job hazard analysis, individual steps of a job are often described by action verbs or action phrases, such as “holding tools,” “using grinders,” and “climbing ladders.” Potential hazards are associated with each step, and control measures are suggested. Similarly, the severity of hazards recognized from images can be more accurately evaluated by recognizing workers' interactions with the tools and the equipment. For instance, not wearing a face shield is not necessarily a noncompliance when the worker is not using a tool that produces sparks, heat, or strong light; not wearing hand protection while using tools is more critical than not wearing hand protection in the office trailer. Based on these observations, the authors propose a learned HOI model which improves the performance of existing vision-based safety checking methods and prevents false alarms. The authors argue that such a model not only improves existing safety gear compliance checking tasks, but also highlights critical noncompliance incidents. The authors present a number of experiments to validate their claims. First, the proposed HOI recognition model is compared and validated with a previous HOI method and a rule-based method. The method is more effective at retrieving actual interaction instances. Second, to demonstrate safety gear compliance checking, the proposed HOI model is compared with alternative checking strategies, such as using a rule-based HOI method and using object detection alone. For checking hard hat and safety coloring compliance, the model achieves better precision and recall compared to the rule-based method. For checking hand protection compliance while using tools, the HOI formulation outputs significantly fewer false positives than when object detection is applied alone. Also for this example, the proposed HOI model achieves better precision and recall compared with the rule-based alternative. These experiment results suggest a learned HOI model has practical value and improves existing vision-based safety checking methods. The authors will discuss these in greater detail in the Experiment Results and Discussions section. Fig. 1 shows an example of the model output. The authors also introduce an approach for collecting and annotating construction images to build a new dataset upon which the proposed HOI models are trained and validated.Download : Download high-res image (825KB)Download : Download full-size imageFig. 1. Example of HOI model output. (Left) The input image. (Middle) The object detector branch recognizes all targeted construction resource instances. (Right) The HOI branch recognizes interactions between detected object instances. Once objects and HOIs are recognized, fall protection safety questions such as “Are worker_1 and worker_3 having personal fall arrest systems?” and “Is worker_2 wearing a hard hat?” can be answered directly using model outputs. Best viewed in color and high definition.
