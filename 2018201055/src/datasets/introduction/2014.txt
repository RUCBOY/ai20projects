With the recent advancements in the digital world, there has been an exponential increase in the volume of the video data. This data explosion has put our existing network infrastructure at risk. As a result, any interested user has to browse through substantial video repositories to find the relevant video data. However, these users cannot decide without viewing the entire video content which is time-consuming. Time is of the essence. A summarized video can aid any interested user to make quick decisions. Moreover, a summarized video data can be stored and retrieved efficiently [1].
Video summarization extracts an abstract representation of the original video by selecting the keyframes of the videos and discarding the redundant parts. The keyframes are those frames which contain prime parts of the video. When a set of keyframes are viewed together, it can convey the essential message of the video. The video abstracts can be stored in less space and users can perceive the contents in less time compared to the original video. The abstracts must also be consistent with the human vision so that humans can understand the information conveyed by a video which spans hours long in few minutes. The video abstracts can be still abstracts (static storyboard representation) [2] or moving abstracts (video skims) [3], [4]. The still abstracts convey the essential message of the video as a sequence of frames. The temporal component in the input video is lost in still abstracts. Whereas, video skims represent summarized output as a short video which retains the temporal component. The still abstracts are simple to put into practice than the moving abstracts.
There exists a number of works for automated video summarization. Avila et al. in [5] emphasize on processing videos based on color histogram. Ejaz et al. [6] used aggregation of global features to detect keyframes. But, the global features fail to capture local characteristics of frames which is relevant for detecting content change between frames. Guan et al. in [7] used local SIFT features for detecting the key-frames. Later, Hannane et al. in [8] combined local features with optical flow for generating a good quality summary. Summarization was also done at content level [9] by selecting frames using dynamic programming approach. Zhu et al. [10] emphasized object level processing of frames to extract key parts of the input video.
Recently, with the development of high performance computers with GPUs, deep learning based methods have been developed for video summarization. Mahasseni et al. in [11] proposed the long short-term memory network (LSTM) to select the key-frames. The network is trained so that the reconstruction error is minimum. Fei et al. in [12] proposed video summarization framework based on entropy and memorability score. The memorability score is computed using Hybrid-AlexNet. The quality of summaries generated by these methods reveal that deep features can represent the frames more efficiently than handcrafted features. The sparse dictionary based method for summarization is explored in [13].
However, most of the existing methods consider the whole video for processing. The computational complexity of these methods is very high since a video has many redundant frames. The duplication of similar visual content in the input is unfavorable for generating a brief but comprehensive representation of the original video. The efficiency of video summarization framework can be improved if ambiguous frames are detected and discarded initially so that set of frames with less redundancy can be used for further processing.
Various techniques have been explored in literature to eliminate redundant frames before summarization. Avila et al. in [14] discarded redundant frames using a uniform sampling method by randomly selecting one frame in a second from the input video. Kuanar et al. in [15] identified that the frames corresponding to the significant valley of mutual information curve carry the essential content of the video. These frames must also be added to the presampled set of frames to prevent information loss. Song et al. in [16] selected those frames with minimum Euclidean distance to the average histogram of a shot as candidate frames for summarization. But these methods fail to achieve consistent results on videos of all categories. Thus, a technique that could remove the redundancy from input videos retaining every distinct frame and which generate consistent results for all categories of video is in demand and attracted researchers in this area.
Recent works on video analysis [17], [18] has proved the significance of motion vectors and temporal analysis in capturing the content change in video data. Moreover, the work in [19] suggests motion vectors are more discriminative when it is calculated after reducing the number of redundant frames from the input videos. Thus, an attempt to reduce redundancy based on motion vectors after an initial sampling would be helpful to reduce the computational burden of subsequent summarization step. So, to reduce running time and redundancy, we propose here a domain-independent redundancy elimination method for video summarization based on flow vectors after performing uniform sampling on the set of input frames. The method makes use of SIFT Flow algorithm to find the magnitude of displacement between consecutive frames after uniform sampling. Then, the redundant frames are eliminated using threshold value which is determined based on local averaging of the displacement magnitude. The method is tested on VSUMM and OVP dataset and it achieves high reduction rate of 97.64% and less error compared to other state-of-the-art methods.
The rest of this paper is organized as follows. Section 2 describes the proposed methodology for eliminating redundancy from input video before summarization. Experimental results and discussions are illustrated in Section 3. We conclude the paper in Section 4.
