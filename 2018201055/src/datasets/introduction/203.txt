As autonomous robotic vehicles (i.e., drones) proliferate, scholars expect them to be valuable to search and rescue (SAR1) Alharthi et al. (2018a); Khan and Neustaedter (2019) as they move from nuisance ABC News (2017); Hutson (2017) to valued teammates. As drones become more automated, we expect it to be useful for one person to direct multiple drones while maintaining situational awareness Endsley (1995). This pushes drones from being an extension of an individual pilot to members of a team and allows human operators to move from direct piloting via physically large, laptop-based user interface (UI) to wearable UIs that support mobility and situational awareness.
Our long-term objective is to build such wearable UIs for human-drone teams, predicting this need in the near future. To do so we build and test an intermediate step: a mixed reality that combines wearable UIs and humans operating in the physical world with a set of virtual drones. Such a mobile laboratory Ashbrook et al. (2009) is an ecologically valid research environment for developing algorithms for drones, UIs for directing drones, and hardware configurations that make up composite wearable computers (i.e., those assembled from heterogeneous pieces of hardware). This intermediate step enables us to design such systems without yet needing to consider the legal and safety ramifications of drone flight Dolgov and Hottman (2011); Dorr (2018); Hobbs (2010), which is outside our scope. The objective of the present research is to narrow down the space of needed devices to achieve such an interface by running a fixed laboratory study Ashbrook et al. (2009) using a 3D game UI, the Virtual Drone Search Game, to represent the physical world and considering wearable computer configurations. We develop the following research questions (RQs):
1.To improve technological mediation of wearable multiple-drone control, we contrast two input modality conditions. Which input modality will work more effectively? As an exploratory study, we do not make a directional hypothesis.2.We aim to further improve our mixed reality system for designing future wearable drone UIs and training operators. How can we improve our system to maximize knowledge gain from designs and to make the experience compelling and stressful (i.e., like real SAR scenarios) to participants?3.As drones become more automated, they transition from tool to teammate. What is a baseline for how participants socially experience the (virtual) drones when working with them?
To address the research questions, we compare input modality design. To narrow the design space, we performed grounded theory on hardware that could be composed to create a wearable computer system. Taking this analysis, along with our target domain, we ran a pilot study with three device configurations. Following the pilot, we identified two candidate input modality combinations that we believed would be effective:
•Gesture condition: Head-mounted display (HMD), wrist-worn touchscreen, Leap Motion free-air gesture controller, and Twiddler 3 chording keyboard; and•Tap condition: HMD, wrist-worn touchscreen, Tap finger-worn keyboard, and handheld mouse.
In this paper, we test human interaction with our UI to team with multiple drones to understand the design of future systems. We compare workload, performance, and situational awareness between two UIs to demonstrate the usefulness of our simulation for user testing and to learn which UI is most effective. We use free response questions to determine how to improve the system and game before we move it to a mixed reality system. Further, we examine how participants perceive the drones as teammates and potentially anthropomorphic agents.
The present research contributes ways to advance designing wearables to support human-drone teams. We find that: (RQ1) the Gesture condition produced better performance and lower workload than the Tap condition. However, across conditions, situational awareness of the drones was low. (RQ2) Based on participant comments, certain aspects of the game were more confusing than others (e.g., drone charging) and could be improved. (RQ3) Finally, participants felt like a group or team with the virtual drones, and rated them as moderately agentic.
The remainder of the paper is organized as follows. Multiple background sections cover SAR, human-drone teaming, wearables, and game design. We describe our research artifact, the Virtual Drone Search Game, providing details on game design and implementation, including how technologies are assembled. Our methods section provides insight into the empirical user study, and the results section organizes results according to our research questions. We then provide a discussion and conclusion, pointing to future work and its value to SAR.
