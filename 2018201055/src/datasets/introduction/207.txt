Recent advancements in the field of computer vision, natural language processing, and audio analysis enabled the deployment of intelligent systems in homes in the form of assistive technologies. These so-called smart homes come with a wide range of functionality such as voice- and gesture-controlled appliances, security systems, and health-related applications. Naturally, multiple sensors are needed in these smart homes to capture the actions performed by household residents and to act upon them.
Sensors for smart home applications — Microphones and video cameras are currently two of the most commonly used sensors in smart homes. The research in the domain of video-oriented computer vision is extensive, and the combined usage of a video camera and computer vision enables a wide range of assistive technologies, including applications related to security (e.g., intruder detection) and applications incorporating gesture-controlled functionalities (Zhang et al., 2019). However, one of the major drawbacks of using video cameras is their privacy intrusiveness (Rajpoot and Jensen, 2015). These privacy-related concerns are, at an increasing rate, being covered by media articles (Staples, 2019). Furthermore, a largely overlooked aspect of video-assisted technologies in smart homes is that video cameras are able to capture both smart home residents and visitors. Therefore, residents of smart homes need to be aware of the statutory restrictions on privacy invasion.
Low-power radar devices, as complementary sensors, are capable of alleviating the privacy concerns raised over the usage of video cameras. In that regard, the main advantages of radar devices over video cameras are as follows: (1) better privacy preservation, (2) a higher effectiveness in poor capturing conditions (e.g., low light, presence of smoke), and (3) through-the-wall sensing (Zhao et al.).
Frequency-modulated continuous-wave (FMCW) radars capture the environment by transmitting an electromagnetic signal over a certain line-of-sight. The reflections of this transmitted signal are then picked up by one or more receiving antennas and converted into range-Doppler (RD) and micro-Doppler (MD) frames (Chen et al., 2014). These frames contain velocity and range information about all the objects in the line-of-sight (for the duration of the recording). Recent studies show that with the help of (deep) neural networks, it is possible to leverage these RD and MD frames to recognize multiple individuals (Vandersmissen et al., 2018, Jalalvand et al., 2019) or to detect human activities with high precision (Wang et al., 2016). The aforementioned studies represent these RD and MD frames in the form of a sequence of mono-color images which are supplied as an input to deep CNNs. Three example RD frames and their corresponding video frames for the gesture swiping left are given in Fig. 1.
Convolutional neural networks — Even though CNNs were applied extensively in the work of LeCun et al. (1998) on the MNIST data set, they only became popular after some revolutionary results were obtained in the study of Krizhevsky et al. (2012) on the ImageNet data set (Russakovsky et al., 2015). That work was further improved by many researchers, showing the efficacy of deep CNNs (Simonyan and Zisserman, 2014, He et al., 2016, Szegedy et al., 2016). One of the main benefits of CNNs is their ability to automatically learn features, thus making it possible to forgo the cumbersome process of having to define hand-engineered expert features. Since CNN architectures are end-to-end differentiable, the features can be learned by an optimization method such as gradient descent. This property allows CNNs to be applied to various types of data beyond images and video sequences, such as text (i.e., natural language processing), speech, and radar data (Graves et al., 2013, Kim and Toomajian, 2016).
Nevertheless, CNNs usually also come with a number of detrimental properties, namely, (1) a high training complexity, (2) difficulty of interpretation, and (3) vulnerability to adversarial attacks.
Adversarial examples — The study of Szegedy et al. (2013) showed that the predictions made by CNNs may change drastically with small changes in the pixel values of the input images. Specifically, when the input data at hand are modified with a gradient-based method to maximize the likelihood of another class (i.e., targeted adversarial attacks) or to minimize the likelihood of the initially correct class (i.e., untargeted adversarial attacks), it is possible to create malicious data samples that are called adversarial examples. These adversarial examples are shown to exist not only in the digital domain but also in the real world (Kurakin et al., 2016), and are thus recognized as a major security threat for models that are used in a real environment (that is, an environment where the input is not strictly administered).Download : Download high-res image (286KB)Download : Download full-size imageFig. 1. (1) RD frames, (2) video frames, and (3) Grad-CAM heatmaps for the action swipe left. The X-axis and Y-axis of the RD frames and the Grad-CAM images, which are omitted for visual clarity, correspond to range and velocity, respectively.
Research on adversarial attacks on CNNs gained traction after the seminal studies conducted by Szegedy et al. (2013) and Goodfellow et al. (2014), with Biggio et al. (2013) providing an in-depth discussion of adversarial attacks on machine learning models deployed in security-sensitive applications. Since then, the susceptibility to adversarial attacks is recognized as one of the major drawbacks of deep learning, along with the lack of clear interpretability of these models. Some studies even go so far as to suggest that these two issues should be investigated as a single topic (Ross and Doshi-Velez, 2018, Tao et al., 2018, Etmann et al., 2019).
On the other side of this story, many defense techniques to prevent adversarial attacks have been proposed, only to be found ineffective (Carlini and Wagner, 2017, Athalye et al., 2018). As it currently stands, there are no defense mechanisms available that reliably prevent all adversarial attacks.
Aim of this study — Given the extensive research in the machine learning community on techniques to prevent adversarial attacks, we analyze the vulnerability of radar-based CNNs to adversarial examples, with the goal of assessing their significance as a security threat in smart homes. For this analysis, we consider the human activity recognition task presented in Vandersmissen et al. (2019), in which the proposed models were able to identify human gestures with high precision using a low-power FMCW radar. Our analysis of adversarial attacks covers a wide range of scenarios, from white-box attacks, in which the adversary is assumed to have all the knowledge about the underlying system, to localized attacks on radar frames under strict conditions, in which the adversary is assumed to have limited knowledge. Furthermore, we also attempt to analyze the connection between adversarial attacks and neural network interpretability by investigating the connection between prediction, perturbation amount, and Grad-CAM (Selvaraju et al., 2016), a popular deep neural network (DNN) interpretability technique.
This paper is organized as follows. In Section 2, we describe the mathematical notation, the data set, and the machine learning models used. In Section 3, we cover the details of the different threat models considered in this study, followed by a discussion of the experiments performed and the results obtained in Section 4. Next, we provide a number of additional experiments in Section 5, exposing the relation between adversarial examples and model interpretability. In Section 6, we conclude our paper and provide directions for future work.
