Studying degraded document images is an important aspect of a character recognition (CR) system. Note that the CR of a poorly degraded document is a challenging task. Moreover, the study becomes more challenging when different types of noise are present in degraded documents. Note that several types of degradation, such as faint text, water-spilling, ink-bleed, fox, and non-uniform illumination, are found in ancient documents. Documents with poor retention process and the digitalization process could lead to degradation in document quality. However, a document can not only suffer from a single degradation type but also from any combinations of degradation types. Fig. 1 shows ancient documents with combinations of degradation types. The document in Fig. 1a suffers from a combination of ink-bleed through, faint text, and yellowing background. The document in Fig. 1b suffers from a combination of faint text, low brightness, water spilling, and non-uniform illumination. Finally, the document in Fig. 1c suffers from a combination of low contrast, faint text, and uneven object. Note that binarization aims to extract significant information from a document's image. Identifying a method that can quickly and accurately binarize an image has received considerable interest from researchers. Moreover, several binarization techniques have been performed for resolving severe and complex degradation. These include traditional thresholding methods such as those reported by Otsu [1], Niblack [2], and Sauvola [3] or hybrid methods such as those reported by Su [4] and Nafchi [5]. A thresholding based approach is the most straightforward binarization technique; however, it fails for binarizing documents that have severe and complex degradation. Some modified thresholding methods such as Bataineh [6] and iNICK [7] were proposed to enhance the binarization performance but issues still remain.Download : Download high-res image (218KB)Download : Download full-size imageFigure 1. Examples of ancient documents suffering from combinations of degradation types. (a) Document suffering from a combination of ink-bleed through, faint text, and yellowing background. (b) Ancient Jawi document suffering from a combination of faint text, low brightness, water-spilling, and non-uniform illumination. (c) Document suffering from a combination of low contrast, faint text, and uneven object.
Hybrid methods undergo several stages to binarize an image. Several binarization methods, such as Gatos [8] and Lu [9], use background estimation as a preprocessing step to remove the non-uniform background of the document; however, iterative processing is required to estimate the appropriate background. Su et al. [4] proposed contrast construction to detect high contrast text and text boundary in a document; however, constrat construction encounters problem when show-through degradation exists. Moreover, an energy minimization-based binarization method was proposed [10] that solves severe complex background using Laplacian energy minimization. Nevertheless, a high computational cost is incurred when processing documents with a complex background.
Ramirez-Ortegon et al. proposed an edge-based binarization method using a smoother pixel transition, i.e. transition pixel [11]. Lelore and Bouchara improved the transition pixel method by reducing the number of existing parameters using two threshold values [12]. The primary drawback of the edge-based technique is that it cannot distinguish between the text and the degradation contour such as hard water-spilling noise, furthermore, most hybrid methods incur a high computational cost for complex degradation.
Recently, binarization was performed using deep learning (DL); however, DL has two primary drawbacks: long processing time and ground-truth (GT) image availability. Processing time is vital when binarization becomes part of a framework such as optical character recognition (OCR). However, to produce a reasonable result, training the DL model using the GT of the target dataset is necessary [13].
To summarize, existing techniques still have multiple drawbacks in terms both of performance and computational cost [14]. Furthermore, combinations of degradation types on a single document still pose an issue in binarizing ancient documents [5], which need to be addressed because old documents rarely suffer from only one degradation type. Preliminary studies have been conducted to binarize ancient Jawi documents. An ancient Jawi document is a form of Southeast Asian heritage that contains more information and knowledge compared to other heritages such as tombstone and calligraphic wall. As shown in Fig. 1b, many of these documents suffer from severe combined degradation. Existing methods, including the document image binarization contest (DIBCO) competition winner have poor performances in binarizing documents with such combined degradation.
In this study, our objective is to improve the performance of state-of-the-art methods for binarizing documents with severe combined degradation, particularly for ancient Jawi documents, and to propose a new binarization method for handling combined degradation on ancient documents based on histogram uniformity.
The proposed method comprises a new approach for improving degraded background to be uniform using histogram analysis; a new technique of improving contrast quality using histogram shifting and stretching; a novel local adaptive thresholding using a combination of local and global mean value; and an artifact removal stage.
The rest of the paper is organized as follows. In Section 2, we present the proposed method. In Section 3, we discuss the experimental setup. In Section 4, we present the results and discussion. Finally, in Section 5, we provide the conclusion of this study.
