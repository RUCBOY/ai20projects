Based on a large number of studies, it is found that attractors have good nature[1], [2], [3], [4], and no one can deny the importance of attractors. According to the distribution, attractors can be divided into two kinds: discrete attractors and continuous attractors. Discrete attractors are discrete located in the state space [5], [6], [7], [8]. There exists an attraction region around each attractor, and the trajectory from the attraction region converges to the attractor. A continuous attractor is a set of connected stable equilibrium points[9], [10], these stable points are continuous in state space.
Continuous attractors are quite different from discrete attractors. For example, discrete attractors can be used for associative memory. The famous Hopfield network[11] is a typical example of discrete attractor neural network. Continuous attractors are mainly used to describe the short-term memory mechanism in biological activities. Discrete attractors canâ€™t deal with continuous variables, but continuous attractors can solve this limitation well and can be used to code continuous external stimuli, such as the spatial location[12], [13], [14], moving direction [15], [16], [17], or those continuous features underlying the categorization of complicated objects[18], [19]. So continuous attractors reveal the essence of information processing and further characterize the ability of the brain to process information [20], [21], [22], [23], [24]. The dynamic behavior of continuous attractors in recurrent neural networks (RNNs) shows many interesting characteristics.
In the past, most studies focused on the continuous attractors of a single network [25]. Returning to our brain itself, in many cases, the brain uses multiple perception modules to perceive the surrounding environment, and now there are many researchers who focus on it [26], [27], [28]. In view of this, we should also pay attention to the situation of continuous attractors with multiple network coupling. Based on the inspiration of Long Short Term Memory (LSTM) architecture, a highway network structure is constructed in Refs. [29], [30], which can be trained directly by using stochastic gradient descent (SGD). This Recurrent highway networks model incorporates highway layers inside the recurrent transition, which designed to take advantage of increased depth in the recurrent transition while retaining the ease of training of LSTM.
Inspired by the above model, we have explored some issues related to parallel processing of multiple networks and studied the modularization behavior of networks. There are not only one stimulus received by neural network, but also many [31]. For this reason, a new model was born in Ref. [32], and the conditions for the neural network to produce continuous attractors were discussed. On this basis, we further deduce and prove the results of the coupled network.
The organizational structure of this paper is as follows. The model is shown in Section 2. Section 3 describes the relevant conclusions on the existence of non-zero continuous attractors in the model. We further prove the correctness of the theory by simulation in Section 4. Next, we discuss the effect of constant coefficient. Finally, the conclusion is drawn in Section 5.
