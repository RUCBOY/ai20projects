Deblurring is a typical problem for low-level vision tasks because of its importance in both the academic community and industrial applications. The low-quality image or video causes visually poor quality, which hampers some high-level vision tasks (Zhang et al., 2020c, Lee et al., 2011). In general, great progress has been achieved in the problem of image deblurring, while the problem of video deblurring has not been well explored because of its relatively complicated settings. Compared with image deblurring, the modeling of temporal information should be handled appropriately.
Naively, one can apply modern methods of single image deblurring to each single frame one by one to obtain the results as the deblurred video frames. However, the independence among the processing of multiple frames would inevitably introduce artifacts into the resulting video. To solve this problem, some traditional methods model the temporal information by explicitly forcing the continuous frames to be consistent after warping. However, there exist several drawbacks to these methods. First, forcing continuous frames to be consistent with warping involves many heuristics. For example, it requires the estimation of motion between continuous frames, which is time-consuming. And there are occlusion areas even if the motion is estimated correctly. Second, this kind of solution is typically very complicated and not end to end. It is difficult to tune and diagnose.
With the popularity of deep learning, several video deblurring approaches that using deep neural networks are proposed. For example, Su et al. (2017) propose a neural network that stacks five successive frames as inputs to produce the center sharp frame. Hyun Kim et al. (2017) concatenate the multi-frame features to recover the current frame by a deep recurrent network. These recent deblurring approaches often work on a fixed spatiotemporal scale and do not fully use spatial information from the center blurry frame. However, the spatiotemporal scale of blurs can vastly vary. This is because that most of the camera shake blur is short, spatially uniform and temporally uncorrelated (Su et al., 2017, Xu et al., 2014), while object motion causes long, spatially localized and temporally smooth blurs (Pan et al., 2016). Therefore, we propose a spatiotemporal pyramid module to process the input frames both in spatial and temporal dimensions via a pyramid mode that taking the advantage of the fact that the spatiotemporal scale of blurs in a video can vary vastly. The spatiotemporal pyramid module works on different temporal scales. It first takes the middle frame as the center, and divides five successive input frames into three different lengths in pyramid mode. Then it uses 2D convolution to process the center blurry frame, and captures temporal information from the successive subframes via 3D convolution. Finally, it dynamically fuses the spatiotemporal information to sharpen blurry ones. We put this module in front of the network to help the network (SPN) not only focus on the spatial information of the center blurry frame, but also learn the different scales of temporal information from nearby frames. With the help of this module, the deblurring performance of SPN is improved, as Fig. 1 shows.Download : Download high-res image (2MB)Download : Download full-size imageFig. 1. Exemplar video frames on the DVD dataset (Su et al., 2017) processed by DBN (Su et al., 2017), SPN and SPGAN. The input blurry frames are in the first row. The second row illustrates the deblurring results of DBN. The third row displays the deblurring results of SPN. The fourth row shows the deblurring results of SPGAN. The values of PSNR and SSIM are shown at the bottom of each frame. The higher values demonstrate better performance.
Moreover, with the emergence of generative adversarial networks, the problem of video deblurring has been addressed with the help of generative adversarial networks by existing methods to boost the realness of the deblurred video. This is conducted by discriminating the generated videos against the real-world sharp videos via an adversarial loss. This is effective in some cases, but with only limited improvement. We suspect that this is partly due to the following reason. The space of natural images can be hardly covered by the limited number of training examples for video deblurring. Thus the volume within which the discrimination between deblurred videos and real-world videos is conducted in fact is limited in the whole visual image space. To this end, we propose to alternatively discriminate the generated videos against real-world videos in the differential space of images. Existing researches (Chen et al., 2019, Pan et al., 2014) have shown that gradient images of sharp images and blur images are completely different. Due to the blurring process, the values of neighboring pixels tend to be closer to each other. Thus, for sharp images, values in their gradient images are usually greater than values in the gradient images of blurred images. In other words, the distribution of gradient images of sharp images is different from those corresponding to blurred images, and the blurring effect is well-observed in the gradient of a blurred image. In this paper, we discriminate the combination of gradient images against those of the real sharp videos. The insight behind is, the freedom degree of differential space of image is much smaller than that of the original visual image space, thus it is easier for the deblurring algorithm to generate videos with the gradient images sharing the same space with those from sharp videos. We call it as the adversarial gradient prior. Fig. 1 shows exemplar results.
Our main contributions are threefold. First, we propose a spatiotemporal pyramid module as a new tool to model spatiotemporal dynamics within the video for the specific video deblurring task. Second, we introduce the gradient space of the image into the discriminator of the generative adversarial network. With the goal of fooling the discriminator in the differential space of image, it is easier for the deblurring method to generate sharp videos. Finally, the proposed methods achieve state-of-the-art results by comparing with the existing methods on benchmark datasets.
The rest of the paper is organized as follows. Section 2 briefly introduces the related work of deblurring. Section 3 represents the proposed method. Experimental results are reported in Section 4. Conclusions are drawn in Section 5.
Download : Download high-res image (519KB)Download : Download full-size imageFig. 2. The architecture of the proposed SPN. The input is five consecutive blurry frames. The output is the central deblurred image frame. The orange arrow indicates the short-cut from the input to the output, so that the network focuses on residual learning. Zoom in for better visibility.
