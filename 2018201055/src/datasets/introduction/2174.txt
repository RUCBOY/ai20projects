1.1. Contextual overviewArtificial Neural Networks (ANNs) are known for their ability to accurately represent data in highly non-linear and multivariate systems. In fact, ANNs have already been successfully used to model chemical reactions (Ahmadi et al., 2009, Maltarollo et al., 2013), control small modular reactors (Manic and Sabharwall, 2011), optimize chemical systems (Polikar et al., 2001, Zhang et al., 2010), predict stock markets (Zavadskaya, 2017) and even identify malignancies in histological images (Xue and Ray, 2017). It has been demonstrated in the literature that ANNs offer certain benefits over traditional numerical approaches, both in accuracy, speed of execution and resiliency to sudden changes in input parameters (Ahmad et al., 2017, Parkale, 2012). Additionally, and perhaps most exciting, is that ANNs can self-train through algorithms. This means that in a day and age of distributed modular systems, exploiting lab-on-a-chip (LoC) technologies or advanced materials (such as in photochemical reactors), system control can be greatly simplified, and tuning can be merged with quality assurance in a production environment. Finally, ANNs can process significantly more data in a reasonable amount of time with modest computing resources. This can be leveraged either to control based on multivariate trends or to increase the quality of the input data. ANNs can easily be combined with unorthodox sensors that produce very large amounts of data through capture at high resolution, high speed or a combination of both. Overall ANNs offer many exciting benefits for process control in a variety of laboratory systems that can also aid in the design of production-scale processes.ANNs have been used in the past for process control quite successfully using various different approaches (Dayal et al., 1994, Hagan et al., 2002, Khalid and Omatu, 1992, Psaltis et al., 1988, Vamvoudakis et al., 2015, Zribi et al., 2015) and the NARMA-L2 neurocontroller algorithm has gained prevalence in the control of highly-nonlinear systems in the last two decades. The issue with many of these previous control schemes has been the difficulty of selecting an appropriate model and translating the system behavior into a data form understandable by the computer. For example, the NARMA-L2 control algorithm works by canceling nonlinearities to transform nonlinear dynamics into linear ones and PID-neural net systems require rigorous models of the system, just like first-principles PID tuning. While these existing algorithms have worked well where they have been applied, the fact remains that they need a rigorous model of the system, significant computational power, and expertise to implement. In this work we present a different approach to ANN-based control using computer vision and fully autonomous training for implementation in a thermal control system for a microfluidic reactor.The use of IR thermography in research is a popular method for monitoring reactions and receiving data from these reactions. Over the years, advanced IR thermography methods have been developed to maximize the efficiency of scientists and their research. IR thermography has demonstrated the ability to provide useful data beyond temperature control, such as enthalpy and kinetics measurements (Romano et al., 2015, Zhang et al., 2016). There have also been studies which have used IR thermography to monitor reactions and indicate the location of their intensity across a two-dimensional system (Fu et al., 2016). These methods have proven IR thermography to be very successful in providing data and monitoring reactions in microreactors and microchannels.IR thermography has attracted much attention among scientists allowing for further advancements of its applicability and methods to be made. Over the years many new methods and techniques have been developed. Infrared technology led to the development of precision thermometry to track temperature profiles of fast and highly exothermic reactions (Haber et al., 2013). Recently, a non-intrusive method to measure fluid temperature and two-phase flow patterns was developed (Liu and Pan, 2016). IR thermography has also been coupled with temperature frame processing methods to estimate heat distribution of chemical reactions along a channel in a microreactor (Pradere et al., 2006). The development of such measuring and monitoring techniques demonstrate the ability to collect a lot of spatially-dependent information in a short amount of time, hinting at the broad-scale acceptance of IR thermographic studies for micro-scale systems, and validates the benefits of using IR thermography in this study.Similarly, Artificial Neural Networks have proven to be beneficial for process control in scientific studies and have become frequently used in basic chemistry research. ANN's are highly effective at monitoring process systems with a lot of variables that altogether are too complex to manage without an ANN (Uraikul et al., 2007). Artificial intelligence has been widely applied to fields of study ranging from aquaculture to pollution minimization (Chan and Huang, 2003, Lee, 2000). ANNs have also had much success in representing phenomena in unit operations that are difficult to model. For example, recent research has focused on studying heat exchangers and modeling power consumption (Dudzik, 2011). Although ANN's have become a staple in scientific research the development of new methods involving ANN's have slowed down. Some methods that are currently used have been developed over a decade ago with very few studies involving modern advancements in ANN process control, despite the potential enabled by miniaturization of silicon technologies leading to much smaller, power-efficient and inexpensive processing systems. Despite the lack of many recent publications of ANN-based control, it is still a topic of great interest both to chemical engineers, and those outside the field.Overall, having resilient process controllers which can adapt to a variety of circumstances and operating parameters is of great interest to industry. In the past a lot of work has been devoted to the formulation of robust multivariate controllers (Ingham et al., 2015, Kourti and MacGregor, 1995, Kresta et al., 1991) and to the understanding of complex system dynamics. By enabling highly multivariate data input into the ANN algorithm, it is expected that multivariate control, especially in complex industrial environments, can be enabled and simplified, making it competitive with traditional univariate controls. Additionally, due to the increased simplicity of writing and optimizing neural networks, along with new developments in specialized neural network computing platforms, networks that collect, analyze, and act upon data can be distributed throughout a plant on small, low-power computers.
1.2. Artificial Neural Network overviewAn ANN is a synthetic representation of the biochemistry seen in nature, whereby the summation of weighted inputs, if accumulated to a threshold value, leads to a firing of the neuron. In turn this first neuron triggers other downstream neurons with each connection having a given weight. Represented mathematically the activation function of a neuron j at time t+1 would be:(1)aj(t+1)=f(aj(t),pj(t),θj)where aj symbolizes the activation of a neuron, θj represents the threshold/bias value determined during net training, and pj is the input of the network. The signal then propagates through the network with the function:(2)pj(t)=∑ifout(aj(t))ωijwhere ωij is the weight of a connection. Due to the simplicity of these arithmetic operations a shallow neural network can run very quickly, while a deep neural network can process unfathomable amounts of input data. ANNs like these are also known as multilayer perceptrons.In practice several different types of threshold functions for neurons are used in ANNs, including step functions, linear functions, sigmoid functions, hyperbolic tangents, and Rectified Linear Units (ReLU). These various forms of neurons are employed in different parts of the network including the hidden and output layers. Often, a network would be composed of a variety of different neurons linked to each other. A basic representation of a network with three hidden layers can be seen in Fig. 1.Download : Download high-res image (254KB)Download : Download full-size imageFig. 1. General structure of an Artificial Neural Net. Top: macro overview of input to output data flow, middle: inputs to individual neuron, bottom: different possible threshold functions for a neuron. Usually a normalized sigmoid is used as it gives the greatest flexibility to the training algorithm and thus best represents the data.
1.3. Artificial Neural Network trainingBefore a network can be used, it needs to be trained. This is accomplished via a variety of different possible training algorithms for which the general goal is to adjust the weight ω and bias values θ of the various neurons until the desired output is achieved. Adjustments to the weights change the contribution of each input to a neuron, while adjusting bias values will shift the threshold function/ adjust the steepness of a sigmoid. In practice, the training is carried out much like any standard optimization problem would be solved in engineering- a cost function is defined, and then parameters are adjusted until the ‘optimal’ solution is found within certain performance and accuracy targets. The cost function defines how far the current solution is from the optimal one, implying that the most optimal solution has the lowest cost. Although ANNs can usually achieve arbitrarily good fit, certain parameters are used to determine when to end training as to enable computation in a reasonable amount of time. There are numerous training algorithms available for network training, generally divided into conjugate gradient and quasi-Newtonian methods. It is important to remember that these algorithms require cleanly differentiable weight, input, and transfer functions to compute successfully.
1.4. Training algorithmsThe various training algorithms investigated herein include:1.Levenberg–Marquardt (Damped Least Squares) with forward training: first, the performance of a network is assessed with respect to the bias and weight variables. Backpropagation is used to calculate the Jacobian (Li et al., 2016), from where each variable is then adjusted to the Levenberg–Marquardt rules (Levenberg, 1944). Training occurs until either maximum computation heuristics are exceeded (time, number of epochs, etc.), training parameters are not met (performance gradient, mu or validation failure), or the network reaches its performance goal.2.quasi-Newton backpropagation: as engineers, we are all familiar with Newton's method, which relies upon computation of a Hessian matrix. However, the Hessian is very computationally intensive to compute as it involves the second derivatives. In quasi-Newtonian methods the Hessian is replaced with an approximation based on the gradient. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (summarized in Dennis and Schnabel, 1985) uses this approximation to compute updated weights and biases for the network. Training is continued until performance goals are reached or the number of epochs, or maximum time are exceeded.3.Resilient backpropagation (RProp): essentially, this is a basic first-order optimization algorithm, the weights in a network are updated based on the sign of the partial derivative of the error function (Riedmiller and Braun, 1993). If the sign of the error is equivalent between two consecutive iterations then the update factor is positive, if it changes sign then the update factor is negative. Training is continued until either performance goals are met, or computational heuristics are exceeded.4.Scaled conjugate gradient backpropagation: first backpropagation is used to calculate the derivatives of performance, then a scaled conjugate gradient algorithm is used (Møller, 1993). The conjugate gradient algorithm is based on conjugate directions, but a line search is not performed during each iteration of training. Training is continued until computation heuristics (number of epochs, minimum gradient or maximal failure rate) are exceeded or the performance goal is met.5.Conjugate gradient backpropagation with Fletcher–Reeves updates: again, this algorithm is similar to the conjugate gradient method, but it computes the search direction by dividing the norm square of the pervious and current iterations (Fletcher, 1964). Training is continued until computation heuristics (number of epochs, minimum gradient or maximal failure rate) are exceeded or the performance goal is met.6.Conjugate gradient backpropagation with Powell–Beale restarts: this algorithm is very similar to the conjugate gradient method, except it uses a search algorithm at each iteration (Powell, 1977). This search algorithm computes a search direction from the gradient and previous search direction. The search direction is reset based on a numerical test. Training is continued until computational heuristics (number of epochs, minimum gradient or maximal failure rate) are exceeded or the performance goal is met.7.Conjugate gradient backpropagation with Polak–Ribiére updates: again, this algorithm is similar to the conjugate gradient method, but it computes the search direction by using a formula involving the norm square combined with the gradient (Khoda et al., 1992). Training is continued until computation heuristics (number of epochs, minimum gradient or maximal failure rate) are exceeded or the performance goal is met.8.One-step secant backpropagation: first, backpropagation is used to calculate the derivatives of the performance vector with respect to weights and biases. Next, the variables are adjusted according to a search algorithm where the direction is calculated as a function of the gradient, step changes in the weights from the previous iteration and the gradient change from the previous iteration (Battiti, 1992). Training is continued until computation heuristics (number of epochs, minimum gradient or maximal failure rate) are exceeded or the performance goal is met.9.Gradient descent with momentum and adaptive learning rate backpropagation: as in the other methods, first backpropagation is used to compute the necessary derivatives. Next, the variables are adjusted based on the gradient descent with momentum (Moreira and Fiesler, 1995). After each iteration if the performance of the network is closer to the goal then the learning rate is increased, if it is further away the learning rate is decreased and the change is not kept. Training is continued until computation heuristics (number of epochs, minimum gradient or maximal failure rate) are exceeded or the performance goal is met.A final but important factor to consider about ANNs is their ability to self-train during operation. This is accomplished by sequentially updating the weight and bias values of the various neurons during use of the network. This functionality is important for longer-term instillations where variables such as wear and tear, weather fluctuations, input power quality or other transient factors can introduce a drift in the system. It is also useful for “teaching” the system how to respond to process changes such as flowrate or composition. By using each run to keep the training accurate, the set point accuracy of the controller algorithm can be kept more constant over time.
1.5. Introduction to IR imagingInfrared is as much part of the electromagnetic spectrum as is visible light, but unlike visible light its intensity is related to the temperature of an object through the concept of a black body emitter, tying it directly to the object's temperature. Due to advances in Vanadium Oxide and Uncooled Focal Plane Arrays (Li et al., 2011), thermal imaging cameras have transitioned from being bulky contractions requiring cryogenic cooling units to handheld webcams powered from a USB port. This has resulted not only in greater simplicity, but also in drastic price decrease, making them competitive with high accuracy thermocouples. This offers several distinct advantages to the chemical engineer, primarily ease of application, acquisition speed, accuracy, and resolution. Traditional thermocouples need to be placed all around a reactor, resulting in increased complexity, especially where long shielded wiring runs are required due to the analog nature of the signal. Thermal cameras can cover a large area of a reactor, or even an entire process, providing multiple measurements with minimal hardware. Also, IR cameras are capable of non-contact measurement, giving a distinct benefit when dealing with either extreme temperatures or sensitive environments such as bioreactors. This is also particularly useful for in-situ measurements as an entire reactor can be constructed of IR-transparent material. Finally, thermal cameras can capture data very quickly; standard models can capture 30 FPS and specialty units are capable of greater than 3000 FPS. This provides much higher granularity to the data as opposed to traditional thermocouples. Overall, IR imaging offers many benefits over traditional forms of temperature measurement, and it is useful in a variety of scenarios for which a detailed comparison can be seen in Table 1.Table 1. Comparison of the accuracy and data speed of IR cameras with traditional forms of thermal measurement.Measurement typeAccuracy (standard)Accuracy (specialized)Data speed (standard)Data speed (specialized)Number of measurement pointsIR Camera±0.2 °C< ± 0.1 °C33 ms/frame5 ms/frame300k to millions per frameThermocouples (T, J, E, K, N, R, S, B, C’’)±1-2.2 °C±0.25-1 °C1000 ms/ measurement50 ms/ measurement (aerospace grade)1Thermistors±5 °CVery high over narrow range––1
1.6. Computer vision conceptsComputer vision is the application of numerical algorithms to image data for the purpose of extracting important parameters. There are a few important concepts to understand; all are based on the fact that an image from the IR camera is basically an M × N matrix of values corresponding to the temperature measured pixels in the array. The first concept is focused on histograms, which are plots of the pixel quantity and a given intensity. Histograms are useful for finding the probability of a certain value as:(3)P(i)=hist(i)/(M·N)Next, basic algebraic operations can be performed on the matrix for the extraction of useful parameters. Subtracting two images can give information about motion (as only certain pixels will change value), multiplication by an array of 0 or 1 values in given shapes can mask off part of an image, and division by a matrix of intensities can be used to compensate for non-uniformities in the lighting or optics. Another important concept is pixel neighborhoods, or in other words, a pixel and the four pixels directly bordering it. Neighborhoods conform to the principle of symmetry such that:(4)(i,j)∈N↔(−i,−j)∈Nand a path is defined as a set of ordered indices where consecutive indices are adjacent, or P=(I1,I2,…,IN) such that Ii∼Ii=1 where ∀i=1,…,n−1. Finally, thresholding can be used to segment an image based on a given condition,(5)g(x,y)={1iff(x,y)>T0iff(x,y)≤TUsing these concepts an image can be manipulated to extract important information, such as the flow regime (slug, laminar or turbulent), and the position and properties of given fluid slugs.In laminar flow the standard deviation of measurements across a channel should be relatively small, and a consistent thermal gradient should be seen along the walls. This is because non-turbulent (low Reynolds Number) flow has a parabolic flow profile, with a consistent velocity in the center and decreased velocity near the edges. In turbulent (high Reynolds Number) flow the standard deviation of measurements would be higher and a consistent thermal boundary would not be observed near the walls. This is due to fluid mixing during flow and lack of a clear flow profile. Finally, slug flow should also be clearly visible due to the different thermal nature of the different fluids. Certain fluids would change temperature at different speeds, and thermal gradients could be observed between slugs, making identification and segmentation rather straightforward. Overall information from thermal measurements could be used to elucidate the flow regime in a microfluidic system.Canny edge detection is another important topic to consider in Computer Vision. This algorithm uses a set of operations to find edges in images for segmentation purposes (Canny, 1986). The first step in the algorithm is to apply a Gaussian filter to remove noise from the image. Noise can greatly impact the performance of the algorithm and even cause false detections, so it is important to smooth it out. Gaussian filters with a various kernels can be used depending on the application and signal quality. Next, the Hessian Matrix is computed such that(6)[∂∂x2∂∂x∂y∂∂x∂y∂∂y2]·I(x)=[IxxIxyIyxIyy]where the principle 2nd derivatives give then eigenvalues(7)[λ1λ2]and the principle directions are given by the eigenvectors [ev1,ev2]. This is implemented in the discrete form by first calculating the 2nd derivatives in the 4 raster directions, then choosing the direction with either the minimum or maximum 2nd derivative, and applying a discrete mask of either [1,−2,1] for linear changes or 1/2·[1,−2,1] for diagonals. Finally, the peak intensity in an image can be found by taking(8)n−=∇(G⊗I)/|∇(G⊗I)|then finding the neighborhood closest to n¯, finding the angle α to this neighborhood, and continuing the search until a peak is found. Overall, the methods of Canny edge detection are very useful for isolating segments of interest in a 2D image.
