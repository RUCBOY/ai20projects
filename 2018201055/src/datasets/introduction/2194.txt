According to the Scopus database—the largest abstract and citation database in the world—more than 60% of computer science papers are published in conference proceedings, compared to 2%, 2.5%, and 9% in health, life, and social sciences, respectively. The review process of major conferences in the field of computer science is rigorous and the work presented in these venues enjoy high visibility and attract large numbers of citations, with the advantage of disseminating ideas and research results more quickly than could be done through journals (Almendra et al., 2015). The fundamental role of conferences in computer science is emphasized in the Best Practices Memo for evaluating computer scientists and engineers for promotion and tenure published in 1999 by the U.S. Computing Research Association.1 These facts make determining the quality of conferences in which computer scientists publish their work essential because it can help in manuscript submission decisions, as well as decisions related to distribution of funds, evaluation of research proposals, faculty hiring, promotion, and tenure, among others (Freyne, Coyle, Smyth, & Cunningham, 2010; Vrettas & Sanderson, 2015). This task is also important because there is no widely accepted empirical method for evaluating conferences (Almendra et al., 2015).
Previous attempts at assessing the quality of conferences have relied on two main methods: consultation with subject specialists and application of citation-based metrics. The first method involves collecting and analyzing the opinions of experts in a given field. These exercises generally result in good quality assessments, especially when the number of experts consulted is significant. The experts’ domain knowledge and experience provide informed opinions about the quality of conference venues in their fields. Examples of projects that employ this method in computer science, the subject matter of this study, are:
1CORE (http://www.core.edu.au/), a rating system first developed in 2006 and last updated in 2018 (7th edition) by the Computing Research and Education Association of Australasia where 1505 conferences are classified into A*, A, B, and C; and2CCF (http://history.ccf.org.cn/paiming.jsp.htm), a rating system for conferences and journals developed in 2010 and last updated in 2015 (3rd edition) by the China Computer Federation, dividing the venues into 10 subfields and classifying them into three categories: A, B, and C. CCF covers 338 conferences.
One problem with these ratings is that they tend to favor conferences that are more popular among local computer researchers (Li, Rong, Shi, Tang, & Xiong, 2018). In the dynamic field of computer science, new venues are created or cease to exist on a regular basis and the quality of some venues may change frequently, further complicating this method (Martins, Gonçalves, Laender, & Ziviani, 2010). The cost and effort associated with consulting and collecting the opinions of a large number of subject specialists are high (Martins et al., 2010). Finally, such ratings are largely influenced by the manuscript acceptance rate—a good predictor for identifying some but not all top-tier conferences, given that the indicator is hard to obtain or is unreliable for many conferences (Küngas et al., 2013) and is influenced by many variables (Freyne et al., 2010). These aforementioned problems with expert-based rating systems may explain why CCF and CORE omit dozens of top-tier conferences as shown below.
In citation-based methods, researchers have applied various metrics and strategies to measure the quality of conferences. For example, Rahm and Thor (2005) analyzed the citation frequencies in Google Scholar of two main conferences (SIGMOD and VLDB) and three journals (Sigmod Record, ACM Transactions on Database Systems, and VLDB Journal) over a period of 10 years. They found that conference papers had a larger average number of citations than journal papers. Freyne et al. (2010) conducted an analysis of Google Scholar citations of 3258 conference papers and 5506 journal articles from 15 conferences and 15 journals in artificial intelligence and machine learning. They concluded that the impact of papers in top-ranked computer science conferences matches that of papers in middle-ranking journals and is only slightly above the impact of papers in journals in the bottom half of the Journal Impact Factor rankings. In contrast, Franceschet (2010) used Web of Science (including the Conference Proceedings Citation Indexes) and found that in computer science the impact of journal publications is significantly higher than that of conference papers. Vrettas and Sanderson (2015) analyzed Google Scholar citations of over 195,000 conference papers and 108,000 computer science journal articles. They found that A* conferences obtained a significantly higher average citation rate than A* journals, while conferences and journals graded A showed no statistical difference, and journals graded B and C had a significantly higher citation impact than conferences with the same rating. In an analysis of over 68,000 software engineering papers covered in Scopus up to year 2014, Garousi and Fernandes (2017) found that on average a journal article in the field is cited 12.6 times and review articles 18.5 times whereas conference papers on average attract only 3.6 citations. Results in these citation-based studies differed greatly due to different methods of analyses and different sources of data (Meho & Rogers, 2008; Meho & Yang, 2007).
In other types of citation-based studies, Zhuang, Elmacioglu, Lee, and Giles (2007) proposed using program committee members’ citation characteristics as a proxy for estimating the quality of a conference. In another study, Yan and Lee (2007) considered that in each field there is a set of recognizable papers of good quality (the seeds) and that these highly cited papers can be used to determine the quality of a conference. Martins et al. (2010) used the sum of Conference Impact Factor, the Conference Citation Impact, the Conference Size and the Conference Longevity to compute what they called the Combined Conference Factor. Loizides and Koutsakis (2017) proposed a conference classification approach based on its papers’ impact and their authors’ h-indexes. All of these proposals have merit but none seems to be universally adopted by the scientific community.
Commercial companies, too, have used citation data to rank conferences. In 2014, Google Scholar introduced its Metrics service, which enables the comparison of journal and conference citation performances.2 However, the service uses a measure, the h5 index, that is significantly influenced by the size of the venue (Vrettas & Sanderson, 2015), a limitation that also applies to conference rankings provided by Microsoft Academic, which was relaunched in 2016.3 To conclude, the scientific community still lacks an evaluation tool or method that is as standardized, influential, popular, and easy to use as the Impact Factor is for journals.
In December 2016, Elsevier announced the development of CiteScore, a database and method meant to be used as alternatives to the over-40-year-old Journal Citation Reports database and the Journal Impact Factor method.4 CiteScore measures the citation impact of journals, conferences, book series, and trade journals covered in the Scopus database (Colledge, James, Azoulay, Meester, & Plume, 2017). The result is a list of publication venues with a CiteScore value for each (we call this the CiteScore database). Table 1 provides a summary of the main differences between the CiteScore database and Journal Citation Reports, which publishes the Journal Impact Factor. For more details on these differences, see Fernandez-Llimos (2018).Table 1. Main differences between CiteScore database and Journal Citation Reports.CiteScore databaseJournal Citation ReportsPublishing historyElsevier, 2016-Institute for Scientific Information, 1975-1992; Thomson, 1992-2008; Thomson-Reuters, 2008-2017; Clarivate Analytics, 2017-Publication frequency and venueAnnually (June) + CiteScore Tracker, a monthly release of a provisional calculation via CiteScore list in ScopusAnnually (June-July) via Journal Citation ReportsData sourceScopusJournal Citation Reports database which is extracted from Web of ScienceMaterials covered (2017 edition)22,337 journals, 551 book series, 292 trade journals, and 180 conferences indexed in Scopus database11,655 journals indexed in Web of Science database. Does not cover conferencesMaterials omitted from 2017 editions of CiteScore and JIF although covered in Scopus and Web of Science595 journals, 52 book series, 10 trade journals, 2365 conference proceedings indexed individually, and hundreds of conference proceedings published in book series, such as the Lecture Notes in Computer Science1,345 journals and all conference proceedings indexed individually or within book series, such as the Lecture Notes in Computer ScienceCalculation method for each publication venue (using 2017 as an example)Citations received in 2017 to ALL items published in previous three years divided by ALL items published in previous three yearsCitations received in 2017 to ALL items published in previous two years divided by “citable items” published in previous two yearsAccess typeFreeStand-alone subscriptionNumber of fields used to classify materials334252
This study explores whether CiteScore provides the kind of tool or method the scientific community has been seeking for evaluating the quality of computer science conferences. More specifically, this study addresses three questions:
1How effective is CiteScore as a tool or method for assessing the quality of computer science conferences? How many and which conferences does CiteScore identify as top-tier and how do they compare with journals in the field?2How do CiteScore results for conferences compare with those of expert-based ratings and citation-based rankings?3Why do expert-based ratings and citation-based rankings such as Google Scholar Metrics and Microsoft Academic give low grades or do not rate/rank certain top quartile conference venues?
