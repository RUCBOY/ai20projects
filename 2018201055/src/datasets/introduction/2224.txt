Persons with impaired communication capabilities rely on constant help of human care-takers. Robotic service assistants can re-establish some degree of autonomy for these users if they offer adequate interfaces and possess a sufficient level of intelligence. Generally, such systems require adaptive task and motion planning modules to determine appropriate task plans and motion trajectories for the robot to execute a task in the real world. Moreover, it requires a perception component to detect relevant objects or to avoid accidental collisions with obstacles. With increasing capabilities of autonomous systems, intelligent control opportunities also become more important. Typical interfaces, such as haptic (buttons), audio (speech) or visual (gesture) interfaces, are well suited for most users. However, for persons with impaired communication skills these control opportunities are unreliable or impossible to use.
In this paper, we present and evaluate a novel framework, schematically depicted in Fig. 1, that allows closed-loop interaction between users with minimal communication capabilities and a robotic service assistant. To do so, we record neuronal activity elicited in the human brain with electroencephalography (EEG). Furthermore, we employ a deep convolutional neural network (ConvNet) approach for online co-adaptive decoding of neuronal activity, to allow users to navigate through a graphical user interface (GUI), which is connected to a high-level task planner. The GUI allows the intuitive selection of goals based on the generation of referring expressions that identify the objects to be manipulated. The set of feasible actions displayed in the GUI depends in turn on the current state of the world, which is stored in a central knowledge base and continuously updated with information provided by the robot and a camera perception system. Once a goal has been selected, the high-level planner decomposes it into a sequence of atomic actions. Subsequently, low-level motion planning techniques convert these into executable trajectories for the mobile manipulator. This approach minimizes the cognitive load of the user, which is a crucial aspect in the design of a BCI. Furthermore, the intelligence and autonomy of the system make it possible to communicate goals from a non-invasive BCI, which currently have low information transfer rates, to our robotic assistant composed of 11 degrees-of-freedom (DOF). In the following, we present the related work, describe the components shown in Fig. 1 and present a quantitative evaluation of the system regarding its performance and user-friendliness.Download : Download high-res image (438KB)Download : Download full-size imageFig. 1. Our framework that unifies decoding of neuronal signals, high-level task planning based on referring expressions, low-level motion and manipulation planning, and scene perception with a centralized knowledge base at its core. Intuitive goal selection is provided through an adaptive graphical user interface.
Download : Download high-res image (470KB)Download : Download full-size imageFig. 2. Detailed overview of our framework. It uses a brain–computer interface to decode the thoughts of the user. Thus, the user has control over a goal formulation assistant which is connected to a high-level planner. The commands sent by the high-level planner are then processed by low-level motion planners and executed on the robot. A perception system determines information on object poses, the user’s mouth position and liquid levels. Finally, a central knowledge base stores and provides data to connect all components.
