As we celebrate fifty years of the International Journal of Human–Computer Studies it is timely to reflect on how far human–computer interaction and symbiosis has evolved in five decades: what has changed in the underlying technology; what has changed in the personal and social impact of computer-based systems; what has been realized of the potential envisioned by the early pioneers; what new technologies and phenomena have emerged that were not envisioned; what negative side-effects have become apparent that now need to be managed; and what are our aspirations and challenges for the foreseeable future.
When we signed the publishing contract for this journal in December 1967 the cutting-edge issue for computer systems research was to facilitate and utilize interactivity as we migrated back from the batch processing1 of the 1950s, where the time cycle of interaction between those creating programs or solving problems and the computer’s response was hours or days, towards direct human–computer interaction where the time cycle was seconds, emulating person to person conversation.
Licklider’s (1960) vision of man-computer symbiosis, McLuhan’s (1964) of media as extensions of man, Engelbart’s (1963) of the augmentation of man’s intellect and his demonstrations of what was now possible through graphic interaction and Weizenbaum’s (1966) of how natural language conversation could be emulated, had stimulated research on and development of, the next generation of interactive computer systems and applications world-wide. Conferences and books on the Mechanisation of Thought Processes (NPL, 1959), Computer Augmentation of Human Reasoning (Sass and Wilkinson, 1965), Conversational Computing (Orr, 1968), and the Computer Utility (Parkhill, 1966) had encouraged the formation of a new community of practice focused on interactivity.2
The technology of stand-alone interactive computers in 1968 is best represented by minicomputers and microcomputers originally designed for instrumentation and control applications such as the PDP8,3 ELBIT1004 and MINIC.5 These offered high reliability and fast interactivity at a reasonable price and the ELBIT100 and MINIC were small enough to operate in the office or home without requiring a special environment. However, the initial human interface to these systems was ASR33 upper-case, 10cps teleprinters, and such typewriter-like terminals were the norm for several years.6
Experiments with time-sharing partitions on existing computer systems commenced in the early 1960s with Corbató et al.’s (1962) implementation of the Compatible Time-Sharing System (CTSS) on the vacuum-tube IBM709 and the transistorized IBM7090 mainframes, and BBN’s implementation of a similar system on the smaller and cheaper PDP1 minicomputer (McCarthy, Boilen, Fredkin, Licklider, 1963, Walden, Nickerson, 2011). The techniques developed were simple and widely applicable, illustrating how “any computer can be used for on-line systems” (Cooper and Heckathorne, 1967, p. 39), and time-sharing systems proliferated rapidly (Karplus, 1967).7 They often offered special programming languages designed for interactive development, debugging and applications, such as BASIC, JOSS, TELCOMP, BASYS, APL and so on (Schur, 1973).
1.1. Breakthroughs in the technology of interactivityImproved person-computer interactivity and accessibility have been continuing objectives since those early years. The first major breakthrough was in accessibility through the development of personal computers (Freiberger and Swaine, 1984) such as the Altair 8800 in 1974 (Roberts and Yates, 1975), Apple II in 1977 (Wozniak, 1977), and IBM PC in 1981 (Morgan, 1981), which were sufficiently low in cost as to make dedication of a computer to the tasks of one interactive user cost-effective.8 Low-cost personal computers encouraged software entrepreneurs to develop text-processing capabilities that provided a word processor at a lower cost than specialist hardware (Bergin, 2006), and added enhanced capabilities such as WYSIWYG and proportional spacing (Rubinstein, 2006). These, and other innovations such as the spreadsheet (Grad, 2007, VisiCalc, 1984), in turn expanded the market for personal computers, a classical positive feedback process leading to exponential growth.The second major breakthrough was in interactivity as the graphic user interfaces envisioned by Engelbart (Bardini, 2000) and implemented on the Xerox Alto in 1973 (Wadlow, 1981) became widely available in 1984 through the Apple Macintosh personal computer (Williams, 1984). This new mode of human–computer interaction was the culmination of world-wide research to improve the human interface beyond the keyboard and printer (Bardini, 2000, Bitzer, Slottow, 1966, Howard, 1963, Lehrer, Ketchpel, 1966).The third major breakthrough was in connectivity as computer-computer networking was integrated with human–computer interaction in the mid-1990s when commercial services were allowed on the Internet (Abbate, 1999, Gaines, 1998), Andreessen and Bina implemented forms supporting interactive widgets and programmed in HTML in their Mosaic web browser (Andreessen, 1993, Gaines, 1999), and applications were ported to operate through the web (Gaines, 1995) where HTML (and later CSS) provided a powerful human interface programming language. Multiprocessing techniques developed for timesharing enabled servers on the Internet to provide a wide range of services to large numbers of users through client processes on their personal computers, an efficient factoring of the computational workload.The fourth major breakthrough was in portability with the advent of the smartphone (Merchant, 2017) in 1997 as cell phone data transmission capabilities advanced to a stage that provided digital access at a reasonable cost, and enabled interactive computers to be made available as lightweight portable devices connected to the Internet anywhere that had access to the cellular network.A fifth breakthrough has occurred in the past decade as interactivity has been extended beyond computers and people to an increasingly wide range of smart devices on the Internet of Things (Evans, 2011, Miller, 2015) as part of the Internet of Everything (Di Martino, Li, Yang, Esposito, 2018, Meridou, Papadopoulou, Kapsalis, Kasnesis, Delikaris, Patrikakis, Venieris, Kaklamani, 2017). This includes not only instrumentation and control in industry, public utilities, home and office, but also autonomous mobile entities such as delivery drones, automobiles, trucks (Miller, 2015) and cyberwarriors (Allenby, 2018). By 2009 the number of such devices connected to the Internet surpassed the number of human users (Evans, 2011) and now greatly exceeds it.
1.2. Interactivity today — hyperconnectivity and malevolent usersWhen one examines our current world of interactivity some five decades later it is apparent that the initial visions of the potential of human–computer interaction have been largely realized. Interactive computing is now embedded in our everyday life and has become taken for granted (Ling, 2012) as an essential component of our modes of existence. We have long surpassed Wells’s (1938) vision of a world brain with all the advantages that he foresaw, but have encountered negative side-effects that he did not expect.The pressing current issues are ones that arise as byproducts of the hyperconnectivity (Quan-Haase and Wellman, 2006) that we have achieved. The pioneers focused on the individual and social good that could emerge but may not have adequately addressed the potential for harm in a world that has become highly dependent on information utilities (Parkhill, 1966) that connect everyone world-wide including a growing number of increasingly skilled malevolent users. Combating malevolent usage has now become the major problem and is far from resolution (van Bavel, Rodríguez-Priego, Vila, Briggs, 2019, Jansen, van Schaik, 2019, Williams, Hinds, Joinson, 2018).Many of the current issues of security and privacy were already recognized 50 years ago. I attended a session with that title at the Spring Joint Computer conference in 1967. Dash et al.’s (1959) Eavesdroppers, an exposé of illegal and semi-legal wiretapping, was cited in the papers and their discussion, and Westin’s (1967) influential analysis of the issues of legislating acceptable interception of communications was published shortly after the conference. There seemed to be widespread awareness of the potential for a surveillance society, but rather less of the potential for fraudulent exploitation. In addition, governments were seen as the primary instigators of surveillance, and the rise of what Zuboff (2018) has termed surveillance capitalism where commercial organizations track the lifestyles, political views, buying habits, and so on, of a high proportion of the population and use this for behavioural manipulation was not foreseen.I recollect one particular recommendation by a speaker from NSA: “Every user should be subject to common discipline and authority. He shall know and understand the conventions which are required of him to support the security system” (Peters, 1967, p. 284). Our studies of human skills training indicated that this was not a realistic requirement if access to interactive computing became widespread. For example, flying an aircraft required meticulous attention to procedures, and we tested for this capability to qualify those wishing to learn to fly. However, the majority of the population failed such tests. Rather than “meticulous attention to procedures,” a prevalent human behaviour is that of “muddling through” (Fortun and Bernstein, 1998). The technical recommendations in the 1967 NSA paper encompass much of what we are doing today to improve computer security, but one might want to replace the quoted human factors recommendation with a more realist objective based on Norman’s (Norman, 1990, Norman, 1999) notion of affordances: “Users should be supported by affordances that enable them to achieve their objectives in a secure manner”.9
1.3. Our problématique — protecting legitimate users from malevolent onesThe problématique of our current era is how to continue to facilitate and enhance interaction in a hyperconnected world threatened by malevolent users, whilst recognizing that patterns of interactive usage are now so widespread and ingrained that changing our intended users’ habits is probably not a viable solution, certainly not a complete one (Akhgar, Brewster, 2016, Ghosh, Turrini, 2010, Hartzog, 2018, Kshetri, 2010, Tropina, Callum, 2015). We cannot take a draconian approach that undermines functionality, usability or likeability (Shackel, 1991) to an extent that usage is no longer attractive. Malevolent users are a natural and unavoidable phenomenon of the dynamics of human society, and we need to understand their motivations, activities and skills if are to prevent the harm they intend or unwittingly cause — they are also users, albeit unwanted ones.It is not only deliberately malevolent users who are a problem. Unintentionally malevolent ones can also be highly disruptive, not only as end-users but, even more problematically, as those who control system functionality in their roles as designers or maintainers. Our everyday dependency on computer services may create serious problems when someone who has control over them is careless or incompetent and makes an error that affects large numbers of users. For example, e-commerce or financial websites that we use routinely can become dysfunctional because someone has decided to ‘improve them’ without understanding how they are used, or carrying out adequate user evaluations. The digital ecosystem is fragile and easily damaged. In addition, ill-considered and ill-managed attempts to go digital and replace systems developed over a long period of time that operate effectively through skilled people may have disastrous consequences, as, for example, with the “incomprehensible failure” (Committee on Public Accounts, 2018) of an attempt to centralize and computerize the federal payroll system in Canada.To understand these issues requires a conceptual framework going beyond current psychological and technological schemas developed for man-machine systems in the initial stages of human–computer interaction studies. The following sections provide a range of perspectives on the dynamics of civilization and the role of technology, none of which alone can provide definitive answers, but which together form a framework for understanding much of what is happening as we move towards a hyperconnected civilization. One major lesson of the history of innovation in knowledge and technology is the need to be defensive about possible adverse side-effects and prepared to react quickly to the unexpected — often through a positive feedback cycle of further innovations (Bijker, Hughes, Pinch, 1987, Tenner, 1996, Wojciechowski, 2001). In an era of rapid technological change, we have long learned the Red Queen’s lesson that it takes all the running you can do, to keep in the same place (Carroll, 1871, Chapter 2).
