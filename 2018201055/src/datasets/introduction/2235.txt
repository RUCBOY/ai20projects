Since the advent of computers, humans have dreamed of one day having a natural conversation with computers. In the beginning of the 21st century, the information service represented by call center represents the arrival of the era of highly intensive voice and text information. It has provided information support for the social service industry and economic benefits. It was predicate by Negroponte Nicholas in his well-known book “Being Digital”[1] that “in 1000 years, people and machines will talk more than people”. With no more than 20 years, with the development of speech recognition, speech synthesis and natural languages understanding technologies, natural conversation between human and computer has made great progress. After decades of development, the man-machine dialogue system has developed from the early telephone voice system, such as language learning, ticket and hotel booking, etc,2, 3, 4 to the current inaccurate questions and answers, such as speech assistant: Apple Siri and Google Duplex.
In traditional human computer dialog techniques, human-computer interaction focused on speech and language information processing. While in people’ daily face-to-face communication, their information is often transmitted from multiple channels, including facial expressions, emotional voice, postures and gestures. For example, in the process of human interaction, when one’s voice or tone is not enough to reflect the specific meaning of expression, human tend to express their intentions using not only speech but also facial expressions, body movements or gesture[5]. In these cases, a simple expression, such as gestures’ fast and slow movement, amplitude changes in smile, contain rich interactive information in human computer dialog. The multi-modal human-computer dialogue is superior to the traditional single mode in the efficiency and integrity of information expression.
Compared with traditional single-channel interaction, multimodal human-computer interaction has a wider and potential application in mobile interaction and natural interaction6, 7, such as smart home[8], smart human-computer dialogue9, 10, somatosensory interaction11, 12, 13, education[14], etc. In recent years, artificial intelligence technology contribute to largely improvement for single channel behavior perception, including speech recognition15, 16, face recognition17, 18, emotional understanding19, 20, 21, 22, 23, gesture comprehension24, 25, 26, 27, posture analysis12, 28, 29, handwriting comprehension30, 31, 32, eye trace33, 34, 35, touch36, 37, 38, 39, etc. Nowadays, the computer is able to understand the users well for one channel behavior.
Traditional single channel human computer interaction methods, such as mouse and keyboard, or graphic interface, obtain users’ input signals accurately from capture device. However, under the condition of multimodal human dialog and natural interaction, the machine needs to judge users’ intention from multiple channels, for example in the field of home service robots, when a user points to an apple on the table and says, “please give me the apple”, the robot needs to accurately understand the user’s target “apple” from the speech context and the “position” demonstrated by the gesture. Understanding of user intentions from users’ multi-modal information fusion plays key role in the nature interaction in human-computer dialog. Therefore, Multimodal data fusion in multimodal human-computer dialog is a very dynamic and extensive research field. The goal of this paper is not to present a complete summary of this field, but to analyze the characteristics of multimodal information processing in multi-modal human computer dialog, and then to introduce how multi-modal information fusion can increase the interaction feeling of human computer dialog.
The remainder of this paper is organized as follows: we first introduce the concept of multi-modal fusion and the assumption of multimodal information fusion in cognitive science in section 2; in section 3, we will introduce some multi-modal information fusion methods in practical applications; discussion and conclusions are presented in section 4 and section 5.
