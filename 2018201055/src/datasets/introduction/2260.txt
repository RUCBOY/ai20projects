The idea of quantum computing was first put forward in a rather vague form by the Russian mathematician Yuri Manin in 1980. In 1981, it was independently proposed by Richard Feynman. Realizing that (because of the exponential increase of the number of quantum states) computer simulations of quantum systems become impossible when the system is large enough, he advanced the idea that to make them efficient the computer itself should operate in the quantum mode: “Nature isn’t classical and if you want to make a simulation of Nature, you’d better make it quantum mechanical, and by golly it’s a wonderful problem, because it doesn’t look so easy”. In 1985, David Deutsch formally described the universal quantum computer, as a quantum analogue of the universal Turing machine.
The subject did not attract much attention until Peter Shor in 1994 proposed an algorithm that could factor very large numbers on an ideal quantum computer much faster compared to the conventional (classical) computer. This outstanding theoretical result has triggered an explosion of general interest in quantum computing and many thousands of research papers, mostly theoretical, have been and still continue to be published at an increasing rate.
During the last 20 years one can hardly find an issue of any science digest magazine, or even of a serious physical journal, that does not address quantum computing. Quantum Information Centers are opening all over the globe, funds are generously distributed, and breathtaking perspectives are presented to the layman by enthusiastic scientists and journalists. Many researchers feel obliged to justify whatever research they are doing by claiming that it has some relevance to quantum computing. Computer scientists are proving and publishing new theorems related to quantum computers at a rate of one article per day. A huge number of proposals have been published for various physical objects that could serve as quantum bits, or qubits. As of September 25, 2018, Google gives 71,400,000 results for “quantum computing”, 1,280,000 results for “quantum computer”, and 331,000 results for “quantum computing with”, and these numbers increase every day. The impression has been created that quantum computing is going to be the next technological revolution of the 21st century.
When will we have useful quantum computers? The most optimistic experts say: “in 10 years”; others predict 20–30 years (note that those expectations have remained unchanged during the last 20 years), and the most cautious ones say: “not in my lifetime”. The present author belongs to the meager minority that has been answering “not in any foreseeable future” [1], and this point of view is explained below.
The idea of quantum computing is to store and process information in a way that is very different from that used in conventional computers, which basically operate with an assembly of on/off switches, physically realized as tiny transistors.
At a given moment the state of the classical computer is described by a sequence (↑↓↑↑↓↑↓↓…), where ↑ and ↓ represent bits of information – realized as the on and off states of individual transistors. With N transistors, there are 2N different possible states of the computer. The computation process consists in a sequence of switching some transistors between their ↑ and ↓ states according to a prescribed program.
In quantum computing one replaces the classical two-state element by a quantum element with two basic states, known as the quantum bit, or qubit. The simplest object of this kind is the electron internal angular momentum, spin, with the peculiar quantum property of having only two possible projections on any axis: +1/2 or −1/2 (in units of the Planck constant ħ). For some chosen axis, we can again denote the two basic quantum states of the spin as ↑ and ↓.
However, an arbitrary spin state is described by the wave function ψ = a↑ + b↓, where a and b are complex numbers, satisfying the condition |a|2 + |b|2 = 1, so that |a|2 and |b|2 are the probabilities for the spin to be in the basic states ↑ and ↓ respectively.
In contrast to the classical bit that can be only in one of the two states, ↑ or ↓, the qubit can be in a continuum of states defined by the quantum amplitudes a and b. This property is often described by the rather mystical and frightening statement that the qubit can exist simultaneously in both of its ↑ and ↓ states. (This is like saying that a vector in the x-y plane directed at 45° to the x-axis simultaneously points both in the x- and y-directions – a statement that is true in some sense, but does not have much useful content.)
Note that since a and b are complex numbers satisfying the normalization condition, and since the overall phase of the wave function is irrelevant, there remain two free parameters defining the state of a single qubit (exactly like a classical vector whose orientation in space is defined by two polar angles). This analogy does not apply any longer when the number of qubits is 2 or more.
With two qubits, there are 22 = 4 basic states: (↑↑), (↑↓), (↓↑), and (↓↓). Accordingly, they are described by the wave function ψ = a(↑↑) + b(↑↓) + c(↓↑) + d(↓↓) with 4 complex amplitudes a, b, c, and d. In the general case of N qubits, the state of the system is described by 2N complex amplitudes restricted by the normalization condition only.
While the state of the classical computer with N bits at any given moment coincides with one of its 2N possible discreet states, the state of a quantum computer with N qubits is described by the values of 2N continuous variables, the quantum amplitudes.
This is the origin of the supposed power of the quantum computer, but it is also the reason for it’s great fragility and vulnerability.
The information processing is supposed to be done by applying unitary transformations (quantum gates), that change these amplitudes a, b, c… in a precise and controlled manner.
The number of qubits needed to have a useful machine (i.e. one that can compete with your laptop in solving certain problems, such as factoring very large numbers by Shor’s algorithm) is estimated to be 103–105. As a result, the number of continuous variables describing the state of such a quantum computer at any given moment is at least 21000 (∼10300) which is much, much greater than the number of particles in the whole Universe (only ∼1080)!
At this point a normal engineer, or an experimenter, loses interest. Indeed, possible errors in a classical computer consist in the fact that one or more transistors are switched off instead of being switched on, or vice versa. This certainly is an unwanted occurrence, but can be dealt with by relatively simple methods employing redundance.
In contrast, accomplishing the Sisyphean task of keeping under control 10300 continuous variables is absolutely unimaginable. However, the QC theorists have succeeded in transmitting to the media and to the general public the belief that the feasibility of large-scale quantum computing has been proven via the famous threshold theorem: once the error per qubit per gate is below a certain value, indefinitely long quantum computation becomes feasible, at a cost of substantially increasing the number of qubits needed (the logical qubit is encoded by several physical qubits). Very luckily, the number of qubits increases only polynomially with the size of computation, so that the total number of qubits needed must increase from N = 103 to N = 106-109 only (with a corresponding increase of the atrocious number of 2 N continuous parameters defining the state of the whole machine!) [2].
In this context, Leonid Levin, professor of mathematics at Boston University, has made the following pertinent remark: What thought experiments can probe the QC to be in the state described with the accuracy needed? I would allow to use the resources of the entire Universe, but not more!
