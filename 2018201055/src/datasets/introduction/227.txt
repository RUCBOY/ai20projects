The amount of multimedia data is large and is growing at an unprecedented rate. For instance, more than five hundred hours of video are uploaded every minute on YouTube. Detailed metadata of, for instance, music, video, and images is typically not available due to the high cost of manually describing these objects. This problem is often addressed by representing multimedia objects with automatically computed high-dimensional feature vectors or descriptors, such as Scale Invariant Feature Transform (SIFT), Gist, Bag-of-Features (BOW), Vector of Locally Aggregated Descriptors (VLAD) and deep features (100–1000+ dimensions) [1], [2], [3].
Searching in these databases, such as performed by content-based multimedia retrieval (CBMR) services, is a fundamental operation for several multimedia applications [4]. The most time consuming step in these searches consists in finding the k nearest neighbors (k-NN) of a query object descriptor(s) in the database. The exhaustive exact search is costly due to, not only the large databases used by web applications, but also to the high dimensionality of the descriptors. An alternative to the exhaustive search is to employ data structures, such as kd-trees [5] and k-means trees [6], to partition the space and efficiently prune data partitions during the search. However, the pruning efficiency and, consequently, the performance of these techniques degrade as the data dimensionality grows due to the well known curse of dimensionality [7].
The approximate nearest neighbors (ANN) search has been proposed for those applications in which the exact search is not strictly necessary, allowing for accuracy to be traded off for speed. This motivated the development of several ANN algorithms [6], [8], [9], [10], [11]. The product quantization nearest neighbor search [9] has received special attention among due to high performance and ability to minimize its memory footprint. This was attained by representing descriptors with small quantization codes and by using an inverted file to avoid exhaustive search in the quantized space. This algorithm received the name of IVFADC, as detailed in Section 3, and is used here with product quantization, although it may employ other quantization approaches. Thus, mentions to IVFADC in the rest of the paper refer to the indexing with product quantization.
Most of the previous work, however, has focused on optimizing ANN algorithms for batch execution using sequential execution on a single machine. This approach does not meet the demands of modern online CBMR services, which must handle increasingly large databases that are much larger than a common single node main memory’s capacity. Also, while online applications are concerned with minimizing the query response times of individual queries (queryresponsetime=queuewaitingtime+processingtime), those algorithms were developed to maximize throughput in batch scenarios where several queries are bundled and processed together as a single task. Besides, there are additional challenges in the online settings as these systems experience fluctuating workloads, and must adapt at run-time to minimize response times.
One way to adapt to fluctuating query rates is to vary the number of queries (query block size) that are dispatched for execution on the GPU in each device call. For instance, when the input query rates are low, the queue waiting time is negligible, and minimizing the processing time is the best approach for reducing the query response time. This is attained by reducing the query block size so that more computing resources or cores in the GPU are assigned to process each query. As the system load increases, the queue waiting time grows and dominates the query response time. Thus, the system should be configured to maximize throughput in order to minimize the queue congestion. A larger the number of queries executing concurrently may improve overall throughput due to amortization of synchronization, communication and startup overheads, and ability to increase its occupancy until a certain number of queries is used.
This work addresses these challenges with a distributed-memory parallelization of IVFADC targeting hybrid machines equipped with CPUs and GPUs. The proposed parallelization avoids data replication and allows for large databases to be searched. We have also developed strategies to execute query searches using CPU and GPU cooperatively. Even though the GPU can typically reach higher query processing rates, the CPU is able to significantly reduce the query response times under several query load configurations. We have additionally proposed an out-of-GPU memory execution approach in which the GPU is efficiently used when the database index does not fit into its memory, but can be stored in the CPU memory. In this case, the index is divided into disjoint partitions that are load independently to the GPU for processing input queries. The queries are then evaluated against all partitions whose results are merged to create the final answer. This configuration also allows for the GPU to be used in cooperation with the CPU, and work stealing is employed to minimize the load imbalance among the devices.
In summary, this paper makes the following major contributions:

•An efficient distributed memory version of IVFADC for hybrid CPU–GPU systems. The execution on a CPU–GPU machine can answer 6364 queries/sec with a single GPU. It also attained an almost linear scalability on a distributed execution with up to 256 V100 Nvidia GPUs for in- and out-of-GPU memory scenarios using a database containing up to 256 billion SIFT descriptors.•The DQPP strategy for adjusting the system at run-time under fluctuating workloads. It is responsible for selecting the best number of queries to execute concurrently on the GPU according to the system load. DQPP improved response times compared to the best static approach by 1.29× and 1.35×, respectively, for the GPU-only and CPU–GPU configurations.•An out-of-GPU memory execution scheme that uses work-stealing for maximizing performance on the CPU–GPU cooperation. The CPU–GPU execution with work stealing improved the throughput and response times compared to the GPU-only execution by 1.26× and 2.4×, respectively.
The remainder of this manuscript is organized as follows: Section 2 presents the problem statement and background on high-dimensional k-NN search. Section 3 details the product quantization nearest neighbor search. Section 4 presents the IVFADC parallelization on distributed memory and hybrid CPU–GPU equipped machines and details optimizations to deal with out-of-GPU memory execution. The response time aspects are presented in Section 5. The experimental results are discussed in Section 6, and our conclusions are in Section 7.
