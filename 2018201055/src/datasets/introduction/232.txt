As an indispensable way of communication toward the deaf persons, hand gesture-based sign language has been pervasively adopted in the communication toward the deaf mute. The study of understanding complex hand gesture and changeable gestures can accelerate the development of human–computer interaction (HCI) techniques by leveraging human hand gesture. Nevertheless, however, due to the complexity of human hand gesture and its complicated HCI application context, the study of human hand gesture recognition has always been a highly difficult problem in the past decades. Conventional human hand gesture recognition systems typically require the sign language users to put on expensive data gloves for sign language visual clue capture or color gloves to facilitate the feature extraction as well as other operations of hand gesture motions. Although this type of hand gesture understanding can achieve ideal recognition accuracy under controllable environment toward users. It is worth emphasizing that, the generalization ability of these hand gesture recognition models is poor. In practice, the HCI system designers have to manually engineer HCI features when adopting a different sign language data set.
By leveraging the position of human hand gesture in semantic information expression, human hand gesture can be divided into the autonomous gestures and non-autonomous gestures. The user’s hand gesture can comprehensively expresses the idea of mute performer in the process of the mute presentation. Meanwhile, hand gesture plays a key role in the semantic communication, which can be considered as the autonomous human gesture. Each independent user sometimes adopt hand gesture to facilitate the semantic expression so as to better express his/her emotion during the real-world communication. But human hand gesture only can better express the meaning and supplement each user’s expression ideas. This type of hand gesture belongs to the non-autonomous one and will not dominant during human semantic understanding. By leveraging the figures of human hand gestures during their communication activities, it can be divided into the centrifugal gesture and centripetal gesture. As an example, when each speaker sends a command, the figure gesture of the receiver belongs to the centrifugal gesture. As another example, the human hand gesture of traffic police in traffic control belongs to the centrifugal gesture. When the listener sends the emotional response when receiving a message, the human gesture will be considered as a centripetal one. As an example, when you agree to shake your hand, this belongs to a centripetal gesture. Typically, there are two ways for hand gesture maker to express his/her emotions. One is to deliver the emotion of the hand gesture maker using the fingers and palm movements when the arm stays still. Meanwhile, the second way is to ignore the movement of fingers and express his/her emotions using the hand movement track. In a real-world hand gesture understanding system, we will encounter the problem that the hand and fingers move at the same time. To facilitate hand gesture understanding, we have to enforce the corresponding constraints. When the hand is moving, then the finger movement should be ignored. When the fingers are moving, then the hand movement will be tracked. We observe that the final face of human hand gesture understanding is based on 2D image appearance. When the hand motion feature is extracted, it will hurt the performance of human hand gesture understanding.
In the literature, many human hand gesture understanding frameworks have been proposed and some methods are even commercialized. Nevertheless, the conventional hand gesture understanding models are still deficiently effective in many circumstances. Successful applications are only constrained in the controlled environment. In detail, these conventional hand gesture understanding models are frustrated due to the following shortcomings:

(1)To our best knowledge, there are a rich number of shallow/deep features characterizing each human hand gesture. However, how important of each kind of feature remains unknown. Based on our experiences, the weights of different features are determined on a specific hand gesture extraction environment. For example, for hand gesture recognition under poor lighting conditions, we have to emphasize more on the silhouette feature while suppress the rest ones. Comparatively, when the human hand gesture motion is ultra-fast, then we can assign a large weight to the optical flow feature, while the other features can be deemphasized. In practice, we want a dynamically tuned feature weighting scheme, wherein the importance of different feature channels can be adjusted by leveraging a specific application data set.(2)To optimally fuse the multimodal visual features, it is important to preserve the inherent local structure of feature distribution toward each single feature channel, as well as encoding the global structure of feature distribution toward multiple feature channels. In practice, maximally preserving either the local or global feature structure is not a difficult task. Nevertheless, it is difficult to simultaneously preserve the local and global structure of the features from multiple channels. Moreover, we observe that it is a challenging to design a solvable objective function that can maximally preserve the intra- and inter-channel feature structures.(3)To our best knowledge, the existing human hand gesture understanding models are usually based on a particular application context, which depend on lots of optimal settings. For example, the background is pure, the light condition is stable, and there are little occlusion. These requirements make it difficult to design a general human hand gesture understanding model that can be applied to different application contexts. Practically, we want to develop a general framework that can be easily applied to different human hand gestures. And besides, we want to make the system parameters not depend on each specific data set.
To solve or at least alleviate the above three shortcomings, we propose a novel framework for human hand gesture recognition toward the modern HCI systems. An overview of our proposed pipeline is presented in Fig. 1. More specifically, give a rich set of human hand gestures collected from multiple users, we first extract the silhouette, optical flow, and active-learning-based finger location of each hand. Afterward, we propose a novel local–global-based feature fusion algorithm to optimally combine the multiple visual features from each human hand gesture. This feature fusion algorithm can both maximally preserve the intra-feature structure and the inter-feature structure. Based on the fused feature, we formulate a large-scale image kernel, which is subsequently fed into a multi-class SVM for human hand gesture recognition. Notably, we use the kernel LDA to reduce the long kernel-based feature vector into a short one, which can be efficiently processed. Based on the predicted human hand gesture category, we can adopt it to enhance plenty of applications such as 3D games and virtual reality-based 3D architecture design.Download : Download high-res image (267KB)Download : Download full-size imageFig. 1. The pipeline of our proposed human hand gesture understanding framework.
The key novelties of our work can be summarized into three fold: (1) we propose a new local–global feature fusion scheme, wherein both the intra-feature and inter-feature structures can be optimally preserved; (2) we propose to extract multiple robust visual features to capture the complicated and varied human hand gestures; and (3) an image kernel-based vector is calculated to integrate the fused features, which is subsequently converted into the corresponding short vector for visual categorization.
The reminder of this article is organized as follows. In Section 2, we briefly review the previous work closely related to ours and claims the differences between them. In Section 3, we detail the pipeline of our proposed human hand gesture recognition models by introducing the technical detail of the three key contributions. Section 4 empirically shows our proposed method and demonstrates its superiority. The last section concludes the whole article and suggests some future work.
