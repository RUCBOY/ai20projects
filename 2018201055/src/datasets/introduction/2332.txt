Gestures are a natural form of human expression, and hands a natural mode of interaction with the physical world and objects in it (ZIMMERMAN et al., 1987, BUCHMANN et al., 2004). Gestures are used for communication and accompany speech in many different forms. They range from gestures that do not convey a specific meaning and simply follow the rhythm of the speech, to those enriching its meaning and symbolizing specific concepts (MCNEILL, 1992, QUEK, 2004).
Hand gestures in particular, including use of fingers and arms, are widely explored as a natural and intuitive interaction modality for a variety of applications. They are used as a sole, or one of the modes for interaction interfaces. It is believed that gesture based interfaces can reduce the complexity of interaction between humans and computers (New et al., 2003). Motivations behind the decision to use gestures in an interface can be varied. Gesture based interfaces used for computer applications can be more intuitive than established WIMP (Windows Icon Mouse Pointer) based interfaces, and allow inexperienced users to interact with computer applications, without undertaking extensive training (BUCHMANN et al., 2004, KIM et al., 2005, BEYER and MEIER, 2011). In medical applications or industrial environments, they enable touchless operation guaranteeing sterility or safer interaction. Gesture interfaces for Virtual Reality (VR) or Augmented Reality (AR) environments provide better immersion and do not require conscious attention dedicated to the specific gestures being performed (Deller et al., 2006). Spatial concepts can be expressed using gestures, and they are used in design and engineering, when externalising ideas (Vinayak et al., 2013). Interaction with comfort functions in a car can be achieved without taking the eyes off the road (Riener et al., 2013). Gestures can be used to help older population achieve easier interaction with electronic devices (Bhuiyan and Picking, 2011). These are just some of the examples, and new applications are constantly being developed. Use of gestures for these applications is supported by a variety of technologies. Development of Kinect (Kinect, 2018) and LEAP sensors (LEAP MOTION INC., 2018), which are portable and supported by Software Development Kits (SDKs) enabling simpler implementation, seems to have contributed significantly to the expansion of the field on gesture based interfaces since 2013.
While the gesture-based interfaces are being developed for various applications clear standards which could guide their further development are not apparent. For example, while interfaces supporting three-dimensional (3D) object manipulation exploring use of intuitive, affordable and non-intrusive interfaces are ubiquitous, none of the approaches used have been established as the baseline for future development (Vinayak et al., 2013). Investigation of patterns of gesture use, identifying commonalities and differences between different fields would be an initial step towards development of a standard framework for gesture elicitation for interaction interface development.
Review by Rautaray and Agrawal (2015), provides a survey of the gesture based research published up to and including 2012, and the content covered is largely that published prior to the uptake of Kinect and LEAP in the gesture research community. Reviews by HASAN and MISHRA, 2012, SUAREZ and MURPHY, 2012, and Pisharady and Saerbeck (2015) focus on recognition approaches. Three systematic reviews have been identified: one that focuses on usability guidelines for “health serious games” (Milani et al., 2017), one that focuses on data exchange formats (Santos¹ et al., 2015), and one that focuses on vision based gesture systems and algorithms for gesture recognition (Al-Shamayleh et al., 2018). The first one is in Portuguese and reports on only 16 studies. Reviews identified in the literature either cover recognition based research questions, rather than gestures themselves, or are not systematic, and information on patterns of gesture use cannot be extracted from them.
Further to this, the underpinning gesture theory found in the literature is heavily based on gestures observed as speech aid, gestures used in parallel with verbal communication. Classifications and definitions present in it may not be capable of fully describing the free-form in-air gesture interaction often used for interaction with 3D/VR/AR environments for example, or be aligned with its goals. This will be explored further in Section 2.2.
This paper reports findings from a systematic review aiming to answer the following research question: What are the patterns of touchless hand gesture use during gesture based interface design for interfaces which have reached the prototype stage? To answer this research question, the field of touchless gesture based interfaces is first mapped, and then patterns and commonalities between different approaches are explored. The nature of gestures used in different interaction interfaces is explored, along with the reasoning, if present, behind the gesture elicitation choices made in them. The nature of gestures is characterised by the motions used and the role they serve in the interface. The impact of the application field, technology, and recognition techniques on gesture use is also observed. The aim was to identify and classify types of gestures used in interaction interfaces, and explore similarities and differences encountered in their elicitation and implementation. While one of the inclusion criteria is the existence of a prototype, the interest was in the nature and type of gestures used in these prototypes, rather than implementation details. They are analysed in terms of the role the gestures used play, and classified based on established gesture theory. Admittedly, the goals of work focusing on application of gestures in an interface and theoretical classification of gestures based on their role in a communicative process are fundamentally different. However, articles reporting on prototypes of gesture-based interfaces often classify gestures using gesture theory, and exploring how well they can describe the gestures used was deemed worthwhile.
Section 2 provides the context and history of gesture research, and establishes the terminology and the classification approach that will be followed in this paper. Section 3 provides a description of the methodology followed in the article. Section 4 classifies the types of gestures identified in the articles reviewed. Section 5 summarizes application types encountered, and technologies and recognition approaches that facilitate them. Analysis of the role and nature of gestures used in different applications is reviewed in Section 6, and patterns of gesture use are covered in Section 7. Discussion in Section 8 establishes links between the gestures and applications, technologies and recognition approaches, and discusses the potential paths for future research.
