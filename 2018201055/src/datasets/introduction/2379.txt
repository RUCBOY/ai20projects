Computer simulations enable the study of complex models beyond what is possible analytically. The primary goal of a simulation therefore is the scientific or engineering result obtained from the computer experiment. The majority of time, however, is usually spent developing, testing, and optimizing the simulation codes. This is mainly due to the “knowledge gap”, stating that the increasingly specialized knowledge required to efficiently use modern supercomputing platforms is found in only a small group of people [1]. The knowledge gap can be reduced by abstracting the algorithmic implementation of the simulation from the computer system platform.
Such abstraction has a long tradition in computational science. It is typically provided as programming language extensions, high-level programming languages, software libraries, or as a framework combining programming language extensions and libraries. For high-performance computing (HPC), programming language extensions include OpenACC [2] and OpenMP [3] that provide a directive-based parallel programming model, CUDA [4] and OpenCL [5] for GPGPU and accelerator programming, co-array Fortran (CAF) [6], High-Performance Fortran (HPF) [7], and Unified Parallel C (UPC) [8]. Examples of high-level programming languages for parallel computing include Linda [9], providing a model for coordination and communication between parallel processes, Vectoral [10] for direct vector-processor programming, and Julia [11] designed for high-performance numerical analysis. Examples of software libraries for parallel HPC include implementations of the Message Passing Interface (MPI) [12] standard like OpenMPI [13] and MPICH [14], HPX [15] as a runtime system for parallel and distributed applications, the DASH implementation of the PGAS model providing general-purpose distributed data structures [16], and Charm++ [17] as an example of a framework for distributed parallel programming. Beyond HPC, a number of languages, libraries, and problem-solving environments [18] for simulation exist that focus on sequential processing, including the equation-based simulation language Modelica [19], a Matlab-based compiler [20], and the scientific computing environment FALCON [20]. Such languages and libraries can greatly reduce code-development overhead and render hardware platforms more accessible to a wide user base.
However, the abstractions on which these libraries and languages are based cannot be universal and concise at the same time. Fine abstractions, like those in MPI [21] can be universal (i.e., every parallel algorithm can be implemented with MPI), but may remain hard to use. Coarse abstractions, like the ParMetis library for parallel graph partitioning [22], are easy to use, but provide only limited flexibility (i.e., ParMetis cannot do parallel FFTs). The knowledge gap reduction that is achievable with general-purpose abstractions, like the ones above, is therefore limited by the trade-off between generality and ease of use.
Further reduction of the knowledge gap is possible using abstractions that are specific to a certain application domain. This has successfully been exploited by numerical simulation libraries like OpenFOAM [23] for finite-volume simulations, DUNE [24] and Trilinos [25] for finite-element simulations, DualSPHysics [26] and others [27], [28] for Smoothed-Particle Hydrodynamics (SPH) simulations, NAMD [29] and LAMMPS [30] for Molecular Dynamics (MD) and Dissipative Particle Dynamics (DPD) simulations, LibGeoDecomp [31] for cell-based decomposition codes, and AMReX [32] for Adaptive Mesh Refinement (AMR). Examples of domain-specific programming languages for parallel numerical simulations include DOLFIN [33], the programming language of the FEniCS framework for finite-element simulations [34], and Liszt [35], a domain-specific language for mesh stencil codes. Among numerical simulation frameworks, particle methods are particularly appealing from a software-engineering viewpoint, because they can be used to simulate models of all four kinds: discrete, continuous, deterministic, stochastic. For particle methods, mainly three frameworks for distributed parallel computing exist: POOMA [36], FDPS [37], and the Parallel Particle Mesh (PPM) library [38], [39] with its domain-specific Parallel Particle Mesh Language (PPML) [40], [41]. While these have successfully provided abstractions for rapid development of scalable parallel implementations of particle and particle-mesh methods, two of them seem to be discontinued (POOMA, PPM) and the third (FDPS) seems specifically focused on N-body problems. However, all these languages and libraries have successfully demonstrated the benefits of domain-specific abstraction, and helped close the knowledge gap in scientific high-performance computing.
Based on this past success, we here present OpenFPM, an open-source C++ framework for parallel particles-only and hybrid particle-mesh codes. OpenFPM is intended as a successor to the discontinued PPM Library [38], [39] using advanced methods from scientific software engineering, such as template meta-programming (TMP). OpenFPM combines the most general formulation of particle methods with classical mesh-based approaches. A particle is defined as a point in an arbitrary-dimensional space that carries an arbitrary number of arbitrary data structures. A mesh is a regular division of the space into polyhedra (Cartesian at the time of writing) with support for local refinement. OpenFPM uses C++ TMP to transparently handle simulation domains of any dimension, and to allow particles to carry any C++ object as a property, including objects of user-defined classes. OpenFPM also provides its own memory allocators and runtime dynamic load balancing in order to transparently distribute the data and the work among the processors, and to dynamically adapt to changes in local mesh resolution or particle density during a simulation. The functionality of OpenFPM therefore goes beyond that of the PPM Library, and it relaxes PPM’s most salient limitations, including the limitation to 2D and 3D simulations and the limitation to primitive types as particle properties. It also extends the capabilities by adding transparent dynamic load balancing, support for accelerator hardware, and automatic memory layout optimization. Using OpenFPM is further facilitated by a complete and up-to-date documentation, as well as a series of tutorial videos and example codes. It is actively supported in the long term with new functionality continuously being added. OpenFPM implements the same abstractions as the PPM Library [1], rendering it easy to understand for experienced PPM users and for novice developers alike.
