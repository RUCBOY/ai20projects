Over the past several years, labeled image datasets have played a critical role in high-level image understanding [1], [2]. However, the process of constructing labeled datasets is both time-consuming and labor-intensive [3], [4]. To reduce the time and labor cost of manual annotation, several works have focused on active learning. For example, Collins et al. [5] proposed to label some seed images to train the initial classifiers. Then, they leveraged these classifiers to carry out classification on other unlabeled images and find low confidence images for manual labeling. The process was iterated until sufficient classification accuracy was achieved. In [6], a system for online learning of object detectors was proposed. This system refined its models by actively requesting annotations on images. However, active learning methods require pre-existing annotation, which often is one of the most significant limitations when it comes to scalability.
To further reduce the cost of manual annotation, learning directly from web images has attracted more and more attention [7]. Compared to manually-labeled image datasets, web images are a rich and free resource. For arbitrary categories, potential training data can be easily obtained from an image search engine [8], [9] like Google1, Bing2, or Baidu3. Unfortunately, the precision of images returned from these search engines is still unsatisfactory. For example, Schroff et al. [8] reported that the average precision of the top 1000 images for 18 categories from the Google Image Search Engine is only 32%.
One of the main reasons for the noisy results is the problem of visual polysemy. As shown in Fig. 1, visual polysemy means that a word has multiple semantic senses that are visually distinct. For example, the keyword “coach” can refer to multiple text semantics and visual senses (e.g., “bus”, “handbag”, sports “instructor”, or “company”). This is commonly referred to as word-sense disambiguation in Natural Language Processing.Download : Download high-res image (371KB)Download : Download full-size imageFig. 1. Visual polysemy. The keyword “coach” can refer to multiple text semantics, resulting in images with various visual senses being included in the returned results by an image search engine.
Word-sense disambiguation is a top-down process arising from ambiguities in natural language. The text semantics of a word are robust and relatively static, and we can easily look them up from a dictionary resource such as WordNet [10] or Wikipedia [11]. However, visual disambiguation is a dynamic data-driven problem that is specific to images collection. For the same keyword, the visual senses of images returned from the image search engine may be different at different time periods. For example, the keyword “apple” might have mainly referred to the fruit apple before the “Apple” company was founded.
The traditional way of handling visual polysemy falls back to expert knowledge, WordNet or Wikipedia. However, this human-developed knowledge suffers from the problem of missing visual information and still requires manual annotation to bridge the text semantics and visual senses [12], [13]. Some existing works attempt to reduce the influence of visual polysemy by filtering out irrelevant images [14], [15]. For example, Li et al. [15] utilized the few top-ranked images returned from an image search engine to learn the initial classifier. Then, the classifier refined its model through an incremental learning strategy. With the increase in the number of positive images accepted by the classifier, the learned model would reach a robust level. Hua et al. [16] leveraged a clustering-based strategy to remove “group” noisy images and a propagation-based mechanism to filter out individual noisy images. These existing methods have the advantage of eliminating manual intervention. However, none of them can directly address the problem of visual polysemy.
Since the text semantics and visual senses of a given keyword are highly related, recent works have also concentrated on combining text and image features [17]. Most of these methods assume that there exists a one-to-one mapping between semantic and visual senses for a given keyword. However, this assumption is not always true in practice. For example, while there are two predominant text semantics for the word “apple”, there exist multiple visual senses due to appearance variation (green vs. red apples). To deal with the multiple visual senses, Chen et al. [12] adopted a one-to-many mapping between text semantics and visual senses. This approach can help us discover multiple visual senses from the web but overly depends on the collected webpages. The effectiveness of this approach is greatly reduced if we fail to collect webpages that contain enough text semantics and visual senses [29].
Instead of relying on human-developed resources, we focus on automatically solving visual disambiguation in an unsupervised way. The motivation behind this work comes from the fact that keyword-based image search may yield multiple visual senses, and those returned results change dynamically. Therefore, the proposed approach should have a better time adaptability. Unlike the common unsupervised paradigm, which jointly clusters text and image features to solve visual disambiguation, we present a novel framework that resolves it by dynamically matching candidate text queries with images retrieved for the given keyword. Compared to human-developed and clustering-based methods, our approach can adapt to the dynamic changes in the search results. Our proposed framework includes three major steps: we first discover and then dynamically select the text queries according to the keyword-based image search results, we employ the proposed saliency-guided deep multi-instance learning (MIL) network to remove outliers and learn classification models for visual disambiguation. To verify the effectiveness of our proposed approach and demonstrate its superiority, we conduct extensive experiments on visual polysemy datasets CMU-Poly-30 [12] and MIT-ISD [18]. The main contributions of this work can be summarized as follows:
1) Compared to existing methods, our proposed framework can adapt to the dynamic changes in search results and carry out visual disambiguation accordingly. Therefore, it has a better time adaptability.
2) We propose a saliency-guided deep MIL network to remove outliers and jointly learn the classification models for visual disambiguation. Compared to existing approaches, our proposed network achieves state-of-the-art performance.
3) Our work can be used as a pre-step before directly learning from the web, which helps identify appropriate visual senses for sense-specific image collection, thereby improving the efficiency of learning from the web.
The rest of the paper is organized as follows: Section 2 elaborates the related works of visual disambiguation. We propose our framework and associated algorithms in Section 3. The experimental evaluations and ablation studies are presented in Sections 4 and 5, respectively. Section 6 concludes this paper.
