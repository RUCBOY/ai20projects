Biometrics is the automated recognition of an individual on the basis of her/his distinctive physiological or behavioral characteristics and has been progressing at an accelerated rate in the past decades. So far, various forms of biometrics systems, such as face, iris, fingerprint (FP), finger vein (FV), palmprint, voice, gait, and so on, have been successfully used in practical applications [1], [2]. However, unimodal biometrics systems are often limited due to their susceptibility to changes in personal biometric characteristics, noisy data, intra-class variations, and spoofing attacks [3]. Therefore, multimodal recognition systems, which generally fuse two or more traits, have attracted extensive attention in the field of identity authentication [4], [5].
Compared with the unimodal finger biometric system, the multimodal finger biometric system, which contains more diverse finger characteristics, is expected to improve the recognition accuracy, security, and reliability of the system [6], [7]. In recent years, finger-based multimodal biometircs technology has developed rapidly in the personal identification community. As one of the most commonly used finger traits, finger knuckle print (FKP) are collected at the joints on the back or inner surface of the finger, with highly distinctive features, such as rich line and texture feature [8]. Unlike finger knuckle print, which can be seen or touched, finger vein are located underneath the skin surface making veinous replication or stealing impossible in real applications [9]. To this end, a finger vein trait has inherent anti-counterfeiting and active liveness, which exhibits remarkable advantages in identity authentication. Although there are advantages demonstrated by these two finger patterns, there is also inadequate research on multimodal biometric systems by fusing finger vein and finger knuckle print traits. Hence, in this paper, we propose a multimodal finger feature recognition approach, which integrates finger vein and finger knuckle print features.
In the design of such a multimodal biometrics, we must resolve two challenges: feature representation and feature fusion. There is no doubt that a suitable feature representation method is significantly beneficial for improving the performance of multimodal finger recognition. Existing finger feature representation approaches can be roughly divided into two categories: learning based [4] and non-learning based [10]. Most the non-learning based methods are hand-crafted by design and usually require strong prior knowledge to establish the feature descriptors. Besides this, the learning-based approaches, such as principal component analysis (PCA) [11] and linear discriminant analysis (LDA) [12], which usually learns a projection subspace to make the features more compact and discriminative, have been widely used for biometrics recognition. For example, Swati et al. [12] proposed a finger knuckle print feature extraction method by combining PCA and LDA principles. According to the fusion level, the feature fusion approaches usually can be divided into: sensor-level fusion, feature-level fusion, score-level fusion, and decision-level fusion [13], [14]. Recently, there have been plenty of feature level-based fusion methods proposed for multimodal finger recognition [15], [16]. For example, Shekhar et al. [15] developed a sparse representation based feature level fusion method for multimodal (iris, fingerprint, and face) recognition, which is robust to noise and occlusion situations. Khellat-K et al. [16] proposed a kernel fisher analysis (KFA) based feature selection method for multimodal fusion of FP, FV, and FKP, which can reduce the feature dimensions and time complexity. Yang et al. [17], [18] proposed the granulation computing based feature fusion strategies, which separately extracted the finger features from different modalities. Unfortunately, these feature fusion methods only consider the relationships of within-class and between-class samples, while ignoring the implicit correlation between the inter-modality of the within-class samples.
Considering these above mentioned limitations, in this paper, we proposed a joint discriminative feature learning (JDFL) method for multimodal finger recognition. The basic idea of the proposed method is shown in Fig. 1. Specifically, given the training finger vein and finger knuckle print images, we first formed the dominant direction vectors (DDVs) of the multimodal images by the unified descriptor. For the obtained DDVs, we jointly learned a bank of feature mapping functions that can convert multimodal features into discriminative binary codes. In the feature learning strategy, we established the objective function by enforcing three constraints to make intra-class samples more compact and inter-class samples more separate. After the projection functions are obtained, for the testing FV and FKP images, we first calculated the multimodal DDVs and then transformed them into discriminative feature maps. Finally, we concatenated the block-wise histograms of the multimodal feature maps to form the feature descriptor of the testing samples. Experimental results on a multimodal finger database demonstrated that the proposed method exhibited better recognition performance than state-of-the-art finger feature representation and recognition methods.Download : Download high-res image (582KB)Download : Download full-size imageFig. 1. The flow chart of the proposed JDFL-based method. 1) We first obtain the DDVs of the multimodal finger images by using the unified representation. 2) Then, we jointly learn the feature projection matrix by the training samples. 3) Based on this, we convert the multimodal DDVs of the test sample into binary codes by using the learned projection matrix. 4) Finally, we calculate and fuse the local histograms of the binary feature maps for multimodal recognition.
Based on the above consideration, the main contributions of our work is highlighted below: i. We formed the dominant direction vector (DDV) of a finger to further feature learning and representation, which can provide more discriminative and informative features than the raw pixel. ii. We proposed a joint discriminative feature learning (JDFL) framework to automatically learn and encode the DDVs of the multimodal images, which simultaneously considered the correlations of intra-modality and inter-modality samples. iii. We developed a robust feature level fusion strategy for multimodal finger recognition. Exhaustive experiments on a multimodal finger database showed the effectiveness and robustness of the proposed method.
The rest of this paper is organized as follows. Section 2 introduces the related work about finger-based feature representation and recognition. In Section 3, we present in detail the proposed joint discriminative feature learning method. Section 4 proposes the JDFL-based feature fusion for multimodal finger recognition. The obtained experimental results are discussed and analyzed in Section 5. Finally, conclusions are drawn in Section 6.
