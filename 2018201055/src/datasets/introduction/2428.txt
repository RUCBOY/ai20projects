Uncertainty can be found in any experimental measurement or mathematical model due to assumptions/simplifications in the modeling process, bias of experimental devices, or lack of understanding of a physical process. For simple problems at which the process can be modeled with high accuracy both theoretically and experimentally, the uncertainty can be controlled to have small effect on the response. However, for complex phenomena, uncertainty could come from various sources, and it can propagate causing a large spread in system response. Nuclear reactor modeling and simulation entails coupling of various computer codes (e.g. mathematical models) which represent different physical phenomena (e.g. reactor physics, reactor kinetics, thermal-hydraulics, fuel depletion, fuel performance). The mathematical model forming each physical phenomenon contains various uncertainty sources. Neglecting such uncertainties in nuclear modeling can severely affect prediction of responses in real-world problems. Uncertainty is usually classified into two main categories: aleatoric (inherent which cannot be controlled) and epistemic (systematic which can be controlled) [1]. In this study, epistemic uncertainty is classified into five main categories as: (1) parametric/input, (2) experimental/measured, (3) predictive/model discrepancy, (4) model-form, and (5) interpolation/statistical uncertainties.
Parametric uncertainty results from the model parameters that are used as input to the computer model. Some model parameters have inexact values, cannot be measured in a lab, or difficult to be inferred by statistical/fitting methods. Experimental uncertainty is inevitable in any measurement setup due to the systematic uncertainty of the instruments used in the experiment and random uncertainty that can be reduced by repeating the same experiment several times. Predictive uncertainty results from the fact that any physical model is a reduced representation of reality, and hence it cannot match the experimental data exactly. Therefore, predictive uncertainty represents the model deficiency or discrepancy. For some problems, exact/analytical solutions to the mathematical problem of interest are not available, at which a numerical or Monte Carlo technique is used instead. Model assumptions, numerical approximations, lack of physics understanding feature the model discrepancy/uncertainty. Model-form uncertainty is the multi-dimensional form of the model discrepancy source. Due to the lack of knowledge about the physical phenomenon, various models are developed to simulate the same physical process, but none of them can accurately represent the reality. These models could have different structure, physics involved, solution methods, and/or approximations. Model-form uncertainty arises from model selection in which several qualified models should be considered in the UQ and response prediction (since we believe all models are deficient). Additional discussions on the differences between model-form and predictive uncertainties are presented later in Section 2.2. Interpolation uncertainty can occur for cases when there is lack of data available from a computer code. For example, some computer codes take extensive time to run which limits their usage for UQ applications. A common practice is to replace such high-fidelity code by a reduced order model (ROM), also called surrogate or metamodel, which takes negligible time to run. Using ROM instead of the original high-fidelity model introduces what so-called interpolation uncertainty. Hypothetically if the parametric, experimental, and interpolation uncertainties are zero, both predictive and model-form uncertainty are still causing a discrepancy between the model prediction and the true value.
Quantification of model-form and predictive uncertainty is a widely investigated topic due to its importance for UQ applications using computer models. Due to the difficulty and complexity of quantifying and modeling the model-form uncertainty, it has been neglected in many studies that perform UQ of nuclear reactor simulations. Neglecting predictive and model-form uncertainty can cause significant underestimation of the final response uncertainty, especially if the computer model is inaccurate and/or not validated against experimental data. Lets assume that there are different models (e.g. computer codes) M1,M2,…,MN which simulate similar phenomena. The analyst usually picks one model without considering the uncertainty that results from this model selection. Bayesian Model Averaging (BMA) is a common technique to work around such a problem [2]. Given a response of interest at which experimental data is available, different mathematical/statistical models are used to predict that response. BMA uses new observations to update model probabilities using the Bayesian framework, for which the model with highest probability is considered the best model for this dataset [2]. The process of model averaging in BMA refers to estimating the response under each model in the full model set and then averaging the estimates according to each model weight [3]. A recent systematic review of BMA applications in science can be found in [4]. As updating the model probability can be performed by finding the maximum likelihood estimate (MLE) of the variance that maximizes the probability of observing the data in the model prediction, various techniques were used to search for such MLE. For example, [5] used expectation-maximization (EM) algorithm and BFGS non-linear optimization techniques to search for the MLE. The two prescribed methods were compared to Markov Chain Monte Carlo (MCMC). [6] proposed alternative approach to EM (since EM convergence is not always guaranteed), by using a recently developed MCMC algorithm called DREAM (Differential Evolution Adaptive Metropolis) to calculate BMA required weights and variances. Aside from BMA, another approach to quantify model-form uncertainty relies more on expert judgment and evidence theory. Zio and Apostolakis [7] introduced the theoretical and mathematical formulation for two expert-opinion-based approaches, and they were applied to assess the model uncertainty for high-level nuclear waste repository simulation tools. The first approach relies on constructing a set of plausible hypothesis based on expert elicitation, and then evaluating the validity of each hypothesis based on the available data. The second approach, which is more common, is called the adjustment factor approach, at which a reference model (best) is selected at the beginning from the model ensemble, usually by expert opinion. The predictions of the reference model are then adjusted by a factor that is unknown and should be determined. The unknown factor is uncertain by nature and accounts for the uncertainty in the other models. Clearly both of the previous two approaches rely on ad-hoc expert judgment, for which defending them from scientific point of view could be difficult [7].
Several studies have been conducted to quantify model-form uncertainty in other fields. Raftery et al. [8] applied a detailed BMA to ensemble of linear regression models which are used for prediction of different physical processes. Park et al. [9], [10] utilized BMA for model-form uncertainty quantification (UQ) through MLE, and the adjustment factor approach for its propagation into the system response. The methods were applied on a concrete creep problem and laser peening process. Riley and Grandhi [11] developed a framework based on the existing methods such as BMA and adjustment factor approach to quantify various uncertainty forms including the model-form. The framework was applied on a full-scale simulation in aerodynamics. Park and Grandhi [12] quantified the model-form and parametric uncertainties using the evidence theory through expert evidence. Evidence theory is flexible in describing the imprecise human knowledge in which numerical measures can be assigned to the uncertainty of various subsets of a model. The method was applied to spring-mass and laser peening problems. Droguett and Mosleh [13] developed a rigorous BMA framework for fire risk modeling when considering both homogeneous and heterogeneous performance data sources. Another application to quantify the model-form uncertainty of fatigue reliability models can be found in [14].
Our previous and current efforts in promoting UQ in various nuclear areas included developing sampling-based UQ framework of the reactor kinetic parameters [15], [16], UQ in criticality safety and spent fuel analysis [17], [18], [19], inverse UQ for thermal-hydraulics physical model parameters [20], and another inverse UQ application to fuel performance code BISON is presented in [21]. Most of the previous efforts did not consider analyzing both predictive and model-form uncertainty which are usually neglected in scientific modeling and UQ applications, even though they could account for most of the response uncertainty. Therefore, due to the emergent needs for thorough UQ of nuclear codes and experimental data, we are introducing and applying the BMA technique in various forms to nuclear computer models. In this study, various uncertainty forms can be quantified and accounted for during the UQ process to improve our understanding about the models and data. The experimental data and the codes (i.e. models to be evaluated) are described and validated first before performing UQ. For demonstration, the methodology is applied to three different computer codes used to simulate two-phase flow and thermal-hydraulics phenomena inside nuclear reactors, which are important for nuclear reactor system safety. Different cases and experimental data to quantify different forms of uncertainty are analyzed, and conclusions are drawn based on the results. The current approach demonstrated in this study can be efficiently extended to other computer models simulating other phenomena, given good-quality experimental data is available.
The remaining sections of this paper are organized as follows: Section 2 discusses the theory and the methodology behind Bayesian statistics and model averaging, with their adaptation to nuclear applications. Section 3 describes the reactor thermal-hydraulic codes (models) used to demonstrate the methodology as well as the experimental data used to assess the models. In Section 3, the codes are verified and validated against experimental data to demonstrate their validity to predict the response of interest with accepted uncertainty margin (i.e. to ensure that the models give reasonable prediction of the investigated process). Results and discussion of the advanced UQ framework are presented in Section 5 through a set of three case studies, followed by a discussion in Section 6, and concluding remarks in Section 7.
