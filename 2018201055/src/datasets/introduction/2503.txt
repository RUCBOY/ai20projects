Data-intensive science gateway applications in fields such as bioinformatics, climate modeling, and particle physics are becoming increasingly popular. These science gateway applications present unique requirements in terms of deployment of distributed heterogeneous infrastructure and use of advanced cyberinfrastructure technologies/protocols such as high-speed data transfer protocols, end-to-end virtualization, local/remote computing, Software-defined Networking (SDN) with OpenFlow support, Network Function Virtualization (NFV), end-to-end performance monitoring [1], and federated identity & access management [2]. In most cases, such resources are shared among federated user sites and are handled as ‘component’ solutions that can be combined for transforming a local applications (frequently at desktop scale) to a hybrid cloud application (i.e., infrastructure composed of distributed/federated cloud resources) [3].
Traditional infrastructure deployment approaches use a five-step waterfall model [4] involving: sequential abstraction, analysis, component deployment, recursive benchmarking and infrastructure deployment steps. Such a tedious five-step approach generally requires involvement of experts to accurately identify the application requirements, compose feasible solutions, and re-tune the components post-deployment to improve upon sub-optimal outcomes. Alternatively, automation of such a waterfall model through expert systems can be performed using suitable ‘abstractions’ of heterogeneous resource components and a ‘reusable’ solution model. The lack of abstraction and reusability of configurations in the approaches makes provisioning of heterogeneous resources for data-intensive applications quite time-consuming and prone to guesswork. This subsequently leads to sub-optimal and cost-prohibitive outcomes for data-intensive application users and can impede wide-adoption of dynamic distributed resource management.
There are existing infrastructure-level automated resource deployment approaches that offer automation and reusability to some extent. These solutions include: VMware Appliance [5] (vApps) from VMware; National Science Foundation (NSF) sponsored cloud Global Environment for Network Innovations (GENI) RSpecs [6] using XML files; and Amazon Machine Images [7] by Amazon Web Services (AWS). However, due to their proprietary nature, they assume a homogeneous deployment environment that features only their API (Application Programming Interface) that hides the complexity, and thereby, obviates the need for abstractions to facilitate heterogeneous resource provisioning. There are also existing application level approaches such as, VMware ThinApp, Citrix XenApp and Microsoft App-V that are limited in terms of platform and operating system independence. In addition, these technologies are mostly targeted towards commercial use. Due to issues such has portability and licensing, they are yet to gain wider acceptance within the data application owners and science gateway community. Thus, a suitable abstraction, virtualization, and orchestration approach is needed to fully comprehend the reusable heterogeneous distributed resource deployments for data-intensive applications for science gateways.Download : Download high-res image (595KB)Download : Download full-size imageFig. 1. Abstraction of federated cloud infrastructure components to compose solutions that integrate heterogeneous cloud resources.
In this paper, we build upon our recent prior work in [8] in order to address the above limitations of abstraction efforts and deployment approaches for heterogeneous distributed computing infrastructures. More specifically, we present a novel middleware approach shown in Fig. 1, for integrating federated/distributed cloud resources to support data-intensive application user needs. The goal of the design of our middleware is to enable it to create data-intensive science gateway application resource requirement ‘abstractions’ to foster pertinent cloud resources recommendations, coupled with ‘reusable’ approaches and automated resource deployment to save time and effort. Our middleware implements the Component Abstraction Model which is designed to abstract and group different cloud resources into categories of heterogeneous resource components through a corresponding Application Requirement Abstraction. It generates reusable, and extensible Custom Templates of components to fulfill the diverse data-intensive application requirements. Our middleware builds upon existing technologies, and protocols and uses a “Custom Template Catalog” for storage and reuse of these Custom Templates.
Our middleware features a Recommender system with a set of recommender schemes that enable the reuse of existing custom templates. An offline initial recommender module improves the user productivity by narrowing down cloud resource composition to the most appropriate options. In addition, an online iterative recommender module regularly monitors the application behavior, and dynamically provides automated resource adaptations based on application demands. The adaptations can be fine-grained changes to an existing configuration that involves e.g., adjusting the virtual machine (VM) count. Alternately, adaptations can be coarse-grained changes that involve recommendation of a completely new custom template. Lastly, the chosen template can be provided to a Resource Deployment Engine that uses CSP-specific APIs, Docker technology [9] (other frameworks such as Terraform engine [10] could be substituted) and automates the process for cloud resources deployment in a federated-distributed cloud environment.
We evaluate our middleware recommendation scheme with simulated interactions and a real-world data-intensive case study in the Advanced Manufacturing domain scope. We simulate a series of user interactions for diverse applications requirements, considering ‘novice users’ (users who provide reduced or incomplete information to the KIS) and ‘expert users’ (users who are more aware of their data-intensive application requirements and provide more inputs to the KIS). Next, our evaluation of the middleware implementation features a real-world data-intensive manufacturing science gateway application with computing workflow requirements involving a cluster of systems/nodes. Our experiment results demonstrate significant improvement for novice/expert users to effectively express their data-intensive application requirements to the KIS, and subsequently access federated cloud resources that are automatically deployed. This whole process reduces the resource provisioning time drastically and the guesswork in selecting appropriate cloud resource allocations. We also show the benefits of using dynamic scaling knowledge to motivate application adaptations in terms of performance, agility and cost factors.
For our experiment scenarios, we use the Amazon Web Services (AWS) and GENI cloud [6] platforms as part of a public–private cloud testbed with distributed heterogeneous resources that can be discovered and configured by using the geni-lib, a GENI API capability. Through extensive simulations, we quantify the accuracy metric in terms of the percentage of satisfied user requests for a set of novice/expert user requirements. Accuracy is measured with respect to the increase in the amount of missing infrastructure requirement parameters within user preferences, and is also based on the catalog knowledgebase maturity. The simulation results demonstrate that the increase in catalog size results in an increase of the search space complexity, and consequently further hinders the accuracy of the resource recommendation.
The reminder of the paper is organized as follows: Section 2 presents related works. Section 3, describes our Component Abstraction Model. In Section 4, we present our Recommender Algorithm. In Section 5, we present details of our middleware implementation and resource deployment methodology. Section 6 discusses the performance evaluation and Section 7 concludes the paper.
