Gesture recognition is to recognize category labels from an image or a video which contains gestures made by the user. Gestures are expressive, meaningful body motions involving physical movements of the fingers, hands, arms, head, face, or body with the intent of: conveying meaningful information or interacting with the environment.
Hand gesture is one of the most expressive, natural and common type of body language for conveying attitudes and emotions in human interactions. For example, in a television control system, hand gesture has the following attributes: “Pause”, “Play”, “Next Channel”, “Previous Channel”, “Volume Up”, “Volume Down” and “Menu Item”. While in a recommendation system, hand gesture can express “Like” or “Dislike” emotions of users. Thus, it is one of the most fundamental problems in computer vision and pattern recognition, and has a wide range of applications such as virtual reality systems [15], interactive gaming platforms [16], recognizing sign language [17], [18], [19], enabling very young children to interact with computers [20], controlling robot [21], [22], practicing music conducting [23], television control [24], [25], automotive interfaces [26], [27], learning and teaching assistance [28], [29], and hand gesture generation [30].
There has been significant progress in hand gesture recognition, however, some key problems e.g., fast and robust are still challenging. Prior work usually puts emphasis on using whole data series, which always contain redundant information, resulting in degraded performance. For examples, Wang et al. [15] present a superpixel-based hand gesture recognition system based on a novel superpixel earth mover’s distance metric. Ren et al. [16] focus on building a robust part-based hand gesture recognition system. Hikawa and Kaida [17] propose a posture recognition system with a hybrid network. Moreover, there are many approaches are also proposed for action or video recognition task, such as [31], [32], [33], [34], [35], [36], [37], [38], [39], [40]. Liu and Shao [31] introduce an adaptive learning methodology to extract spatio-temporal features, simultaneously fusing the RGB and depth information, from RGB-D video data for visual recognition tasks. Liu et al. [40] propose to combine the Salient Depth Map (SDM) and the Binary Shape Map (BSM) for human action recognition task. Simonyan and Zisserman [41] propose a two-stream ConvNet architecture which incorporates spatial and temporal networks to extract spatial and temporal features. Feichtenhofer et al. [42] study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. In sum, all these efforts endeavor to decrease the computation burden in each solo frame, while overlooking all processing schemes in the whole frames would incur more computation burden than a few selected representative frames, which is a fundamental way to decrease the computation burden, greatly. This paper is devoted to bridge the gap between fast and robust hand gesture recognition, simply using solo popular cue e.g., RGB, which ensures great potential in practical use.
Key frames, also known as representative frames, extract the main content of a data series, which could greatly reduce the amount of processing data. In [2], the key frames of the video sequence are selected by their discriminative power and represented by the local motion features detected in them and integrated from their temporal neighbors. Carlsson and Sullivan [43] demonstrate that specific actions can be recognized in long video sequence by matching shape information extracted from individual frames to stored prototypes representing key frames of the action. However, we regard every frame in a video as a point in the 2-D coordinate space. Since we are focusing on distinguishing dynamic gesture from a data series while not reconstructing it, we simply introduce a measure to find which frames are more important for distinguishing and which are not. In consideration of information entropy [4], [5], [6] could be a useful measurement to quantify the information each frame contains, we introduce frame entropy as an quantitative feature to describe each frame and then map these values into a 2-D coordinate space. How to describe this 2-D space is a hard nut to crack for its uneven distribution. Therefore, we further propose an integrated strategy to extract key frames using local extreme points and density clustering. Local extreme points includes the local maximum and local minimum points, which represent the most discriminative points of frame entropy. Shao and Ji [1] also propose a key frame extraction method based on entropy. However, the differences between [1] and the proposed method are two-folder: (i) The entropy in [1] is calculated on motion histograms of each frame, while the proposed method directly calculate on each frame. (ii) [1] simply to find peaks in the curve of entropy and use histogram intersection to output final key frames, while the proposed method first selects the local peaks of entropy and then use density clustering to calculate the cluster centers as the final key frames. Density clustering [7] is the approach based on the local density of feature points, which is able to detect local clusters, while previous clustering approaches such as dynamic delaunay clustering [8], k-means clustering [9], spectral clustering [10] and graph clustering [11] cannot detect local clusters due to the fact that they only rely on the distance between feature points to do clustering.
In order to promote the accuracy, we also present a novel feature fusion method that combines appearance and motion cues. After extracting key frames, we replace the original video sequence with the key frames sequence, which could greatly enhance the time efficiency at the cost of accuracy. This feature fusion strategy takes advantage of both the motion and the appearance information in the spatiotemporal activity context under the hierarchical model. The experimental results show that the method proposed is accurate and effective for dynamic hand gesture recognition on four datasets. To summarize, the main contributions of this paper are:
•A novel key frames extraction method is proposed, which improves efficiency of hand gesture processing.•A feature strategy is presented in which appearance and motion cues are fused to elevate the accuracy of recognition.•Experiments demonstrate that our method achieves the balance between efficiency and accuracy simultaneously in four datasets.
