Since facial expressions play an important role in reflecting human feelings, automatic facial expression recognition (FER) has been a popular topic in the fields of computer vision and multimedia, etc. The facial action coding system [1] provides an objective way to describe expressions in terms of both appearance and geometrical facial changes, which is further extended for automatic FER with convolutional neural networks [2].
However, when the expression database used for network training is not diverse enough, the training may result in over-fitting and poor generalization performance on other databases due to the large variations across different persons. Currently, different sparseness regularization approaches have been proposed to address the overfitting to improve the network’s generalization ability. Sparse representation, namely, compressed sensing, can not only decrease the redundancy and extract common features among different person identities [3], but also help to decrease the computational complexity of the training. Sparseness of weights, hidden units, features and dropout were often considered.
However, different sparseness strategies may be applicable to only specific databases. Although the fusion of the metric strategies can balance the performance for different databases [4], the regularization coefficient of each sparseness term should be provided before the network training for each database. Meanwhile, each sparseness strategy can introduce multiple hyper-parameters. For example, the dropout ratio for dropout [5], the sparseness ratio for weight pruning [6] and the set of the hidden unit layers selected for hidden unit sparseness, are all hyper-parameters to be adjusted since they can yield largely different performances for different databases. Hyper-parameter optimization provides a solution of the challenge for different databases. While grid search provides a greedy search into the hyper-parameter space, it requires large time complexity since each network training may demand much computation resource. Population evolution [7] and derivative-free optimization framework [8] were proposed to reduce the runtime cost of the grid search. Since the original network training often demands much runtime cost, the idea of surrogate network was introduced to approximate the computation of the original network and simplify the hyper-parameter optimization.
Regarding to the optimizers for hyper-parameter optimization in the surrogate network, the gradient-based solvers use the gradients of the objective function with respect to (w.r.t.) the hyper-parameters for the iteration. However, these gradients can not be analytically solved. The discrete approximation of the gradients demands a number of network training, which requires much computation resources for even a shallow network. While a meta-heuristic algorithm adopts greedy strategy inspired from behavior patterns of creatures to approach global optimum from multiple positions of the searching region, it is a practical alterative for a variety of non-differentiable and non-convex problems, such as the optimization of the hyper-parameters in the surrogate network. Among the meta-heuristic algorithms, differential evolution (DE) and particle swarm optimization (PSO) optimizers [9] have attracted lots of attention, since they are simple in structure, easy to implement, and perform relatively well on non-differentiable and non-convex problems. In this work, DE and PSO are used for hyper-parameter optimization.
1.1. Related worksFor FER with network sparseness, the sparse weight or hidden unit is achieved by imposing the sparseness constraint on the weight matrix W [6] or the feature maps [10] of a deep network. The feature sparseness constraint on the fully connected (FC) layers is a specific case of the hidden unit sparseness, which can largely decrease the complexity of the sparseness term. For feature sparseness, the inputs or outputs of the last two FC layers or their mathematical transformations are often embedded in the sparseness losses for generalization ability improvement [11]. Dropout [5] and its variants, such as weight dropout is an alternative strategy for the network sparseness. Alam et al. [12] used dropout learning in deep simultaneous recurrent networks for generalization ability improvement and model size reduction in FER.The optimization of network hyper-parameters is usually required to adapt various sparseness strategies to different databases. Ilievski et al. [13] used radical basis function (RBF) as the surrogate of hyper-parameter optimization to reduce the complexity of the original network. As multiple network re-training are required for RBF-based surrogate fitting, Talathi [14] employed sequential model-based optimization to tune the hyper-parameters of seven convolution layers of a deep network.For these traditional surrogates without using the network models, the structure of the losses in the original network is not preserved. As only hyper-parameters are used for the modeling of the validation performance, the approximation performance to the original model is limited. Network-based surrogates, such as neural network surrogate for Bayesian optimization [15], can be used as the surrogate of network hyper-parameter optimization. Compared with traditional surrogate, network-based surrogates can use more information like network weights and features for the mapping construction, and preserve the network structure related with the hyper-parameters. Thus, network-based surrogates can more accurately approximate the original model computation. Eq. (1) compares the difference between the constructing formulas of the traditional and network-based surrogates.(1){TraditionalSurrogate:accv=f(λ),Network-basedSurrogate:accv=f(Wλ,xλ).where accv, λ and f( · ) are the validation accuracy, network hyper-parameters and mapping function, and Wλ and xλ are network parameters and features relied on the hyper-parameters of λ. In this work, shallow networks are deployed to surrogate the optimization of sparseness hyper-parameters for the original deep network.
1.2. ContributionAs the hyper-parameters tuned for training dataset may not work well on different test datasets, this work proposes a new algorithm to enable current deep learning approaches to dynamically adapt their hyper-parameters to new datasets. Meanwhile, it is revealed in the state-of-the-art studies [12], [16] that FER performance can largely benefit from sparseness strategies, which motivates us to explore the optimization of the hyper-parameters in the deep sparseness strategies for FER problem. More precisely, an iterative algorithm based on a surrogate network for the optimization of hyper-parameters in deep sparseness strategies is proposed, where the surrogate network shares the same loss structure as the original network and is used to surrogate the original network computation. The fitting of the surrogate network is embedded into the training of the original network based on four Euclidean losses with unilateral back propagation. The hyper-parameters optimized with gradient-free optimizers, i.e. DE and PSO, based on the surrogate network, are then used to compete with the previous best and applied to the original network training in the next stage. The main contributions of this work are summarized as follows:•A new iterative framework for the hyper-parameter optimization in deep sparseness strategies is proposed to adapt hyper-parameters to different databases in FER;•A simplified network is deployed to surrogate the original network for hyper-parameter optimization, where Euclidean losses with unilateral back propagation are introduced to approximate the original network;•The hyper-parameter optimization algorithm achieved competitive performances on six public benchmark expression databases.This paper is structured into the following sections. The proposed approach is demonstrated in Section 2. The experimental results and the corresponding illustrations are demonstrated in Section 3. Finally, the conclusions and a discussion are presented in Section 4.
