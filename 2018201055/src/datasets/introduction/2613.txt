As multi-linear generalization of vector and matrix, tensor can naturally represent many multi-dimensional real-world data [1]. For instance, the electroencephalography(EEG) data [2], Hyperspectral Image(HSI) [3], and video [4], [5] can be naturally described as tensors. Although a tensor can be vectorized in traditional data analysis techniques like principle component analysis (PCA), the vectorization ignores the intrinsic structures ubiquitous in multi-dimensional data. Tensor analysis provides a direct way to exploit rich intrinsic structures in multi-dimensional data. With the development of tensor algebra [6], tensor analysis has been extensively applied in the society of pattern recognition[7], signal processing [2], computer vision [8], machine learning [9], bio-informatics [10] and some other areas.
Tensor recovery aims to recover a tensor with some intrinsic structures from partial or corrupted observations. Extensive methods consider low-rankness as the intrinsic structure of the underlying tensor [8], [11], [12]. Different from the matrix rank, there are multiple definitions of tensor rank, such as the CANDECOMP/PARAFAC (CP) rank [13], [14], the Tucker rank [15], the Tubal rank [16], [17], the Tensor Train (TT) rank [18] and the Tensor Ring (TR) rank [19]. Thus, a tensor may be modeled with different low-rank structures. The mostly widely adopted tensor low-rank structure should be the low-Tucker-rankness which assumes the underlying tensor to be simultaneously low-rank among each mode [8], [20], [21], [22]. Due to the NP-hardness of general rank-minimization problems, the tensor nuclear norm [8] is proposed for designing tractable algorithms to solve many tensor recovery problems. The tensor nuclear norm is defined as the sum of nuclear norm of matricizations along each mode, and has been broadly applied for tensor completion [8], [20], tensor robust principle component analysis (TRPCA) [23], to name a few.
In many applications, the underlying multi-dimensional data may occupy more than one kind of intrinsic structures simultaneously [11], [24]. Recently, a new model called regularized multi-linear regression and selection (Remurs) which assumes that the underlying tensor is both low-rank and sparse was proposed in [25]. It is reported that Remurs have superior performance compared to Lasso, Elastic Net and their multi-linear extensions for the classification and interpretation of fMRI data [25]. The Remurs takes into account the integration of the tensor nuclear norm and the l1-norm, which imposes both low-rankness and sparsity in the final solution. The optimal solution is obtained through the alternating direction method of multipliers (ADMM) algorithm [26], [27].
Despite the practical success, the statistical performance of Remurs-like models has not been explored. To fill in the gaps, we mainly study the statistical performance of a Remurs-like tensor recovery model which assumes the underlying tensor is simultaneously low-Tucker-rank and sparse in this paper. The proposed model is based on a newly defined nuclear-l1-norm. Then based on the M-ADMM framework [28], which takes advantage of both Gauss-Seidel ADMM and Jacobian ADMM, an efficient algorithm is developed. Further, motivated by Tomioka and Co-authors [20], [29], the statistical performance of the proposed model is rigorously studied by establishing both deterministic and non-asymptotic bounds on the estimation error. Specifically, we analyze two specific configurations, i.e., the noisy tensor decomposition and the random Gaussian design. To the best of our knowledge, this is the first paper that explores the statistical performance of recovery models for simultaneously low-rank and sparse tensors. Numerical experiments are conducted respectively for both the noisy tensor decomposition case and the random Gaussian design case, and illustrate the correctness and validity of the proposed theorems.
The remainder of this paper is organized as follows. Basic notations and the tensor nuclear-l1-norm are introduced in Section 2. The observation model and the M-ADMM-based algorithm are provided in Section 3. Section 4 is devoted to the deterministic and non-asymptotic upper bounds on the estimation error. Numerical experimental results are shown in Section 5. This work is concluded in Section 6. Proofs of Theorem 1 and Lemma 2 are given in Appendix A.
