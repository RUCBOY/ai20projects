Modern computer systems are widely employing heterogeneous hardware components such as graphics processing units (GPUs), digital signal processors (DSPs), and various controllers to accelerate a certain task. Though such an acceleration can be employed for a wide range of workloads, one of the most well-known candidates for acceleration is a memory (data)-intensive workload. With general-purpose processors, typical memory-intensive workloads cause a number of cache misses, resulting in worse performance and energy efficiency mainly because of frequent off-chip data transfer operations. Though many modern well-known workloads (e.g., graph analytics [5] and neural network processing [6]) are memory-intensive, one of the most memory-intensive workloads is memory-to-memory data transfer over a large memory footprint with linearly increasing memory addresses or array indices (we call it as ‘linear memory streaming’ in this paper). The linear memory streaming is often used to quantify a memory bandwidth of computer systems with benchmark programs[10]. In addition, many real-world workloads (e.g., image processing [2], 1  ×  1 convolutions and bias addition/scale in deep neural networks (DNNs) [7], [8], and bulk data transfer [15]) already contain memory streaming operations (often with simple arithmetic or logical operations). These workloads are frequently executed in contemporary embedded/mobile systems [7] as the computing capability of these devices becomes more powerful.
A conventional way to accelerate linear memory streaming is adopting processing-in-memory (PIM) or near-memory processing. Several recent works [4] have explored the PIM or near-memory processing architecture to mitigate the data transfer overhead from memory-intensive workloads. Though they could be efficient in high-performance system with emerging memory technologies such as high bandwidth memory [9], it may not be desirable for embedded or mobile systems as they have a tight power and cost (i.e., resource) budget. In [16], [17], direct memory access (DMA) is used for data transfer, which has better performance and energy efficiency for memory data transfer than the CPU, they only utilize either existing resources or dedicated DMA-based accelerators, posing a limitation on improving performance and energy efficiency. If we could further utilize idle hardware resources such as multi-core central processing units (CPUs) along with a dedicated linear memory streaming accelerator (i.e., a near-memory accelerator that can be executed in parallel with multi-core CPUs), we would further improve performance of the system. For example, modern embedded/mobile systems-on-chip (SoCs) are known to have a non-negligible number of idle cores in CPUs [14]. As memory streaming can be performed on CPUs as well as on hardware accelerators, we can utilize both hardware resources in the system for better performance and energy efficiency.
In this paper, we propose a technique based on a hardware/software co-design for linear memory streaming acceleration in embedded/mobile systems. Compared to the conventional approaches that use either a CPU or a hardware accelerator, we exploit both hardware resources for fast and energy-efficient linear memory streaming. In the hardware perspective, we propose to utilize a linear memory streaming accelerator that efficiently performs several simple arithmetic operations. In the software perspective, we devise a software support (in the form of a pseudocode) to utilize both the CPU and the accelerator for linear memory streaming. For the proof of concept, we implemented our prototype hardware accelerators and software supports on a Zynq7020-based FPGA-SoC platform [19]. With our experimental results for linear memory streaming operations with a large memory footprint (STREAM benchmark [10]) and case studies (image processing, 1  ×  1 convolution, and bias addition/scale), we demonstrate the superiority of our acceleration technique in terms of performance and energy efficiency. To the best of our knowledge, this is the first work that utilizes both an accelerator and idle hardware resources for linear memory streaming operations with a hardware/software co-design approach in embedded/mobile systems. Moreover, our work is not based on simulation-based experiments but based on implementation and prototype-based experiments, meaning that the experimental results are more accurate than the simulation-based results.
We summarize our contributions as follows:
•We propose a linear memory streaming acceleration technique via a hardware/software co-design approach that uses both an accelerator and idle hardware resources (CPU in this work).•We implemented our prototype hardware accelerator and software supports on a Zynq7020-based FPGA-SoC platform [19].•When running linear memory streaming operations with a large memory footprint (STREAM benchmark [10]), our technique improves the performance and reduces the energy consumption by 73.1% and 45.5%, respectively, compared to the case where we use only CPU.•When performing image processing tasks, 1  ×  1 convolution, and bias addition/scale, our technique also improves performances by up to 1.3 × , 2.3 × , and 2.5 × , respectively, while reducing the energy consumptions by up to 17.7%, 57.7%, and 58.5%, respectively, compared to the case where we use only CPU.
The rest of this paper is organized as follows. Section 2 introduces recent studies closely related to the topic of this paper. Section 3 explains our acceleration technique. Sections 4 and 5 demonstrate our experimental results when running linear memory streaming operations with a large memory footprint and case studies on image processing, 1  ×  1 convolutions, and bias addition/scale. Section 6 discusses several important design tradeoffs, the method to apply our technique to more complex kernels, and limitations. Lastly, Section 7 provides the conclusions of this paper.
