Enabling natural human-computer interaction through speech involves endowing the machine with the ability to distinguish between speech that is addressed to it from speech that is addressed to others (Tsai et al., 2015). Designers of spoken dialogue systems (Batliner, Hacker, Nöth, 2006, Batliner, Hacker, Nöth, 2009) refer to speech not addressed to the system as “off-talk” (Oppermann et al., 2001), and to speech addressed to the system as “on-talk”. The latter exhibits distinctive linguistic behaviours. Previous studies have shown that the user’s talking behaviour varies depending on whether the interlocutor is a machine or a human being (Branigan, Pickering, Pearson, McLean, 2010, Fischer, 2011), and that talking to a computer is similar to talking to a person who has a hearing impairment (Batliner et al., 2009). Human-computer communication tends to be more “exaggerated” than human-human communication (Branigan et al., 2010).
In applications such as speech-to-speech machine translation, users often experience communication difficulties due to the errors caused by system misinterpretation of off-talk, especially when facial and gestural cues that normally aid communication are reduced or absent from the system (Cerrato et al., 2016). In such settings, the speaker adapts different strategies such as overarticulating and slowing their speech rate when automatic speech recognition (ASR) fails. However, as modern ASR systems are trained on normal paced speech, this strategy often results in worse ASR performance (Hayakawa, Cerrato, Campbell, Luz, 2015, Goldwater, Jurafsky, Manning, 2010). Self-talk, or soliloquy which is an example of off-talk, may occur in a dialogue system or speech translation system due to a number of factors. Failure of speech recognition or machine translation components, for instance, often causes users to talk to themselves in amusement or frustration. In a speech-to-speech machine translation system, users might read out loud the system’s textual output, such as feedback or back-translation displayed to them during interaction with the system. Talking to others (“other-talk”) is also quite common in several contexts of use of dialogue systems, where addressee detection is an active research area (Tsai et al., 2015). Therefore, equipping the system with the ability to detect off-talk could enhance system performance by avoiding the processing of off-talk utterances, and using this information (i.e. which utterances are off-talk) as a feedback to the ASR module and other system components. In an audio conference where participants use a machine translation system, for instance, off-talk detection could prevent irrelevant and potentially confusing utterances from being translated and transmitted to the remote participants.
This study extends our previous work (Hayakawa, Haider, Luz, Cerrato, Campbell, 2016a, Haider, Akira, Luz, Carl, Campbell, 2018) where the EEG signal is analysed in overt speech (during articulation) rather than in covert (prior to articulation) as in the present study. We also analyse overt speech (during articulation) in combination with a very high dimensional set of acoustic features in previous studies (Hayakawa, Haider, Luz, Cerrato, Campbell, 2016a, Haider, Akira, Luz, Carl, Campbell, 2018). This study proposes a new approach for automatic detection of on- and off-talk which could decrease the response time of an interactive speech driven system in accepting or rejecting a speech utterance. This model employs electroencephalography (EEG) features collected prior to articulation (covert speech setting). While EEG signals have been employed before in speech-related brain-computer interaction (BCI), these applications tend to focus on the interpretation of “silent” or covert speech (Sereshkeh et al., 2017), where the user “imagines” the words or phonemes to be produced but does not physically articulate them. This focus on covert speech is due to the fact that muscle activity during speech articulation produces noise that contaminates the EEG signal. By focusing on the preparatory phase of speech production (Levelt, 1989, van Turennout, Hagoort, Brown, 1997) and processing the EEG signal before articulation starts, our approach avoids this difficulty. We implemented and assessed models that employ pre-articulation EEG features in isolation and in combination with prosodic features gathered during articulation for on- and off-talk detection. The system architecture underlying our method is depicted in Fig. 1, where a Voice Activity Detection (VAD) component detects the start and end time of a speech utterance from the incoming audio stream and then extracts acoustic features from the speech utterance for off-talk detection. The EEG based off-talk detection system is triggered as soon as it detects 10-ms of speech and processes the EEG features corresponding to a time window immediately prior to articulation from the memory buffer for off-talk detection.Download : Download high-res image (228KB)Download : Download full-size imageFig. 1. The system architecture where the system processes the EEG features prior to articulation as soon as it received 10 ms of audio which is detected through voice activity detection (VAD) using audio features.
In summary, the main research contributions of this article are:
1.the introduction of a novel method for automatic detection of on- and off-talk utterances;2.a demonstration of the usefulness of EEG signals recorded up to 2 s prior to articulation for on- and off-talk detection;3.an analysis of the predictive power of the fusion of audio and EEG features with regard to this detection task, and4.a demonstration of the discriminating power of EEG potential generated by the right mid-front and right mid-back positions of brain for off-talk detection.
At a practical level, improvements in response time yielded by the proposed method could contribute towards the design of interactive speech systems that show attentive behaviour to users and, in the specific case of machine-translated audio conferencing, improve the flow of conversation. In the scope of this study, we mainly focused on prosodic information. Therefore we have focused on electrical signals from the right brain-hemisphere and on basic prosodic features, as it is generally assumed that the right brain-hemisphere preferentially processes prosody (Kreitewolf, Friederici, von Kriegstein, 2014, Heilman, Leon, Rosenbek, 2004, MacNeilage, Rogers, Vallortigara, 2009, Friederici, Alter, 2004, Shapiro, Danly, 1985, Weintraub, Mesulam, Kramer, 1981, Ross, Mesulam, 1979). For instance, Heilman et al. assessed the brain of a subject with a right medial frontal cerebral infarction and observed an impairment in expressing emotions using prosody, and in comprehension and repetition of prosody (Heilman et al., 2004). The left brain-hemisphere is largely involved in speech production aspects other than prosody control (Flinker, Korzeniewska, Shestyuk, Franaszczuk, Dronkers, Knight, Crone, 2015, Blank, Scott, Murphy, Warburton, Wise, 2002). For instance, Flinker et al. showed that the Broca area activates prior to articulation and stays inactive during articulation. The motor cortex, on the other hand, activates during speech production but remains inactive prior to articulation (Flinker et al., 2015). Focusing on the right brain-hemisphere also minimises possible confounding from brain patterns related to hand control as the user uses the mouse, since the left brain-hemisphere controls the right hand (MacNeilage, Rogers, Vallortigara, 2009, Neuper, Pfurtscheller, 2001), and the subjects of this study are right-handed. As regards the analysis of prosody on the speech signal for distinguishing on-talk from the two types of off-talk (other-talk and self-talk). It is noted that although other talk and self talk seem to differ prosodically, talking to a system has a distinctively less natural character, marked by features such as over-articulation, louder and slower speaking etc. While further investigation is still needed, these features of off-talk seem to persist across the different languages recorded in our experiment.
