Vision-based autonomous driving becomes trendy and has recently attracted much research enthusiasm from the computer vision community. Compared with other sensors like LIDAR or ultrasound, vehicle-mounted cameras are low-cost and can either independently provide actionable information or complement other sensors. Key achievement has been observed in academia and in industrial R&D branches in the past decade, which can mainly attribute to several interlocking factors including the revolutionary progress of deep learning techniques [1], [2], [3], [4], the powerful computing infrastructure, and the accumulation of high-quality annotated data obtained through crowd sourcing [5], [6], [7]. For solving various visual perception tasks in autonomous driving, one may notice two methodologies: mediated perception and behavior reflex. The former [8] decomposes the entire task into a group of more tractable atomic sub-tasks (such as image classification [9], [10] and object detection [4], [11]) and combines all outputs to obtain a holistic understanding of the visual surroundings. In contrast, the latter [12] often relies on some black-box model (such as end-to-end deep neural networks) to mimic human expert’s behavior (such as safely steering a car [13]). This research follows the first methodology and tries to solve a basic and important problem in vision-based autonomous driving: estimating vehicle orientation from a single picture captured by in-vehicle dashcams, which is a relatively unexplored task in the fields of computer vision, robotics and machine learning.
Fig. 1 illustrates the input and expected output in this task. This research targets a single-shot setting, namely only one picture will be provided. The system is desired to estimate the vehicle’s orientation with respect to the road lane. To our best knowledge, this task is nascent. Existing research mostly aims to estimate the orientation of nearby vehicles or pedestrians, rather than that of an interested autonomous car itself. Such a technique is particularly useful in mediated perception based autonomous steering (namely predicting accurate wheel angles, braking or acceleration etc. from visual perception). Most datasets used for autonomous steering, such as those provided by Udacity1 or drive.ai2, postulate the vehicle is moving parallel to the road direction. Despite this eases learning the model, it has severe adverse effect during model deployment. When the vehicle deviates from the road direction, the learned model may produce sub-optimal predictions since the situation is not part of the training data. Our proposed technique can be used for turning the vehicle back to its canonical orientation.Download : Download high-res image (675KB)Download : Download full-size imageFig. 1. Illustration of single-shot vehicle orientation estimation. We draw two pictures taken by cameras mounted in the vehicles, and graphically illustrate the estimated orientation of the interested vehicle with respect to the road direction. The road directions are highlighted in red arrows and the estimated orientations (derivation angles to the road direction) are displayed in grey color.
