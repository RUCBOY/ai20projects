Quantum computers promise to provide computational power required to solve classically intractable problems and have significant impacts in materials science, quantum chemistry, cryptography, communication, and many other fields. Recently, much focus has been placed on constructing and optimizing Noisy Intermediate-Scale Quantum (NISQ) computers [1], however over the long term quantum error correction will be required to ensure that large quantum programs can execute with high success probability. Currently, the leading error correction protocol is known as the surface code [2], [3], which benefits from low overheads in terms of both fabrication complexity and amount of classical processing required to perform decoding.
A common execution model of machines protected by surface code error correction requires a process called magic-state distillation. In order to perform universal computation on a surface code error corrected machine, special resources called magic states must be prepared and interacted with qubits on the device. This process is very space and time intensive, and while much work has been performed optimizing the resource preparation circuits and protocols to make the distillation process run more efficiently internally [4], [5], [6], [7], [8], relatively little focus has been placed upon the design of an architecture that generates and distributes these resources to a full system.
This study develops a realistic estimate of resource overheads of, and examines the trade-offs present in, the architecture of a system that prepares and distributes magic states. In particular, instead of using a single large factory to produce all of the magic states required for an application, the key idea of our work is to distribute this demand across several smaller factories that together produce the desired quantity. We specifically characterize these types of distributed factory systems by three parameters: the total number of magic states that can be produced per cycle, the number of smaller factories on the machine, and the number of distillation rounds that are executed by each factory.
The primary trade-off we observe is between the number of qubits (area/space) and the amount of time (latency) spent in the system: we can design architectures that use minimal area but impose large latency overheads due to lower magic-state output rate, or we can occupy larger amounts of area dedicated to resource production aiming to maximally alleviate application latency. The two metrics, space and time, are equally important as it is easy to build small devices with more gates or large devices with few gates. This concept is closely related to the idea of “Quantum Volume” [9], when machine noise and topologies are taken into consideration. To capture the equal importance of both of these metrics, we use a space-time product cost model in which the two metrics simply multiply together. This model has been used elsewhere in similar analysis [7], [8], [10], [11].
Fig. 1 illustrates the opposing trends for space and time when we increase the magic-state production rate. Our goal is to find the “sweet spot” on the combined space-time curve, where the overall resource overhead is at its lowest. In summary, this paper makes the following contributions:1.We present precise resource estimates for implementing different algorithms with magic-state distillation on a surface code error corrected machine. We derive the estimates from modeling and simulating the generation and distribution of magic states to their target qubits in the computation.2.We quantify the space and time trade-offs of a number of architectural configurations for magic-state production, based on design parameters including the total number of factories, total number of output states these factories can produce, and the desired fidelity of the output magic states.3.We study different architectural designs of magic-state distillation factory, and present an algorithm that finds the configuration that minimizes the space-time volume overhead.4.We highlight the nontrivial interactions of factory failure rates and achievable output state fidelity, and how they affect our design decisions. We analyze the sensitivity of these optimized system configurations to fluctuations in underlying input parameters.5.We discover that dividing a single factory into multiple smaller distributed factories can not only reduce overall space-time volume overhead but also build more resilience into the system against factory failures and output infidelity.Download : Download high-res image (168KB)Download : Download full-size imageFig. 1. Space and time tradeoffs exist for distributions of resource generation factories within quantum computers. These trends are shown assuming same total factory output capacity. By explicit overhead analysis, we can discover optimal space-time volume design points.
The rest of the paper is structured as follows. In Section 2, a basic background of quantum computation, error correction, magic-state distillation and the Bravyi-Haah distillation protocol, as well as the block-code state-distillation construction are described. Section 3 describes previous work in this area. Sections 4 and 5 discuss important space and time characteristics of the distillation procedures that we consider, and derive and highlight scaling behaviors that impact full system overhead analysis. Section 6 describes in detail how these characteristics interact, and shows how these interactions create a design space with locally optimal design points. Section 7 details the system configurations we model, describes a novel procedure for discovering the optimal design points, and discusses the simulation techniques used to validate our model derivations. Section 8 shows our results and the explains the impacts of optimizing these designs. Sections 9 and 10 conclude and discuss ideas to be pursued as future work.
