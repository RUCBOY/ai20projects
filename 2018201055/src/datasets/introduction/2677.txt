The undeniable success of computing accelerators in the supercomputing scene nowadays, is not only due to their high performance, but also due to their outstanding energy efficiency. Interestingly, this success comes in spite of the fact that efficiently programming machines with these devices is far from trivial. Not long ago, the most powerful machines would have a set of identical processors. To further increase the computing power, now they are sure to integrate some sort of accelerator device, like GPGPUs or Intel Xeon Phi. In fact, architects are integrating several such devices in the nodes of recent HPC systems. The trend nowadays is towards highly heterogeneous systems, with computing devices of very different capabilities. The challenge for the programmers is to take full advantage of this vast computing power.
But it seems that the rapid development of heterogeneous systems has caught the programming language stakeholders unaware. As a result, there is a lack of a convenient language, or framework, to fully exploit modern multi-GPU heterogeneous systems, leaving the programmer to face these complex systems alone.
It is true that several frameworks exist, like CUDA [24] and OpenCL [34], that can be used to program GPUs. However, they all regard heterogeneous systems as a collection of independent devices, and not as a whole. These enable programmers to access the computing power of the devices, but do not help them to squeeze all the performance out of the heterogeneous system, as each device must be handled independently. Guided by the host-device model introduced by these frameworks, programmers usually offload tasks, or kernels, to accelerator devices one at a time. Meaning that during the completion of a task the rest of the machine is left idle. Hence, the excellent performance of these machines is tarnished by an energy efficiency lower than could be expected. With several devices in one system, using only one at a time is a considerable waste. Some programmers have seen this flaw, and have tried to divide the computing tasks among all the devices of the system [4], [14], [27]. But it is an expensive path in terms of coding effort, portability and scalability.
This paper proposes the development of a means to assist the programmer with this task. Even leaving code length and complexity considerations aside, load balancing data-parallel applications on heterogeneous systems is a complex and multifaceted problem. It requires deciding what portions of the data-set of a given kernel are offloaded to the different devices, so that they all complete it at the same time [3], [12], [39].
To achieve this, it is necessary to consider the behaviorof the kernels themselves. When the data-set of a kernel is divided in equally sized portions, or packages, it can be expected that each one will require the same execution time. This happens in well behaved, regular kernels but it is not always the case. The execution time of the packages of some kernels may have a wide variation, or even be unpredictable. These are considered irregular kernels. If how to balance a regular kernel can be decided prior to the execution, achieving near optimal performance, the same cannot be said about irregular ones. Their unpredictable nature forces the use of a dynamic approach that marshals the different computing devices at execution time. This however, increases the number of synchronization points between devices, which will have some overhead, reducing the performance and efficiency of the system. In conclusion, the diverse nature of kernels prevents the success of a single data-division strategy in maximizing the performance and efficiency of a heterogeneous system.
Aside from kernel behavior, the other key factor for load distribution is the configuration of the heterogeneous system. For the load to be well balanced, each device must get the right amount of work, adapted to the capabilities of the device itself. Therefore, a work distribution that has been hand-tuned for a given system is likely to underperform on a different one.
The OmpSs programming model is an ideal starting point in the path to hassle-free kernel co-execution. It provides support for task parallelism due to its benefits in terms of performance, cross-platform flexibility and reduction of data motion [8]. The programmer divides the code in interrelating tasks and OmpSs essentially orchestrates their parallel execution maintaining their control and data dependences. To that end, OmpSs uses the information supplied by the programmer, via code annotations with pragmas, to determine at run-time which parts of the code can be run in parallel. It enhances OpenMP with support for irregular and asynchronous parallelism, as well as support for heterogeneous architectures. OmpSs is able to run applications on symmetric multiprocessor (SMP) systems with GPUs, through OpenCL and CUDA APIs [31].
However, OmpSs can only assign kernels to single devices, therefore not supporting co-execution of kernels. An experienced programmer could decompose the kernel in smaller tasks so that OmpSs could send them to the devices. But there would be no guarantee that the resources would be efficiently used or the load properly balanced. The programmer would also be left alone in terms of dividing the input data and combining partial results. This would lead to longer code, which would be harder to maintain.
As a solution to the above problems this article presents an OmpSs extension which enables the efficient co-execution of massively data-parallel OpenCL kernels in heterogeneous systems. This has the advantage of providing a natural way to program using all the available resources that was not previously available in OmpSs. Manually achieving an equivalent functionality would require rethinking the applications themselves to account for the heterogeneous devices, creating different tasks with adequate granularities and even implementations. Moreover, these extra manual work would need to be repeated if the system configuration changed. By automatically using all the available resources, regardless of their number and characteristics, the proposed extension presents an easy way to perform kernel co-execution and extracting the maximum performance of these systems. It takes care of load balancing, input data partitioning and output data composition.
The experimental results presented here show that, for all the used benchmarks, being able to co-execute kernels on multiple devices has a positive impact on performance. In fact, the results indicate that it is possible to reach an efficiency of the heterogeneous system over 0.85. Furthermore, the results also show that, although the systems exhibit higher power demand, the shorter execution time grants a notable reduction in the energy consumption. Indeed, the average energy efficiency improvement observed is 53%.
The main contributions of this article are the following:

•The OmpSs programming model is extended with a new scheduler, that allows a single OpenCL kernel instance to be co-executed by all the devices of a heterogeneous system.•The scheduler implements two classic load balancing algorithms, Static and Dynamic, for regular and irregular applications.•Aiming to give the best performance on both kinds of applications, two new algorithms are presented, HGuided and Auto-Tune, which is a parameterless version of the former.•An exhaustive experimental study is presented, that corroborates that using the whole system is beneficial in terms of energy consumption as well as performance.
The remainder of this paper is organized as follows. Section 2 presents background concepts key to the understanding of the paper. Next, Section 3 describes the details of the load balancing algorithms. It is followed by Section 4, that covers the implementation of the OmpSs extension. Section 5 presents the experimental methodology and discusses its results. Finally, Section 7 offers some conclusions and future work.
