Probabilistic Graphical Models(PGMs) play a significant role in modern machine learning, in which nodes are presented by random variables and connections denote the statistical dependences between nodes. The main task of deep learning is to learn the high-level representation of input data. PGMs have big advantages in data representation, and have made great successes in pattern recognition and artificial intelligence, such as image recognition [1] and segmentation [2], human motion generation [3] and recognition [4]. There are three advantageous properties of PGMs. The first one is that they provide a simple way to specify the structure of a probabilistic model via graph representation. The second is that the conditional independences between the nodes can be intuitively obtained. The last one is that the computational executions of probabilistic inference and learning can be possessed by graphical manipulations in an implicit way [5].
Restricted Boltzmann machines (RBMs) [6], deep belief networks (DBNs) [7], deep Boltzmann machines (DBMs) [8] are three well-known examples of PGMs. These models are all probabilistic and generative, that can be used to build the data and feature distributions. A RBM has a simple bipartite architecture. It has been successfully applied to collaborative filtering [9] and human motion modelling [3]. A DBN is deep probabilistic network that can generate high quality image samples after unsupervised learning. It has revived the research area of neural network and deep learning since 2006. A DBM is hierarchical Markov random field, which shows superior performance in speech recognition [10], multimodal learning [11], document modelling [12] and wind speed prediction [13]. Variational Auto-encoders (VAEs) [14] are another important examples of PGMs, which have their prosperities in image generation and language modeling [15]. These models have similar properties of being hierarchical, layer-wise and probabilistic. In terms of their hierarchical architectures and close relationships with conventional neural networks, we regard them as deep probabilistic graphical networks (PGNs).
A deep model, no matter whether it is probabilistic or deterministic, is hard to avoid severe parameter explosion as the number of hidden layers and nodes increases. Therefore, parameter redundancy is a common phenomenon in modern deep deterministic neural network models, such as CNN based deep models including AlexNet, ResNet, GoogleNet and VGGNet [16]. Parameter redundancy will lead to two serious problems. One is that the complexity of learning will increase exponentially. That means it often takes several days or weeks to train a deep CNN even with GPU acceleration. The other risk is the possible overfitting, which may offset the performance of the deep model. Han [17] has showed more than 50% of model parameters in most deep CNNs are redundant weights in computer vision and neural language processing tasks. To reduce the risk of overfitting, regularization, dropout, and early stopping are often necessary for these deep models. Recently, deep compression has become an alternative approach to alleviate this two inherent drawbacks. It is desired to remove the useless connections and nodes as far as possible, while keeping the loss of accuracy as less as possible. There are many methods and approaches proposed, such as parameter pruning [18], [19], low-rank factorization [20], knowledge distillation [21] and so on.
In deep PGNs, the number of parameters is more tremendous compared with deep deterministic neural networks due to the fully connections between adjacent layers. For example, a DBN with structure of 784−500−500−2000−10 has over 1.5 million parameters. Is parameter redundancy also a significant problem in deep PGNs? How severe and how to compress them if it is certain? These questions still have no answers so far. This paper is the first attempt to investigate these open problems. We firstly confirm and also give an insight of the parameter redundancy of popular PGNs. Whereafter, we combine deep PGNs and deep compression techniques together to derive sparse versions of the deep probabilistic models. The performance of compression is verified by three benchmark dataset MNIST, Fashion-MNIST and CIFAR-10. However, there are some differences between the compression of deep PGNs and deep deterministic networks. The most notable one is that connections in deep PGNs representing the conditional independences between different probabilistic nodes [22]. If some of the connections or nodes are removed, the conditional independences and the joint probabilistic distribution over visible and hidden nodes will be changed dynamically. The influences of the network changes with respect to the probabilistic distribution are not clear. This means the compression of deep PGNs is a non-trivial task, and it may be more challenging than its deterministic counterparts.
One popular approach for the compression problem is to construct sparse versions of these deep PGNs, such as convolutional DBNs(CDBNs) [23], Sparse Group Restricted Boltzmann Machines(SGRBMs) [24] and Sparse Boltzmann Machines(SBMs) [25]. CDBNs employ local connections and probabilistic max-pooling, that are appropriate and efficient for large-scale image classification. SGRBMs adopt L1/L2 regularization to yield sparsity at both the group and hidden layers of RBMs, and achieve higher performance over RBMs. SBMs explore the correlations between nodes to determinate the sparse structure of an RBM, and show good performance in text analysis. For general shadow PGMs, graphical lasso [26], [27] is widely used to select sparse versions of probabilistic models. However, it is unable to deal with hidden variables that are not directly corresponding to real data. The main drawback of the above methods is that they are too heuristic to generate sparse structures before or during the learning processes. This leads to more complex functions to be optimized. Therefore, it is infeasible for this kind of method to evaluate the redundancy degree of deep PGNs and compress the trained deep PGNs. The additional disadvantage of these methods is that they are limited to specific forms of graphical models and can not be used in general cases.
There are many compression methods developed for deep deterministic neural networks, and some equivalences for pre-constructing sparse versions of shadow probabilistic networks. To our knowledge, there is a vacancy of using pruning methods to explore parameter redundancy in deep PGNs and perform model compression. Magnitude and percentage based pruning is one of the most common compression methods for deep deterministic neural network. However, the compression of deep convolutional models with magnitude and percentage approach is often post-pruning and in global manner. Post-pruning is not suitable when it comes to deep PGNs, because it is necessary to get and train a redundant deep network first. Therefore, we develop a pruning and retraining approach in the paper, which is adherent to the two-stage training of deep PGNs, i.e., unsupervised pre-training and fine-tuning. The pruning and retraining approach in this paper is a layer-by-layer and pre-pruning one. That is, the final PGNs grow in a layer-by-layer and compressed manner. This compression approach has several advantages over complex sparse pre-construction methods as
•The developed pruning approach is a layer-by-layer and pre-pruning one. That is, the final PGNs grow in a layer-by-layer and compressed manner.•It can easily specify the pruning rate, and provide a spectrum of pruned model with different percentages of redundancy. There is no need to set weight threshold, which is difficult to select.•It is very efficient in redundancy reduction. The whole pruning only takes a few seconds to conduct, while other complex methods take much more time to select sparse architectures or conduct model compression, such as regularization and low-rank factorization approaches. These model compression techniques are more time-consuming compared with pruning due to the complexity of regularization and factorization.•This approach can be used to compress trained PGNs to achieve a lightweight network which can be embedded in on-chip memory of small mobile devices.•It is flexible to implement and can be applied to every deep PGN.
The rest of this paper is arranged as follows. In Section 2, a brief formulation for the pruning problem of the deep PGNs is presented. Several representative PGN models and their learning algorithms, i.e., parameters estimation, will be discussed in Section 3. The pruning approach for parameter redundancy problem of PGNs will be shown in Section 4. The exploration of redundancy and the compression results of different models are shown in Section 5. Finally, a conclusion is drawn in Section 6.
