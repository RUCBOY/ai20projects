The monitoring of environmental change has become increasingly important for environmental managers at a time of accumulating and accelerating stress on ecosystems. However, scientific monitoring is costly, especially if vast areas are to be covered over long periods. To facilitate cost-effective data collection at greater scale, members of the public are encouraged to contribute observations to centralised databases, often managed by scientists or non-governmental organisations. The potential benefits of data collection through citizen science are broadly accepted (Lodia and Tardin, 2018; McKinley et al., 2017; Tiagoa et al., 2017); however, further evidence is needed to assess how well these types of data complement, or integrate with, more professional monitoring systems.
Citizen science programs refer “to the inclusion of members of the public in some aspect of scientific research” (Eitzel et al., 2017, p. 1) can have different levels of order and structuring (Welvaert and Caley, 2016). Structured programs provide observations or perceptions specific to particular locations and times, using a purpose-built monitoring and reporting tool. The majority of studies to date focused on structured citizen science programs (e.g. Theobald et al., 2015). In contrast, information supplied unwittingly, such as that related to social media communication, generates unstructured data. Social media information is often only indirectly relevant to the particular monitoring interest. However, with the appropriate filtering mechanisms these unstructured data can potentially generate useful insights with a high geo-temporal resolution (Becken et al., 2017). Indeed, Daume and Galaz (2016) refer to twitter conversations as “embryonic citizen science communities” (p. e0151387).
Citizen science platforms and social media sharing have benefitted from substantial progress in information technology and Internet availability, and the transmission of digital data ‘from the field’ has become possible at large scale. This has led to an increasing interest in less structured ‘crowd sourced’ data that are transmitted at a high-volume and velocity. Twitter data has attracted considerable research activity (for a review see Steiger et al., 2015), in particular with a focus on event detection (predominantly acute crisis and disaster, e.g. Vivacqua and Borges, 2012). A small number of researchers have explored the usefulness of Twitter posts for environmental monitoring (e.g. Becken et al., 2017; Daume, 2016; Daume and Galaz, 2016).
The opportunity to collect data from members of the public is particularly pertinent for places that face rapid environmental change, and that – at the same time – are visited by large numbers of people that have the potential to deliver a high volume of data (ElQadi et al., 2017). The Great Barrier Reef (GBR) in Australia is an example of a natural asset that faces environmental decline, yet is a major tourist destination visited by over 2 million people each year (Becken et al., 2017). Recognising the urgent need for a comprehensive and integrated monitoring program, the role of citizen science has been acknowledged and has explicitly been advocated (Addison et al., 2015). Hence, the Great Barrier Reef Marine Park Authority (GBRMPA) is assessing the role of citizen science programs as part of the wider goal of ensuring adequate and cost-effective coverage to help evaluate progress towards long-term sustainability targets as outlined in the Reef 2050 Plan (Department of Environment, 2015).
Our research therefore aims to increase the understanding of how traditional monitoring programs can work alongside, or be integrated with, a range of citizen science programs to enhance the overall monitoring capacity in a cost-effective way. We draw on multiple data sets related to the GBR as a case study, contrasting and comparing the spatial coverage and environmental information they contain We have specifically focussed on coral monitoring, but future research could seek to compare other aspects of GBR monitoring, for example of particular fish species, cetaceans, dugongs, water quality, beach erosion or environmental incidents.
This research has three objectives: 1) to review the broad range of citizen-based data collection for environmental monitoring, 2) to understand the availability and nature of multiple types of data sources that provide information on the state of coral at the GBR; and 3) to compare the spatial coverage and the bleaching patterns of the GBR based on the different data sources. The paper concludes by making recommendations for developing a hybrid monitoring system that integrates citizen-derived with professionally collected data.
1.1. Categories of environmental monitoringEnvironmental monitoring typically requires the expertise of trained scientists who understand the details of the ecosystem they are observing. Expert knowledge ensures that changes are recognised and interpreted correctly. Some monitoring requires precise measurement (e.g. water temperature, salinity, turbidity) using specific technology and instruments, whereas other monitoring relies on human (expert) assessment of environmental conditions (e.g. coral cover, species abundance).Hiring professional staff is expensive, and transporting them to monitoring sites, often in remote areas, is costly and at times logistically difficult. It has therefore become increasingly attractive to involve members of the public in monitoring activities. Building on a tradition of utilising lay observations of the environment (Kearns et al., 2003), an increasing number of programs have formalised public involvement. Collecting data involves three elements: the observation, time, and geographic location (Resch, 2013). Several authors refer to these elements as ‘volunteered geographic content’ (Connors et al., 2012). To date, such volunteered data has mainly informed research projects related to biodiversity and conservation biology (e.g. McKinley et al., 2017), and has been less prominent in the context of monitoring environmental change (Connors et al., 2012).The literature uses multiple terms to describe approaches that involve members of the public, including ‘citizen science’, ‘citizen surveillance’, ‘human sensing’, and ‘crowdsourcing’. Often these terms are used inconsistently (or as subsets of each other) and there is limited consensus in relation to how different approaches could be classified within one conceptual framework (relevant work is presented in See et al., 2016 and Liu et al., 2015). This paper provides an attempt towards a classification based on a coherent set of variables, and specifically for the purpose of environmental monitoring. We note, however, that the field is “in a state of flux” (Eitzel et al., 2017, p.1) and involves researchers from many disciplines, who bring with them different definitions and research motivations (e.g. Boulos et al., 2011). Moreover, this paper acknowledges – but will not resolve – the ongoing discussion on whether citizen science (broadly or with its sub-forms) is a tool, a specific research method, or a new way of developing or implementing research collaborations (Eitzel et al., 2017).Despite conceptual and definitional challenges, we suggest a framework that serves as an underlying logic for this present paper. Our classification begins by differentiating structured from unstructured approaches (Welvaert and Caley, 2016). More specifically, Welvaert and Caley argue that reporting can be intentional or unintentional (i.e. involuntary), and detection of observations can be controlled (e.g. through a rigorous framework) or opportunistic. Social media, for example, generates unintentionally supplied data that are collected with an opportunistic detection method. The different types of approaches discussed in the literature cover the whole spectrum of these two dimensions. In addition, existing approaches differ in terms of their resource requirements, the nature of data, potential for bias, the types of people involved in data generation, training needs and expertise, and data processing (Table 1).Table 1. Categories of environmental monitoring and key differences (Sources: after Resch (2013) and Welvaert and Caley (2016)).Collective sensingHuman sensorsCitizen scienceProfessional monitoringIntentional reportingUnintentionalIntentionalIntentionalProfessionallyDetection mechanismUnstructuredSomewhat structuredStructuredHighly structuredExpertise/knowledgeNone requiredNone to someSome to highConsiderableCost and infrastructureLow, but requirement for IT infrastructureModerate, investment into data collection mechanismHigh, as training and supervision requiredVery highData volumesPotentially very largeLargeLow to mediumLowTime-space resolutionDepends on several factorsDepends on several factorsLow to mediumTypically very lowData qualityPoorPoor to mediumMediumHigh
1.2. Collective sensingCollective Sensing – also referred to as crowdsourcing1 by Welvaert and Caley (2016), passive crowdsourcing by See et al. (2016) or e-participation by Gharesifard et al. (2017) – draws on aggregated data that stem from an open source network, typically accessed via mobile technology. Liu et al. (2015) introduce the term social sensing, referring to the use of big data with a spatial reference and socio-economic context (alluding to the established remote sensing). However, different to this study Liu et al. do not differentiate between unintentional (i.e. collective) and intentional sensing (i.e. human sensing, see below). Collective sensing data could originate, for example, from mobile phone usage or transport smart card patterns (Liu et al., 2015), but the most commonly used sources evident in the scientific literature comes from social media. In particular, Twitter and Flickr data have been used widely because they can be accessed without a password and they provide actual content beyond geographic coordinates. Twitter is a microblogging platform where users can post short messages, photos or short video clips. In 2017, there were about 330 million monthly active users of Twitter (Statistica, 2018). Twitter releases the equivalent of 1% of the total tweets free of cost to researchers, who can choose a random sampling approach for data collection using specified filtering mechanisms. Flickr is an online photo management and sharing application that is commonly used by travellers and photographers.For both Twitter and Flickr, users share information for a range of purposes that are unrelated to subsequent research questions. Thus, the analysis of data contained in posts or photographs occurs without the knowledge or intent of the provider. Subject expertise is coincidental; instead data provide insights into what ‘matters to people’ at a particular location and time. Since these types of social media posts are often geo-time stamped, it is possible to extract geographic information in combination with specific content of interest (e.g. Barve, 2014).Earlier research noted that people are more likely to share positive content (Alaei et al., 2017; Mitchell et al., 2013) or experiences that are outside the norm. Unusual weather events, for example, feature more often in social media than expected weather (Hyvarinen and Saltikoff, 2010). Thus, whilst not being useful as a monitoring vehicle for every day conditions, social media posts may add detail to scientific records for extreme events. The implication of such information is that observations posted on social media are not representative of all types of conditions, and the fact that a particular state is not commented on does not mean it did not occur.Given that social media user profiles also contain a significant amount of metadata it is possible to provide some context around posts (e.g. past behaviour, favourite topics, location of residence), and this may help understand possible sources of bias and levels of expertise. In this sense, social media could be superior to other more structured approaches that often collect very little information on the providers of data (See et al., 2016). Some projects are not primarily interested in the content provided but only focus on the related metadata. Mapping Ocean Wealth, for example, is a project by The Nature Conservancy (2017) that uses the locations of photographs posted on Flickr to derive estimates of visitation to coral reefs, and associated economic impact.Due to the open source nature of many collective sensing data, no investment is required in producing them; however, there is a cost in developing a computing system that can access, download, store and process the large volume of data. Data cleaning and filtering systems are required to eliminate redundant or irrelevant data. Despite such processes, the information extracted from collective sensing often lacks accuracy and reliability, but as stated by Hyvarinen and Saltikoff (2010), the volumes are potentially large and the benefit of real time information compensates for some of the data shortcomings. Thus, despite considerable uncertainty it may still be possible to draw important conclusions from the data.
1.3. Human sensorsIncreasingly, organisations obtain data from people who are willing to voluntarily provide information for a particular purpose. Eitzel et al. (2017) define a human sensor as an “individual who is part of a network by sending data and observations that are often taken and transmitted via modern communication tools, like smartphones, to a central database” (p. 1). Often, specialised web apps help collect the relevant data, but observation sheets and clip board-type approaches are also being used. In most applications, there is little or no expertise required in providing the information as systems rely on common sense and general types of human observations. Due to a predefined format for data collection, the quality is higher than collective sensing. Similar to collective sensing, an upfront investment into the digital infrastructure and computing system is required. Ongoing expertise is needed to maintain the system and analyse incoming data.The ‘Did You Feel It?’ (DYFI) website developed by the U.S. Geological Survey (USGS) to understand the effect of earthquakes is a primary example of how human sensors can complement traditional measurements (Crooks et al., 2013). Thus, by sharing (often real-time) observations or measurements, people can contribute to targeted data collection. Another example is a dedicated Facebook-based citizen science group that reports sightings of cetaceans off the coast of Brazil. A comparison with scientifically collected data showed acceptable levels of correlation (Lodia and Tardin, 2018).
1.4. Citizen scienceThis approach involves an active involvement of citizens in the collection of scientific data (Resch, 2013), and as such it is quite different from both collective and human sensing. Eitzel et al. (2017) emphasise that this distinction is particularly relevant for funding agencies who need to understand that the ‘more engaged participatory approaches’ of citizen science are superior in their data quality outcomes. Indeed, citizen science is a targeted approach that requires significantly higher levels of involvement and expertise. McKinley et al. (2017) argue that citizen science does not differ substantially from professional science. They define it “…as the practice of engaging the public in a scientific project—a project that produces reliable data and information usable by scientists, decision makers, or the public and that is open to the same system of peer review that applies to conventional science” (p. 16). The contribution by citizens can be substantial. Theobald et al. (2015) reviewed 388 citizen science projects related to conservation and biodiversity. They found that collectively these involved 1.3 million volunteers, representing a value of US$2.5 billion in-kind every year.Citizen science projects often involve some training, and this can take several shapes. Most often, citizens collect data as part of an existing and well-supported scientific project, for example in relation to birds, fish or butterflies, or water quality monitoring (Connors et al., 2012). One of the oldest examples of data collection involving citizens is Audubon's Christmas Bird Count, whereby volunteers have counted birds in the Western Hemisphere since 1900 (Connors et al., 2012). Another well-known example is eBird, which provides an online program for bird watchers to report and access information about bird abundance and distribution (Cornell Laboratory of Ornithology, 2018). Hyder et al. (2015) provide a review of marine citizen science projects and their relevance to specific policy areas of marine management.A new form of citizen science project has evolved with increasing computing power. It involves the interpretation of large amounts of data delivered by personal computers or devices of citizens. For example, participants view and classify photographs of animals and their behaviours, taken by automated cameras. Snapshot Serengeti is one of the most prominent examples where 30,000 volunteers helped to identify rapidly images from 200 cameras installed in the park (Swanson, 2015).
1.5. Integrated approachesA small number of projects have compared the reliability and accuracy of citizen-derived data with professional monitoring. Romana et al. (2017), for example, examined observations of street trees and found that generally the consistency between experts and citizens is high (e.g. 90% for site type, land use, dieback, and genus identification), but lower for a number of specific parameters (e.g. wood condition). They recommended to either drop overly complex variables or use additional techniques, such as photo collection. Similarly, Tiagoa et al. (2017) compared the data collected by citizens on reptile and amphibian species in Portugal with a scientific dataset. They concluded that the citizen science project delivered relatively accurate predictions of species distributions. In a marine context, the benefits of using citizen-supplied sightings to expand scientific databases was recognised by Lodia and Tardin (2018), who obtained records on dolphins and whales from citizens for areas that were not covered by their survey. Citizens also identified new species not previously present in the database.Limited work has been published that compares several forms of citizen-based and scientific data. One exception is the OakMapper.org (Connors et al., 2012), which constitutes an online platform at the intersection of citizen science and crowdsourced data (i.e. collective sensing). OakMapper.org collects and visualises information on a disease called ‘sudden oak death’. The platform integrates and visualises official data monitoring with an iPhone application, and Flickr and Twitter data. Connors et al. conclude that using environmental monitoring data from a broad spectrum of sources and intentionality can be suitable for “other cases of highly visible environmental problems” (p. 1285). For citizen-supplied data to enhance scientific data, it is important to build or strengthen the connections between the different contributors to scientific enquiry (Theobald et al., 2015). This current paper interrogates and compares several types of data sources for the case of coral bleaching at the GBR in Australia to explore how a broad spectrum of citizen-based data can be integrated and calibrated with relevant scientific data.
