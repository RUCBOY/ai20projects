The analysis of energy systems has been recently affected by two significant developments. First, the ongoing transition towards a renewable supply of energy brings rapid and continuous changes to energy systems from technology to society [1]. The energy system that traditionally used to be operated and planned as a unilateral top-down system is gradually turning into a bilateral top down and bottom-up system. Liberalization, decentralization, and the increasing volatility of electricity generation represent major challenges for the planning and investigation of energy systems [2]. Second, advances in computer and data sciences allow energy system models to include more detail and become more complex. This increase in computing power expands the possibilities for energy system analysis for which optimization models have become the most popular approach [3], [4], [5].
The attractiveness of optimization models (ESOMs) among energy system models lies in their capability of finding optimal solutions (e.g. in terms of cost) for energy systems under predefined constraints [6]. They are commonly used for short-term dispatch planning and for investigating future scenarios in terms of long-term investment planning [7]. Constraints in ESOMs usually reflect technical and physical necessities, such as the balancing of power supply and demand as well as restrictions on capacity such as on generation or transmission capacity. Furthermore, constraints in ESOM are often used for socioeconomic or environmental limitations, such as on greenhouse gas emissions [8].
Currently, ESOMs are becoming more and more complex. This is often motivated by the assumption that a more complex ESOM gives more accurate results. However, according to George Box and Norman Draper, ‘all models are wrong, but some are useful’ (cited by [9]). If models are wrong in general but might still be useful to represent reality, the question arises as to how simple an ESOM should be without failing to achieve a required level in accuracy [10]. Choosing the model with the best trade-off between complexity and accuracy comes with a process that starts with (1) formulating a research question and choosing (2) the conceptual model, (3) necessary model components, (4) their relations, and (5) the level of detail (e.g., in terms of temporal or spatial resolution). In fact, the combination of these steps (1) to (5) define the minimum requirements to answer the research question [11]. In our opinion, the implementation of these steps should follow an inter- (step (2)) and intra-model complexity comparison (steps (3), (4), and (5)). Hereby, an appropriate conceptual model might be selected and further specified in its complexity and level of detail according to the individual requirements. To our knowledge, such a systematic procedure is currently still uncommon in energy system modeling.
Inter-model complexity comparison can be found in different disciplines. García-Callejas and Araújo [12] compare different ecological system models for their complexity and their accuracy in representing ecologic system behavior. Venkataraman and Haftka [13] analyze different structural models for buildings. Different hydrological models are compared by Orth et al. [14]. Bale et al. [15] compare different models for their capability of representing complex system behavior in energy systems. Meta-modeling can be considered as a type of inter-model complexity comparison that replaces the original model by a less complex representation. Ikeda and Ooka [16] apply meta-modeling for optimization models of a building energy system and observe the potential for a large reduction of computing times. Nolting et al. [17] demonstrate a use case of meta-modeling approaches to assess the security of electricity supply. Martinez-Moyano [18] presents a documentation tool for system dynamics models that includes information on the complexity of the respective models. Finally, Scheller and Bruckner [19] review different ESOMs for their incorporated complexity and level of detail. They recommend making the investigated models more complex as they assume that the results will become more accurate.
In terms of intra-model complexity comparison, most studies focus on specific system components that cause complexity problems. The degree of complexity is varied by analyzing different alternative implementations and the resulting accuracy is compared. Lin et al. [20] evaluate different piece-wise linearization methods by comparing the number of variables and constraints to the approximation error. Milan et al. [21] apply two approaches for linearizing partial load efficiencies in investment ESOMs and compare them by the resulting energy system layouts and the complexity measures CPU time as well as the number and type of variables and constraints. Kotzur et al. [22] aggregate time series to typical days using different clustering techniques and compare the solving times to the deviation in objective function value (OFV). Palmintier and Webster [23] aggregate generation units by applying different clustering methods and compare the results among other measures by the deviation in OFV, dispatch schedule, and solving times.
Studies applying holistic and empirical examinations of complexity and accuracy across multiple components within one conceptual model are rare. Pollok and Bender [24] introduce a workflow to find a Pareto frontier for the trade-off between complexity and accuracy and apply it to Modelica1 models. Sun et al. [26] propose a systematic procedure for defining the right level of detail in agent-based models. However, there are no comprehensive studies that we are aware of that holistically and empirically examine ESOMs in terms of complexity and accuracy by comparing different more or less complex model formulations for multiple system components (i.e. intra-model complexity comparison). Such an examination would make the modeling process of ESOMs more efficient by minimizing the time required for modeling and computation while at the same time providing a sufficiently accurate answer to the problem [27].
In this paper, we present a methodology to systematically analyze the trade-off between complexity and accuracy in ESOMs. Our empirical analysis investigates the complexity for a case study of power system optimization models (PSOMs) with regard to economic dispatch and investment planning. In addition to the cost of required computing capacity and computing time, complexity assessments in dispatch models are further motivated by the need for time-efficient solutions to support short-term operational decisions. We develop a modular and scalable PSOM that allows an analysis of the relationship between degree of complexity and of accuracy. The analysis of these models provides the empirical basis for our complexity and accuracy assessment.
The specific research questions that we will address in this paper are:
(1)Are complex power system models more accurate?(2)What are the complexity and accuracy drivers in power system optimization models and which modeling recommendations can be derived?
Section 2 sets out the literature-based theoretical framework. Resting on this framework, in Section 3 the procedure for systematic management of complexity in ESOMs is introduced. Further, the modular and scalable PSOM is formulated and validated. Section 4 presents the optimization results, which are subsequently used to discuss recommendations for the modeling process of PSOMs in Section 5.
