Face image synthesis is one of the most significant topics in computer vision and has a wide range of applications, such as face recognition, photo recovery, and virtual social network. Take face recognition as an example. A small-sample-size problem, which can be a bottleneck of practical face recognition, can be overcome to some extent by generating the related training data with various variations. Despite the development of image synthesis technologies, how to generate facial image with various variations and preserving the subject identity is still very difficult, due in part to the technical challenge of mapping from a variation factor (e.g., adding or removing some kind of expression, disguise, and even pose) to a high dimensional face image, but also due to the fact that humans are extremely attuned to subtle details in the facial region. There are two main challenges in face image synthesis. On the one hand, there are various semantic face variations, such as facial disguises, lighting changes, expression, and pose, which are hard to synthesize in the image space. How to effectively learn a complex transformation from the original image space to a suitable latent semantic space is a still an open question. On the other hand, different from the common image synthesis, it is usually required that facial image synthesis should preserve the subject identity of the face image. Although face recognition has been well developed, how to jointly learn the generation of face image and preserve the subject identity is still not well solved.
As a hot research topic, face image synthesis has attracted much attention from researchers. One direction of face image synthesis is the cross-modal image generation, such as face sketch synthesis. Wang et al. [1] proposed a Bayesian face sketch synthesis framework, in which the task is divided into two parts, the neighbor selection model and the weight computation model. Wang et al. [2], [3], [4] also introduced several effective methods to improve the neighbor selection process or proposed a new model-driven face sketch synthesis framework to speed up the synthesis process. In our paper, we focus on the facial variation synthesis in the face images. An ordinary strategy to repair the occluded parts of face images is to approximate the appearance of other facial parts using a morphable model [5]. Mo et al. [6] firstly used a dictionary of frontal face images to improve the work of morphable models. Wang et al. [7] proposed a new scheme based on principal component analysis and represented a target image by a linear combination of eigenfaces. Burgos-Artizzu et al. [8] introduced cascaded pose regression and sparse coding [9] to generate different kinds of poses and expressions. However, the quality of images generated by those ordinary strategies is poor due to the low resolution, artifacts, and the absence of personal facial feature details. Overall, it is difficult for those ordinary strategies to generate a face image with both a good visual verisimilitude and a required variation.
Recently, deep neural networks have been applied successfully to face generation. Xie et al. [10] proposed a new structure of deep neural networks, combing with sparse coding, to repair the blind occluded face parts. Pathak et al. [11] proposed a deep model of context encoder to learn a useful representation of appearance and shape of a face. Yeh et al. [12] introduced generative adversarial networks (GANs) to generate image texture of occluded facial parts and achieved a great improvement. As one of the most significant improvements on the research of the deep generative models, GANs have achieved state-of-the-art results in the fields of image generation, image editing and style translation [13], [14], [15], [16], [17], [18], [19], [20], [21], [22]. In GANs [23], there are a generator G and a discriminator D. By inputting a random noise z sampling from the Gaussian distribution pz, the generator G aims to generate an image G(z), which should be as similar to a desired real image x as possible. Meanwhile, the discriminator D should distinguish the generative image G(z) from a real image x following the real data distribution pd as accurate as possible. Thus the competition between generator and discriminator is like a two-player minimax game and its value function V(D, G) can be learned by solving the following minimax optimization:(1)minGmaxDV(D,G)=Ex∼pd(x)[log(D(x))]+Ez∼pz(z)[log(1−D(G(z)))].The adversarial learning is an effective learning method and has an advantage that it does not need to compute any Markov chains or approximate inference networks [23]. The value function can obtain the global optimality when pz=pd. It is proved that given any fixed generator G, the discriminator D can get its optimal result via DG*(x)=pd(x)pd(x)+pz(x). With the optimal discriminator D, minimization of the value function turns out to be the minimization of the Jensen–Shannon divergence between the model and data distribution: V(DG*,G)=−log(4)+2·JSD(pd∥pz), which will finally be at the Nash balance when pd=pz. Generative Adversarial Networks (GANs) show its excellent ability to generate a complex and high-resolution image in a wide range of research fields.
The original Generative Adversarial Networks generate the target samples from a random vector sampled from the Gaussian distribution, ignoring the input of prior knowledge information. Recently, many domain related prior knowledge, optimization algorithms, and modified construction strategies are proposed to integrate with adversarial networks, making a big success in several image generation tasks, such as style transfer [14], [16], [19], [24], super-resolution [25], [26], [27], [28], text2image [29], and future prediction [13], [30]. Several works have been proposed to promote the original GANs in both network structure and adversarial theory. Radford et al. [17] firstly applied the convolution networks to construct the generator and proposed a deep convolution GANs (DCGAN), which is more powerful to generate high-resolution images. In order to improve and stabilize GANs, Zhang et al. [29] proposed a more effective chain cooperation pattern with two GANs, named StackGAN, followed by the High-resolution GAN [25], which developed a more complex structure of GANs to obtain super-resolution images, e.g., 1024 × 1024 resolution. There are also several works focusing on the mathematical deduction and improvements on the theory of GANs, in which the Earth Mover distance is applied in [31], [32] and the negative log likelihood objective is replaced by a least square loss in [33]. Meanwhile, class information is introduced into the model of GAN. Mirza and Osindero [13] extended the original GANs to the Condition-GAN, by adding a class label and requiring the discriminator to classify images with not only a True or False scalar but also a class label vector. In order to generate images with more semantic information, researchers proposed several new but more complex methods to achieve better results [18], [20], [34]. For instance, DRGAN [18] represented the pose factor as an one-hot vector, in which each element of the vector represents a given pose direction. TPGAN [20] imitated the behavior of a human recognizing an object (e.g., a stranger’s face) and synthesized a frontal face with two separate pathways, e.g., a local pathway for local texture generation and a global pathway for global structure generation. Although the variants of GANs has improved the performance of image generation, the visual quality and generated variations in the synthesized image are still not satisfactory.
Recently, Zhu et al. [16] proposed a model of CycleGAN, which introduces a cycle constraint for two GANs to learn the mapping between source domain and target domain in an unpaired dataset. Compared with other variants of GANs, CycleGAN not only retains the advantages of single GANs but also introduces an effective cycle reconstruction between two GANs (e.g., GAN X and GAN Y). In CycleGAN, when inputting the output image generated by GAN X, GAN Y is required to generate the input image of GAN X. Although CycleGAN has shown a promising performance in face synthesis because of the cooperation between its two GANs, it still has the following concerns to be solved, as shown in Fig. 1. Firstly, CycleGAN cannot well preserve the class of the generated image (e.g., zebra-stripe is wrongly generated in the horse riders in Fig. 1(b), and there are artifacts in the mouth region in the generated face image in Fig. 1(e)) because it cannot directly regularize the intermediate synthesis result of cycle image translation process between a pair of images for its single GAN. Also the cycle reconstruction procedure in CycleGAN easily loses stability and generates a poor-quality image (e.g., Generated face GX(X) loses balance with Generated face GY(Y) Fig. 1(i) and (j).) especially when facing a difficult image synthesis task. Inspired by the great potential of CycleGAN and motivated by the issues above, in this paper we proposed a novel model of triple translation GAN with multi-layer sparse representation (TTGAN). In the proposed TTGAN, we proposed a multi-layer sparse representation model for face image preserving, in which the L1-norm representation constraint enhances the ability of identity preserving and the robustness of the generated facial images to reconstruction error (e.g., noise, disguises, outliers and other variations). Moreover, triple translation consistent, including two existing translations and a designed third image translation (i.e., the generation from the reconstructed original input to the desired output), is introduced into the model, which can further improve the stability of the model optimization, with good visual quality reported. The face synthesis experimental results on the AR face database [35] and CAS-PEAL face database [36] clearly shows the superior performance over the recent CycleGAN and Pix2Pix GAN [19].Download : Download high-res image (756KB)Download : Download full-size imageFig. 1. The remaining issues of image generation by CycleGAN. zebra-stripe is wrongly generated in the horse riders in (b); there are artifacts in the mouth region in the generated face image in (e); Generated face GX(X) loses balance with Generated face GY(Y) (i) and (j).
The main contributions of our work lie in three areas: (1) we design a new model of GANs to effectively handle the problem of face image synthesis with pairs of images as inputs. (2) We propose a new framework of triple translation GAN with a novel translation constraint which is proved useful to repair facial parts by extensive experiments. (3) We develop a novel multi-layer sparse representation on the translation from a source domain to a target domain, which can effectively preserve the face identity and generate face images with various variations.
The rest of this paper is organized as follows. Section 2 presents a brief review of the related works. Section 3 gives the proposed TTGAN method. Section 4 describes the optimization algorithm of TTGAN. Section 5 conducts the experiments and Section 6 concludes the paper.
