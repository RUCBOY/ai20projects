Graph-based representations have already gained considerable popularity thanks to their competence of characterizing structured data in general. Over the last decades the research in structural pattern recognition has mainly focused on the solution of the correspondence problems [1], [2] as the essential means of assessment of structural similarity. For the sake of the analysis and comprehension of these data, it is necessary to have some graph similarity measure methods.
The famous kernel trick [3], [4] has switched the problem from the vectorial representation of data to a representation of similarity. Then, it became possible to use standard statistical learning techniques for data with no easy vectorial representation. With the similarity being at hand, similarity-based pattern recognition techniques such as graph kernel can be adopted to execute the tasks of recognition and classification, or graphs may be embedded in a high-dimensional pattern space using kernel mapping techniques. So, in last years the pattern recognition community has been increasingly interested in structural learning utilizing graph kernels. Nontheless, owing to the ample expressiveness of graphs, this task has been demonstrated to be hard as well.
Graph kernels are used heavily for the classification of structured data. Images could be regarded as graphs if components in an image are considered as node and their semantic relations as edges. A great deal of work has been witnessed for image classification by adopting methods like graph edit distance [5] or marginalized kernels [6]. Other application fields include bioinformatics (graph models of molecular structures [7] in molecular biology), chemoinformatics (modelling molecular compounds in chemistry, prediction of features of molecules from their graph structures, e.g. toxicity or effectiveness as a drug [8]), financial data analysis [9], social network analysis [10], and HTML, XML, Internet for graph models [11].
1.1. Entropy and applications in pattern recognitionEntropy theory was put forward by German physicist Rudolf Clausius in 1865 [12]. He gave the entropy in a reversible thermodynamic system: ds=(dq/t)r, where r indicates the reversible process, ds symbolizes the entropy change of the system, dq indicates the heat change of the system, and t is the absolute temperature of the system. Clausius associated entropy with the heat exchange of the system from a macro perspective, and found that entropy is a state function, independent of the thermodynamic path experienced by the research object, only related to the initial state and the final state.In 1896, Boltzmann, an Austrian physicist, connected entropy with the number of microscopic states and used statistical theory to explain thermodynamics [13]. In a system composed of a large number of particles, entropy is the disorder degree of the system, which can be used to express the disorder degree of particle motion. According to Boltzmann the function of entropy can be expressed as: S=−klnp, where k is Boltzmann constant, p is thermodynamic probability. Boltzmann’s microscopic study makes the entropy theory popularized. By the 1950s, entropy was found to be able to describe information. In 1948, Shannon defined the uncertainty of information in the communication process as information entropy, which was used to measure the uncertainty of signal source and laid the theoretical foundation of modern information theory [14].Recently, entropy is widely used in pattern recognition. For example, George et al. [15] provided a comprehensive analysis, making the most of the entropy rate created by a Markov chain over a connected graph of order n and subject to a prescribed stationary distribution. Aggarwal et al. [16] presented an entropy-based method to model a decision-maker’s subjective utility for a criterion value. Zhou et al. [17] showed an ave-entropy based weight assignment process considering the risk preference of decision making to deal with manifold attribute decision making problems, where indefinite subjective judgments such as belief distributions are included. Moreover, other approaches for information entropy are such as: Min-entropy latent model [18], Multi-layer entropy-guided pooling [19], Semi-supervised multi-view maximum entropy discrimination [20], and so on.It is worth noting that in recent years, there has been a lot of research literature about the applications of Rényi entropy on graphs. For example, Pál [21] presented simple and computationally efficient nonparametric estimation of Rényi entropy and mutual information based on generalized nearest-neighbor graphs. Livi [22] presented different variants of the improved optimized dissimilarity space embedding graph classification system by exploiting the α-order Rényi entropy of the dissimilarity representation to execute the classification on labeled graphs. Moreover, Ran [23] showed the connections between Renyi entropy Principal Component Analysis, kernel learning and graph embedding. This theoretic development enables a close relationship between information theoretic learning, kernel learning and graph embedding. In this paper, we will explore the utility of a h-layer deep Rényi entropy graph kernel.
1.2. Related workGraph kernel methods are strong learning algorithms that can be effortlessly used for every input domain and are supported from statistical learning theory that provides powerful theoretical guarantees on the output hypothesis. One of the common ideas of graph kernel methods is that of deconstructing two distinct objects and comparing some simpler substructures. Examples include Random Walk Kernels [24], Marginalized Graph Kernels [6], Graph Hopper Kernels [25], Deep Graph Kernels [26], and Multiscale Laplacian Graph Kernel [27]. There are other convolution kernel functions: Shervashidze et al. [28] gave a feature extraction scheme rooted in the Weisfeiler-Lehman test of isomorphism on graphs. It establishes the mapping of the initial graph to a sequence of graphs whose node attributes capture topological and label information. Another significant graph kernel is studied in recent years [29]. Zhao et al. [27] proposed a labeled graph kernel for behavior analysis, Xu et al. [30] defined two reproducing graph kernels. They further proposed a hybrid reproducing graph kernel, and used it as a measure for establishment of the similarity resemblance between a couple of graphs. Formerly suggested graph kernels that undergo implicit computation typically back up assigning arbitrary kernels for vertex annotations, but do not scale to large graphs and datasets. Even when people know approximative explicit feature maps of the kernel on vertex annotations, it is still unclear how to gain feature maps for the graph kernel. Recently, Bai et al. [3], [31] built a graph kernel based on multilayer representation as well as the entropy-based isomorphism test. The applied strategy has a significant implication upon the operation time of the graph kernel method at the higher level. The running time of graph kernel methods discussed above holding up implicit kernel computation is often longer for training support vector machines. Therefore, it is necessary to find more efficient and faster graph kernel functions.
1.3. Contributions and paper organizationIn contract to previous works we use the Rényi entropy to structure a deep representation of graphs, and focus on the combination of Rényi entropy and probability of the steady state random walks on graphs visit vertex to generate a novel graph kernel. In detail the primary contributions of our work are highlighted as follows:(1)We use the L’Hospital’s rule to derive the Shannon entropy of graph by generalized entropy with q=1, and deduce the expression of second-order Rényi entropy by generalized entropy with q=2. We further give some properties about Shannon entropy and second-order Rényi entropy. Based on these properties, we draw on the second-order Rényi entropy to measure a graph established on the probability of the steady state random walks visit vertex on graphs.(2)The entropy has an important application in graph representation. We gauge the deep information by means of a family of h-layer expansion subgraphs rooted at a vertex, and define a h-layer depth-based second-order Rényi entropy representation for each vertex of graph as a point coordinate. The h-layer depth-based second-order Rényi entropy representation around vertex v of graph is a h-dimensional vector, where h is the length of the shortest paths from v to other vertices in the graph.(3)We propose a new vertex alignment kernel based on the existing framework of the depth-based matching procedure [31] associated with the Rényi entropy representations. To this end, we develop a matching method for point set matching, which makes the computation of an affinity matrix with regard to the Euclidean distances between points. Meanwhile, by second-order Rényi entropy of a graph above, we further define a deep second-order Rényi entropy graph kernel (SREGK), and this graph kernel is based on second-order Rényi entropy and Euclidean distance. For graphs with n vertices, the time complexities for the our kernel are O(n2), in contrast to O(n6) for the classic Gärtner graph kernel. This low computational complexity makes it possible for our subgraph kernels to work with much larger graphs and thus overcome the size limitation occurring in the most advanced graph kernels.Our experimental outcomes upon fourteen benchmark datasets demonstrate that our SREGK outperforms or matches twelve state-of-the-art graph classification algorithms including graph kernels and representations of graphs through deep learning networks.The remainer of the paper is organized as follows. In Section 2 we discuss the fundamentals including graph kernel and Shannon entropy. The Rényi entropy is reviewed and deduced in Section 3. It contains the generalized entropy, first-order entropy, second-order entropy and their properties. In Section 4 we show deep Rényi entropy representation and graph matching, and further propose deep Rényi entropy graph kernel. The experimental evaluation is given in Section 5. Finally, some discussion in Section 6 concludes the paper.
