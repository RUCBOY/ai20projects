The text-to-image synthesis task is defined to generate diverse photo-realistic images conditioned on an input sentence. That is this task aims to learn a mapping from the discrete semantic text space to the continuous visual image space. Therefore, this task has many practical applications, e.g., editing images, designing artworks, restoring faces. Particularly, generated images by text-to-image models are required not only to be highly realistic, but also semantically consistent and richly diverse. As a result, generated images are different from each other in the pixel space, but all of them should match semantically the input sentence.
These requirements are exactly what Generative Adversarial Network (GAN) [1] attempts to meet. Original GAN is composed of one generator and one discriminator, and these two models play an adversarial game. Through such a game, GAN has shown significant results in synthesizing real-world images [2], [3], [4], but the content generated by GAN is uncertain only taking the noise as input. Compared with original GAN, conditional GAN (cGAN) can effectively control the content by offering the extra condition to both generator and discriminator. For this reason, cGAN has become the only solution to the text-to-image synthesis task to generate the controllable and certain image. And the generated content matches semantically with the input text. Thus, all of state-of-the-arts in Table 1 adopt cGANs as basic models.Table 1. The details of models for the text-to-image task. Scale indicates the resolution of generated images. G and D represent how many generators (G) and discriminators (D) in a model. Supplement demonstrates how, where and how frequently linguistic and visual features are fused. Others means whether the model uses extra condition besides text. End2End means whether a model is trained end-to-end.ModelsDetailsScaleGDConditionEnd2EndSupplementOtherGAN-INT-CLS6411Concat/Input/1GAWWN12822Concat/Input/1StackGAN64,128/25622Concat/Input/2StackGAN++64,128,25633Concat/Input/3TAC-GAN12811Concat/Input/1HDGAN64,128,25613Concat/Input/1PPAN64,128,25613Concat/Input/1Ours64,12812CM-M/All/5
These models tend to deepen structures to guarantee the generative capacity in practice. Therefore, all of them have unstable training processes and tend to produce nonsensical results. To resolve these problems, StackGAN [5] and StackGAN++ [6] adopt the stage-by-stage strategy for image generation. Both models firstly synthesize coarse 642 images by taking the sentence as input, and then supplement again the text to the 642 image feature to generate fine 1282 and 2562 images. Nonetheless, these two models simply concatenate the text to the image feature. Due to noneffective of this operation, models have to stack several residual blocks to ensure the capacity. Hence, StackGAN and StackGAN++ attain numerous parameters and computations. HDGAN [7] designs a hierarchical-nested structure to simplify the network. Similar to StackGAN and StackGAN++, the generator of HDGAN concatenates only once the text.
Actually, all above state-of-the-art models adopt the traditional method of the generator structure to design, which concatenates two-modal representations and then stacks consecutive residual blocks to guarantee the capacity. However, they have no explanation for this design [8]. In this work, we attempt to infer and regularize the compact network structure for lightweight, stable, vivid image generation. To reduce the network parameters and complexity, we infer from the information compensation theory, and further propose that degrees of semantic information required are different at different scales of image generation. For example, in the process of generating low-scale images to high-scale images, the semantic information required by the generator changes from the object information, e.g., beak and petal, to the state information, e.g., small beak and pink petal. Therefore, modules in LD-CGAN of Conditional Embedding and Conditional Manipulating Block are proposed for the semantic disentanglement and entanglement process, respectively. So far a few works have focused on semantic disentanglement, and they all lack theoretical explanation for the problem, why learned representations are separated by degrees of semantics. Only [9] introduces the evaluation metrics to measure the disentanglement property of the output space. Here, we attempt to explain this problem from weight parameters in Section 3.3.1.
All state-of-the-art models regard the input text as a semantically intertwined whole and do not consider the semantic process. Here, Conditional Manipulating Block makes disentangled attributes directly guide image representations to learn. As a result, different scale layers are fed with required scale-specific information. Based on information compensation based approach, LD-CGAN no longer needs residual blocks to ensure the capacity and reduce dramatically the network complexity. In summary, for the text-to-image synthesis task, LD-CGAN provides a new idea, that is from the explainable disentanglement to the effective entanglement.
More specifically, Conditional Embedding (CE) in Fig. 1(b.I) is introduced to disentangle the input intertwined semantically information into multi-level attributes. Because not all attributes have specific physical meanings, there is no practical method to supervise these scale-specific meanings. Therefore, the whole process leads to be unsupervised. Here, we propose the Conditional Manipulating Block (CM-B) in Fig. 1(b.II) consisting of an up-sample layer, a conv3 × 3 layer and a Conditional Manipulating Modular (CM-M) in Fig. 1(b.III) to entangle the disentangled semantics and image features.Download : Download high-res image (1MB)Download : Download full-size imageFig. 1. The framework of LD-CGAN. It is composed of the following major components: Conditioning Augmentation (CA) for producing integrated global semantic condition c0; the generator consisting of Conditional Embedding (CE), Conditional Manipulating Block (CM-B) and Pyramid Attention Refine Block (PAR-B) for supplementing disentangled linguistic condition s0 to each-scale feature to gradually generate 642 and 1282 images; and two discriminators for distinguishing ground-truth and generated images.
Meanwhile, to fully enhance spatial features between multi-scale context, we improve the pyramid structure in Gao et al. [10] to propose Pyramid Attention Refine Block (PAR-B) in Fig. 1(b.IV). This module employs the attention mechanism to fuse multi-scale features rather than adding them together [10]. Leveraging the pyramid structure enriches multi-scale feature representations on computer vision tasks [11], [12], [13], [14], [15]. Networks for these tasks are constructed from down to top and connected laterally to combine low-scale, semantically weak features with high-scale, semantically strong features. Therefore, these networks attain rich semantics at all-scale features and can be built quickly from a single image scale. Here, attentively relative importance between multi-scale features could be captured by PAR-B. This module takes two-scale features (e.g., 642 and 1282) as input, and enables semantically strong 642 features support the generation of semantically weak 1282 features. In consequence, PAR-B makes the whole network take full advantage of multi-scale context. Notably, all proposed CE, CM-M in CM-B and PAR-B are so flexible and compatible that they can be adopted in any cross-modal tasks.
Generators of [5], [6], [16], [17] are only optimized indirectly with feedbacks from discriminators. Here, we employ another loss, perceptual loss L1 [18], to update directly the generator to improve the quality of generated images. Besides, images of different scale should attain similar basic sketches and colors of an object conditioned on the same text description. Therefore, the image-consistency loss L2 [6] is used between 642 and 1282 images to keep generated two-scale images more consistent. In this paper, we draw on the basic hierarchically-nested adversarial structure [7] and wholeheartedly focus on minimizing the model complexity, without compromising the quality of generated images. Thus, our method is orthogonal to the ongoing discussion about GAN regularization and hyper-parameters [2], [19]. Specifically, we simplify the adversarial structure, and adopt multi-purpose losses [7]. The discriminators (e.g., LR Discriminator (LR-D) and HR Discriminator (HR-D) in Fig. 1(a)) in LD-CGAN attain the matching-aware pair loss L3 [20], local image loss L4 to ensure semantic consistency and visual fidelity. Specially for 1282 images, their discriminator (HR-D) adopts an additional class information loss L5 to guarantee class invariance.
In conclusion, our major contributions can be summarized as follows: (1) Instead of deepening the network, we propose Conditional Manipulating Modular (CM-M) in Conditional Manipulating Block (CM-B) to compensate multiple times semantic information to the entire visual processing. Meanwhile, we further introduce Conditional Embedding (CE) to disentangle integrated information into separated attributes to input multi-scale CM-M. These two modules in our LD-CGAN make the best use of text information to simplify the network; (2) To enrich feature representations in the highly structured model, we develop Pyramid Attention Refine Block (PAR-B) to capture multi-scale context. At each pyramid level, PAR-B takes coarse-resolution and fine-resolution features as input to attentively select relative importance between spatial locations; (3) Except for adopting all adversarial losses to indirectly optimize the generator, the perceptual loss L1 and image-consistency loss L2 are used by LD-CGAN to directly optimize the generator to improve the quality of generated images. (4) Experiments show our 1282 images can attain comparable performance with 2562 images generated by the state-of-the-arts on two benchmark datasets: CUB and Oxford-102. Besides, our generated images have more vivid details than others. Meanwhile. LD-CGAN reduces dramatically network parameters and computation by 86.8% and 94.9% over state-of-the-art HDGAN, respectively.
