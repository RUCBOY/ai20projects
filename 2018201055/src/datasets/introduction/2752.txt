Pronunciation teaching often includes practice with a teacher, who can guide learners individually and provide feedback in the correct manner and amount when necessary (Hincks, 2003). Yet this is often time- consuming and expensive when the educational institutions’ benefits are taken into consideration. Additionally, this does not match up well with the way that teachers usually approach pronunciation teaching. Research shows that most teachers approach pronunciation teaching in an ad-hoc manner, that is, they address pronunciation issues mostly in presence of a salient error or an error causing a communication problem. This is mostly either because teachers do not have sufficient training (Burgess and Spencer, 2000) or self-confidence (Couper, 2017; MacDonald, 2002) in pronunciation teaching. Another common belief among teachers is that pronunciation improvement will take care of itself with sufficient input and it does not require teaching in the way that other language skills do. This is a belief that was motivated by the principles of communicative language teaching which emphasized fluency over accuracy (Levis and Sonsaat, 2017).
However, providing instruction and feedback on immediate production in pronunciation teaching is an essential pedagogical requirement for learners’ improvement, even though it can demand extensive instructional interventions (Warren et al., 2009). One solution to the lack of time and training of teachers is computer-assisted pronunciation training (CAPT) systems, which have been utilized to support learners to study autonomously and help teachers provide learners with individual feedback without using large amounts of time in class (Egan and LaRocca, 2000; Eskenazi, 1999; Rypa and Price, 1999). CAPT may also be motivating for many learners, both because of their interest in technology and because of learning preferences that make working with a computer program more comfortable than interacting with a real person. CAPT gives learners the chance to work on their pronunciation in a stress-free environment, at their own time and pace. For instance, pronunciation is a skill that may require extensive listening and repetition. Some learners may feel uncomfortable about asking for a repetition in class more than once, but with a CAPT program it is easier to make use of extensive repetition (Hardison, 2004). All said, CAPT offers great promise for individualized pronunciation instruction, more consistent practice, and greater comfort in learning (Levis, 2007).
With advancements in speech technologies such as automatic speech recognition (ASR) and speech synthesis, CAPT can also provide practice opportunities that a face-to-face class cannot. For example, the use of speech visualizations that adapt to each person's speech (Bliss et al., 2018), the use of multiple voices in perceptual training (Barriuso and Hayes-Harb, 2018; Thomson, 2011; Thomson, 2012), or the use of personalized voices (Probst et al., 2002) all provide learning opportunities that classroom pronunciation training cannot. The latter idea (i.e., personalized voices) has resurfaced several times in the CAPT literature. It was first proposed nearly thirty years ago by Nagano and Ozawa (Nagano and Ozawa, 1990). In their pioneering study, Japanese learners were asked to practice with a model of their own voice that had been modified to match the prosody of a reference English speaker. Post-training utterances from these learners were rated as more native-like than those for a second group of learners who instead had practiced with the reference English voice. More than a decade later, Probst et al. (2002) published a study in Speech Communication where L2 learners were asked to practice with a native speaker voice that had different characteristics. Participants who imitated a well-matched voice (i.e., one with characteristics similar to their own voice) improved more than those who imitated a poor match. This result led the authors to suggest that each learner has an ideal speaker voice to imitate, a so-called “Golden Speaker.” Nearly ten years later, and in an article also published in Speech Communication (Felps et al., 2009), we proposed that each learner's “Golden Speaker” should be their own voice, resynthesized to have a native accent. Most notably, in that study we presented an accent-conversion technique that was able to correct not only the learner's prosody (as Nagano and Ozawa had done) but also their segmental errors (i.e., phoneme substitutions, additions and deletions). Missing from our study, however, was a validation of the technique on pronunciation-training experiments. This is a clear next step. A decade since the first paper has shown that refining the accent-conversion technique for successful deployment in pronunciation training was more challenging that expected. The improvenment we have seen in accent-conversion quality makes us optimistic for further successful deployment of the Golden Speaker algorithms.
The manuscript describes a web application (Golden Speaker Builder; GSB) and the underlying speech analysis/synthesis algorithms that allow L2 learners to generate their own personalized voices. In a first step, we conduct a series of listening tests to determine the extent to which the synthesized voices mirror the learner's own voice with an American English accent. Then, we validate GSB in a language-instruction setting with a population of Korean L2 learners of English. The study was guided by two research questions:
•RQ1: What is the effect of using the GSB on learners’ improvement of their comprehensibility and fluency?•RQ2: What features of the GSB did learners find useful, and what did they find in need of improvement?
