Brain-computer interface (BCI) is a powerful technology that endows the human ability to communicate with the outside, especially severely disabled people and those with brain dysfunction [1]. Many applications of BCI systems are exploited to serve as rehabilitation tools for patients with communication disabilities and reinforcement tools for healthy people [[2], [3], [4], [5]]. Due to the high temporal resolution and easy operation, electroencephalography (EEG) signals are widely used to construct the BCIs. In the task paradigms, based on the stimuli and EEG signal components, EEG BCI can be categorized by motor imagery (MI), P300, Steady-State Visual Evoked Potentials (SSVEP), motion onset visual evoked potential (moVEP), etc. Among these BCIs, MI attracts wide attentions because of the potential applications in extending healthy peopleâ€™s ability, assisting for disabled people, and motor function rehabilitation. MI is the mental process of controlling the normal movement of the limbs by means of brain imaging in the absence of normal movement [6]. Various studies have proven that MI can lead to similar brain activities in the motor system as actual physical action, which is the fundamental neural mechanism for the MI BCI based rehabilitation [7]. However, there are two main limitations for MI BCI: relatively lower accuracy and the long duration in subjects training.
Recently, there have been rapid developments in deep learning. The deep learning framework with deep neural networks (DNN) is widely employed in feature extraction and has achieved better performance than traditional classification algorithms in image, video, speech, and text etc. [8,9]. Motivated by the successful applications of deep learning in these fields, some researchers began to apply this advanced technique to the EEG analysis including feature extraction and classification. Ma et al. [10] utilize DNN and compressed sensing to extract features of the moVEP, achieving approximately 87.5% accuracy improvement compared to the conventional approaches. In addition, the convolutional neural network (CNN) is adopted to conduct the emotion recognition, which results in a higher accuracy [11]. The deep learning has also emerged as an effective algorithm in MI applications. Specifically, CNN and stacked autoencoders (SAE) have been used to classify motor imagery signals [12]. Therefore, we adopted deep learning to improve the classification performance in current work.
In order to alleviate the burden of long training durations for subjects, we expect to construct a model by the information of previously recorded subjects, which was used to classify a new subject. The current methods mostly use the training and test data of one subject because of the variability of different subjects, which imposes the intensive burden for training on subjects. Moreover, in some real situations, it is impossible to collect the data of the subject beforehand. With the developments of BCI experiment processes and the continuous improvement of the classification algorithms, some works have begun to focus on the subjects-to-subjects or sessions-to-sessions problem for BCI, which transfers knowledge from previous subjects to new ones [13]. Because the statistical distribution of the data varies across subjects as well as sessions within individual subjects, many researches define this method as the subject-to-subject transfer learning. Transfer learning is a problem that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem in deep learning. In current work, we refer to this as training free or transferring subjects learning. Krauledat et al. [14] proposed an effective training free approach that uses the common space pattern (CSP) filters to transfer sessions to sessions. Fazli et al. [15] also addressed the subject-independent mental state classification by building CSP filters. Recently, as a result of the powerful ability of deep learning, researchers try to explore the training free method using deep learning framework. Lin et.al [16] proposed a conditional transfer learning for improving emotion classification performance.
Indeed, the transferability of training data or trained models among different subjects has two limitations. Firstly, the high dimension and various statistical distribution of BCI data cause challenges in extracting useful abstract features from raw data. Another issue is the low number of samples per experiment and the uncontrollable noise during the experiment. It is a long quest for effective BCI systems that can overcome these challenges. Consequently, many effective features are used to reduce the dimensionality and capture special information in the different domains, such as frequency domain, time-frequency, and space domain. Considering that CSP is one of the most popular feature exaction methods in EEG applications particularly in motor imagery classification [[17], [18], [19], [20], [21], [22]], in current work, we also adopt the CSP feature. The fundamental assumption in transferring subjects learning is that the transfer based on subjects-to-subjects or sessions-to-sessions is the set of linear filters that are conducted in the process of CSP computation should be invariant across subjects or sessions. Moreover, there are many effective methods can implicitly learn the abstract features of data and use the regularization tricks at hand.
However, the CSP features in previous works are based on the logarithmic normalization, which may lose some time-varying information. In this work, we abandon the operation of the logarithmic normalization but retain the information in the time domain. The details will be described in the following section. In order to mine the discriminative information of the time-varying CSP series, we design a separated channel convolutional network, called SCCN, to encode the CSP channels. The structure can reduce the dimension of the CSP data, and obtain more discriminative information than the logarithmic average. Nextly, a convolution neural network classifies the concatenated features developed by a set of encoders. The remainder of the paper is organized as follows. In Section 2 we introduce the background knowledge of the proposed method. Section 3 shows the pipeline of method and the structure of deep model. The data and results are shown in Section 4. We discussed the results in Section 5 and draw conclusions in Section 6.
