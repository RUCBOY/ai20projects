In the past few years, incorporating structured priors in statistical model has become a popular technique for coping with various estimation tasks since it can provide a natural characterization over the relationships between data. As the successful applications of this view, low rank minimization has shown great potential in numerous fields such as machine learning, signal processing and so on.
It is known that using low rank function to constrain a matrix variate can capture its global structure, which leads to the increasing interest in matrix completion and subspace segmentation problem since the intrinsic dimensionality of high-dimensional data is in fact much smaller, i.e., they often lie in low dimensional structures. The most representative models around this view are Low Rank Representation (LRR) [1] and Robust Principle Component Analysis (RPCA) [2]. Their common characteristic is regarding samples to be correlated, but the stretching of each sample inevitably destroys its spatial structure. In fact, there also exist correlations between pixels of a single sample in some applications. Taking advantage of this fact, nuclear norm based matrix regression (NMR) [3] and Nuclear Norm-Based 2-DPCA (N-2-DPCA) [4] integrated the relationships of pixels in the error images caused by the spatially contiguous variations into modeling for face recognition, which is carried out by virtue of low rank assumption of the error matrix. Chen et al. [5] and Luo et al. [6] considered the similar problem from the viewpoint of the dependent matrix distribution. Subsequently, Luo et al. [7] and Luo et al. [8] further extended the above models resorting to Schatten p-norm and tree structure, respectively. In addition, Xu et al. [9] exploited the low-rank structure of the multi-label predictor in multi-label learning. Yan et al. [10] used Maximum-Margin Matrix Factorization to accomplish collaborative prediction of rating data by emphasizing the low-rank structure of the desired data. For more related studies on rank function minimization, please see [11], [12], [13], [14].
The promising results in the above work demonstrate that low rank can effectively characterize the global structure of a matrix variate, but the local structure for a matrix variate also exists and is very crucial for the face recognition problem. For instance, the face images of 120 individuals from AR database are resized to 45 × 30. For each individual, a clean face image and a face image with sunglasses are chosen (as shown on the left side of Fig. 1). Thus, 120 noise images caused by sunglasses are generated. Each noise image is partitioned into 9 × 6 blocks and the size of each block is 5 × 5. To describe the local correlation structure, we focus on the local block of each error image. Firstly, a certain block in the error image is chosen and marked as the red box. The correlation map of pixels in this block is presented in Fig. 1(a). It is clear that most pixels in red box are highly correlated. And this trend will become more evident with the narrowing of red box area. Next, we further investigate the relationships between blocks. Note that the structured attribute of nuclear norm, we attempt to use nuclear norm to measure each block, i.e., nuclear norm of each block is calculated to represent the corresponding block. Using the nuclear norm of each local block as an element and keeping their relative position, we can form a new matrix B, which to some extent reflects the global structure of the original error image. As a result, 120 random matrices of dimensions 9 × 6 are acquired. The correlation map of elements in B is shown in Fig. 1(b). It is found from Fig. 1(b) that these blocks are also correlated. By the above analysis, there is no doubt that both local structure and structure among local blocks in an error image exist and the local structure factually plays a dominant role. Accordingly, how to merge these useful structure information into a unified framework to realize the performance promotion turns into a valuable and challenging issue.Download : Download high-res image (641KB)Download : Download full-size imageFig. 1. (a) The illustration for showing the local correlation of the pixel-errors, where the right image is the correlation map of pixels in the red box area of the left error image; (b) The illustration for showing correlation of blocks, where the right image is the correlation map of nuclear norm of blocks (i.e., pixels in matrix B) in left error image.
It is worth noting that some methods mentioned above, including LRR, RPCA and NMR, replace rank function with its tightest convex surrogate over the unit spectral norm ball, namely nuclear norm, to facilitate the design of algorithm. They suffer from the high computational cost to compute the singular value decomposition (SVD) in each iteration, especially for the large-scale matrix. Therefore, reducing the time complexity in SVD is extremely helpful for speeding up the algorithm. Toward this end, Lu et al. [15] presented a fast SVD method for multilevel block Hankel matrices. They used Lanczos process to reduce the MBH matrix into a bidiagonal or tridiagonal matrix and the SVD is implemented on the reduced matrix using the twisted factorization method. Majumdar et al. [16] decreased the complexity by computing a Cholesky decomposition instead of SVD. Cai et al. [17] computed the singular value thresholding (SVT) for a given matrix without SVD, which is carried out by two steps, namely, the polar decomposition step and the projection step done by Newton's method. Oh et al. [18] proposed a fast approximate SVT method by exploiting the property of iterative NNM procedures, which avoids the direct computation of SVD. The interested reader is referred to [19], [20], [21] for more techniques about the approximated SVD. These ways to some extend decrease the computation complexity in the traditional nuclear norm minimization. Nonetheless, they either focus on the specific cases, or are only the approximation of SVD, which may be far away from the essential attributes of nuclear norm characterizing the structure, leading to an impractical result.
On the basis of the above analysis, this paper will establish a nesting structure for a matrix and use it to induce a nesting-structured nuclear norm minimization model. This not only takes local and global structures of a matrix variate into joint consideration, but also affords the lower time complexity than traditional nuclear norm minimization. Specifically speaking, we partition an original matrix A1 into several blocks. Differing from the structured sparsity inducing norm [22], [23], [24], [25], which stretches each block into a vector and uses L2 or L∞ norm to constrain it, here we keep the original matrix form of each block and use nuclear norm to directly characterize it. Due to the structure attribute of nuclear norm, this can exploit effectively the structure of each block which can be viewed as the local structure of the matrix A1. As the experiment in Fig. 1(b), we can obtain an external matrix A2, each element of which coincides with the nuclear norm of the corresponding block in A1. Such a process is repeated continually. We eventually obtain a matrix Ak which only includes an element. Obviously, each matrix Al, where l=1, …, k, possesses the smaller size than the original matrix Al. Then, the nuclear norm is acted on each external matrix Al to further exploit the structure of the original matrix A1, which produces the nesting-structured nuclear norm. Using nuclear norm to characterize these external matrices factually captures the global structure of the original matrix variate since it incorporates all local structures into modeling. We establish a unified framework for nesting-structured nuclear norm minimization by developing an improved Sub-gradient method to solve the proposed model. This adds a popular accelerated scheme and a forcing descent technique into the traditional sub-gradient to stabilize and speed up the iterative process. Unlike the traditional nuclear norm minimization models, which compute the SVD on the original matrix, the proposed approach only needs to implement SVD on some generated small-scale matrices. Under a matrix variate distribution, the statistical meaning of the proposed framework is provided by seeking its maximum a posteriori probability estimation solution. Additionally, the essence of the proposed distribution for characterizing some structural variates is also revealed. The proposed method is applied on matrix regression and completion, respectively. A series of experiments on face recognition and large-scale matrix completion show the advantages of our method over some existing methods.
NotationsThroughout this paper, the bold capital and bold lowercase symbols are used to represent matrices and vectors, respectively. tr(X) denotes trace of a matrix X and exp ( · ) represents the exponential function. If a square matrix X is positive semi-definite, we denote it by X ≥ 0. ‖X‖1 denotes L1-norm of the matrix X. ‖X‖F denotes Frobenius norm of the matrix X, which is equal to the L2-norm of Vec(X) (i.e., ‖Vec(X)‖2), where Vec(⋅) is an operator converting a matrix into a vector. ∂h(B)∂xi denotes the partial derivative of matrix function h(B) associated with xi.
The remainder of this paper is organized as follows: in the next section we first give the definition of nesting-structured nuclear norm and present a nesting-structured nuclear norm minimization framework, which is solved by an improved sub-gradient method. Then, the statistical meaning and rationality of the proposed method are investigated in Section 3. The proposed framework is applied to matrix regression and matrix completion in Section 4. The convergence and complexity analysis for the proposed algorithm are presented in Section 5, and some experimental results are reported in Section 6. Section 7 contains the conclusions.
