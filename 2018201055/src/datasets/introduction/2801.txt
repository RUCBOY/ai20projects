Virtual reality is growing to be an important platform for various types of scenarios (e.g., games[1], training[2] and education[3]), where target selection is a common and basic interaction task. For example, a game player often picks up game props, a mechanic might need several tools (e.g., a hammer). Currently, target selection tasks require users to switch to the menu interface and select the target out of a list of candidates. This method requires a series of manipulations including invoking the menu, choosing the target category, scanning the items in the menus until the user pinpoint the desired one, and finally getting back to the ongoing task. This interaction process could be time-consuming and distracting, especially when users are new to the interface or when the target is buried deeply in a hierarchical menu[4].
Compared to selecting targets through menus, gesture-based interaction can be a potential solution to simplify the searching process. Gesture-based approach has the advantage of enabling eyes-free and direct input[5] and does not require an additional interface. Eyes-free input refers to the interaction that does not require the participation of users’ visual focus on the input devices, e.g., typing on the keyboards to input words while looking at the screen. In this paper, we explore to design intuitive hand gestures to retrieve virtual objects, head-based gestures to trigger control commands, and using eyes-free pointing gestures to acquire targets in the interaction space around the body. A key to gesture-based target selection is designing the mapping between the gestures and the targets, which greatly influence the learnability of the gestures. An ideal mapping should satisfy several criteria: it should be easy to discover and memorize6, 7, be easy to perform, be consistent with the acquired experience of users[8], gains high consensus across users[9], and be associated with high productivity and low error. To meet these criteria, we study how users build connections between gestures and targets, which we refer to as users’ mental model. Based on the results, we leverage users’ acquired interaction experience and sense of proprioception in the gesture-target mapping design.
Related researches on gesture mapping design applied two major approaches. One way is to specially design the appearance of the target to suggest the assigned gesture. The gestures were mapped to the shapes, colors, motions of the targets, or directly overlaid onto the targets8, 10, 11, 12, 13. In these cases, the gestures were cued by the appearance of the targets, and thus are easier to discover and remember[13]. However, using these techniques, users need to observe the targets to find the gestures, which introduces a high visual load. The second but more widely used solution is the user-defined approach, which was first introduced by Wobbrock et al.[9] to design gestures for interaction on an interactive surface. This approach portrays the effect of a command (e.g., to delete an item), and then asks a group of users to design their own gestures to issue this command. The gesture with the highest consensus will be assigned to the command. In this way, the elicited command-gesture mappings reflect daily behaviors and experience of users, which results in a more contextual connection between gestures and commands[9]. The approach has been successfully applied to many areas14, 15, 16, 17. In this paper, we follow the approach of user-defined gesture to elicit gesture-target mappings that users feel most intuitive.
In addition to intuitive mappings, the recognition of the input gestures is another challenge to gesture-based target selection approaches. In the process of performing gestures, user controls her body parts to mimic the gesture in her mind. However, limited by the motor control accuracy, the performed gesture is usually not exactly the same as the desired one. Meanwhile, the system senses the performed gesture through several information channels (e.g., camera and initial sensors). The sensing devices also introduce noise and errors due to limited sensing accuracy. What we need to achieve is to detect and recognize the user’s intended gesture in spite of these noises and errors, and accurately return the target that the user intends to select. We refer this part to be the understanding of user’s behavior model. To achieve this goal, we sample target positions in the interaction space around users, collect selecting data and regress the position offset to help predict the desired target of the user.
In the process of performing gestures, user’s control accuracy relies on a number of factors, and two important factors among them are spatial memory and proprioception[18]. Spatial memory is the part of memory that is responsible for recording information about different locations and the spatial relations between objects[19]. It can help users efficiently retrieve positions of targets[20] in acquisition tasks. Previous work studied the ability and effectiveness of users to build the spatial memory, both in 2D[21] and 3D[20] spaces. In addition, proprioceptive feedback is important for human’s movement control[18]. Proprioception is the sense of position and orientation of one’s body parts with respect to each other[22]. With the help of proprioception, users could perform eyes-free acquisitions of the targets on various platforms. For example, Face-Touch[23] visualizes targets in the virtual world via a VR headset, and enables users to select them by tapping onto the back of the headset at the according position to the target in their view. Although users cannot see their hands or the target location on the headset in the real world, they can still estimate the target location and reach to the vicinity of it without aiming. Similarly, with the help of proprioception, users can perform target selection on a remote screen (Air Pointing[18]), select different directions by orienting a mobile device (VirtualShelves22, 24), or control the body posture as an input modality (Pose-IO[25], FootGesture[26]).
The process of gesture-based target acquisition is as followed: User has a desired target as the intention while she may have other ongoing tasks. Considering the intention, tasks and the current environment as the context, user will decide a gesture to acquire the target. By controlling her hand, head or other body parts, the user performs the gesture. The computer senses the gesture in different ways, including using visual-based, inertial sensor-based sensing techniques. Limited by the accuracy of user’s movement control and the sensing accuracy, the sensed gesture data is not exactly the intended gesture in user’s mind. In most cases, it can be represented as a set of gesture candidates with different estimated possibility to match the intention. Within this set, computer will extract temporal, spatial and frequency features and run algorithm to recognize the original performed gesture and user’s intention. As with many intelligent input algorithms, the input prediction model that estimates the possibility of different candidate gestures should be trained based on user data. As we will show in this paper, the mental model and behavior model of different users share some similar characteristics, but are distinct from each other in other aspects. Therefore, to achieve reliable performance in real use, the algorithm should not only consider patterns emerged from the data of a number of different users, but also be able to continously adapt to each individual user during usage. In practice, the training process demands a varying size of training data, depending on the specified error tolerance, the signal-noise ratio and the tasks itself. For example, as we will show in Section 2.1, using data from 12 participants and a list of top-5 candidates, users could reach an accuracy over 94% when performing object retrieval by grasping gesture. Through this process, the two models we described above play very important role. Mental model describes how users choose a gesture given intention, task and context. Behavior model reveals the connection between the performed gesture and the sensed gesture. Leveraging these models, we can improve the understanding of the user’s intention and then provide the intended target more efficiently and more accurately.
Based on the two models, we developed three novel target selection approaches in VR and AR. First, we designed intuitive grasping gestures to retrieve virtual objects in VR through a gesture elicitation experiment[27]. Evaluation results showed that novice user successfully retrieve targets with accuracy of 75.51% without any training. Second, we designed head movement based gesture to trigger command on AR devices. Through participatory design process, we generated nine head gestures and assigned them to trigger basic commands (e.g., select, drag and drop). This approach supported controlling the device in a hands-free way. Third, we studied how user acquire targets in the interaction space around the body without turning head to look at them[28]. By analyzing the distribution of acquisition points, we generated the connection between the acquisition points and the desired target positions and then improved the acquisition accuracy.
