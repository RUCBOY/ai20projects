Recently, many successful applications such as business intelligence (BI) and customer data-driven marketing (CDM) have emerged with the age of big data. In these kinds of applications, the most important information is about relationships among people, or customers, and their behaviors. With this information, we can achieve successful results in a wide variety of fields. For instance, we can promote a very efficient and effective viral marketing for business, figure out the propagation of epidemic diseases for social security, and design a robot that avoids interrupting conversations between customers to provide satisfactory service. However, such information is difficult to infer from raw data.
For decades, computer vision has been successfully applied to intelligent surveillance, to robots, and to many other fields to help us understand situations. With the explosion in the number of cameras due to the ubiquity of mobile phone cameras, and thanks to social network services, plentiful images of people’s social activities are created every day. These images contain important information about interactions or social relations between individuals. In order to obtain the aforementioned highly desirable information, a computer vision technique detecting groups of interacting people in images of everyday social activities is essential.
Given these technical demands, few studies focused on determining interacting groups in an image [1], [2], [3], but they mainly focus on reasoning based on scenes by obtained surveillance cameras. However, CCTV footages involve different features from the images that are easily acquired in everyday life or from a social network service (SNS) including view angles, a distance from subjects and scene environments. Therefore, it is not trivial, and is difficult to directly apply the existing methods directly to the images of everyday social activities.
Existing methods that detect interacting groups are categorized into two groups, namely human behavior-based methods and positional structure-based methods. The behavior-based methods mainly focus on detecting pre-defined behaviors in video sequences. Subsequently, the interacting group is detected by determining individuals who interact with each other. Kong et al. [4] proposed a method to detect the following six behaviors in the UT data set: handshake, hug, kick, pointing, punching, and pushing. In [4], the presence of a specific behavior was detected as created by a support vector machine (SVM) [5] with handcrafted features that describe whether individuals extended their hands or feet. Zhou et al. [6] and Lathuilière et al. [7] detected five types of individual behaviors appearing in a video including crossing, waiting, forming a queue, walking, and talking. In [6], [7], a generation model and energy function minimization were used to detect the appearance of each behavior. Sun et al. [8] proposed a method to detect six different types of behaviors including cross-tracking, queuing, talking, dancing, and jogging by multi-target tracking based on the tracklet association. Extant studies based on the behavior analysis mainly use the sequential information from videos that are not obtained from a single image. In addition, they do not detect interactions between people who do not perform any recognizable actions. Therefore, these methods exhibit limitations with respect to their application to detect interacting groups in everyday social activity images from SNS [4], [6], [8], [9], [10].
The methods based on the positional structure of individuals analyze additional types of interactions including static situations when compared to behavior-based methods. The methods in this category initially determine the planar position of each individual in an image on a top-view and also estimate the head orientation of each individual. Subsequently, they detect interacting groups by detecting the appearance of pre-defined formations among individuals on the top-view. The most widely studied formation is the F-formation that corresponds to a sociological theory [1], [2], [3], [11], [12], [13], [14], [15]. In the F-formation, a space that is formed between individuals with a mutual relationship is defined as three types of spaces, namely O, R, and P. An initial study based on this theory was introduced in [13] to detect the social interaction among people in an image, and this was followed by various approaches that used F-formations for grouping individuals [1], [2], [3], [15]. Hough Voting for F-formation (HVFF) has been adopted in a set of methods [1], [13], [14] to identify F-formations by finding the maxima of accumulated voting spaces. There are three types of accumulations in this set of methods: linear combination (HVFF-lin)[13], using the weighted Boltzmann entropy function (HVFF-ent)[14], and applying multi-scale approach (HVFF-ms)[1]. Graph-Cuts for F-formation (GCFF) [2] iteratively finds F-formations with jointly optimize a graph-cut for grouping people an image, and the center location of each group. The convergence of this joint optimization is guaranteed. Game-Theory for Conversational Groups (GTCG) [3], [16] detects groups with a game-theoretic framework that models the uncertainty associated with people’s positions and orientations. In  [3], [16] the affinity between pairs of people is calculated with their distributions over the most probable area of the center of people’s attention. However, these methods are based on the assumption that the top-view locations of individuals in an image are easily obtained. This assumption is satisfied with surveillance camera footages albeit not with snapshots such as social activity images from SNS.
In order to resolve this limitation, Choi et al. [17] proposed a method that automatically extracted the top-view locations of each individual in snapshots. The method then detects seven types of pre-defined formations of individuals: standing in line, standing facing one another, standing side by side, sitting on chairs side by side, sitting on the chairs facing one another, sitting on the floor facing one another, and sitting on the floor side by side. In [17], the energy function was defined to determine which group of individuals belongs to which of the seven types of groups. Although this method is advantageous as it determines a group even in a variety of situations including static situations, it exhibits limitations wherein it is necessary to define the type of the group to be searched in advance. The orientation of faces plays an important role in determining the formation that is presented in the image, and the final result is very sensitive to the error of the head orientation estimation. Therefore, there are limitations to applying it to a large number of individuals appearing in everyday images.
One interesting alternative is based on texture classification [18]. In [18], texture information is used as a feature to capture the crowded area of an input image, and motion and spatial information are used to verify the initial detection of groups. This method is very efficient for finding isolated groups, however, it is impossible to separate a group into subgroups according to interactions.
In this paper, we propose a novel method to extract and infer various information necessary to determine an interacting group in images of everyday social activities and to then determine actual interacting groups based on this information. We first use the geometric relationships between the heads of individuals in an image to create the candidates of actual interacting groups. Subsequently, we quantitatively calculate the extent to which the individuals interact with each other in each of the generated groups. For this purpose, we designed a novel measure to consider physical information, such as location and gaze of each individual, as well as emotional relevance that is deduced through facial expression, sex, and age. In the analysis of the geometric relation, we calculated the distance between individuals by considering both the straight line distance between the points where individuals are located and the relative face sizes of individuals. In order to analyze the social relations, the image is classified into eight types based on the interpersonal circle theory of sociology [19]. We used the social relation network (SRN) [20] and applied the result to the social relation measure.
In order to evaluate the proposed method, we constructed a new dataset that consists of 363 images of everyday social activities. This is because, to the best of the author’s knowledge, a proper dataset has never been released for the study of interacting group detection. In the experiments, we evaluated the grouping performance of the proposed method and baseline method based on the existing grouping methods according to the performance measures proposed in [14]. In contrast to the baseline methods that require the application of a pre-processing step to everyday social activity images, the proposed method outperformed the baseline methods without requiring any pre-processing and exhibited an acceptable performance on the constructed dataset.
The contributions of this paper are as follows: (1) We first propose a method to detect interacting groups in images of everyday social activities by designing new interaction measures for it; (2) We construct a new dataset to evaluate the performance of interacting group detection in everyday social activity images.
This paper is organized as follows. Section 2 describes the problem formulation. Section 3 presents objective functions for designing the geometric relation and social relation measure, and the optimization method to determine optimal interacting groups. Section 4 presents the experimental results, and this is followed by conclusion in Section 5.
