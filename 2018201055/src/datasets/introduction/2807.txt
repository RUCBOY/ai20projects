Safety inspection aims for proactive management of hazardous conditions. The goal is to achieve “zero accidents” [1]. However, the proportion of hazards identified by human inspectors has reached a plateau [2]. Automated workplace safety monitoring and diagnosis based on computer vision and deep learning technologies could achieve reliable and real-time detection of construction workplace hazards [3], [4], [5], [6]. A step toward automated workplace safety monitoring is to enable computer algorithms to learn the logic behind numerous safety rules through the use of example workplace images labeled by humans as containing safety-rule violation scenarios or not. A dataset consisting of images labeled as violating or following certain safety rules is essential to serve as the input of machine learning algorithms for training automated computer-vision-based workplace safety monitoring systems. However, manually labeling numerous workplace images by safety engineers was tedious and costly due to the limited availability of experienced safety engineers and huge manual image labeling workloads [4]. Even experienced engineers could only achieve 70% accuracy in labeling safety rule violations of complex scenes due to (1) distractions caused by a large number of objects in images, (2) subtle differences between safety rule definitions, and (3) misperceptions about objects and relationships in scenes [7].
Crowdsourcing is a promising alternative for labeling large amounts of images that contain scenes that comply with or violate certain safety rules [8], [9]. A crowdsourcing approach invites randomly recruited online annotators to collect safety-rule violation responses of images and then aggregates their contributions into safety-rule-violation labels. Such aggregation integrates the recognition power of people without professional safety knowledge into formal safety-related reasoning capacity. Unpredictable human behavior and dynamic workspaces pose challenges of hazard detection in complex contexts, while crowdsourcing approaches are promising for addressing these challenges [8], [10], [11], [12].
Image labels crowdsourced from non-professional annotators need proper “annotation aggregation” –a process of resolving conflicting information from multiple labels of an image and obtaining the final label of that image as certain violations. In other words, the aggregation model sorts out the correct labels despite various errors in the crowdsourced annotations. However, existing methods for such aggregation become unreliable when a significant portion of labels are wrong. One commonly used method for aggregating the answers of crowdsourced participants is “majority vote”; this algorithm identifies the correct answer as the answer agreed on by the most participants [13], [14], [15], [16]. While majority votes yield accurate results in many cases, such methods often fail when a significant portion of participants are wrong [15], [17], [18], i.e., the majority vote mistake.
Another category of annotation aggregation methods is Expectation Maximization-Based (EM-based). EM-based methods have the similar difficulties encountered by the majority-vote methods while handling cases where significant portions of answers are wrong [19]. When over half of the workers choose the wrong answer while dealing with complex scenes, EM-based methods will likely converge to that incorrect answer. For example, Demartini et al. found that ZenCrowd, an elaborate EM-based method, cannot achieve precision higher than 85% no matter how large the number of crowd workers is [19], [20]. In many cases, unreliable annotations produced by people who lack professional inspection training can cause biases in the aggregated results: many images with no safety-rule violation would be labeled as having violations, or “false alarms”.
High rates of false alarms in safety-violation labeling of photos, called “high false-positive rates,” can seriously mislead the training process of deep-learning-based computer vision algorithms. In fact, without additional field information depicting operational contexts, the use of training images with many false positives to train deep learning networks for automatic labeling of photos with safety violations will result in algorithms that have a large number of mislabels [21]. Computer vision researchers have been tackling noisy labels of training example images for a decade. Many researchers developed methods for tolerating false-negatives in the training data [22], [23], [24], [25]. Unfortunately, these studies found that few machine learning methods have yet to overcome high false-positive rates in the training data set [26]. Therefore, reducing false positive rates of crowdsourced safety-rule violation labels, along with minimizing the increase of the false negative rates, is critical.
This paper examines a new low-cost crowdsourcing approach that can achieve the lowest possible false alarm rates with acceptable false negative rates in labeling workplace images as containing certain safety-rule violation scenarios. The new approach aggregates the annotations contributed by ordinary people into a labeled workspace image dataset with the minimum false alarms and acceptable false negatives in the image labels of safety-rule violations. The authors expect that this workspace image dataset with crowdsourced labels can serve as a reliable training data set for deep-learning-based computer vision algorithms that can automate safety rule compliance checking based on field videos. The presented study investigates the fundamental crowdsourcing methodology in the domain of vision-based safety surveillance through the following:
1)Characterizing the impacts of six-rule and ten-minute long training processes on the labeling accuracy of the crowdsourced annotations, compared to published works on crowdsourced annotations without any training processes;2)Examing a new crowd consensus model that automatically aggregates the violation annotation results from briefly trained annotators into image labels. The aggregated image labels have a low false positive rate and an acceptable false negative rate for training vision-based safety surveillance algorithms.
Results from an online image-based safety-violation labeling experiment showed that the proposed innovations result in a new crowdsourcing approach, one that achieved reliable safety-violation labeling results in twenty target photos when compared with the results generated by the majority votes. The Background Research section below introduces previous studies related to construction safety-rule violation and crowdsourcing techniques while highlighting their contributions to this paper. The Methodology section describes the details of the new crowdsource approach that integrates the short training process and the new crowd consensus model. The Validation section compares the performance of the proposed new crowdsourcing approach against the conventional majority vote approach for annotation aggregation. The performance metrics used in this comparison include true positive, true negative, false positive, false negative, and correct rate. The Discussion and Conclusion sections present the results, conclusion, and future research plans.
