With the development of artificial intelligence, Knowledge Graphs (KGs), such as NELL [4], Freebase [1] and WordNet [23] play an increasingly critical role in many downstream applications, e.g., question answering [13], [2], [11], information retrieval [22], personalized recommendations [34], machine reading [38], etc. Knowledge graphs are graph-structured facts, which are usually in the form of a triple (head entity, relation, tail entity) denoted as (h,r,t), e.g., (begin, Antonym, end).
The original entities and relations in a new knowledge graph are always obtained from the Internet or gathered manually, thus, knowledge graphs tend to suffer from incompleteness [27], which would affect the performances of the downstream tasks. For example, 75% of the persons in Freebase are missing their nationality [12]. Moreover, many links could also be lost among the entities in a KG. Predicting such missing links is referred to as knowledge graph completion. The lost links among the entities carry lots of logical reasoning information, which is critical in many tasks based on reasoning. Many machine learning systems must be able to reason with the existing entities and links, and learn to infer an unknown relation or link. The task is named as knowledge graph reasoning. The reasoning on a KG is an effective and common method to infer missing links or detect false links based on observed connectivity patterns. For instance, if athletePlaysForTeam(X,Y) and teamPlaysInLeague(Y,Z) both exist in the KG, then we can infer that athletePlaysInLeague(X,Z), i.e, filling the missing edge athletePlaysInLeague between X and Z. After the reasoning process, the system should be able to automatically complete the missing links between the pair of entities. This kind of reasoning method on a KG can be treated as an essential component for the various downstream tasks, such as QA systems and information retrieval systems. Thus, it becomes an important and challenge task on how to complete the KG by predicting the missing links between the entities through the method based on reasoning.
The mainly approaches to accomplish this task are from three different frameworks, such as Rule-Based [33], [39], Embedding-Based [3], [21] and Path-Based [19]. For example, the embedding-based methods for reasoning on KGs embed both entities and relations into low dimensional vector spaces, where a score for the plausibility of a triple can then be computed based on these embeddings. The path-based reasoning methods follow a different philosophy, where the paths between entity pairs are inferred to explore the relations of the entities for missing links, such as path ranking algorithm [19]. Recently, reinforcement learning is also brought into the task of predicting the missing links by combining with the path-based reasoning, such as DeepPath [37] and MINERVA [9]. The DeepPath is the first work to incorporate deep reinforcement learning into KG reasoning, where an agent picks relational paths between entity pairs. The reinforcement learning framework provides a new perspective on the KG reasoning task.
However, the main drawback of the above reinforcement learning-based methods is that the memory component is ignored when the pre-training is resorted to in the training process. The pre-training takes advantage of many given paths (head and target entities) to the model training. Before the model training, people need to prepare the triples with the head entity and target entity in a path to generate the state of the agent, which means that the target entity needs to be given in a reasoning task [8]. While, the brute-force operation may make the model prone to overfit on the given paths from pre-training, where there is no memory component to keep the training records. Even the MINERVA improves the problem by querying the target entities in the KG, it assumes that there must be a path from the head entity to the target entity, which limits the performance when the reasoning task is complex, especially when the candidate nodes in a path can not lead to the right tail entity. Thus, it is worthy to find out a way to remove the dependency of the pre-training in the KG reasoning task.
To tackle this issue, we propose a novel deep reinforcement learning framework for KG reasoning named MemoryPath, where a memory component is considered by incorporating Long Short Term Memory (LSTM) and graph attention mechanism. The memory component is used to get rid of the pre-training so that the model doesn’t depend on the given target entity for path reasoning. The framework is based on reinforcement learning, and an agent is built to cooperate with the memory component for finding the most promising path in a reasoning task. To better guide the RL agent for learning relational paths, we also present two different metrics, Mean Selection Rate (MSR) and Mean Alternative Rate (MAR), to measure the complexities of the replaceable paths of a relation, which provides an effective way for model fitting. Meanwhile, three different training mechanisms, Action Dropout, Reward Shaping and Force Forward, are proposed to optimize the training process of the proposed MemoryPath, by which the agent in the reinforcement learning framework can achieve state-of-the-art performances on the reasoning tasks, such as fact prediction, link prediction and finding the best reasoning paths. Specially, the contributions can be summarized as follows:
•A novel deep reinforcement learning framework for KG reasoning is proposed, where the reasoning task is driven by an agent with continuous states based on the knowledge graph embeddings and well-designed reward function. The experimental results demonstrate that the proposed framework for KG reasoning can effectively improve the performances of the downstream tasks such as fact prediction and link prediction.•To address the issue of dependency of model pre-training, a novel memory component based on Long Short Term Memory(LSTM) and graph attention mechanism is presented to get rid of the dependency of the given triples. The memory component formed by LSTM and graph attention mechanism can store the learning process and automatically find the promising paths for a reasoning task during the training.•In contrast to prior works, two metrics are defined (MSR and MAR), to quantitatively measure the complexities of the relations when the model learns the alternative paths in the proposed framework. Meanwhile, three novel mechanisms of reinforcement learning are proposed to optimize the training process of the MemoryPath, action dropout, reward shaping and force forward.•The proposed MemoryPath is evaluated on two famous datasets, FB15K-237 and NELL-995, with the downstream tasks. Experiments on fact prediction and link prediction demonstrate that the proposed MemoryPath outperforms state-of-the-art methods based on path reasoning. Also, the experiments on qualitative analysis also show the effectiveness of the memory component and the graph attention mechanism in finding reasonable paths.
The remainder of the paper is organized as follows. Section 2 surveys the related research on knowledge graph reasoning. Section 3 proposes the MemoryPath, and gives the deep reinforcement learning framework on KG reasoning. Section 4 elaborates on the optimization of model training by presenting the proposed training mechanisms. Section 5 gives the discussion of the proposed model, comparing with other related works. Section 6 reports the experiments on fact prediction, link prediction, and finding the paths, together with the qualitative analysis. Section 7 concludes the whole paper and shows future work.
