Visual tracking is one of the important issues in computer vision with numerous applications such as video surveillance, medical imaging, auto-control systems and humanâ€“computer interaction. Given the initialized bounding box of a target object in a frame of a video, the aim of visual tracking is to estimate the states of the target in the subsequent frames [1,2]. Although it has been studied for years and has made considerable progress, the performance of the existing visual trackers is still not satisfactory in practical applications due to significant appearance variations [[3], [4], [5]] such as occlusion, deformation, motion blur, illumination variation and background clutter.
Over the past years, a series of visual trackers have been proposed. The proposed trackers can be generally divided into two categories: tracking by matching [4,5] and tracking by detection [[6], [7], [8], [9], [10], [11], [12], [13], [14]]. The former tracks the target by matching the interested pixels or points of the tracked target across consecutive frames. The latter treats visual tracking as a classification problem, and trains a discriminative classifier with a series of positive and negative samples to separate the target candidate from the background in each frame of a video.
In tracking by detection, correlation filter (CF) based visual trackers [[8], [9], [10]] have recently attracted much attention and research, which can deal with various sorts of appearance variation challenges. An influential work by Henriques et al. [10] developed a novel kernelized CF-based tracker, which abstracts visual tracking as a linear regression model, trains the classifier with vertical and horizontal cyclic shifts of the base sample, and diagonalizes the resulting data circulant matrix with the discrete Fourier transform into the frequency domain to improve the tracking efficiency. Experimental results show that the tracking performance and efficiency was improved significantly. Not surprisingly, many kernelized CF-based visual trackers [[11], [12], [13], [14]] are developed in recent years. For example, Chen et al. [12] handle partial occlusion by modeling the object appearance based on multiple local parts and features.
Although the performance (e.g. accuracy, robustness) of existing CF-based trackers has improved significantly compared with other trackers, there are still some limitations. Firstly, existing CF-based trackers are vulnerable to the influence of surrounding background [13]. This is because the positive samples used to update the classifier normally contain unnecessary background context information, especially in the case of occlusion or background clutter. Secondly, existing CF-based trackers with fixed scale template can easily lead to tracking failure when the object scale changes frequently [14].
To overcome the above issues, we propose a CFs-based visual tracker, CFs with Gabor energy filter, variable-scale template and features fusion (CFGVF). The main features of the proposed CFGVF tracker are as follows. Firstly, we propose to use the Gabor energy filter to preprocess every frame of a video, which largely eliminate the influence of frequently illumination variation. Secondly, a multi-feature fusion schema combining features including Gabor energy (GE) feature, Histogram of Oriented Gradient (HOG) and color naming (CN) is proposed to enhance the ability of the CFGVF tracker in dealing with significant appearance variations of the target object. Thirdly, in order to overcome the limitation of existing CF-based trackers with fixed scale template, a variable-scale template method is proposed to estimate the scale of the target object. The proposed scale estimation approach is based on the observation that object scales of the adjacent two frames in a video change relatively small and smoothly except for the sudden and rapid movement or deformation of the target object [12]. Finally, an online updating schema based on Peak-to-Sidelobe Ratio (PSR) [15] is employed. Experiments are conducted to compare both the qualitative and quantitative performance of ten trackers on frequently used benchmark dataset OOTB [1], and the proposed CFGVF tracker outperforms other state-of-the-art visual trackers.
The rest of this paper is organized as follows. The most relevant work that motivated this paper is reviewed in Section 2. In Section 3, the proposed CFGVF tracker is presented in detail. In Section 4, experimental results are shown and the performance evaluation is conducted. Finally, we conclude the paper in Section 5.
