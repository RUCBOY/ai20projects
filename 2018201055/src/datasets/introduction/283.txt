Video super-resolution (VSR) aiming at recovering high-resolution (HR) frames from low-resolution (LR) frames attracts extensive attention in research and industrial communities. It can be used in many real-world applications, such as surveillance [8], medical image processing [9], recognition [10]. However, VSR is an inherent ill-posed problem since its one-to-many mapping nature. In another word, a variety of HR images can be mapped to the same LR image.
In the VSR task, the temporal motion information modeling is a key problem. Temporal motion information underlying consecutive frames can provide beneficial priors for reconstructing current frame and keeping visual consistency between super-resolved frames. As a typical graph matching problem [11], explicit motion estimation (e.g. optical flow) is commonly formulated in the state-of-the-art VSR methods. Kappeler et al. [1] adopted motion compensation with convolutional neural networks to build temporal relations of sequential frames. Caballero et al. [6] collaborated motion compensation and super-resolution network in an end-to-end manner. However, the precise motion estimation is challenging and time-consuming. Many motion estimation algorithms rely on the brightness constancy assumption, but they may fail due to lightness/pose variation and the presence of motion blurs and occlusions. In another aspect, some efforts [3], [5], [12] utilize recurrent architectures to build temporal dependence and show excellent efficiency. In [7], a fast spatio-temporal residual network was proposed, and they adopted three-dimensional convolutions to simultaneously exploit spatio-temporal relations. These VSR methods do not specially consider frames frequency information, and noise disturbances are not technically eliminated.
In this paper, we propose a novel spatio-temporal matching network (STMN) for video super-resolution in wavelet domain. It mainly contains three components: a temporal fusion wavelet network (TFWN), a non-local matching network (NLMN), and a global wavelet domain residual connection (GWDRC). TFWN is designed to aggregate consecutive frames for implicitly modeling motion information, instead of explicitly formulating complicated and time-consuming motion information as in motion compensation methods [1], [6]. Then we transform the aggregated features into wavelet domain, which retains spatial information and meanwhile provides knowledge of different frequencies. In NLMN, we adopt a pyramid design to regularly augment feature channels along with network going deeper. Besides, a novel non-local matching residual block (NLMRB) is elaborately designed in our NLMN. Our NLMRB is highly motivated by the non-local matching strategy, and can produce a denosing weight matrix [13], so that it can well integrate super-resolution and denosing process. Last but not least, we design the GWDRC which reconstructs the super-resolved frames with global wavelet domain residual information. We show an example of VSR results of different methods in Fig. 1, and our method showcases very promising performance compared with others.Download : Download high-res image (1MB)Download : Download full-size imageFig. 1. Visual comparisons of different video super-resolution methods on the Calendar dataset with 4 ×  up-scale factors; our method can clearly recover the “words” and “lines”.
In summary, the main contributions are four-fold:
•To the best of our knowledge, this paper presents one of the very first attempts towards wavelet domain deep video super-resolution.•Proposing a temporal fusion wavelet network (TFWN) to adaptively produce temporal fusion wavelet maps so that avoids motion compensations.•Constructing a non-local matching network (NLMN) with an innovative non-local matching residual block (NLMRB), which specially targets at integrating super-resolution and denoising on wavelet domain.•Extensive experiments conducted on publicly available datasets demonstrate the state-of-the-art performance of our method.
The rest of our paper is organized as follows: In Section 2 we overview related work. Our method is illustrated in Section 3. In Section 4 the experimental results are shown and discussed. In Section 5, we conclude this paper.
