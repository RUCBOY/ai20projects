The perception of textures is an essential field of research that has a broad range of applications in numerous vision problems. While textures play a decisive role in the visual appearance of materials, they also serve as a cue in many understanding tasks such as judging material attributes (e.g., [1]), material recognition and classification (e.g., [2], [3], [4]) and object detection (e.g., [5]). The main challenge when dealing with textures is that their effect on human perception is very complex and highly subjective. In this paper, we focus on the problem of perceived texture similarity. Questions like “are textures A and B more similar than textures C and D”? arise in many applications, for instance when reasoning whether objects are made of the same or a similar material or when searching materials that are perceptually in-between certain other given samples. Reliable texture similarity metrics could greatly facilitate the development of efficient user interfaces for retailing web stores that are consistent with human perception, or could be used to guide design processes, find suitable replacements for unavailable materials, and so on.
In color science, perceptually uniform color spaces such as CIELAB and CIELUV have reached a mature state and enabled highly efficient digital solutions for applications like gamut mapping for display and print [6]. Textures, however, introduce additional spatial structure and possibly semantic context, which makes any measure of similarity much less well-posed and objective. Pioneering research in perceptual texture similarity has aimed at identifying a meaningful, low-dimensional and perceptually uniform space for textures [7], [8], where the axes represent meaningful quantities and distances between points in this space are proportional to variations in the human perception. These studies and many who have built upon them rely on free-grouping experiments to collect perceptual data. This input is later shaped into a similarity/distance matrix from which a perceptual texture space (PTS) is constructed by means of dimensionality reduction techniques. Although this approach has proven useful in the task of deriving different similarity metrics, the experimental procedure may become impractical for large datasets due to the substantial duration of the grouping process, which may lead to fatigue and boredom issues. In fact, arranging e.g., a large collection of printed images into clusters has been shown to be a time-consuming experiment [9], where only six out of thirty participants were able to complete the whole assignment. Moreover, this experimental design does not allow the addition of unseen data points to the resulting similarity matrix, whose sparsity increases along with the size of the dataset. Several valid alternatives have been proposed to collect similarity data while partially overcoming the previous issues e.g., progressive grouping, pairwise comparisons or forced-choice combinations. However, they depend on direct user queries for pairwise similarity between textures, a concept that is ill-defined and depends on the individual subjective experience, being therefore subject to bias.
In this paper, we introduce a novel strategy for the measurement of mutual perceptual similarity between textures from materials. Our proposal offers precise control of the data collection for specific pairs while mitigating some of the obstacles from the previous techniques. Specifically, we rely on a localization task inspired by the notion of just-noticeable differences (JND) and employed a well-established technique in computer graphics, patch-based texture synthesis, to produce a gradual transition, or gradient, between two textures A and B. An additional intermediate texture exemplar C, generated using the same method, is then given to the user, along with the task to locate it on the continuum from A to B. The working hypothesis for our metric is that the more similar the two textures at the end of the gradient are, the less certain the user will be about the placement of C. For multiple users tasked with the same question, this will result in a variance which we expect to correlate with the perceived similarity between textures. As the texture interpolation itself is not driven by perceptual metrics, the synthesized transition between the textures may not represent a smooth, even gradient between materials. We therefore propose a preliminary study that allows the generation of perceptually linear interpolations. Our approach addresses the medium and high end of the similarity scale described by Zujovic et al. [10], is easily parallelizable among multiple subjects, extensible to additional samples and produces approximately interval-scale measurements. Thanks to these characteristics, the methodology is suited for implementation on human intelligence platforms like Amazon Mechanical Turk and results in a space of textures that complements previous investigations.
Altogether, the main contributions of this paper are:
•The introduction of a novel formulation for the perceptual similarity between textures that exploits the noise inherent to a JND-like task. To generate the stimuli used for the experiments, we make use of state-of-the-art texture interpolation.•To derive a perceptually linear interpolation between textures that is so far not guaranteed by current techniques, we present a linearization method based on user judgments.•From our procedure, a meaningful perceptual space for material textures is computed. We determine the soundness of our approach by comparing it with the perceptual space resulting from a free-grouping experiment.
