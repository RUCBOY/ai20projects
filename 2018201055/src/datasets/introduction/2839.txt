Motion provides essential information for a wide variety of visual tasks, and directly affects the subsequent image processing. Consequently, the computation of motion information from image sequences is an important issue in computer vision and image processing. Optical flow, one of the most successful motion estimation methods, has been widely investigated. Since Horn and Schunck (HS) [1] as well as Lucas and Kanade (LK) [2] proposed the differential method to calculate optical flow in 1981, a great deal of extensions and modifications have been proposed. More recently, a new line of optical flow algorithms considers the use of Convolutional Neural Networks (CNNs). The two classes of algorithm have become the predominant ways to calculate optical flow [3], [4], [5]. In this survey, we review both and discuss challenges and opportunities.
1.1. Optical flowThe relative movement of a 3D scene observed with a camera leads to changes in the 2D projected images. The estimation of this motion comes down to the computation of a projection of the actual motion onto the image plane [6]. To this end, algorithms have relied on the analysis of brightness variations of pixels in pairs of subsequent images. The 2D displacement field that describes apparent motion of brightness patterns between two successive images is called the optical flow [1]. The optical flow field is often considered as the projected scene flow field, which is the true motion of the objects in the scene as viewed from the image plane [4]. The optical flow field is ideally a dense field of displacement vectors (see Fig. 1), which maps all points of the first image onto their corresponding locations in the second image.The concept of optical flow was proposed by Gibson [7]. Poggio and Reiehardt [8] presented an approach to compute the motion of each pixel in an image, which can be considered a rough flow method. A first practical optical flow model was established by the classical work of Horn and Schunck (HS) [1]. It is based on the assumption that the brightness of a pixel keeps constant during a short time interval, which is known as the brightness constancy assumption (BCA).Download : Download high-res image (411KB)Download : Download full-size imageFig. 1. Illustration of optical flow: (a), (d) frames 10 and 11 of the RubberWhale sequence on the Middlebury benchmark [5], respectively; (b) color-coded ground truth flow (black regions indicate unknown flow vectors); (c) vector plot ground truth flow. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)
1.2. Applications of optical flowAlthough the optical flow field is an approximate projection of the true motion of the scene, it provides valuable information about the spatial arrangement of the viewed objects and the change rate of the arrangement. We discuss several domains where optical flow is used.Visual Surveillance. Visual surveillance systems are often designed as modular systems of functional modules such as motion detection, depth estimation, object tracking, and object behavioral analysis [9]. Optical flow is effective in separating the foreground and background, and to identify moving objects [10]. This allows for the detection and tracking of objects through the scene. Optical flow is extensively used for visual surveillance tasks including tracking [11], [12], segmentation [13], [14], action recognition [15], [16] and anomaly detection [17], [18].Robot Navigation. Navigation can be roughly described as the process of determining a suitable path between a robot’s start and goal location [19]. As optical flow is the apparent motion of the brightness patterns of image sequences, it not only provides important information about the unknown environment, but it is also helpful to determine the direction and the speed of the robot. Due to this characteristic, optical flow is one of the two primary techniques for mapless robot navigation [20]. Using optical flow for robot navigation is inspired by emulating the flying behavior of bees [21]. Optical flow is widely used for obstacle detection [22] and collision avoidance [23].Image Interpolation and Super-Resolution. High definition TVs (HDTVs) have significantly improved our visual experience [24]. Image sequence interpolation [25], [26] is the process of creating intermediate images between two given consecutive images, to increase the frame rate. It has broad applications in the fields of video post-processing and restoration. Image sequence super-resolution [27], [28] aims at combining the available information from a sequence of low-resolution (LR) images to produce a high-resolution (HR) image with more details. Both image interpolation and super-resolution require accurate and detailed alignment of pixels in which the dense optical flow field plays a crucial role.Physical and Metrological Applications. Optical flow can be applied as a measurement in modeling and simulation, including weather forecasting [29] and fluid dynamics [30], [31].
1.3. Optical flow techniquesIn this section, we introduce the primary optical flow techniques and give a detailed analysis of their characteristics.1.3.1. Differential techniqueThe differential technique also refers to the gradient-based approach. It computes the velocity from spatial and temporal derivatives of the image brightness [4]. The constraint relies purely on the BCA. This leads to an aperture problem: one equation cannot uniquely determine two unknown components (the horizontal and vertical pixel displacement) of the flow field. An additional constraint encoding a priori information on the flow field needs to be incorporated with the BCA to make the problem well-posed [1], [2]. Normally, the prior takes the form of spatial coherence imposed by either local or global constraints. Generally, we identify two approaches [32], [33]:•Global Differential Method introduces a global smoothness constraint (also called regularization term) to solve the aperture problem. The assumption is that neighboring pixels come from the same object and consequently undergo a similar motion. As a result, the displacement field of the brightness patterns varies smoothly. The variational method proposed by Horn and Schunck [1] is generally considered as the typical global method. It is one of the most successful techniques to compute optical flow [6]. Solving the optical flow estimation requires the minimization of the associated Euler–Lagrange equations. These form a system of partial differential equations that can be derived analytically using the calculus of variations [34]. In practice, the system can be solved with numerical methods such as Successive Over-Relaxation (SOR) [35], or efficient multi-grid methods [36]. In general, there are two main ways to minimize the global energy function. One is a continuous optimization [6], [37] such as gradient descent [38], [39] and the variational method [1], [35]. The second approach is a discrete optimization [40], for example using graph cuts algorithms [41], [42] or message-passing algorithms [43], [44]. The discrete optimization method approximates the continuous space of solutions and enables a more thorough and complete search of the state space. Since discrete methods do not require differentiation of the energy, they can handle a wider variety of data and regularization terms than continuous methods [3], [45]. On the other hand, discrete optimization methods are generally limited in terms of accuracy and efficiency by the number of labels and the size of the label space [46], [47]. For optical flow, sub-pixel accuracy is often required [40], and real-time computation is necessary for many applications [48], [49].•Local Differential Method assumes that the motion within a local neighborhood can be described by a parametric model [2], [50]. For each pixel in a local neighborhood, an equation relating brightness and flow can be derived. The set of equations for all the pixels within the local neighborhood is then solved to estimate the optical flow by performing least-squares minimization. Setting the proper size of the neighborhood is a challenge. A small-sized neighborhood might not contain sufficient information to handle the aperture problem. In contrast, a large-sized neighborhood allows the integration of information over many pixels but may include pixels from other motion surfaces. Moreover, the local method experiences difficulties in homogeneous regions and regions is motion discontinuities [51].The global and local methods differ in the interaction of neighboring pixels due to the constraints. Global methods constrain a pixel’s flow by its neighboring pixels’ flow vectors, whereas local methods constrain a pixel’s flow by its neighboring pixels’ intensity values. Also, the two methods are computated differently. The global method aggregates the constraint residual over the whole field. Therefore, the flow recovery at one pixel relies on the flow recovery at other pixels. In contrast, the local method integrates the flow constraint of one pixel into a linear system, and solves each pixel’s flow independently.1.3.2. Region-based techniqueThe region-based technique relies on searching matching patches between two consecutive images. When two corresponding patches have the largest correlation, the optical flow is defined as the shift of the patches [4], [52]. Popular measures include the sum of absolute differences (SAD), the sum of squared differences (SSD) [1], and cross-correlation metrics [53]. The region-based technique is more robust to noise than differential techniques. In addition, the region-based technique works well even when images are interlaced or decimated.1.3.3. Feature-based techniqueFeature-based techniques attempt to link sparse but discriminative image features in successive images over time [6], [54]. This technique ignores ambiguous areas and, as a result, the calculated flow field is sparse but robust. Discriminative features, in particular corners and edges, as well as low-contrast features such as flat regions, can be matched to determine the optical flow [55]. Feature-based methods consists of two steps: feature detection and correspondence matching. Currently, feature-based optical flow methods are widely used, especially for large displacement matching [56]. The technique has two main drawbacks. First, the optical flow is very sparse when the background or the objects contain features that are not discriminative. Second, the selected features may not be reliable and disappear in subsequent frames.1.3.4. Frequency-based techniqueThe frequency-based technique, or velocity tuned filters, calculates optical flow using velocity-tuned filters in the Fourier domain. The advantage is that motion-sensitive mechanisms operating on spatiotemporally oriented energy in the Fourier domain can estimate motion that cannot be estimated using matching approaches [57]. For example, the technique can deal with the motion of random dot patterns. According to the output of the velocity-tuned filters, the frequency-based technique can be classified into two groups:•Energy-Based Method. This method is based on the output energy of the filters [4], [58]. The energy of a continuous translation in space is concentrated on a plane in the spatiotemporal frequency domain with the orientation related to the velocity [59]. The first computational model for the perception of motion [60] consists of a quadrature Gabor filter pair. Later, the model was extended to compute optical flow [58]. The optical flow is formulated as the least-squares fit of spatiotemporal energy to a plane in frequency space.•Phase-Based Method. It defines the velocity component in the output of band pass velocity tuned Gabor filters [57], [61]. The method is based on the decomposition of the original image into band-pass channels, similar to those produced by quadrature-pair filters in steerable pyramids [62]. Multi-scale representations are typically used for flow computation. A further decomposition into orientation bands yields more local constraints with a generally higher Signal-Noise-Ratio (SNR). Phase-based methods can be considered superior to the energy-based method in three aspects. First, subpixel accuracy can be achieved. Second, the velocity resolution is preserved by taking responses from neighboring filters. Finally, the technique is more robust to changes in viewpoint and illumination, as phase information is insensitive to changes in speed and contrast. Download : Download high-res image (278KB)Download : Download full-size imageFig. 2. End-to-end CNN learning model for optical flow estimation.Table 1. MPI-Sintel special evaluation protocol.MethodEPE allEPE matchedEPE unmatchedd0–10d10–60d60–140s0–10s10–40s40＋Table 2. KITTI special evaluation protocol.MethodSettingOut-NocOut-AllAvg-NocAvg-AllDensityRuntimeEnvironment1.3.5. CNN-based techniqueCNNs have achieved impressive success in a wide variety of image processing tasks, including optical flow estimation. CNNs are increasing used to replace hand-crafted features by learned features [63], [64]. The CNN is applied to extract deep features of the input images. These features are then integrated into common optimization algorithms to calculate optical flow [65], [66]. End-to-end methods are one class of approach where the CNN is not only used for learning the image features, but also for matching these features in the images [63], [67]. In this way, the whole optical flow process is performed by the CNN (see Fig. 2). While CNNS are typically trained in a supervised way by providing pairs of subsequent frames and the resulting flow field, recent work has also addressed the unsupervised training of CNNs for optical flow [64].
1.4. Evaluation measuresEvaluation measures are aimed at revealing properties of optical flow algorithms and helps to better understand the relative strengths and weaknesses of each. They not only provide information to improve the algorithms but also supply insight to select more suitable approaches to handle special challenges such as dealing with noise, large displacements or motion boundaries. In addition, evaluation measures support researchers to establish more realistic and complex benchmarks, and to develop ways to ensure continuous progress. The measurement of the performance of optical flow algorithms remains the subject of scientific debate, and have led to the introduction of various evaluation measures. We discuss the most relevant ones.1.4.1. Mathematical measuresWe define the ground truth (GT) optical flow as g(x,y)=(uT,vT) and the estimated flow as e(x,y)=(uE,vE). Some mathematical criteria are proposed to evaluate the performance quantitatively.•Average Angular Error (AAE): (1)AAE=1MN∑arccos(uTuE+vTvE+1(uT2+vT2+1)(uE2+vE2+1))where M and N are the numbers of columns and rows of the image, and MN is the number of pixels. The AAE measure is defined as the flow error deviation of angle between the GT flow and the estimated flow [61]. The drawback of this measure is that errors in small flows are penalized relatively severely.•Average Endpoint Error (AEE): (2)AEE=1MN∑(uE−uT)2+(vE−vT)2This error measure calculates the Euclidean distance between the endpoints of the GT flow and the estimated flow [68], [5]. The AEE discounts errors in regions of small flow while strongly penalizing large estimation errors.•Average Magnitude Error (AME): (3)AME=1MN∑|uE2+vE2−uT2+vT2|The AME is defined as the average magnitude of the velocity difference between the GT flow and the estimated flow [69]. Since it does not account for errors in direction, it is usually used together with the AAE.•Normalized Average Magnitude Error (NAME): (4)NAME=1MN∑|uE2+vE2−uT2+vT2|uT2+vT2This measure is defined as the average ratio between the magnitude of the velocity difference and the magnitude of the ground truth. It is somewhat unreliable for small GT vectors [70].•Error Normal to the Gradient (ENG): (5)ENG⊥((uE,vE),(uT,vT))=‖((uE−uT),(vE−vT))f⊥(x,y)‖where f⊥(x,y)=(−∂yI(x,y),∂xI(x,y))T. This error measure aims to examine the effectiveness of an algorithm to compensate for the aperture problem [69]. Recently, it was pointed out that the residual error between the GT and the estimated warped interpolation (Iwarp), which is interpolated by the estimated optical flow, is also a good way to evaluate the performance [5].•Root-Mean-Square (RMS) error: (6)RMS=[1MN∑(x,y)(Iwarp(x,y)−I(x,y))2]1∕2This error measure is defined as the root-mean-square (RMS) difference between the warped interpolation image Iwarp(x,y) and the GT image I(x,y) [5].•Normalized Interpolation Error (NIE): The NIE between a warped interpolation image Iwarp(x,y) and a GT image I(x,y) is given by [71]: (7)RMS=[1MN∑(x,y)(Iwarp(x,y)−I(x,y))2‖∇I(x,y)‖2+ε]1∕2Here, ε is an arbitrary scaling constant, typically set to 1. Interpolation error measures (Eqs. (6), (7)) require a robust interpolation algorithm. In addition, they rely on the homogeneity of the scene, as any incorrect flow vector pointing to a location with an identical brightness value is considered incorrect.1.4.2. Color-coded visualizationTo more vividly perceive the performance of optical flow, color coding approaches were introduced to visualize the optical flow and the error map. Fig. 1(b) and (c) show two types of flow field. Each provides a qualitative insight of the accuracy of the estimation. The color-coded flow field (Fig. 1(b)) is a dense visualization of the optical flow field. A color hue is associated to each direction and the saturation of the color increases with the magnitude of the flow vector (see Fig. 3). The vector-plot flow field (Fig. 1(c)) directly represents the displacement vectors and provides a good intuitive perception of physical motion.Errors in the estimation of the optical flow can also be visualized [72]. Fig. 4 shows the correct pixels are colored in blue while the error pixels are colored in red. Such visualizations provide a readily understood means of understanding the performance of an optical flow algorithm.Table 3. Characteristic comparison of the available public datasets.DatasetsMiddleburyKITTIMPI-SintelFlying ChairsImage pairs72194104122872Ground truth (GT)08194104122872GT density per image (%)100∼50100100 Download : Download high-res image (189KB)Download : Download full-size imageFig. 3. HSV color space: Direction is coded by hue, length is coded by saturation. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)Download : Download high-res image (971KB)Download : Download full-size imageFig. 4. A flow error map colorization in KITTI 2015 dataset. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)
1.5. Evaluation datasetsPublicly available datasets with ground truth optical flow allow researchers to compare their novel algorithms to existing work, and understand relative strengths and weaknesses of the methods. There is a steady progression in the size and realism of these benchmark datasets. We briefly discuss here the four most common optical flow datasets. The characteristics of the datasets are compared in Table 3.1.5.1. MiddleburyThe Middlebury benchmark dataset [5] provides ground truth for both real and synthetic sequences with complex conditions such as motion discontinuities, non-rigid motion, motion blur, and multiple independent motion. It does not contain very large displacements.1.5.2. MPI-SintelThis benchmark contains long photo-realistic sequences with extremely difficult cases [73]. It includes large motion, specular reflections, motion blur, defocus blur, and atmospheric effects. For evaluation, the basic EPE and some variants derived from EPE are used. The MPI-Sintel protocol measures the error distribution with respect to occlusion and large motion by different thresholds, see Table 1.1.5.3. Flying ChairsThe Flying Chairs datset [63] is sufficiently large to train CNNs for optical flow computation. The dataset consists of 22,872 image pairs with corresponding optical flow fields. Images are collected from Flickr on which the segmented images of chairs from [74] are overlaid. Random affine transformations are used to chairs and background to acquire the second image and ground truth optical flow fields.1.5.4. KITTIThe KITTI benchmark [75] consists of real sequences taken from a driving platform in an uncontrolled environment. The scene is much more challenging compared to other benchmarks because it includes non-Lambertian surfaces, different illumination conditions, a large variety of materials and large motion. Recently, background and foreground annotations were provided for the dataset [72]. A specific evaluation protocol was introduced for the KITTI benchmark. It uses an EPE threshold of τ pixels (τ∈(2,…,5)), and computes the percentage of pixels whose EPE is above the threshold. With occlusion ground truth, the metric is shown in Table 2.
