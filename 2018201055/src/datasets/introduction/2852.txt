Person re-identification (re-id) is becoming a hot topic for computer vision in smart city gradually, which aims to match the same person across different disjoint cameras and can save lots of human efforts in many video surveillance applications and forensics [1], [2], [4], [5], [6]. A number of methods have been presented to tackle the problem of person re-identification. These methods can be roughly divided two categories: feature learning based methods and metric learning based methods. The former aims to learn a robust and discriminative feature from different cameras by single image or multiple images [7], [8], [9], [10], [11], [12], [13], [14], [15] for person re-id. The metric learning based methods seek to learn an effective metric for matching persons from different cameras [16], [17], [18], [19], [26]. In recent years, deep learning based methods [20], [21], [22], [23], [24] are presented for person re-id problem. The deep learning based methods usually learn deep feature representations based on Convolutional Neural Networks (CNNs) framework. They often utilize multiple layers to learn effective representations from huge data samples. These methods achieve good performance in computer vision and recognition applications etc.
Although these methods tackle the problem of person re-id to some extent, they utilize single type of feature or do not consider the differences of different cameras. As shown in Fig. 1, we can observe that there exist large differences among the same persons from different cameras, due to the complex scenes, e.g., the poses changing, illumination changing, the cluttered background, partial occlusions and variant viewpoints.Download : Download high-res image (322KB)Download : Download full-size imageFig. 1. The typical image examples from real scenario on a new pedestrian dataset HRPID, and the PRID 2011 and iLIDS-VID datasets. The images corresponding to columns come from two disjoint cameras. Due to the changing poses and variant viewpoints, there exist large differences among the same persons from different cameras.
1.1. MotivationAs the above analysis, due to the different poses of pedestrians and viewpoints of cameras, there exist large differences among different types of features and between different cameras. Most existing methods do not fully consider these differences. In practice, multiple categories of visual appearance features can be extracted from each pedestrian image, e.g., texture feature and color feature. However, how to make full use of these different types of features and deal with differences among cameras has not been well studied. Although different types of features can be concatenated into one feature vector, the characteristics of corresponding to the features may be neglected by this manner to some extent. Intuitively, it is necessary and meaningful to preserve the characteristics of corresponding to the features by learning more effective projection to reduce the differences of disjoint cameras and the influence of different features.Inspired by the above analysis, we seek to design an approach, which can make better use of different visual appearance features and reduce the differences between different cameras in this paper.
1.2. ContributionThis is an extended version of our conference paper [25]. The major contributions of this work are summarized as follows:(1) We propose a multi-view coupled dictionary pair learning (MVCDL) framework, which learns the category-specific dictionary pair for different categories of features. With MVCDL, multi-view features of image can be transformed into coding coefficients with favorable discriminability.(2) We contribute a newly collected dataset, named High-Resolution Pedestrian re-Identification Dataset (HRPID) on the campus of Wuhan University. The size of person image sample is normalized to 230  ×  560 pixels, which is higher than most existing person re-id datasets.(3) We have conducted extensive experiments on a newly collected pedestrian dataset HRPID and two publicly available person datasets, including PRID 2011 [27] and iLIDS-VID [28]. Experimental results have shown that our proposed approach can achieve higher matching rates than the representative competing methods, which further demonstrate the effectiveness of our approach.
