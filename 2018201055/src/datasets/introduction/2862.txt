Nowadays, activity recognition is a key requirement in several ICT domains [1], including smart home automation, homeland security, e-health, gaming, manufacturing, pervasive advertising, and smart cities, just to name a few. In particular, advanced healthcare systems rely on continuous monitoring of human activities to support early detection of health issues, to enhance rehabilitation, and to promote healthy and active lifestyles [2], [3]. These applications take into account not only simple physical activities, but also a wide range of complex activities of daily living (ADLs). As a consequence, several techniques to accurately recognize ADLs have been proposed in the last years. A popular approach to activity recognition consists in the use of supervised learning methods applied to datasets of activities and sensor data [4], [5], [6]. Supervised learning proves to be effective in recognizing activities characterized by specific postures or motions, such as physical activities [7]. The supervised learning approach was also successfully applied to the recognition of high-level urban activities based on GPS traces [8], [9]. However, the actual applicability of the supervised approach to recognize complex ADLs at a fine-grained level is questionable, especially when infrequent or sporadic activities are taken into account. Indeed, acquiring large datasets of ADLs is expensive in terms of annotation costs [10], [11]. Moreover, activity annotation by an external observer, by means of cameras or direct observation, severely violates the user’s privacy.
For this reason, different research groups investigated unsupervised approaches for recognizing ADLs. An interesting direction in this sense consists in disregarding the activity semantics, and recognizing recurrent patterns of abstract actions and their temporal variations, as proposed in [12]. That approach can be applied to the early recognition of particular health issues. However, for many other applications domains, knowledge of the activity semantics is of foremost importance. In those domains, most unsupervised methods rely on symbolic modeling of activities in terms of their constituting simpler actions. For instance, the temporal sequence of events “open medicine cabinet; take medicine box; put away medicine box; close medicine cabinet” characterizes the ADL “taking medicines”. A popular approach is to manually define those models through formal ontologies expressed in a description logics language [13], [14], [15]. However, manually defining comprehensive ADLs ontologies is cumbersome, and requires specific expertise in formal ontology languages and knowledge engineering. Moreover, the ontological approach is generally based on rigid activity definitions, that fall short in adapting to dynamic context conditions.
In order to overcome the limitations of current methods, in this paper, we introduce a radically new approach. The approach starts from the observation that, thanks to the widespread popularity of social network applications, users are sharing more and more pictures and videos illustrating the execution of every kind of activity in the most disparate situations. Our intuition is that the ability of extracting semantic tags from those visual resources through computer vision tools may enable novel data mining methods to infer activity models at essentially no cost. Indeed, shared contents are frequently labeled by their owners with tags describing the depicted activity. Those tags provide an implicit activity label. Other tags can be extracted by computer vision tools to acquire semantic information about depicted objects, actors, and context conditions (e.g., time of the day, light level, symbolic location). Hence, by reasoning with activity labels and contextual tags, it is theoretically possible to build a comprehensive set of activity models, without the need of sensor-based training sets and without manual modeling effort. To the best of our knowledge, this is the first work that addresses this research direction, except for our preliminary investigation reported in [16]. Several research issues are involved in this work, including search of the most representative images or videos, computer vision algorithms for tag extraction, and data mining methods to construct the activity models. For the sake of this paper, we concentrate on activity model extraction from still images, where images may include frames extracted from videos. However, the approach can be extended to mine more sophisticated models, considering temporal relationships captured from activities recorded in videos.
We point out that several previous works tried to recognize activities based on images or videos [17], [18], [19], [20]. However, the goal of those works was vision-based activity recognition; i.e., the model extracted from a training set of images or videos was used to recognize the activity depicted in other images or videos. In this paper, we investigate a completely different approach: mining activity models from pictures and videos, and using them to recognize activities based on firing of sensor events. Thus, in our work, the domain of mined data (visual contents) is disjoint from the one of data used for activity recognition (sensor data). A preliminary investigation of this approach was presented in [16], where we introduced the use of computer vision tools to extract object information from images depicting activity execution. In this paper, we extend our previous work with (i) activity mining from videos, (ii) a novel activity recognition technique, (iii) use of additional computer vision tools, and (iv) extensive experiments with additional datasets.
The main contributions of this work are the following:

•We introduce a novel approach to unsupervised sensor-based recognition of human activities, which exploits tagged visual contents shared on the Web, and computer vision tools.•We propose a probabilistic-based method to extract activity models from pictures and videos.•We present the results of extensive experiments with two large datasets of real-world ADLs and different computer vision tools, showing the effectiveness and practicality of our approach.
The rest of the paper is structured as follows. Section 2 discusses related work. Section 3 introduces our methodology. Section 4 presents our algorithms. Section 5 illustrates the methods to recognize activities based on the extracted models. Section 6 presents the experimental evaluation. Section 7 discusses strong and weak points of the approach. Section 8 concludes the paper.
