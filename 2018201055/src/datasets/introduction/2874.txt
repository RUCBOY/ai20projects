Scientific discovery drives economic growth, so determining the optimal level of R&D is a main goal of many policymakers and administrators. In developed economies, two broad, secular trends are changing the parameters governing this problem. The first is a shift to university-led R&D.1 The second is increasing reliance on internal rather than government funding sources.2 This issue is particularly salient in the US, where the share of internally funded university R&D has nearly doubled over the last four decades. The policy debate has been contentious. In 2007, US Congress passed the American COMPETES Act—legislation intended to double the level of funding to certain STEM3 fields—but austerity measures delayed and reduced the scope of the original legislation. Recently, deep cuts were proposed by the President's 2019 budget, though these were ignored by Congress. Advocates for R&D funding emphasize path-breaking innovations generated by university research. The director of the National Institute of Health stated, for example, that without added support, “a lot of good science just won’t be done.”4 Critics, on the other hand, cite allegedly unproductive spending at the margin.
This paper addresses two questions at the core of the issue. First, how large of an impact do institutionally funded university research expenditures have on scientific productivity? There is scarce work on the university's knowledge production function, even though understanding it is crucial to informing a policy debate that stretches back at least to works by Nelson, 1959, Arrow, 1962.5 Moreover, there is little work specific to institutionally funded research, which is increasingly relied on but can differ from other funding sources in important ways.6 Second, does the pool of knowledge derived from academic research create valuable downstream technology at the intensive margin or simply accumulate inside the “ivory tower”? This transport of ideas is at the core of the economics of innovation and technology, crucial to endogenous growth (Romer, 1990), and of the motivation for a large literature studying “real effects” of university research, starting with influential work by Jaffe (1989). This literature provides strong evidence of spillovers from academic activity to private sector outcomes but ultimately does not per se provide proof that marginal university R&D expenditures generate valuable downstream technology. For example, while university R&D spending increases the local pool of knowledge, it also increases the local supply of newly minted science and engineering PhDs and the local demand for technical equipment and services.
Two factors make answering these questions difficult. First, identifying the expenditure-output relationship requires—at a minimum—a large set of controls to disentangle the causal effects from unobservable factors that drive both funding and productivity simultaneously. However, their inclusion sweeps away (helpful) exogenous variation as well, augmenting right-hand side measurement error and exacerbating an errors-in-variables problem that biases estimates towards zero (Adams and Griliches, 1998). Second, researchers rarely directly observe the real value of knowledge production — what Griliches (1979) called the “major difficult[y]” of measuring research output. Publications and patents are helpful for understanding the nature of that production but do not speak to precise amounts of private or social value (Griliches, 1990, Jaffe et al., 2000).
We use exogenous variation in research expenditures to assess the impact of internally funded university R&D expenditures on a set of commonly studied outputs, and then we augment this set with technology licensing revenues to estimate the real value of the resulting innovations. The source of this variation is unexpected National Collegiate Athletic Association Football (“NCAAF”) outcomes. Athletic team performance affects cash flow to the university and, in turn, the funds available for research. Even if unobserved school-specific factors that drive research output also influence football team success, they are unlikely to influence unexpected within-season changes in team success. Moreover, they are unlikely to be correlated with measurement error in research expenditures, thus mitigating or eliminating the errors-in-variables problem. We measure football team success using the Associated Press Top 25 Poll, and use the difference between postseason and preseason vote counts as the instrumental variable. Since the individual voting results of the poll are made public, and the professional sportswriters who vote have a significant reputation stake in correctly forecasting teams' true prospects, the difference between postseason outcomes and preseason expectations can be treated as random.7 Also, since each respondent ranks 25 teams, the number receiving positive votes is much larger, ranging between 35 and 52 and averaging about 40.
Three aspects of the setting aid greatly in obtaining results. The first is the great degree to which football impacts overall school finances. Athletics-related revenues derived from the sale of tickets, apparel, broadcasting rights, and trademark licenses provide one channel. These are large in absolute terms (e.g., University of Texas at Austin football revenue tops $150 million each year, seven times higher per game than the median professional baseball team) and in relative terms (e.g., Louisiana State University football revenue is more than one-third of the entire tuition bill). Donations provide another channel that is often tightly tied to football success. As one example, on the night of freshman Johnny Manziel's unexpected Heisman Trophy win, Texas A&M as a university raised more money than it typically receives in a month, setting records for quarterly and annual alumni giving.8 Much of this cash flow is redistributed to the university to fund key activities, including research.9 The second is the unpredictability of within-season footfall outcomes. The third relates to short lags in knowledge production within the STEM fields. Expenditure data indicates that majority of windfalls/shortfalls are reflected in the budget of the subsequent fiscal year, while patent and license data indicate that faculty gain proof-of-concept and initial private sector interest within that subsequent year. This is consistent with our conversations with administrators, who suggest that marginal funds are often allocated to promising projects hamstrung by funding bottlenecks, as well as technology transfer office (“TTO”) reports that indicate aggressive, early protection and promotion of university intellectual property.
We model knowledge production as a function of faculty, facilities, and research support—close analogs to labor, capital, and materials in our setting—as well as a Hicks neutral total factor productivity residual. We assume that the first two inputs are fixed in the short term but the last one is adjustable. The instrument creates unexpected, marginal shifts in the budget used to finance research projects, so it should impact research support only, leaving faculty and facilities unaffected. We show the data is consistent with this, which allows us to isolate the impact of the most contested knowledge production input. In addition, the data provides us with two exogeneity checks. First, we can test whether football success impacts any source of research expenditures other than institutional funds. For example, football should not impact federal grants. Second, we can test if football success impacts contemporaneous R&D expenditures. Since budgets need time to adjust, only subsequent expenditures should be impacted.
These expenditures support research projects. Successful projects yield publications and patents. We estimate the dollar elasticities of scientific productivity for each of these two output measures by two-stage least squares (“2SLS”). When output is measured in articles published, we find an elasticity of 0.28. When measured in patent applications, we find an elasticity around one. All specifications include school fixed effects and school-specific time trends. Standard errors are clustered at the school level, and the estimates are significant at the 5% level. Our 2SLS estimates contrast sharply with lower OLS estimates that result from using the same set of controls. This difference is policy-relevant, since low elasticities could lead to institutional or even national under-investment, and is consistent with speculation by Adams and Griliches (1998), who argued that their OLS estimates were unreasonably low (due to input measurement error). Our estimates also exceed those recovered by Jacob and Lefgren (2011), who study federally funded university expenditures, and approximate those recovered by Azoulay et al. (2014), who study federally funded private sector pharmaceutical firm expenditures.
To answer the more fundamental question of whether investments in academic research generate valuable, downstream technology at the margin, we combine the logic of revealed preference with data on university licensing revenues. That is, the amount that unaffiliated, private-sector firms will pay for research output offers a straightforward lower bound on its value. The availability of this measure is a convenient consequence of the fact that universities rarely commercialize their inventions, resulting in intermediate transactions that price the technology. When output is measured in upfront licensing revenues, we find an elasticity consistent with approximately constant returns to scale.10 Adjusted to incorporate recurring “running” revenues, we estimate universities earn much as between 15 and 35 cents on each dollar of research support at the margin. This comes in addition to other private and social benefits (e.g. institutional prestige and spillovers, respectively). Together these findings tend to reject claims that while university-led research produces “paper” returns, it does not generate genuinely valuable innovation at the intensive margin.
This paper contributes to several literatures. It most closely connects to work on the knowledge production function (Griliches, 1979). We build on Adams and Griliches (1998), who measure the impact of university research expenditures on scholarly articles and their citations. They find that adding institution-specific dummy variables yields implausibly low elasticities and argue this is due to right-hand side measurement error, which echoes problems encountered in the larger literature studying input-output relationships among manufacturing concerns. That is, though the research discoveries and manufacturing activities differ in obvious and important ways, with spillovers, large uncertainty, and long lags much more likely among the former than the latter, the inclusion of fixed effects tend to result in very low factor returns in both settings (Griliches and Mairesse, 1998). Olley and Pakes (1996) instead take a control function approach to the problem and find more sensible estimates, although recent work by Collard-Wexler and De Loecker (2016) suggests that the errors-in-variables problems persists, at least when using data from developing countries. In particular, they show that instrumenting for capital with investment produces much higher coefficients than using measured values in Slovenian and Indian datasets. We complement these findings by reaching analogous conclusions exploiting very different variation: in their framework, our instrument would represent a cost shifter, moving the shadow price of materials investments around within the university bureaucracy. As stated above, Jacob and Lefgren (2011) take an IV approach similar to ours, but study federally funded research expenditures instead, while Azoulay et al. (2014) measure the impact of public grants on patenting for private sector pharmaceutical firms. We discuss these results in more detail below.11
This paper also closely relates to work assessing the “real effects” of academic research (Acs et al., 1992, Furman and MacGarvie, 2007, Hausman, 2013, Henderson et al., 1998, Jaffe, 1989, Jaffe et al., 1993, Kantor and Whalley, 2014). These papers rely on the geographic coincidence of university activity and private sector outcomes, including wages, employment, and profits. Pakes (1986) provides an alternative approach to directly measuring innovative output, which uses patent renewal decisions to bound the underlying value of inventions. In contrast, we measure impact using technology licensing revenues. These are salient to university faculty and administration (Jensen and Thursby, 2001), but might only capture a fraction of the total returns from STEM research, which more recent research highlights. Using UMETRICS data, which provides detailed accounts of expenditures at a growing list of large US institutions (Allen et al., 2015), Weinberg et al. (2014) show that research support finances not only the training of knowledge workers but also the acquisition of specialized equipment whose very development and production is likely to produce spillovers.12
The paper is organized as follows. Section 2 describes the setting, while Section 3 describes the data. Section 4 provides a model of the knowledge production function and the estimation strategy. Section 5 assesses the impact of our instrument on research expenditures. Section 6 estimates the impact of these expenditures on scientific output. Section 7 concludes.
