Recently, Visual Question Answering (VQA) [1] has received extensive attention in both the academia and industry. It requires a high level understanding of visual/textual information, and aims to accurately answer natural-language questions about given images. VQA can significantly benefit a variety of applications, such as robot tutors, smart home management systems and private virtual assistant.
For achieving simultaneous understanding based on given images and questions, multimodal fusion is an indispensable part in VQA. Its purpose is to incorporate image and question features and to generate integrated visual-textual features for answer prediction. Multimodal fusion has been widely studied in VQA. The prevailing fusion schemes utilized in current VQA learning frameworks include MLB [2], MFB [3], MUTAN [4] and MFH [5]. There are two characteristics for these mainstream multimodal fusion schemes: First, most methods rely on latent embedding that designs two-branch networks in which the visual and textual features are embedded into a common latent space, and then using some operations like multiplication or summation to fuse them as shown in Fig. 1(a). Second, these methods are inclined to perform a single interaction between visual and textual inputs, and predict correct answers based on a single fusion feature. For improving such two characteristics, we propose a Multi-stage Hybrid Embedding Fusion (MHEF) network to achieve multi-space and multi-stage interactions in a unified framework.Download : Download high-res image (255KB)Download : Download full-size imageFig. 1. (a) Latent embedding fusion, âŠ™ is the element-wise multiplication; (b) dual embedding fusion; (c) the core idea of MFS.
For the first characteristic, Latent Embedding Fusion (LEF) is an efficient approach for combining two modal features. To achieve better fusion feature than LEF, motivated by dual mapping [6] in the researches of text-image matching, we intend to explore multimodal relationships in diverse feature spaces for feature fusion, and propose a novel Dual Embedding Fusion (DEF) method. As depicted in Fig. 1(b), DEF establishes two-branch layers to project visual/textual inputs into their reciprocal space (textual/visual space) and then fuse with the original inputs from the other modality in each space. In addition, we combine our DEF and the mainstream LEF in parallel to form a Hybrid Embedding Fusion (HEF) network. In the HEF, after generating fusion features in visual, textual and latent embedding spaces, we unify multi-space fusion features into a final fusion feature for answer prediction. We assume that performing multiple interactions in diverse spaces can gain effective and productive multimodal correlations for fusion features.
For the second characteristic, in order to reduce the potential semantic loss and implement more interactions for improving multimodal fusion performance, we attempt to perform a multi-stage fusion method. To this end, we propose a Multi-stage Fusion Structure (MFS). Specifically, the embedded feature is not only utilized for fusion, but also for reconstructing the original input. In this way, we can ensure the embedded features bring less information loss. Additionally, the reconstructed features can be proceeded for the next round prediction as well. Fig. 1(c) illustrates the core idea in our proposed MFS.
Based on the MFS and HEF, we build a Multi-stage Hybrid Embedding Fusion (MHEF) network by constructing the Multi-stage Latent Embedding Fusion and the Multi-stage Dual Embedding into a parallel combination. To be specific, for each fusion stage in the MHEF, we can get fusion features in three spaces (visual, textual and latent spaces), and the same fusion feature processing in HEF are employed to produce a final fusion feature for each stage. Then, to further enhance VQA performance, multi-stage final fusion features are separately exploited to predict answer candidates, and the final result is the average of the predictions from all stages.
In a nutshell, our contributions can be summarized as follows: (1) We propose a novel DEF scheme, and further incorporate the LEF with our DEF in parallel to establish a HEF approach, thereby capturing rich multimodal semantic correlations in multimodal fusion. (2) In order to reduce information loss and further improve fusion performance, we present a novel MFS, and apply it into the HEF model for generating a MHEF network. (3) We carry out extensive ablation studies over each component in the MHEF, and validate the benefits of the HEF and MFS for multimodal fusion. Furthermore, our proposed MHEF remarkably outperforms the dominant multimodal fusion approaches, and yields promising performance on the VQA-v1 and VQA-v2 datasets.
