Recurrent neural networks (RNNs) have a long history in optimization computation, and the first RNN model can be traced to Hopfield neural network [1], [2]. After that, RNNs have been extensively applied in various areas, such as algebraic computation [3], [4], [5], [6], state estimation [7], fault tolerant control [8], [9], speech recognition [10], and robotic control [11], [12], [13], [14], [15]. In algebraic computation, there exist two kinds of RNNs: gradient neural networks (GNNs) [16], [17], [18], [19] and zeroing neural networks (ZNNs) [20], [21], [22], [23], [24], [25], which are paid attention by lots of researchers. The design of GNNs is on basis of the negative gradient information of the error norm [16], [17], [18], [19]. Because of this, GNNs can be used to solve various algebraic equations and optimization problems with time-invariant coefficients. In [26], a GNN model was developed to address linear equations, and the stability of this model was theoretically analyzed. In [27], [28], an improved GNN model with exponential stability was presented to modify the performance of the original GNN model. In [17], such a GNN model was further applied to nonlinear equations, in which it was found that the GNN model is effective when nonlinear equations only have a simple root, while it is no longer effective when nonlinear equations have multi roots. In addition, it was found that GNNs will produce a lagging error when these algebraic equations and optimization problems have time-variant coefficients [19].
For addressing the above problems arisen in GNNs, a kind of RNN called zeroing neural networks (ZNNs) were proposed and systemically studied for time-variant algebraic equations and optimization problems [20], [21], [22], [23], [24], [25]. ZNN is a systematic method to solve time-variant problems, which has big differences from the above mentioned GNN. Generally speaking, the state dimension of ZNN is multiple. If the state dimension of this network is one, ZNN is called zeroing neural dynamics (ZND). Hence, ZND is also a systematic method to solve time-varying problems with scalar situation included. As compared to time-invariant coefficients, time-variant ones will vary when time changes. Hence, time-variant problems solving is much more difficulty than time-invariant ones solving. To achieve this purpose, different from the design method of GNNs, the design of ZNNs is on basis of a vector/matrix-valued error function and an exponential formula, which can monitor the changing process of each error with time, and make them converge to 0 exponentially. For example, in [20], zeroing neural network was first proposed to find the solution of time-variant Sylvester equation, and it was theoretical proved that such a ZNN model can make the time-variant error function decrease to 0 exponentially. Besides, such a ZNN model was successfully applied to the inverted pendulum control of a cart system. In [21], a general framework of ZNN was developed for time-variant matrix inversion, and further studied the role of different activation functions played in the general ZNN framework.
After that, a large amount of work about ZNN has been produced to show its wide applications, especially for time-variant problems solving [22], [23], [24], [25]. For example, in [22], a ZNN model was developed to solve time-variant quadratic program problems and the corresponding exponential convergence property was guaranteed. In [23], an improved ZNN model was presented to find the exact solution of time-variant generalized linear matrix equations and two modified activation functions were developed to accelerate the convergence rate. Especially, in [16], [29], a ZNN model was extended to solve time-variant nonlinear equation with the theoretical analysis provided. In addition, by comparing the corresponding GNN model, such a ZNN model is a better neural model in terms of the solving accuracy and convergence speed. Nevertheless, it must be pointed out that, before 2013, the presented ZNN models can only achieve exponential convergence, which limits some real-time applications that usually require higher computation speed. For achieving a higher computation speed, in 2013, Li et al. [30] proposed a new activation function (termed the sign-bi-power function) to establish a finite-time ZNN model for solving time-variant Sylvester equation, and the upper bound of the convergence time for Sylvester equation was theoretically calculated. This work made much progress in the ZNN field, which has a breakthrough from the prospective of the convergence speed. Inspired by the idea mentioned in this work, in [31], a new ZNN model with finite-time convergence was designed to find the root of time-invariant nonlinear equation by exploiting this sign-bi-power activation function. In [31], [32], [33], [34], [35], [36], by modifying the sign-bi-power activation function, some new activation functions were presented to improve the convergence property of ZNN models for time-variant linear/nonlinear equations, and the upper bound of the convergence time using new activation functions is smaller than the one using the sign-bi-power activation function [35], [36]. More recent advances about ZNN can be referred to a survey published in [37].
External noises are unavoidable in circuit implementation of neural networks [38], [39]. For a valuable neural model, it has to tolerate external noises in practical applications, otherwise the solving accuracy may be seriously disturbed [40], [41], [42]. However, the above mentioned ZNN models are usually conducted in an ideal environment with no external noises considered. For rejecting external noises, in [43], [44], Jin et al. proposed a noise-tolerant ZNN model for time-variant matrix inversion, while it can only achieve the exponential convergence due to the rejection of nonlinear activations. To address the above two issues simultaneously, in the current work, by suggesting a new nonlinear activation function, a robust and fixed-time zeroing neural dynamics (RaFT-ZND) model is proposed and analyzed for time-variant nonlinear equation (TVNE), which can tolerate external noises and even achieve a faster convergence speed (i.e., fixed-time convergence). Note that the fixed-time convergence is a best convergence performance, as compared to the previous mentioned exponential and finite-time convergence, because it does not depend on initial states of systems and the upper bound can be calculated in advance when system parameters are set. In addition, the theoretical analyses about robustness and fixed-time convergence of the RaFT-ZND model are presented in detail. Simulative results demonstrate that the RaFT-ZND model combines such two merits.
The rest of this work is divided as below. In Section 2, as a basis of research, TVNE and ZND are presented. By suggesting a new activation function, the RaFT-ZND model is proposed in Section 3 for TVNE, and the corresponding theoretical analyses are presented in Section 4. Simulative results are provided in Section 5, and the conclusion is given in Section 6. At last of this section, the main contributions include the following aspects.
•A new activation function is developed to modify the comprehensive performance of zeroing neural dynamics. As compared to the previous activation functions, the new activation function can achieve the best results.•On basis of such a new activation function, a robust and fixed-time zeroing neural dynamics (RaFT-ZND) model is proposed and analyzed for time-variant nonlinear equation (TVNE). In addition, the detailed theoretical analyses about robustness and fixed-time convergence of the RaFT-ZND model are presented.•We give a TVNE example to test the effort of the RaFT-ZND model by comparing the previous ZND model with existing activation functions. Simulative results demonstrate that the RaFT-ZND model is a better model, and combines the robust and fixed-time convergent merits.
