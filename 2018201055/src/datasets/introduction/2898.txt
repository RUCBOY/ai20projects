To an individual, gestures are traditionally perceived as simple, natural expressions and rarely complex, but as effortless as a gesture may seem, it is a powerful mechanism that has the ability to convey and encode messages or imply intent. In society gestures are ubiquitous. Additionally, gestures are unique in that they are narratives, for they are descriptive in nature. However, at times, interpreting gestures can be cumbersome for the observer. Furthermore, gestures can be misconstrued and result in misinformation and confusion.
Nonetheless, when coupled with other visual or audial cues, gestures can further enhance and clarify intent. For example, the simple gesture of pointing to a ball while voicing, “Get that,” indicates that the ambiguous indirect object in the command is the ball. In general, these cues — both visual and audial — are used extensively in everyday life to present or indicate an object's affordances or offer further information as in the provided example.
In essence, gestures are observable instances and provide a natural mode of communication. At any point in time, the instantaneous positions of the body and appendages can be ascertained; these snapshots are called frames. Collectively, the successive compilation of ordered frames forms the observed action.
Capturing and storing a gesture on a computer traditionally requires extensive sensory electronics, which monitor the gesture being performed. As the gesture is being transacted, sensors perform many calculations and serialize the action for processing and consumption. The intent of a gesture should be ascertainable and apparent; nonetheless, gestures can be vague or ambiguous, which results in the gesture being taken out of context or misinterpreted. This confusion is amplified when hardware and software components are added to the system. Sensors are not infallible, and they are prone to errors; as such, these sensors must be calibrated and tested, ensuring the measurements are accurate and precise. Also, sensors can log inaccurate and noisy data. Furthermore, the software systems must be vetted for errors; programming flaws are traditionally present and interacting with non-native devices further compounds the confusion and complexity of the proposed system. Thus, perception of a gesture by machine may be complex and taxing. Notwithstanding, there exists a strong desire to couple gestures and machines that creates a relationship that facilitates interaction and recognizes intent.
Gestures are not the only mode of communication with a computer; however, they are seemingly easier than many of the alternatives. Some of the earliest mechanisms of interaction include the keyboard and the mouse. The mouse is an input device tethered to a computer and, by its very nature, relays intent through interaction with the user. While the mouse and keyboard are extremely popular interfacing devices, these mundane interfaces are all too common and do not embrace the natural, fluid intent of gestures. However, new paradigms are extending the notion of gesture perception and recognition. One of the earliest interfaces — the mouse — is receiving renewed attention [1], [2]. The described systems utilize machine learning techniques, algorithms, and hardware to reconceptualize interactions between a computer and a user while also redefining the notion of a mouse.
Recently, there has been a burgeoning of natural user interfaces, or NUIs. These innovations can be found in the gaming industry and are led by Sony's PlayStation Move, Nintendo's Wiimote, and Microsoft's Xbox Kinect. The PlayStation Move and Nintendo Wiimote primarily utilize inertial sensors for tracking positions and movement. These small devices are typically held or attached to the participant while performing an action. Therefore, the device, while relaying the information to the console, is harmoniously participating in the operator's activity.
Small gyroscopes and accelerometers are also interfacing devices and are prevalent in mobile platforms, including cellular phones. With their miniscule size, these sensors encode and transmit data corresponding to a device's movement, which are consumed by numerous applications. While traditionally used to orient a device, there is interest in using gyroscopes and accelerometers for gesture recognition [3], [4], [5].
The Xbox Kinect, on the other hand, operates using an RGB camera and a depth sensor. Therefore, not only is it able to detect, locate, and track appendages, but it is also able to identify the pose, the height, and other quantitative attributes of the subject. Unlike the other devices, the operator is not tethered to a mechanical device, which frees the user from cumbersome devices that are worn or attached to the subject. The Xbox Kinect is truly an effective device for detecting the natural pose and intent of gestures; it creates and compiles the frames for consumption.
Ultimately, the process of recognizing gestures is rather cumbersome and is a quandary. Gestures can be extremely noisy, and sensor data increases this complexity. With its success as a machine learning algorithm, the Recursive Hyperspheric Classification (RHC) algorithm [6], [7] has been modified to give it a temporal mechanism that assists in the classification and recognition processes of gestures.
RHC is a machine learning algorithm used in ascertaining the descriptive function of a state space. It is a supervised learning process that strives to infer and affix labels to unmarked data. Nonetheless, classical RHC is geared toward static, multivariate datasets and cannot digest temporal data.
Prompted by the need of dealing with spatiotemporal datasets, such as gesture datasets, this paper introduces the concepts related to the modifications of RHC algorithm. These adaptations make it effective as a classifier of dynamic, temporal gestures. The newly modified algorithm is termed Spatiotemporal Recursive Hyperspheric Classification (STRHC). Furthermore, this paper demonstrates STRHC's capabilities when classifying and recognizing human gestures when sensed with an Xbox Kinect.
Traditional methods for classifying and recognizing temporal sequences include state models, large forests, or computationally expensive algorithms. Additionally, some algorithms may be perceived as a black box given that the dynamics are complex or ambiguous. Nevertheless, STRHC is novel in that it produces a single hierarchical tree that — when coupled with a queue — is able to classify and recognize temporal sequences. When validating the algorithm, STRHC is computationally faster than Dynamic Time Warping and attains high recognition rates. Ultimately, STRHC possesses a natural ability to capture and assess temporal data in the nodes of the constructed hierarchy.
This paper, in its remaining parts, is structured as follows. Section 2 details related work in classifying and recognizing temporal gestures; several of these algorithms are used in benchmarking STRHC. Section 3 presents the fundamental concepts of the RHC algorithm and its many facets. A complete example that reinforces the described theory of the RHC algorithm is provided in Section 4. Section 5 describes RHC addenda that improve recognition rates. Section 6 details the STRHC algorithm as it pertains to classifying and recognizing temporal gestures. This section also contains details surrounding the temporal mechanism used in conjunction with the RHC algorithm for classification. Section 7 presents the results when validating and testing the algorithm with diverse datasets, and Section 8 comments on the observed results, offering insight and details about the classification and recognition processes. Finally, Section 9 closes with brief suggestions and modifications that are to be addressed in the future enhancements of the STRHC algorithm.
