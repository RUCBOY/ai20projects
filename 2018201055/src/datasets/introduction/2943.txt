Sparse optimization problems with the L1 norm objective functions subject to the L∞ norm inequality constraints are found in many science and engineering applications such as in the filter design problem [11] and in the image representation problem [12]. For the filter design problem, in order to implement the filter efficiently, the total number of the filter coefficients should be minimized. On the other hand, it is important to guarantee that the maximum ripple magnitude of the filter within its passband and its stopband is within an acceptable specification. For the image representation problem, in order to represent the image efficiently, the total number of the transform coefficients should be minimized. On the other hand, it is important to guarantee that the maximum absolute difference between the original image and the represented image is within an acceptable specification. These two problems are actually the sparse optimization problems with the L0 norm objective functions subject to the L∞ norm inequality constraints. However, as an L0 norm optimization problem is nonconvex and its global optimal solution is not uniquely defined, the L0 norm objective function is usually approximated by the corresponding L1 norm objective function. Hence, these two problems are usually formulated as the sparse optimization problems with the L1 norm objective functions subject to the L∞ norm inequality constraints.
Denote ℜa×b and ℜd as the a×b real valued matrix space and the d dimensional real valued column vector space, respectively. Denote the superscript T as the transposition operator. Let A∈ℜm×n be a dictionary. Let y∈ℜm and x∈ℜn be a signal vector and a representation vector, respectively. In order to have an efficient representation [15], [16], the representation vector should be sparse. To yield a sparse representation, the total number of the nonzero coefficients in the representation vector should be minimized. Since a real number to the power of zero is equal to one, the L0 norm of a vector is equal to the total number of the nonzero elements in the vector. Hence, minimizing the L0 norm of x yields a sparse representation. Here, ‖x‖0 denotes the L0 norm of x. However, the L0 norm operator is nonconvex and the global optimal solution of the corresponding optimization problem is not unique. To address this difficulty, the L0 norm operator is approximated by the L1 norm operator. Since a real number to the power of one is equal to absolute value of the real number, the L1 norm of a vector is equal to the sum of the absolute values of the elements in the representation vector. It is well known that approximating the L0 norm of x by the L1 norm of x is valid if the isometry condition is satisfied [15], [16]. Hence, minimizing the L1 norm of x yields a sparse representation if the isometry condition is satisfied. Here, ‖x‖1 denotes the L1 norm of x. Besides, since the L1 norm operator is convex and the corresponding optimization problem can be reformulated as a linear programming problem, the L1 norm optimization problem can be solved easily. On the other hand, the maximum absolute difference between the elements in the original signal vector and that in the reconstructed signal vector should be small [13], [17]. Since a real number to the power of positive infinity is equal to the positive infinity, the L∞ norm of a vector is equal to the maximum absolute element of the vector. Hence, the small maximum absolute difference between the elements in the original signal vector and that in the reconstructed signal vector is equivalent to the small L∞ norm of Ax−y. Here, ‖Ax−y‖∞ denotes the L∞ norm of Ax−y. Therefore, the above problem is formulated as the optimization problem which minimizes ‖x‖1 subject to the constraint on ‖Ax−y‖∞. That is,
Problem (P1)(1a)minx⁡‖x‖1,(1b)subject to‖Ax−y‖∞≤ε. Here, ε denotes a user defined acceptable specification on ‖Ax−y‖∞. Let xε⁎ be the optimal solution of Problem (P1).
Since ‖x‖1 characterizes the efficiency of the representation of the signal vector and ε determines the reconstruction error between the original signal vector and the reconstructed signal vector [1], [2], [3], [4], [5], [6], [7], both ‖x‖1 and ε should be as small as possible. It is worth noting that the feasible set of the optimization problem is convex. If a convex set consists of more than one distinct element, then all convex combinations of any two distinct elements in the convex set are also in the convex set. Hence, the smallest convex set only consists of a single element. Besides, the size of the feasible set of the optimization problem decreases as ε decreases. Hence, the feasible set of the optimization problem becomes a singleton when the value of ε reaches its minimum value. On the other hand, by the definition, the obtained solution of the optimization problem becomes the solution of the corresponding L∞ norm optimization problem when the value of ε reaches its minimum value [14]. Hence, the unique element in the feasible set of the optimization problem is the solution of the corresponding L∞ norm optimization problem when the value of ε reaches its minimum value. Denote the solution of the corresponding L∞ norm optimization problem as x∞⁎=arg⁡minx⁡‖Ax−y‖∞. For the opposite, the size of the feasible set of the optimization problem increases as the value of ε increases. As the total number of the elements in the feasible set of the optimization problem for comparing their objective functional values is increased, the optimal objective functional value of the optimization problem decreases. However, when the value of ε increases to a value where the solution of the corresponding unconstrained optimization problem is in the feasible set of the optimization problem, a further increase of ε would not further decrease the optimal objective functional value of the optimization problem. The obtained solution of the corresponding optimization problem at that value of ε is the solution of the corresponding unconstrained L1 norm optimization problem. More precisely, the obtained solution of the corresponding optimization problem at that value of ε is the zero vector and the corresponding optimal objective functional value is equal to zero. When the value of ε further increases, the zero vector is still in the feasible sets of the corresponding optimization problems. Hence, the optimal objective functional value is still equal to zero even though the value of ε further increases. The obtained solutions of the corresponding optimization problems are also the solution of the corresponding unconstrained L1 norm optimization problem. Denote the solution of the corresponding unconstrained L1 norm optimization problem as x1⁎=arg⁡minx⁡‖x‖1. Here, a tradeoff exists between ‖x‖1 and ε, where the tradeoff depends on the value of ε.
To address this tradeoff issue, it was suggested to reformulate the optimization problem as an unconstrained optimization problem with the objective function being the sum of ‖x‖1 and a weight coefficient multiplying to ‖Ax−y‖∞ [8]. It is worth noting that this reformulated optimization problem can be represented by a linear programming problem. Hence, many efficient algorithms such as the simplex method can be applied for finding the solution of this reformulated optimization problem. However, the obtained solution of this reformulated optimization problem depends on the choice of the weight coefficient. Using different values of the weight coefficient will result to different obtained solutions. Theoretically, the minimum value of the weight coefficient that the corresponding obtained solution satisfies the required constraint should be chosen. However, finding this minimum value of the weight coefficient is challenging.
Since the tradeoff depends on the value of ε, it is important to determine the value of ε in Problem (P1). In other words, it is required to investigate the relationship between ‖x‖1 and ‖Ax−y‖∞. Although the empirical relationship between ‖x‖1 and ‖Ax−y‖2 [9] as well as that between the L2 norm objective functional values and the L∞ norm constraint specifications [10] have been recently investigated, the existing results are not applied to the study on the relationship between ‖x‖1 and ‖Ax−y‖∞. In fact, Problem (P1) can be represented by a linear programming problem. Assume that the solutions of the linear programming problems exist. However, as there is no analytical form for the solution of the linear programming problem, deriving the relationship between ‖x‖1 and ‖Ax−y‖∞ is challenging. This paper aims to develop an efficient method for deriving this relationship.
The outline of this paper is as follows. The relationship between ‖x‖1 and ‖Ax−y‖∞ is presented in Section 2. Computer numerical results on the random data are illustrated in Section 3. Finally, a conclusion is drawn in Section 4.
