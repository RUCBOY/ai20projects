Sequence labeling, named SL, is one of pattern recognition task in the filed of natural language processing (NLP) and machine learning (ML), which aims to assign a categorical label to each element of a sequence of observed values, such as part-of-speech (POS) tagging [24], chunking [22] and named entity recognition (NER) [17] and etc. It plays a pivotal role in natural language understanding (NLU) and significantly beneficial for a variety of downstream applications, e.g., syntactic parsing, relation extraction and entity coreference resolution and etc.
Conventional sequence labeling approaches are usually on the basis of classical machine learning technologies, such as Hidden Markov Models (HMM) [3] and Conditional Random Fields (CRF) [16], which heavily rely on hand-crafted features (e.g., with/without capitalized word) or language-specific resources (e.g., gazetteers), making it difficult to apply them to new language-related tasks or domains. With advances in deep learning, many research efforts have been dedicated to enhancing SL [24] by automatically extracting features via different types of neural networks (NNs), where various characteristics of word information are encoded in distributed representations for inputs [7] and the sentence-level context representations are learned when end-to-end training.
Recently, Recurrent Neural Network (RNN) together with its variants, e.g., long short-term memory (LSTM) or gated recurrent unit (GRU), have shown great success in modeling sequential data [46]. Therefore, many researches have devoted to research on RNN based architectures for SL, such as BiLSTM-CNN [5], LSTM-CRF [15], [19], LSTM-CNN-CRF [24] and etc. Despite superior performance achieved, these models have limitations under the fact that RNNs recursively compose each word with its previous hidden state encoded with the entire history information, but the latent independent relations between each pair of words are not well managed. The sequential way to process the inputs only focuses on modeling the long-range successive context dependencies, while neglecting the discrete context patterns.2
Discrete context dependency plays a significant role in sequence labeling tasks. Generally, for a given word, its label not only depends on its own semantic information and neighbor contexts, but may also rely on the separate word information within the same sequences, which would significantly affect the accuracy of labeling. Without loss of generality, we take the part-of-speech (POS) tagging task as example, as shown in Fig. 1, the part-of-speech tag of word “Industries” in Sentence-1 primarily depends on word “it”, and thus should label with NNP, which refers to singular proper noun. However, if such discrete context dependency is not well modeled, “Industries” may tend to be labeled with plural proper noun (NNPS) mistakenly, since a word ending with “-s” and more so “-es” is more likely labeled with NNPS. Similar to Sentence-2, assigning the part-of-speech tag list item marker (LS) to word “B” should take account of word “A” and “C”, where these three constitute a list. Therefore, it is essential to selectively choose the contexts that have strong impacts on the tag of the given word.Download : Download high-res image (78KB)Download : Download full-size imageFig. 1. Example: the impacts of discrete context dependencies.
Many works demonstrate that self-attention is capable of effectively improving the performance of several NLP tasks such as machine translation, reading comprehension and semantic role labeling. This inspires us to introduce self-attention to explicitly model position-aware contexts of a given sequence. Although encoding absolute positions of the input sequence with attentions [43] has been proven the effectiveness, compared with injecting absolute position embedding into the initial representations, it is more intuitive to incorporate the positional information in a relative manner. Recently, Shaw et al. [34] present an alternative approach to take the account of the relative distance between sequence elements for representation. Nevertheless, their approaches only consider the relative position information independent of the sequence tokens while neglecting the interaction with the input representations. Hence, how to effectively exploit the position information with attentions for better modeling the context dependency is still an open problem.
In this paper, we propose a novel RNN neural architecture for sequence labeling tasks, which employs self-attention to implicitly encode position information to provide complementary context information on the basis of Bi-LSTM. Additionally, we further propose an extension of standard additive self-attention mechanism (named position-aware self-attention, PSA) to model the discrete context dependencies of the input sequence. Differ from previous works, PSA maintains a variable-length memory to explore position information in a more flexible manner for tackling the above mentioned problem. That is, it jointly exploits three different positional bias, i.e., self-disabled mask bias, distance-aware Gaussian bias and token-specific position bias, to induce the latent independent relations among tokens, which can effectively model the discrete context dependencies of given sequence. Additionally, we also develop a well-designed self-attentional context fusion layer with feature-wise gating mechanism to dynamically select useful information about discrete context dependency and also address the self-disabled mask bias problem. Specifically, it learns a parameter λ to adaptively combine the input and the output of the position-aware self-attention and then generate the context-aware representations of each token. The extensive experiments conducted on four classical benchmark datasets within the domain of sequence labeling, i.e., the CoNLL 2003 NER, the WSJ portion of the Penn Treebank POS tagging, the CoNLL 2000 chunking and the OntoNotes 5.0 English NER, demonstrate that our proposed model achieves a significant improvement over the state-of-the-arts. The main contributions of this work are as follows.
•We identify the problem of modeling discrete context dependencies in sequence labeling tasks.•We propose a novel position-aware self-attention to incorporate three different positional factors for exploring the relative position information among tokens; and also develop a well-designed self-attentional context fusion with feature-wise gating mechanism to provide complementary context information on the basis of Bi-LSTM for better modeling the discrete context dependencies over tokens.•Extensive experiments on part-of-speech (POS) tagging, named entity recognition (NER) and phrase chunking tasks verify the effectiveness of our proposed model.
Roadmap. The remaining of the paper is organized as follows. In Section 2, we review the related work, and in Section 3 we presents a background on sequence labeling tasks, as well as a Bi-LSTM-CRF baseline model, followed with the proposed position-aware self-attention mechanism and self-attentional context fusion layer in Section 4. Section 5 presents the quantitative results on benchmark datasets, also includes an in-depth analysis, case study and wraps up discussion over the obtained results. Finally, Section 6 concludes the paper.
