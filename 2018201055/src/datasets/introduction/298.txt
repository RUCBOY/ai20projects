Nowadays, we are seeing a constant growth of scholarlyknowledge, making the access to scholarly contents more and more challenging through traditional search methods. This problem has been partially solved thanks to digital libraries which provide scientists with tools to explore research papers and to monitor research topics. Nevertheless, the dissemination of scientific information is mainly document-based and mining contents requires human manual intervention, thus limiting chances to spread knowledge and its automatic processing [1].
Despite the large number and variety of tools and services available today for exploring scholarly data, current support is still very limited in the context of sensemaking tasks that require a comprehensive and accurate representation of the entities within a domain and their semantic relationships. This raises the need of more flexible and fine-grained scholarly data representations that can be used within technological infrastructures for the production of insights and knowledge out of the data [2], [3], [4]. Kitano [5] proposed a similar and more ambitious vision, suggesting the development of an artificial intelligence system able to make major scientific discoveries in biomedical sciences and win a Nobel Prize.
Among the existing representations, knowledge graphs i.e., large networks of entities and relationships, usually expressed as RDF triples, relevant to a specific domain or an organization [6], provide a great method to organize information in a structured way. They already have been successfully used to understand complex processes in various domains such as social networks ego-nets [7] and biological functions [8].
Tasks like question answering, summarization, and decision support have already benefited from these structured representations. The generation of knowledge graphs from unstructured source of data is today key for data science and researchers across various disciplines (e.g., Natural Language Processing (NLP), Information Extraction, Machine Learning, and so on.) have been mobilized to design and implement methodologies to build them. State-of-the-art projects such as DBPedia [9], Google Knowledge Graph, BabelNet1 , and YAGO2  build Knowledge Graphs by harvesting entities and relations from textual resources (e.g., Wikipedia pages). The creation of such knowledge graphs is a complex process that typically requires the extraction and integration of various information from structured and unstructured sources.
Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. Some examples are Open Academic Graph3 , Scholarlydata.org [10], Microsoft Academic Graph4  [11] (MAG), Scopus5 , Semantic Scholar6 , Aminer [12], Core [13], OpenCitations [14], and Dimensions7 . These resources provide substantial benefits to researchers, companies, and policy makers by powering data-driven services for navigating, analysing, and making sense of research dynamics. However, the current generation of knowledge graphs lacks an explicit representation of research knowledge discussed in the scientific papers. This is usually only described by not machine-readable metadata, such as natural language text in the title and abstract, and in some cases a list of topics or keywords from a domain vocabulary or taxonomy (e.g., MeSH8 , ACM Digital Library9 , PhySH10 , CSO11 ). These data are useful to some degree, but do not offer a formal description of the nature and the relationships of relevant research entities. For instance this representation does not give us any information about what “sentiment analysis” is and how it interlinks with other entities in the research domain. It would be much more useful to know that this is a sub-task of Natural Language Processing that aims at detecting the polarity of users opinion by applying a range of machine learning approaches on reviews and social media data such as twitter posts.
A robust and formal representation of the content of scientific publications that types and interlinks research entities would enable many advanced tasks that are not supported by the current generation of systems. For instance, it would allow to formulate complex semantic queries about research knowledge such as “return all approaches and benchmarks that are used to detect fake news”. It would also support tools for the exploration of research knowledge by allowing users to navigate the different semantic links and retrieve all publications associated with specific claims. It could also enable a new generation of academic recommendation systems and tools for hypothesis generation.
The Semantic Web community has been working for a while on the generation of machine-readable representations of research, by fostering the Semantic Publishing paradigm [15], creating bibliographic repositories in the Linked Data Cloud [16], generating knowledge bases of biological data [17], formalizing research workflows [18], implementing systems for managing nano-publications [19], [20] and micropublications [21], and developing a variety of ontologies to describe scholarly data, e.g., SWRC12 , BIBO13 , BiDO14 , FABIO15 , SPAR16  [22], CSO17  [23], and SKGO18  [24]. Some recent solutions, such as RASH19  [25], and the Open Research Knowledge Graph20  [26] highlighted the advantages of describing research papers in a structured manner. However, the resulting knowledge bases still need to be manually populated by domain experts, which is a time consuming and expensive process. We still lack systems able to extract knowledge from large collection of research publications and automatically generate a comprehensive representation of research concepts.
It follows that a significant open challenge in this domain regards the automatic generation of scientific knowledge graphs that contain an explicit representation of the knowledge presented in scientific publications [26], and describe entities such as approaches, claims, applications, data, results reported in each paper. The resulting knowledge base would be able to support a new generation of content-aware services for exploring the research environment at a much more granular level.
Most of the relevant information for populating such a knowledge graph might be derived from existing textual elements of research publications. To such an aim, in the last years, we assisted to the emergence of several excellent Machine Learning and NLP tools for entity linking and relationship extraction [26], [27], [28], [29], [30]. However, integrating the output of these tools in a coherent and comprehensive knowledge graph is still an open issue.
For instance, different tools may use different lexicalresources, named-entity recognition approaches, and training sets and thus will often label the same entities with different names and disagree on the relation between them.
In this paper, we present a novel architecture that uses an ensemble of NLP and Machine Learning methods for extracting entities and relationships in form of triples from research publications, and then integrates them in a knowledge graph using Semantic Web best practices. The main hypothesis behind this work is that an hybrid framework combining both supervised and unsupervised methods will produce the most comprehensive set of triples (i.e., high recall) while still yielding a good precision.
Within our work, we refer to an entity as a statement that indicates an object (e.g., a topic, a tool name, a well-known algorithm, etc.). We create a relation between two entities when they are syntactically or semantically connected. As an example, if a tool T employs an algorithm A, we may build the triple 〈T, employ, A〉. We compared our approach versus alternative methods on a manually annotated gold standard covering the Semantic Web domain.
The main contributions of the research presented in this paper are therefore the following:

•we propose an architecture that combines various tools for extracting entities and relations from research publications;•we employ Semantic Web best practices, statistics, NLP, and Machine Learning techniques for integrating these entities and triples;•we show the advantage of an hybrid approach versus methods that are only focused on supervised classification (e.g., Luan Yi et al. in [30]) or NLP tools (e.g., OpenIE);•we carry out an evaluation of the resulting triples in terms of precision, recall, and F-measure;•we generated a gold standard of manually annotated triples that can be used as benchmark for this task.
In this paper we focus on the Semantic Web as main domain, but the resulting approach is general and can be applied to any other domain. The code of the framework, the extracted triples, and the gold standard used in the evaluation are available through a GitHub repository21 .
The remainder of this paper is organized as follows. Section 2 formalizes the problem we addressed. The proposed methodology is detailed in Section 3. The evaluation and its discussion are reported in Section 4. Section 5 discusses the related work and highlights the main differences with the proposed approach. Finally, Section 6 concludes the paper, explains limitations that still exist, and defines future research works.
