Real world objects are meaningful only at a certain scale. You might see an apple perfectly on a table. But if looking at the earth, then it simply does not exist. This multi-scale nature of objects is quite common in nature. Scale-space theory is a framework for early visual operations with complementary motivations from physics and biological vision, which has been developed by the computer vision community to handle the multi-scale nature of image data [1]. It is a formal theory for handling visual structures at different scales, by embedding the original image into a one-parameter family of derived images, in which fine-scale structures are successively suppressed. Scale-space representation has a wide application in computer vision. For example, the scale-invariant feature transform (SIFT) [2], a successful hand-crafted feature in computer vision to detect and describe local features in images, includes an important stage of key localization, which is defined as minima and maxima of the result of difference of Gaussians (DoG) function applied in scale space to a series of resampled and smoothed images.
Nowadays deep learning methods have achieved great success in many tasks, owing to its ability to learn features from raw pixels. Taking advantage of scale-space representation, deep multi-scale methods have succeeded in many practical applications of computer vision, such as breast cancer diagnosis [3], video prediction [4], depth map prediction [5], [6], image generation [7], image dehazing [8] and scene deblurring [9]. The major idea is to exploit a coarse-to-fine framework to break the generation or reconstruction into successive refinements, which is capable of maintaining the global structure and the local details at the same time. This strategy can handle the inherently blurry problem while dealing with image generation and signal reconstruction tasks. For example, Petersen et al. propose multiscale denoising autoencoders [3] for scoring breast density from mammograms, which helps separate dense from fat tissue and lets Cumulus [10] compute the ratio between the dense tissue area and the total breast area. Cumulus scoring has repeatedly been shown to be a strong and independent predictor of breast cancer.
Despite achieving good performances in many computer vision tasks, deep multi-scale methods still need further study in terms of, for example, unsupervised representation learning. It is worth while to explore new methods leveraging scale-space representation for unsupervised representation learning. First, learning feature representations at multiple scales can make learning system robust to the unknown scale variations that may occur. There is a basic observation that real-world objects are composed of different structures at different scales. This implies that real-world objects may appear in different ways depending on the scale of observation. Second, unsupervised methods do not rely on any labeled data. Training deep models in a supervised way needs millions of semantically-labeled images which cost lots of manual work. Collecting large labeled datasets is very difficult, and there are diminishing returns of making the dataset larger and larger. Thus unsupervised representation learning has drawn lots of attention for quick access to arbitrary amounts of data, despite its performance is still limited so far. According to our knowledge, there is a deep multi-scale approach, named Laplacian Pyramid of Adversarial Networks (LAPGAN) [7], that can fulfill the task of representation learning without any labeled data. However, LAPGAN is originally designed to generate realistic images, and it did not provide any methodological description and experimental evaluation on representation learning.
In this paper, we propose a multi-scale learning framework, Laplacian pyramid auto-encoders (LPAE), for unsupervised representation learning. LPAE is different with the traditional auto-encoder [11] that tries to reconstruct its own inputs. It uses multi-path encoding–decoding networks to reconstruct the Gaussian pyramid from the Laplacian pyramid, which is a sequence of difference images between two levels of the Gaussian pyramid. According to the structural characteristic of Laplacian pyramid, we design a hierarchical encoding strategy based on the reverse procedure of Laplacian pyramid generation, that can recover the original image exactly by expanding then summing all the levels of the Laplacian pyramid. Each path consists of two parts, an encoding network which outputs a hidden representation encoding partial levels of Laplacian pyramid and a decoding network which attempts to generate the corresponding image of Gaussian pyramid from the hidden representation. LPAE is capable of encoding visual details at specific scales by taking advantage of Laplacian pyramid, which leads to improvements on efficiency and effectiveness of representation learning. The contributions of this work are summarized as following:

1.We propose a novel multi-scale framework that uses multi-path encoding–decoding networks within a Laplacian pyramid framework to reconstruct the original image and the low pass filtered images of Gaussian pyramid.2.The proposed framework is capable of handling visual details at specific scales, which make the obtained feature representations robust to the unknown scale variations.3.Our experimental results prove that the trained model learns semantically meaningful representations, outperforming many baselines of unsupervised methods for representation learning on several datasets. Especially, we evaluate LPAE as generic feature extractors, which is more direct than transfer learning to prove the effectiveness of unsupervised methods for representation learning.4.Our experimental results prove that LPAE is more appropriate than LAPGAN to leverage Laplacian pyramid for unsupervised representation learning due to more stable training procedure.
The obtained features can be used for many computer vision applications, such as image search engine, image organization and image summarization. For example, social images uploaded by internet users on Facebook are accumulating very fast, and it is very attractive to annotate these images with correct attributes for knowledge graph construction, which is important for user profile. Feature is basic for these recognition tasks. Compared with single scale methods, multi-scale methods aim to learn rich hierarchical feature representations for encoding the signal of the visual data. This makes them especially attractive for problems in visual analysis.
The rest of the paper is organized as follows. In section 2, we discuss related works. Section 3 describes the proposed approach. Evaluation of the proposed approach is presented in Section 4. Finally, conclusions are presented and future research is discussed.
