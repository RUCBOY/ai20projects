Peer review is a decentralised, distributed collaboration process through which experts scrutinise the quality, rigour and novelty of research findings submitted by peers before publication. The interaction between all figures involved, including editors, referees and authors, helps to filter out work that is poorly done or unimportant. At the same time, by stimulating a constructive dialogue between experts, this process contributes to improve research (e.g., Casnici, Grimaldo, Gilbert, & Squazzoni, 2016; Righi & Takács, 2017). This is crucial for journals, science and knowledge development, but also for academic institutions, as scientist reputation and career are largely determined by journal publications ([Bornmann and Williams, 2017], [Fyfe, 2015], [Squazzoni and Gandelli, 2012], [Squazzoni and Gandelli, 2013]).
Under the imperatives of the dominant “publish or perish” academic culture, journal editors are called everyday to make delicate decisions about manuscripts, which not only affect the perception of quality of their journals but also contribute to set research standards (Petersen, Hattke, & Vogel, 2017; Siler, Lee, & Beroc, 2015) by promoting certain discoveries and methods while rejecting others (Bornmann & Daniel, 2009; Lin, Hou, & Wu, 2016; Resnik & Elmore, 2016). With the help of referees, they have to make the whole editorial process as effective as possible without falling into the trap of cognitive, institutional or subjective biases, mostly hidden and even implicit (Birukou et al., 2011; Casnici, Grimaldo, Gilbert, Dondio, & Squazzoni, 2017; Lee, Sugimoto, Zhang, & Cronin, 2013; Teele & Thelen, 2017).
Unfortunately, although largely debated and always under the spotlight, peer review and the editorial process have been rarely examined empirically and quantitatively with in-depth, across-journal data (Batagelj, Ferligoj, & Squazzoni, 2017; Bornmann, 2011). While editorial bias has been investigated in specific contexts (e.g., [Hsiehchen and Espinoza, 2016], [Moustafa, 2015]), the role of bias due to hidden connections between authors, referees and editors, which are determined by their reputation and position in the community network, has been examined rarely empirically (García, Rodriguez-Sánchez, & Fdez-Valdivia, 2015; [Grimaldo and Paolucci, 2013], [Squazzoni and Gandelli, 2013]). A noteworthy exception has been Sarigöl, Garcia, Scholtes, and Schweitzer (2017) who recently examined more than 100,000 articles published in PLoS ONE between 2007 and 2015 to understand whether co-authorship relations between authors and the handling editor affected the manuscript handling time. Their results showed that editors handled submissions co-authored by previous collaborators significantly more often than expected at random, and that such prior co-author relations were significantly related to faster manuscript handling. In these cases, editorial decisions were sped up on average by 19 days. However, this analysis could not look at whole editorial process, including rejections and referee selection, and could not disentangle editorial bias from submission authors’ strategies of editors’ targeting.
Our study aims to fill this gap by presenting a comprehensive analysis of eight years of the editorial process in four computer science journals. First, these journals were comparable in terms of scope and thematic areas, so providing an interesting picture of a community and its network structure. Secondly, we looked at the whole editorial process, with a particular focus on editorial decisions and referee recommendations from all submissions. While we did not have data on all characteristics of authors and submissions and so could not develop intrinsic estimates of a manuscript's quality, we used network data to trace the potential effect of authors’ centrality on editorial decisions once checked for the effect of review scores. In this respect, our study offers a method to examine editorial bias without in-depth and complete data, which are rarely available from journals (Squazzoni, Grimaldo, & Marusic, 2017). More substantively, our analysis revealed that although these four journals were in the same field, their editorial processes were journal-specific, e.g., influenced by rejection rates and impact factor. Secondly and more interestingly, our findings show that more reputed authors were less penalised by editors when they did not presumably submit brilliant work.
The rest of the paper is organised as follows. Section 2 presents our dataset, including data anonymisation procedures and the construction of our main variables. Section 3 presents our findings, including a Bayesian network model that allowed us to disentangle bias throughout the editorial process, while Section 4 discusses the main limitations of our study and suggests future developments.
