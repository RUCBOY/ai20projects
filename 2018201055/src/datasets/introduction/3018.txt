The strategy of Divide-and-Conquer (D&C) is one of the frequently used programming patterns in computer science, which is commonly utilized to design efficient algorithms. The basic idea behind the D&C paradigm is to first recursively divide an original problem into several sub-problems, until those sub-problems become simple enough to be solved directly, and then the desired solution to the original problem is obtained by combining all the solutions to those sub-problems.
In computer science, the D&C pattern is usually used to deal with large-scale problems. When solving large-scale problems using the D&C pattern, another common programming strategy, parallelism, is also exploited to improve the computational efficiency. That is, the D&C algorithms are implemented in parallel under various parallel computing environments to enhance the computational efficiency.
For example, Horowitz and Zorat [1] theoretically analyzed the potential usage of parallel D&C algorithms, and pointed out that the D&C algorithms would be even more efficient if run on an appropriately designed multiprocessor than those designed in sequential. Atallah et al. [2] presented several general techniques for solving problems efficiently using the parallel D&C paradigm. Wu and Kung [3] specifically studied the relationship between parallel computation cost and communication cost for performing D&C computations on a parallel system of P processors.
Achatz and Schulte [4] presented transformation rules to parallelize D&C algorithms over power lists. Those rules converted the parallel control structure of D&C into a sequential control flow, thus making the implicit massive data parallelism in a D&C scheme explicit. Sreenivas et al. [5] first discussed various techniques for parallel D&C in detail and then extended these techniques to handle efficiently disk-resident data, and also provided a generic technique for parallel out-of-core D&C problems. Mateos et al. [6] described EasyFJP, an approach to simplify the parallelization of D&C sequential Java applications.
Recently, Hijma et al. [7] investigated the feasibility of automatically inserting sync statements to relieve programmers of the burden of thinking about synchronization. Chou et al. [8] proposed an energy and performance efficient Dynamic Voltage and Frequency Scaling (DVFS) scheduling scheme to deal with the inherent load-imbalanced issue in irregular parallel D&C algorithms.
Most of the above introduced efforts attempt to parallelize various D&C algorithms on shared-memory or distributed-memory parallel computers by using the existing programming interfaces such as HPF [9] and MPI [10], [11].
However, several of other research efforts were conducted to design and implement compilers or frameworks to parallelize the D&C algorithms such as that introduced in [12], Satin [13], and DAMPVM/DAC [14]. Satin is a system for running D&C programs on distributed memory systems by extending Java with three simple Cilk-like primitives for D&C programming [13]. The DAMPVM/DAC is implemented on top of DAMPVM and capable of providing automatic partitioning of irregular D&C applications at runtime [14].
With the emerging of programmable Graphics Processing Unit (GPU) [15], [16], some research efforts have also been conducted to implement various D&C algorithms in parallel by exploiting the massively computing capability of modern GPUs. For example, Vomel et al. [17] described several techniques for accelerating D&C algorithms on GPU-CPU heterogeneous architectures, and presented an example on how to develop efficient numerical software on heterogeneous architectures.
Among those efforts, probably the most valuable ones are the parallelization of D&C sorting algorithms [18] and scan algorithm [19] on the GPU. Other efforts were also carried out to parallelize the D&C mesh generation [20], [21], [22], convex hull calculation [23], [24], [25], multibody system dynamics [26], [27], etc.
In literature [28], Tzeng and Owens specifically developed a generic paradigm for parallelizing D&C algorithms on the GPU, and applied the proposed paradigm to implement the famous convex hull algorithm, QuickHull [29], in parallel on the GPU. Tzeng and Owens's paradigm is generic, quite effective, and easy to follow to parallelize D&C algorithms on the GPU.
In this paper, by following the generic paradigm proposed by Tzeng and Owens [28], we provide a new and publicly available GPU implementation of the famous QuickHull algorithm to give a sample for demonstrating the parallelization of D&C algorithms on the GPU.
It should be noted that: the basic ideas behind our sample GPU implementation are derived from Tzeng and Owens's paradigm. However, our sample GPU implementation of the famous QuickHull algorithm is different from the one developed by Tzeng and Owens.
The major difference between our sample GPU implementation and the one developed by Tzeng and Owens is that: Tzeng and Owens strongly utilized the efficient parallel primitives such as parallel sort, scan, and reduction provided by the library CUDPP [30], while in contrast we heavily take the advantages of those efficient parallel primitives offered by the library Thrust [31].
The reason why we use the library Thrust rather than the CUDPP is that: the Thrust is much easier to be utilized than the CUDPP since Thrust has been integrated into the GPU programming model CUDA (Compute Unified Device Architecture). In other words, when parallelizing D&C algorithms on the GPU with CUDA, it is quite convenient and easy for users to use those parallel primitives in Thrust; and thus, fewer efforts are needed.
Our contribution in this paper can be summarized as follows. We develop a Sample GPU implementation of a classical D&C algorithm (i.e., Quickhull) by following Tzeng and Owens's paradigm. We hope this will be helpful for interested readers to develop their own efficient GPU implementations of D&C algorithms with fewer efforts and in less time, for example, to develop efficient D&C algorithms in mesh generation [20], [21], [22], convex hull calculation [23], [24], [25], multibody system dynamics [26], [27], etc.
The rest of this paper is organized as follows. Section 2 briefly reviews Tzeng and Owens's paradigm for parallelizing D&C algorithms on the GPU. Section 3 introduces the basic ideas and details of our sample GPU implementation of the classical QuickHull algorithm. The experimental results and analysis are presented in Section 4. The present sample GPU implementation is briefly discussed in Section 5. Finally, Section 6 concludes our work.
