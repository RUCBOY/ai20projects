Over the past few decades, data collection methodologies have expanded to include computerized electronic devices (Wilcox, Gallagher, Boden-Albata, & Bakken, 2012). This is especially true for the collection of self-report survey research, with increasingly large numbers of traditionally paper-and-pencil questionnaires being moved onto computers and the Internet (Buchanan, 2007). Given that self-report surveys are one of the most important ways of gathering data in the social sciences (Berends, 2006), it is necessary to understand both the degree of comparability of responses received through paper-and-pencil and computer data collection methods as well as what might lead to consequential differences.
Mean score comparability across conditions is considered an essential component of equivalence for paper-and-pencil and computerized assessments (American Psychological Association, 1986, as cited in Green, 1991). The equivalence of means and variances fundamentally addresses the norms of measures and the extent to which these norms can be applied across data collection methodologies (Preckel & Thiemann, 2003). Although scores for specific measures can be transformed to account for differences between conditions (Van de Vijver & Harsveld, 1994), this does not address the issue of whether there are systematic differences between paper-and-pencil and computer conditions that result in such changes needing to be made (i.e., moderating variables).
Due to the importance of assessing mean score comparability, many individual studies have sought to examine such equivalence across various computer and paper-and-pencil conditions. However, results have been inconclusive, with some studies suggesting mean scores were essentially comparable based on nonsignificant findings and/or small effect sizes (e.g., Campbell, Cumming, & Hughes, 2006; Hirai, Vernon, Clum, & Skidmore, 2011; Vinney, Grade, & Connor, 2012; Whitaker, 2007; Whitehead, 2011) and others indicating significant differences (e.g., AbuAlRub, 2006; Barak & Cohen, 2002; Dolnicar, Laessar, & Matus, 2009; Saunders, 2012; Vallejo, Mañanes, Comeche, & Díaz, 2008). There are two main potential reasons for such findings. First, almost all studies suffer from methodological issues, such as sampling biases and procedural differences between conditions (see Weigold, Weigold, & Russell, 2013, for a review). Second, in order to determine comparability, most studies used traditional null hypothesis significance testing (NHST) procedures, which are not suitable for assessing equivalence. Instead, equivalence testing is needed to establish if any group differences can be considered trivial (Rogers, Howard, & Vessey, 1993; Rusticus & Lovato, 2011 ). Consequently, we sought to address these concerns by conducting meta-analyses examining the comparability of mean scores for self-report surveys completed using paper-and-pencil and computer data collection methods that 1) included a broad range of self-report surveys, 2) examined a large number of moderators potentially accounting for differences in effect sizes related to the methodological problems in the existing literature, and 3) correctly examined comparability by employing the recommended equivalence confidence interval procedure (Rogers et al., 1993; Rusticus & Lovato, 2011; Weigold et al., 2013).
1.1. Methodological proceduresSampling biases can take different forms, with two of the most prevalent being the use of different samples depending on the condition and allowing participant self-selection to conditions. These can also interact with procedural differences between conditions, such as participants in different groups completing the questionnaires in different settings (see Weigold et al., 2013). Although some studies have specifically examined the influence of sampling and procedural differences between conditions, such as study location and the presence of experimenters (e.g., Cronk & West, 2002; Evans, Garcia, Garcia, & Baron, 2003), these studies also yielded inconclusive findings that are potentially due to some of the same issues they are trying to examine.Given the disparate results in the literature, a number of meta-analyses have been conducted in an attempt to consolidate the literature's findings for specific types of self-report measures: social desirability or self-disclosure (Dodou & de Winter 2014; Dwight & Feigelson, 2000; Richman, Kiesler, Weisband, & Drasgow, 1999), patient-reported outcomes (Gwaltney, Shields, & Shiffman, 2008), psychopathology (Finger & Ones, 1999), and questionnaires generally including sensitive information (Feigelson & Dwight, 2000; Tourangeau & Yan, 2007). Most of these meta-analyses assessed mean effect sizes, although one reported the average difference between means as a percentage (Gwaltney et al., 2008). The number of potential moderators assessed per study ranged from zero (Feigelson & Dwight, 2000; Finger & Ones, 1999; Tourangeau & Yan, 2007) to ten (Dodou & de Winter 2014).Mean effect sizes differed across the meta-analyses, ranging from −0.14 (Finger & Ones, 1999) to 0.22 (Feigelson & Dwight, 2000), with positive numbers indicating higher scores in the computer condition. Several authors reported that there were meaningful and/or statistically significant differences across paper-and-pencil and computer conditions in at least one of the meta-analyses they conducted (Dwight & Feigelson, 2000; Feigelson & Dwight, 2000), whereas others concluded that the overall effect sizes were practically and/or statistically nonsignificant (Dodou & de Winter 2014; Finger & Ones, 1999; Gwaltney et al., 2008; Richman et al., 1999; Tourangeau & Yan, 2007).The types of moderators assessed differed across studies, making it difficult to determine consistent effects for different meta-analyses. Even the most commonly-examined moderator, year of publication, did not show consistent results across studies. Additionally, the meta-analyses that included moderators generally found significant changes to the effect sizes based on at least one of the moderators assessed, although the directions were inconsistent. For example, the overall effect size for Richman et al.’s (1999) meta-analysis of social desirability distortion decreased from d = 0.01 to d = −0.39 when moderators were included, suggesting that those in the computer condition used less distortion. Conversely, for measures that inferred, but did not directly assess, social desirability distortion, the same authors found that the overall effect size of d = 0.06 rose to d = 0.46 when accounting for moderators, indicating higher levels of distortion in the computer condition. Year of publication was a significant moderator in the former analysis, with larger effect sizes for earlier studies than for later studies, although it was not a significant moderator in the latter analysis.Overall, similar to the individual empirical studies in the field, the meta-analyses examining the comparability of self-report surveys completed using paper-and-pencil and the computer have yielded inconclusive results. There are several potential reasons for this. First, the definition of significance was inconsistent, with some authors calling attention to meaningful trends when statistical significance was not found (e.g., Dodou & de Winter 2014; Tourangeau & Yan, 2007). Additionally, all meta-analyses were limited to a specific type of self-report measure, which might have influenced which moderators were impactful. Third, only one meta-analysis examined the inclusion of both between-groups and within-subjects studies in the same analysis (Dodou & de Winter 2014). Although this was not a significant moderator in their study, mixing designs can overlook potential systematic issues that may be present in the different designs (Borenstein, Hedges, Higgins, & Rothstein, 2009). Finally, all meta-analyses used traditional NHST statistical procedures to assess comparability across paper-and-pencil and computer conditions, although such procedures cannot assess equivalence (Rogers et al., 1993).
1.2. Statistical proceduresThe form of equivalence that is best applied to assessing mean score comparability is quantitative equivalence. Originally defined by Van de Vijver and Poortinga (1991), Preckel and Thiemann (2003) provide a concise definition of the term: “[Q]uantitative equivalence mainly addresses the question whether the norm data of one test version can be applied to the other test version” (p. 132). The statistics involved in determining quantitative equivalence are somewhat esoteric in the social sciences. Consequently, most researchers have incorrectly used some form of traditional NHST assessing for significant differences across groups, with nonsignificant results having been interpreted as indicating comparability (Tryon, 2001). However, traditional NHSTs can only indicate if groups are statistically different, not if they are similar enough to be considered comparable (Rusticus & Lovato, 2011). Several suggestions have appeared in the literature to circumvent documented issues, such as the addition of effect sizes and confidence intervals around mean score differences (Cumming, Fidler, Kalinowski, & Lai, 2012), although these by themselves do not provide sufficient evidence of equivalence (Rogers et al., 1993).Correct statistical procedures for equivalence testing have been used for several decades in drug bioequivalence testing to determine the comparability of proprietary and generic drugs (e.g., Stegner, Bostrom, & Greenfield, 1996; Westlake, 1976) and were introduced to the social sciences in the 1990s (Rogers et al., 1993). Although these are also NHSTs, their purpose is to test for significant similarities across conditions. To be considered equivalent, the differences between two or more conditions must be small enough that they are deemed trivial (Rusticus & Lovato, 2011). An examination of comparability such as this is especially important for the many fields in which small differences between conditions tend to occur naturally, such as comparisons across paper-and-pencil and computer data collection methodologies (see Whitehead, 2011). A practical overview and example of the use of equivalence testing can be found in Weigold and colleagues’ (2013) article on the subject, and a discussion including equivalence testing in meta-analyses can be found in Rogers et al. (1993).Several individual studies have assessed the comparability of means across paper-and-pencil and computer conditions using correct equivalence testing procedures (e.g., Eckford & Barnett, 2016; Epstein, Klinkenberg, Wiley, & McKinley, 2001; Lewis, Watson, & White, 2009; Weigold, Weigold, Drakeford, Dykema, & Smith, 2016; Weigold et al., 2013). For example, Lewis et al. (2009) compared a group of college students completing paper-and-pencil questionnaires with an Internet-based computer sample using measures of attitudes and intentions toward drinking and driving based on different advertisements. The authors used Schuirmann's t-test (Schuirmann, 1987) to assess mean comparability and found that the groups were equivalent for all pretest and several of the posttest measures, although paper-and-pencil means were higher for two of the posttest measures. A corresponding traditional NHST 2 × 2 × 2 ANOVA indicated no significant effects. Overall, results generally indicated comparability between the conditions, with differences for two measures. Additionally, Weigold et al. (2016) collected responses to measures assessing the five-factor personality model, social desirability, and computer self-efficacy in an older adult sample. Participants completed the study in a lab setting using either paper-and-pencil or a computer. They assessed equivalence using the confidence interval method, which yields the same results as Schuirmann's t-test (Rogers et al., 1993). The confidence interval method indicated comparability for approximately half of the scales.In summary, the few studies that have used correct statistical procedures to compare surveys completed via paper-and-pencil and the computer have generally been consistent in indicating comparability, with a few differences having been obtained across questionnaires, populations, and data collection methodologies. The differences found between some of the mean equivalence results and corresponding traditional NHSTs support the assertion that traditional NHST alone is not suitable for testing equivalence (Lewis et al., 2009; see Rogers et al., 1993; Tryon, 2001). Additionally, no meta-analyses have been conducted using equivalence testing on this topic, although some authors have interpreted interesting trends that did not reach significance (e.g., Dodou & de Winter 2014; Tourangeau & Yan, 2007). Overall, these findings underscore both the necessity to use equivalence testing in additional future individual empirical studies, as well as the need to start using it in meta-analyses.
1.3. The current studyThe current study sought to address three goals. The first was to determine the overall comparability of mean scores across paper-and-pencil and computer conditions by conducting comprehensive meta-analyses. This means that the current meta-analyses are not limited to a specific type of measure, which makes this the sole comprehensive meta-analytic study on the topic. The second goal was to assess the influence of various methodological issues present in both the previous individual studies (i.e., sampling biases, procedural differences between conditions) and meta-analyses (i.e., specific types of measures) by examining them as moderators. The third goal was to use correct statistical procedures (i.e., equivalence testing) to determine the comparability of paper-and-pencil and computer conditions, which have not previously been used in meta-analyses examining mean score equivalence despite recommendations to do so (see Rogers et al., 1993).To achieve these three goals, we conducted two meta-analyses examining the mean score equivalence of paper-and-pencil and computer conditions across a wide range of studies and types of self-report measures using equivalence testing and a large number of moderators based on issues in the existing literature. One meta-analysis examined mean score comparability for between-groups design studies, and the other did so for within-subjects design studies. Although it is possible to combine the designs into one meta-analysis when certain conditions are met (Morris & DeShon, 2002), the possible systemic issues across designs (Borenstein et al., 2009) has seldom been examined in the context of paper-and-pencil and computer mean score meta-analyses.For both meta-analyses, we hypothesized that the overall effect size would be equivalent. However, we expected there would be significant heterogeneity across effect sizes due to the extensive variability in samples and methodological procedures used across studies. We also selected a large list of potential moderators to assess in the case of significant heterogeneity.
