In line with the increasingly key role of technology in all areas of higher education, computer-based (CB) assessment is becoming more and more common in most university disciplines (Newman, Couturier, & Scurry, 2010). In a similar fashion, many international language testing bodies now routinely use computers for various areas of Academic English writing assessment. In a study to compare ESL writers’ performances on pen-and-paper and computer-delivered tests, Lee (2002) noted that test takers now believed a computer test to be more authentic and valid in relation to the target ESL contexts.
The International English Language Test System (IELTS) test, which is one of the most widely used tests of English language proficiency for educational, professional, and migration purposes, does not currently offer computer-based options. However, it seems more than likely, given the increased authenticity and other perceived benefits of CB testing, that in the near future IELTS will need to move towards offering computer-based options alongside traditional paper-and-pencil (PB) modes. In preparation for a possible move towards the CB assessment of IELTS, research was conducted some years ago to investigate differences between the CB and PB testing of IELTS writing (Wise & Plake, 1989). Although that research is still of relevance, in the intervening years students’ increased familiarity with computers in both learning and assessment, as well as developments in test delivery technology, necessitate a fresh look at the questions of equivalence the study raised.
McDonald (2002) identified two fundamental types of equivalence which need to be examined when a pencil-and-paper writing test is offered alongside a computer delivered version and the two versions continue to co-exist side by side. The first, score equivalence, relates to the results of the test takers’ performance and the concern is whether the scores obtained between the two modes are statistically equivalent and interchangeable. While score equivalence is often considered the most important issue in the delivery mode equivalence research, Mead and Drasgow (1993), who conducted a widely referenced meta-analysis of 159 correlations between paper-based and computer-based scores on writing tests, note that one should not assume that test takers use the same writing processes under different delivery conditions, especially when time-constraints are imposed. A second type of equivalence that needs to be examined, therefore, relates to the underlying construct that is being measured. Given that writing is a cognitively complex and socially situated activity, it is clearly impossible to achieve complete equivalence between the two conditions. However, in the context of direct writing assessment, it is essential to establish that the constructs operationalised by the tests are equally comparable between the two modes and in addition match as far as possible what students are expected to do in the target language use (TLU) domain (Bachman and Palmer, 1996).
Some research has been conducted to examine the cognitive processes of writers completing IELTS writing tasks (Yu, Rea-Dickins, & Kiely, 2011 on AWT1), but evidence of the cognitive validity of IELTS writing between the two modes is lacking, as is any comparison of either with the constructs underlying real life writing activities. Our aim is to examine the extent to which the results of computer-based IELTS, as a direct writing assessment, are statistically equivalent and construct valid as compared to the results from the paper-and-pencil IELTS. We also compare writing in both modes to real life writing in a university setting. The findings will contribute to establishing an evidence base of comparability that is a necessary pre-requisite to the introduction of a CB version of the IELTS writing test.
