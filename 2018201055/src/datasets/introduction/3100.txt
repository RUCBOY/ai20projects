Software Testing (ST) can be seen as one of the most important and least known aspects of software development. In fact, it is common that Computer Science (CS) students graduate into industry without knowing how to test a program (Clarke et al., 2014).1 ST is thus one of the dark arts of software development (Myers et al., 2011). Yet, researchers and practitioners have always argued that testing demands a large share of the costs of a software project (Harrold, 2000). For instance, a survey conducted with 1560 senior IT executives and testing leaders from 32 countries revealed that the IT spend allocated to quality assurance and testing was 35% in 2015 (predicted to rise to 40% by 2018) (Capgemini Group et al., 2016).
Another aspect of ST that seems to be overlooked is the following: the mere exposition to its knowledge might help developers produce more correct – and thus, reliable – programs. In fact, there are several ideas in the ST body of knowledge that can produce positive effects in a programmers’ skills. For example, consider the awareness that virtually all programs contain faults (Myers, Sandler, Badgett, 2011, Ammann, Offutt, 2008), a principle taught early in ST courses. Such an idea can instill a healthy skepticism in programmers towards their own code, making them more cautious. Moreover, the formal testing techniques themselves encourage designing programs with diligence. Take, for example, boundary value analysis, a functional testing criterion that requires writing tests for border inputs. Developers exposed to such a strategy can be more attentive about corner cases in their implementations, hence improving the correctness of their code.
Although the effect of testing knowledge on programmers seems to be intuitive, to the best of our knowledge, there is no empirical evidence to support it. In the literature, we can find substantial work on improving ST training in CS programs. For instance, Patterson et al. proposed the integration of testing tools into programming environments (Patterson et al., 2003); Jones has explored the integration of testing into introductory CS courses through testing labs and diverse forms of courseware (Jones, 2001); and Elbaum et al. presented a web-based tutorial to engage students in learning Software Testing strategies (Elbaum et al., 2007). However, to the best of our knowledge, there are no experimental studies into the effects of ST education on the developers’ programming skills per se, in terms of reliability.2
Investigations into this topic are important because recent data shows that computing academic curricula tend to emphasize development at the expense of testing as a formal engineering discipline (Astigarraga, Dow, Lara, Prewitt, Ward, 2010, Clarke, Davis, King, Pava, Jones, 2014, Wong, 2012). In fact, as reported by Astigarraga et al. (2010), the overall academic CS curricula tend to place a heavy emphasis on design and implementation, rather than on quality assurance topics such as ST. On the other hand, even when ST courses are in fact present in curricula, it is unclear the extent to which the techniques that are taught are in fact adopted by the industry (e.g., mutation (Madeyski and Radyk, 2002) and data-flow (Harrold, 2000) testing seem to be rarely put to practice). Empirical evidence showing that ST education might lead to more reliable programming is very important. In particular, it can motivate the creation or maintenance of these courses.
In this paper, we present two investigations related to ST education and software reliability: one involving students and another involving professors. The students investigation comprised a large controlled experiment that evaluated the impact of ST education on reliable programming, when compared to other types of knowledge; while the professors investigation involved a survey to evaluate the level of ST knowledge of instructors that teach introductory programming courses. The mains idea was to check: (1) whether ST education itself can have a significant impact on programming skills toward more correct – and thus reliable – code; and (2) whether CS instructors themselves possess a minimum level of ST knowledge to be able to instill these positive ideas early in their programs.
The students experiment involved a total of 60 senior CS undergraduate students, 8 auxiliary functions in 4 different domains (basic mathematics; array manipulation; string manipulation; and file input/output) with 92 test cases, and a total of 248 implementations. Subjects implemented two different functions before and after learning basic ST concepts and three techniques (functional – or black-box – testing, structural – or white-box – testing, and mutation testing), and the quality of the code produced before and after was compared. Our goal was not to verify how well the techniques were applied afterwards, but how ST knowledge could impact on the subjects’ programming skills in terms of producing more reliable implementations (i.e., we did not measure the quality of the testing code itself, neither which specific techniques were being applied). To evaluate the reliability of the implemented functions in terms of correctness, the produced implementations were executed against systematically developed test sets before and after the training took place. To improve the external validity of our experiment, we included two control groups in the experiment: one with 22 subjects taking a Software Engineering course, and another with eight subjects taking an object-oriented design course. These control groups help improve confidence that the outcomes observed with the ST course are not due to maturation, for instance. A small replication of the treatment group was also conducted to improve confidence in our outcomes.
Our investigation with students provides evidence that ST knowledge can significantly impact on programming skills in terms of reliability. In fact, subjects in the main experiment were more than twice more likely to deliver correct implementations after learning the ST concepts and techniques (150%). Moreover, the correctness of the subjects’ implementations was, on average, 20% higher after the exposition to ST knowledge took place. Interestingly, we noticed that the positive effect is present even when no specific testing technique is explicitly applied, possibly a consequence of the exposure to the testing theory itself. Results of the replication point to the same direction of the main experiment. On the other hand, subjects in the control groups did not perform as well as the ones in the treatment groups: results indicate that their performance in terms of reliability was not significantly affected, even after learning Software Engineering and object-oriented design concepts. Outcomes related to effort also indicate that subjects invest more time in their implementations after taking the ST classes. However, such effort did not result in the production of significantly more lines of application code, implying that the implementations produced afterwards were more reliable but not significantly larger than the ones produced in the first session.
Our investigation with professors, on the other hand, revealed that instructors that teach introductory programming courses lack proper ST knowledge: the mean score in the survey, which was composed of 10 basic ST questions, was only 4.24 out of 10.00. This is problematic because students might not be having the chance of learning concepts that can encourage more reliable programming (as shown in the students’ experiment) early in their academic programs. In particular we noticed instructors lack the important notion of the destructive aspect of ST. For instance, most professors ignore the fact that the most successful test cases are the ones that tend to fail and thus reveal as yet to be discovered faults. As discussed by Myers et al. (2011), these ideas are important for an effective testing practice.
The remainder of this paper is structured as follows. Section 2 presents background knowledge required to understand our study, and Section 3 presents how our study was set up in terms of subjects, experimental design, metrics and statistical procedures. Section 4 presents the results and analysis of our experiment, while Section 5 discusses such results in more details. In the sequence, Section 6 presents our study limitations and Section 7 summarizes related research. Finally, Section 8 concludes the paper.
