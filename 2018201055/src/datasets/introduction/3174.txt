In 2015, Stetson University introduced a Data Analytics interdisciplinary minor for undergraduate students. A new four credit hour course focused on big data analytics was created to serve as an elective for this minor as well as an upper-level elective for computer science majors. This report documents the curriculum, infrastructure, and outcomes of our big data analytics course.
The landscape of big data tools, techniques, and application areas is vast [15]. Popular tools include the software frameworks Hadoop, Spark, and Hive, as well as cloud-based services like Google’s BigQuery. Popular techniques include MapReduce, relational and non-relational data stores, and computations represented as directed acyclic graphs of functions. Sometimes, these tools and techniques include processing overhead that results in slower processing on small data than traditional approaches such as a single-threaded application. Thus, it is important that students understand not only how but also when and when not to use big data technology.
Our course is designed to give students realistic, hands-on practice with big data. The course is project-focused and the projects are organized so that later projects require more sophisticated data processing techniques. Our projects and the parallel and distributed computing topics that they address are listed in Table 1. Each project is described in more detail in the corresponding sections of this report. Our projects may change in the future as the popularity and appropriateness of various technologies change over time.
Students are given access to a “virtual cluster” running on a moderately-sized server as well as cloud computing resources. Each project employs a publicly-available dataset and challenges students to answer simply-stated queries about the data. While the queries may be simple, the steps to extract the answers from the data may be considerably more complex. We emphasize to students that the data processing steps are a means to an end and the ultimate goal of each project is to provide clear, insightful answers to the project’s queries. Students are required to produce a short report documenting these answers with supporting statistical analysis and plots as appropriate. They are expected to explain their findings with language intended for non-experts and to hide all implementation details in an appendix separate from the report.Table 1. Outline of projects in our course, with corresponding sections in this report as well as topics relating to parallel and distributed computing (PDC).SectionProject taskPDC topics3.1Summarize Backblaze dataPerformance analysis3.2Merge and analyze StackExchange datasetsDistributed computing; MapReduce; Hadoop; Hive3.3Visualize NYC taxi tripsCloud computing; BigQuery; Stream processing; Storm3.4Detect and count starsSpark; Threads; GPU computing3.5Capstone project(Variable)
This course is designed for juniors and seniors who have completed prior courses in programming (Java, C++, and Python). Our students have also used Linux in prior courses, and as such the big data course makes extensive use of Linux on-premise and in cloud environments. We do not require that students have a strong statistics background, thus our projects involve only a small degree of statistics such as correlation analysis and hypothesis testing.
Teachers who wish to use some or all of the projects described below should be very comfortable with Linux and learning new processing frameworks written in Java, C++, and/or Python. Typically, the popular frameworks support multiple languages equivalently, so one may use Python, for example, for all projects. Prior experience with Hadoop, Spark, Storm, and OpenCV will help for the projects described.
We used both a virtual cluster and Google’s cloud computing environment. An effective big data course needs some kind of cluster environment, virtual, physical, or cloud-based. A university or college’s IT staff may be able to support these needs. Our virtual cluster technology is made available as a set of Varant and Ansible configuration scripts.1 Additionally, education grants are available from Google, Amazon, and Microsoft for cloud computing. In Rabkin et al.’s [22] experience, such grants are able to cover the computing needs of an entire course.
This course is the only big data analytics course available to our computer science majors. We are not proposing at this time to integrate PDC topics in first- or second-year programming courses, as explored by Bogaerts [2] and Grossman et al. [12], or throughout the curriculum as done by Swarthmore College and reported by Newhall et al. [19], among other institutions. We agree with these other authors that an integrated curriculum has advantages for student learning and skill development. However, we did not have the opportunity at our university to change the curriculum of multiple courses. Instead, we introduced an upper-level course. This report is targeted at educators who are interested in introducing a single course that covers big data analytics.
This report builds off prior works by the author [[8], [9]], which document the same big data analytics course at our institution. Specifically, in the present report, the course projects are described in more detail with learning objectives, “résumé notes” to be adopted by the students for their résumés, and project variations including experimental comparisons of different technology choices to solve the same project. The present report includes an updated experimental evaluation of a virtual cluster environment versus a cloud computing environment. Finally, the present work includes more recent student feedback and outcomes.
Our big data analytics course differs from some other courses that similarly focus on big data processing paradigms and tools rather than low-level methods like MPI. DePratti et al.’s course on big data programming at Eastern Connecticut State University [6] most closely matches our course. Their institution is comparable in size to our own and also considers itself a liberal arts college. The authors determined that the best way to introduce big data tools and techniques to their students was to introduce a single upper-level project-focused course covering Hadoop, MapReduce, and Spark. However, unlike our course, they do not focus on using big data technology to provide insight to non-experts. Their course also requires that students use their own computers to simulate a cluster environment thus limiting the apparent usefulness of big data techniques.
Ngo et al. [20] describe a Hadoop MapReduce module in a broader parallel and distributed computing course. The assignments related to this module focus on developing MapReduce jobs to answer to a query such as “which job in Google Data Center’s system log has the most resubmissions?” Their assignments do not appear to include broader analysis or visualization components. Additionally, their MapReduce module does not cover other big data technologies such as Spark and Hive as we do in our course. Rabkin et al. [22] similarly focus on computation without broader analysis or communicating findings. However, unlike Ngo et al., they include projects and labs that compare the performance characteristics of algorithms implemented in C and Hadoop and run locally and on a cluster. Likewise, in this report we describe alternative implementations for each of our projects and experimentally evaluate the performance of several of these alternatives. We also describe how students may be challenged to experimentally evaluate their intuitions about which data processing techniques are most performant.
Mullen et al. [18] describe a MOOC course that focuses on the technical aspects of building and executing parallel and distributed processing jobs. Like our course, they emphasize practical skill development with programming activities. However, possibly due to the limitations of evaluation in MOOC courses or possibly by design, their course curriculum does not appear to ask students to write reports that summarize their data processing outcomes. Instead, their course focuses on the theory and mechanics of parallel and distributed computing rather than on big data analysis for the sake of providing insight to non-experts.
The rest of this report is organized as follows. Section 2 gives a brief overview of the course curriculum and schedule. Section 3 details five projects that cover a range of parallel and distributed computing tools and techniques and make use of public datasets. We require that students provide a rationalization for their choice of tools and techniques for their capstone project using a “decision matrix” detailed in Section 4. The hardware and software infrastructure used to support our course is detailed in Section 5, including benchmarks comparing on-premise and cloud-based computing resources. Section 6 summarizes outcomes of the course and the students’ anonymous feedback. Finally, Section 7 offers recommendations and concluding remarks.
