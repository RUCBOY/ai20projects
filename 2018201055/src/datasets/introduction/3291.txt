Tree metrics are an especially simple kind of distance function that appear throughout pure and applied mathematics. Informally, tree metrics are derived by breaking the metric space into a tree T of nested subsets F, called folders, and assigning each folder a diameter w(F). The distance dT(x,y) between any two points x and y is then the diameter of the smallest folder containing both points.
There is an extensive theory of harmonic analysis for tree metrics that parallels the classical Euclidean theory. This theory allows us to adapt signal-processing type algorithms to data sets of much more varied structure, and has proven useful in a wide array of problems in machine learning [1], [2], [3]. Tree metrics' simple structure also yields fast algorithms for metric tasks from computer science, such as nearest neighbor searches, the k-server problem, distributed paging, the vehicle routing problem, and many more [4], [5].
Unfortunately, it is rarely the case that the “natural” metric for a given problem in machine learning or computer science will be a tree metric. A basic goal in metric space theory, therefore, is to approximate arbitrary finite metrics by tree metrics. Of course, the extreme simplicity of tree metrics makes it implausible that an arbitrary metric could be well-approximated by a single tree metric. We therefore consider a modified problem, namely finding a probability distribution over tree metrics so that the expected tree distance yields a good approximation, and such that it is computationally feasible to draw a tree from the distribution.
The formal problem, as considered in [6], [4], [5], [7], [8], [9] and elsewhere is as follows. Given a finite metric space (X,d), we seek a family of trees T and corresponding tree metrics dT that have the following properties:
1.Each tree metric is dominating; that is,(1)d(x,y)≤dT(x,y) for every T and for all x,y∈X.2.The expected tree distance satisfies(2)ET[dT(x,y)]≤Kd(x,y) for some constant K≥1.
Bartal's paper [4] describes such an explicit distribution over trees, where the constant K is of size O(log2⁡n) where n denotes the number of points in X; this result was later improved to K=O(log⁡nlog⁡log⁡n) in [5]. With access to such a distribution over trees, many tasks that depend on the original metric can be performed with randomly drawn tree metrics instead, and then combined to produce an approximation to that task for the original metric. Bartal [4], [5] discusses a number of such problems from computer science, while Charikar's paper [10] shows how this method can produce an approximation to the Earth Mover's Distance, a powerful metric between probability distributions widely used in machine learning [11], [12], [13], [14]. We will go into more detail on this particular application in Section 5.
The question that naturally arises is: how small (that is, how close to 1) can we make the constant K from (2)? The paper of Fakcharoenphol, Rao, and Talwar [9] describes a randomized construction of partition trees whose constant of distortion K is of size O(log⁡n). As there are metric spaces for which no family of trees can achieve a distortion smaller than Ω(log⁡n) [4], this result is optimal in the general case.
If n is large, however, a size O(log⁡n) distortion can be too big for practical applications. Indeed, in a statistical or machine learning environment, if X is a data set drawn from a population about which we wish to make inferences, it is critical to be able to handle very large values of n, as well-designed statistical procedures perform better with increasing sample size.
In this paper we show that a broad class of metrics can in fact be approximated by trees with constant of distortion bounded independently of n. These metrics, known as snowflake metrics, are of the form d(x,y)α where 0<α<1 and d(x,y) is itself a metric [15]. We will prove approximation guarantees for two different tree constructions, both of which have appeared previously in the literature, namely in [9] and [16].
More precisely, in Section 3, namely Theorem 1, we prove the following result: for R≥0 and any 0<α<1, the trees defined in [9] and [16] can be used to approximate the snowflake metric d(x,y)α for distances exceeding R with expected distortion bounded independently of the number of points. Rather, the expected distortion depends on the dimension of X at scale R, a quantity that captures the growth of metric balls exceeding radius R (when R is 0, the approximation guarantee holds for all distances).
The proof of Theorem 1 is very simple; it consists of observing that a divergent series of distances multiplied by probabilities becomes convergent when those distances are raised to a power less than 1. Similar observations are important ingredients in proofs of Assouad theorems on embedding snowflake metrics into Euclidean space [15], [17], [18]. However, for certain applications, such as the approximation of Earth Mover's Distance between probability distributions, it is more natural to work with embeddings into trees than into Euclidean space. We will discuss such applications in more detail in Section 5.
In Section 4 we give an algorithm for constructing the trees from [9] whose cost is O(n2), where the constants are universal. In particular, the cost can be bounded independently of other problem parameters, such as the distances d(x,y). The existence of an O(n2) algorithm is stated in [9], though we have not seen it described anywhere in the literature, and it is unclear whether the algorithm referred to in [9] has cost independent of the metric itself.
In Section 5, we illustrate the results of the paper on numerical examples. As mentioned, we also explore how to apply tree approximations to the approximation of the Earth Mover's Distance between probability distributions.
