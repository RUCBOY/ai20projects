The singular value decomposition of a real m×n matrix A is a factorization of the form A=UΣVT, where U is a m×m orthogonal matrix, Σ is a m×n rectangular diagonal matrix with non-negative real numbers on the diagonal, and VT is an n×n orthogonal matrix. The diagonal entries of Σ are known as the singular values of A. The m columns of U are the left-singular vectors of A, while the n columns of V are the right-singular vectors of A. If A is symmetric, the singular values are given by the absolute value of the eigenvalues, and the singular vectors can be expressed in terms of the eigenvectors of A. Here, and in the sequel, whenever we write singular vectors, the reader is free to interpret this as left-singular vectors or right-singular vectors provided the same choice is made throughout the paper.
An important problem in statistics and numerical analysis is to compute the first k singular values and vectors of an m×n matrix A. In particular, the largest few singular values and corresponding singular vectors are typically the most important. Among others, this problem lies at the heart of Principal Component Analysis (PCA), which has a very wide range of applications (for many examples, see [27], [35] and the references therein) and in the closely related low rank approximation procedure often used in theoretical computer science and combinatorics. In application, the dimensions m and n are typically large and k is small, often a fixed constant.
1.1. The perturbation problemA problem of fundamental importance in quantitative science (including pure and applied mathematics, statistics, engineering, and computer science) is to estimate how a small perturbation to the data effects the singular values and singular vectors. This problem has been discussed in virtually every text book on quantitative linear algebra and numerical analysis (see, for instance, [8], [23], [24], [47]), and is the main focus of this paper.We model the problem as follows. Consider a real (deterministic) m×n matrix A with singular valuesσ1≥σ2≥⋯≥σmin⁡{m,n}≥0 and corresponding singular vectors v1,v2,…,vmin⁡{m,n}. We will call A the data matrix. In general, the vector vi is not unique. However, if σi has multiplicity one, then vi is determined up to sign. Instead of A, one often needs to work with A+E, where E represents the perturbation matrix. Letσ1′≥⋯≥σmin⁡{m,n}′≥0 denote the singular values of A+E with corresponding singular vectors v1′,…,vmin⁡{m,n}′. In this paper, we address the following two questions.Question 1When is vi′ a good approximation of vi? Question 2When is σi′ a good approximation of σi?These two questions are classically addressed by the Davis–Kahan–Wedin sine theorem and Weyl's inequality. Let us begin with the first question in the case when i=1. A canonical way (coming from the numerical analysis literature; see for instance [22]) to measure the distance between two unit vectors v and v′ is to look at sin⁡∠(v,v′), where ∠(v,v′) is the angle between v and v′ taken in [0,π/2]. It has been observed by numerical analysts (in the setting where E is deterministic) for quite some time that the key parameter to consider in the bound is the gap (or separation) σ1−σ2′. The first result in this direction is the famous Davis–Kahan sine θ theorem [20] for Hermitian matrices. A version for the singular vectors was proved later by Wedin [57].Throughout the paper, we use ‖M‖ to denote the spectral norm of a matrix M. That is, ‖M‖ is the largest singular value of M.Theorem 3 Davis–Kahan, Wedin; sine theorem; Theorem V.4.4 from [47](1)sin⁡∠(v1,v1′)≤‖E‖σ1−σ2′.In certain cases, such as when E is random, it is more natural to deal with the gap(2)δ:=σ1−σ2, between the first and second singular values of A instead of σ1−σ2′. In this case, Theorem 3 implies the following bound.Theorem 4 Modified sine theoremsin⁡∠(v1,v1′)≤2‖E‖δ.Remark 5Theorem 4 is trivially true when δ≤2‖E‖ since sine is always bounded above by one. In other words, even if the vector v1′ is not uniquely determined, the bound is still true for any choice of v1′. On the other hand, when δ>2‖E‖, the proof of Theorem 4 reveals that the vector v1′ is uniquely determined up to sign.As the next example shows, the bound in Theorem 4 is sharp, up to the constant 2.Example 6Let 0<ε<1/2, and takeA:=(1+ε001−ε),E:=(−εεεε). Then σ1=1+ε, σ2=1−ε with v1=(1,0)T and v2=(0,1)T. Hence, δ=2ε. In addition,A+E=(1εε1), and a simple computation reveals that σ1′=1+ε, σ2′=1−ε but v1′=(1/2,1/2)T and v2′=(1/2,−1/2)T. Thus,sin⁡∠(v1,v1′)=12=‖E‖δ since ‖E‖=2ε.More generally, one can consider approximating the i-th singular vector vi or the space spanned by the first i singular vectors Span{v1,…,vi}. Naturally, in these cases, a version of Theorem 4 requires one to consider the gapsδi:=σi−σi+1; see Theorem 19, Theorem 21 below for details.Question 2 is addressed by Weyl's inequality. In particular, Weyl's perturbation theorem [58] gives the following deterministic bound for the singular values (see [47, Theorem IV.4.11] for a more general perturbation bound due to Mirsky [40]).Theorem 7 Weyl's boundmax1≤i≤min⁡{m,n}⁡|σi−σi′|≤‖E‖.For more discussions concerning general perturbation bounds, we refer the reader to [10], [47] and references therein. We now pause for a moment to prove Theorem 4.Proof of Theorem 4If δ≤2‖E‖, the theorem is trivially true since sine is always bounded above by one. Thus, assume δ>2‖E‖. By Theorem 7, we haveσ1′−σ2′≥δ−2‖E‖>0, and hence the singular vectors v1 and v1′ are uniquely determined up to sign. By another application of Theorem 7, we obtainδ=σ1−σ2≤σ1−σ2′+‖E‖. Rearranging the inequality, we haveσ1−σ2′≥δ−‖E‖≥12δ>0. Therefore, by (1), we conclude thatsin⁡∠(v1,v1′)≤‖E‖σ1−σ2′≤2‖E‖δ, and the proof is complete. □
1.2. The random settingLet us now focus on the matrices A and E. It has become common practice to assume that the perturbation matrix E is random. Furthermore, researchers have observed that data matrices are usually not arbitrary. They often possess certain structural properties. Among these properties, one of the most frequently seen is having low rank (see, for instance, [14], [15], [16], [19], [51] and references therein).The goal in this paper is to show that in this situation, one can significantly improve classical results like Theorem 4, Theorem 7. To give a quick example, let us assume that A and E are n×n matrices and that E is a random Bernoulli matrix, i.e., its entries are independent and identically distributed (iid) random variables that take values ±1 with probability 1/2. It is well known that in this case ‖E‖=(2+o(1))n with high probability3 [7, Chapter 5]. Thus, the above two theorems imply the following. Corollary 8If E is an n×n Bernoulli4 random matrix, then, for any η>0, with probability 1−o(1),max1≤i≤n⁡|σi−σi′|≤(2+η)n, and(3)sin⁡∠(v1,v1′)≤2(2+η)nδ.Among others, this shows that we must have δ>2(2+η)n in order for the bound in (3) to be nontrivial. It turns out that the bounds in Corollary 8 are far from being sharp. Indeed, we present the results of a numerical simulation for A being a n×n matrix of rank 2 when n=400, δ=8, and where E is a random Bernoulli matrix. It is easy to see that for the parameters n=400 and δ=8, Corollary 8 does not give a useful bound (since nδ=2.5>1). However, Fig. 1 shows that, with high probability, sin⁡∠(v1,v1′)≤0.2, which means v1′ approximates v1 with a relatively small error. Our main results attempt to address this inefficiency in the Davis–Kahan–Wedin and Weyl bounds and provide sharper bounds than those given in Corollary 8. As a concrete example, in the case when E is a random Bernoulli matrix, our results imply the following bounds.Download : Download high-res image (361KB)Download : Download full-size imageFig. 1. The cumulative distribution functions of sin⁡∠(v1,v1′) where A is a n × n deterministic matrix with rank 2 (n = 400 for the figure on top and n = 1000 for the one below) and the noise E is a Bernoulli random matrix, evaluated from 400 samples (top figure) and 300 samples (bottom figure). In both figures, the largest singular value of A is taken to be 200.Theorem 9Let E be a n×n Bernoulli random matrix, and let A be a n×n matrix with rank r. For every ε>0 there exists constants C0,δ0>0 (depending only on ε) such that if δ≥δ0 and σ1≥max⁡{n,nδ}, then, with probability at least 1−ε,sin⁡∠(v1,v1′)≤Crδ.Theorem 10Let E be an n×n Bernoulli random matrix, and let A be an n×n matrix with rank r satisfying σ1≥n. For every ε>0, there exists a constant C0>0 (depending only on ε) such that, with probability at least 1−ε,σ1−C≤σ1′≤σ1+Cr.In particular, when the rank r is significantly smaller than n, the bounds in Theorem 9, Theorem 10 are significantly better than those appearing in Corollary 8. The intuition behind Theorem 9, Theorem 10 comes from the following heuristic of the second author. If A has rank r, all actions of A focus on an r dimensional subspace; intuitively then, E must act like an r dimensional random matrix rather than an n dimensional one.This means that the real dimension of the problem is r, not n. While it is clear that one cannot automatically ignore the (rather wild) action of E outside the range of A, this intuition, if true, explains the appearance of the r factor in the bounds of Theorem 9, Theorem 10 instead of the n factor appearing in Corollary 8.While Theorem 9, Theorem 10 are stated only for Bernoulli random matrices E, our main results actually hold under very mild assumptions on A and E. As a matter of fact, in the strongest results, we will not even need the entries of E to be independent.
1.3. Preliminaries: Models of random noiseWe now state the assumptions we require for the random matrix E. While there are many models of random matrices, we can capture almost all natural models by focusing on a common property.Definition 11We say the m×n random matrix E is (C1,c1,γ)-concentrated if for all unit vectors u∈Rm,v∈Rn, and every t>0,(4)P(|uTEv|>t)≤C1exp⁡(−c1tγ).The key parameter is γ. It is easy to verify the following fact, which asserts that the concentration property is closed under addition. Fact 12If E1 is (C1,c1,γ)-concentrated and E2 is (C2,c2,γ)-concentrated, then E3=E1+E2 is (C3,c3,γ)-concentrated for some C3,c3 depending on C1,c1,C2,c2.Furthermore, the concentration property guarantees a bound on ‖E‖. A standard net argument (see Lemma 28) showsFact 13If E is (C1,c1,γ)-concentrated then there are constants C′,c′>0 such that P(‖E‖≥C′n1/γ)≤C1exp⁡(−c′n).For readers not familiar with random matrix theory, let us point out why the concentration property is expected to hold for many natural models. If E is random and v is fixed, then the vector Ev must look random. It is well known that in a high dimensional space, a random isotropic vector, with very high probability, is nearly orthogonal to any fixed vector. Thus, one expects that very likely, the inner product of u and Ev is small. Definition 11 is a way to express this observation quantitatively.It turns out that all random matrices with independent entries satisfying a mild condition have the concentration property. Indeed, if Eij denotes the (i,j)-entry of E and the entries of E are assumed to be independent, then the bilinear formuTEv=∑i=1m∑j=1nuiEijvj is just a sum of independent random variables. If, in addition, the entries of E have mean zero, then, by linearity, uTEv also has mean zero. Hence, (4) can be viewed as a concentration inequality, which expresses how the sum of independent random variables deviates from its mean. With this interpretation in mind, many models of random matrices can be shown to satisfy (4). In particular, Lemma 34 shows that if E is a n×n Bernoulli random matrix, then E is (2,12,2)-concentrated, and ‖E‖≤3n with high probability [53], [54]. However, a convenient feature of the definition is that independence between the entries is not a requirement. For instance, it is easy to show that a random orthogonal matrix satisfies the concentration property. We continue the discussion of the (C1,c1,γ)-concentration property (Definition 11) in Section 6.
