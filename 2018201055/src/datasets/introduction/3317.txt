Imagine you are driving and approach a stoplight. You see eight cars in front of you, all of which are black, and begin to wonder why all of these vehicles are the same color. At the next stoplight you see eight cars all of varying colors and perceive the situation as happenstance, thinking nothing of it. These two events strike us differently because of our intuitions about randomness. The second event seems clearly a result of chance, but the first event seems to suggest another explanation. But where do these intuitions about randomness come from?
One naïve explanation purports that non-random events are simply low-probability events, but multiple studies have confirmed that humans do not judge equally likely events as equally random (Kahneman and Tversky, 1972, Tversky and Kahneman, 1974). One of the classic examples concerns flips of an fair coin: Asked to choose which of the following coin flip sequences of length eight is more likely to occur, HHHHHHHH or HTHTTHTT, most people will choose the latter, despite the fact that each sequence has the same probability of occurring, 128. This intuition is surprisingly strong. Indeed, even trained statisticians would likely be surprised to see a coin turn up heads eight times in a row. Despite knowing that the two coin flip sequences are both equally probable, we still question whether we are actually witnessing random flips of fair coin; it takes cognitive effort to believe the coin is truly unbiased.1
For this reason, randomness has emerged as a central and persistent topic in the cognitive sciences. Many studies (e.g., Falk and Konold, 1997, Lopes and Oden, 1987) suggest that what people usually mean by randomness is the absence of certain detectable patterns. Because this notion is at odds with ideas of randomness used in probability and statistics, people seem to reason poorly about chance. As Nickerson (2002) puts it, “The general conclusion that the results of these experiments in the aggregate seem to support is that people are not very good at these tasks – that they find it hard to generate random sets on request and to distinguish between those that have been produced by random processes and those that have not” (p. 71). Bar-Hillel and Wagenaar (1991) conclude “People either acquire an erroneous concept of randomness, or fail to unlearn it” (p. 448).
But are people really bad at reasoning about chance? Or are they solving a different problem from merely judging the probability of different outcomes under a random process? If subjective randomness is about detecting patterns, then it is intimately related to intelligent action: “random” stimuli provide no help in predicting outcomes (e.g., what information do eight cars of all different colors confer?), whereas “non-random” stimuli aid our thinking (e.g., if you see eight black cars in a row you might infer that a foreign dignitary is visiting your town). Consistent with this idea, several papers have connected subjective randomness to formal frameworks for characterizing the amount of structure in a stimulus, such as algorithmic complexity (Falk and Konold, 1997, Feldman, 2004, Gauvrit et al., 2014, Gauvrit et al., 2016, Griffiths and Tenenbaum, 2003, Griffiths and Tenenbaum, 2004) and Bayesian inference (Griffiths and Tenenbaum, 2001, Hsu et al., 2010, Williams and Griffiths, 2013).
In this paper, we present a formal framework that unifies these previous approaches, demonstrating that subjective randomness can be explained as a form of statistical inference about the process that generated a stimulus. The key challenge for this approach is characterizing the kinds of structure that people might identify in a stimulus – a problem that algorithmic complexity theory solves by considering all regularities generated by short computer programs (e.g., Li & Vitányi, 2008). This notion is too general to capture human subjective randomness judgments, but by reformulating algorithmic complexity as a statistical inference we identify two ways in which it can be adapted: simplifying the kinds of computing machines considered and restricting the set of possible regularities. We use these two approaches to develop models of human randomness judgments for three different kinds of stimuli: sequences of coin flips, binary matrices, and dot patterns in spatial arrays.
