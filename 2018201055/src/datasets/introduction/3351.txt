Increasing number of real-time applications generate vast number of streaming time-series every day. These time-series comprise of data from computer networks, real-time traffic, electrocardiogram (ECG), electroencephalogram (EEG), stock price, and weather information. In contrast to traditional historical time-series, updates over streaming time-series happen in real-time. Such real-time property poses a significant challenge for data mining researchers in designing efficient algorithms for processing streaming time-series. In particular, streaming time-series online is often fast. The fast updates demand for lightweight algorithms that are designed to execute very quickly, using less memory space, and perhaps without any data preprocessing. In recent years, many techniques on the processing of streaming time-series were proposed. These techniques include classification [11], [19], clustering [26], [30], rule discovery [25], motif discovery [29] and subsequence monitoring [4], [20], [23].
Among all the processing techniques on streaming time-series, subsequence monitoring is considered one of the most fundamental methods since it is widely used in a number of real-world applications, such as technical pattern monitoring in stock market trends. A technical pattern is thought to be a special subsequence in the stock time-series, which can be used as signal indicators for the upward/downward direction of the stock price movement. Therefore, monitoring technical patterns in stock markets helps traders and analyzers to predict the forthcoming stock market trends. Given a sequence (a time-series), the objective of subsequence monitoring is to identify the subsequences which are the most similar to the query sequence in real-time. In streaming time-series, the demand on the efficiency of the subsequence monitoring method is significantly higher than other types of time series processing because streaming time-series cannot be preloaded and preprocessed from some static data archive. In 2007, Sakurai et al. [23] proposed a method called SPRING to monitor subsequences in streaming time-series. SPRING is faster than the naive method by five orders of magnitude without sacrificing accuracy. It was reported that the time complexity of SPRING is O(nm), where n is the length of sequence and m is the length of query sequence. However, SPRING does not support normalization which is prerequisite for obtaining meaningful results from some of the data sets [8], [20]. To alleviate this problem, Gong et al. [4] proposed an algorithm called Normalization-supported SPRING (NSPRING) to support normalization while maintaining the temporal and space complexity of SPRING.
However, in multi-subsequence monitoring problems, instead of monitoring just one subsequence, thousands of subsequences are required to be monitored at the same time. In such cases, an algorithm which is much faster than the currently available one would be desirable. For example, a quantitative trading system may need to monitor thousands of stock prices simultaneously for detecting technical patterns. In such circumstances, the system needs to react in seconds or even in milliseconds as stock prices change at any moment. Any delayed reaction may miss buying or selling opportunities. To address these problems, Forward-propagation NSPRING (FPNS) algorithm for multi-subsequence monitoring is proposed in this paper. The proposed method is inspired by the forward propagation mechanism in Artificial Neural Network (ANN). Instead of using propagation for tuning parameters in NN, it is used for indexing necessary calculations in the FPNS thereby excluding all unnecessary calculations to improve the execution time. Specifically, FPNS needs to calculate a n by m matrix. Among the matrix, necessary cells in the matrix are indexed and calculated while all unnecessary ones are ignored. In some extreme scenarios, the time complexity of FPNS is O(nm). In normal cases, FPNS can effectively prune at least half of the calculations.
A similar idea is also used in the method called UCR-DTW proposed by Rakthanmanon et al. [20]. UCR-DTW does not reduce the time complexity of original algorithm (i.e. O(nm2)). However, it aims to accelerate the Dynamic Time Warping (DTW) distance calculation by pruning unnecessary computations. Interestingly, UCR-DTW takes only 34 h to run on the sequence of length trillions (e.g. 1,000,000,000,000). Thus, pruning unnecessary calculations can significantly improve the algorithm’s performance.
In this paper, we first compare the scalability of FPNS with NSPRING and UCR-DTW on synthetic datasets. Next, FPNS, NSPRING and UCR-DTW are tested on the same benchmark datasets from UCR archive [2]. Finally, FPNS, NSPRING and UCR-DTW are compared on UCI repository [12] to validate their capability on multi-variate sequences. From the experiment results, we find that FPNS is three times faster than NSPRING and one order of magnitude faster than UCR-DTW. The time complexity of FPNS and NSPRING are both linear (i.e. O(lnm), where l is the number of query sequence, n is the length of time-series and m is the length of query sequence). For UCR-DTW, we observe that the pruning ability of UCR-DTW depends on the convergence of minimum distance Dmin (Section 4). Specifically, the fast convergence rate of Dmin to a smaller value enables UCR-DTW to reduce as much calculations as possible. However, this property is less useful in multi-subsequence monitoring since Dmin  will converge from the initial state for each query sequence. In other words, the UCR-DTW prefers a query sequence and a time-series of length 1,000,000,000, rather than 1000 query sequences and a sequence of length 1,000,000. Thus, The scalability of UCR-DTW is quadratic based on time complexity O(lnm2).
We review the related work in Section 2. In Section 3, we first introduce the notations and definitions used in this paper. Next, the FPNS algorithm is described in details. Finally, the experiment results are discussed in Section 4 while Section 5 concludes the paper.
