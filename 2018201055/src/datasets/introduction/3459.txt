Understanding semantic similarity among images is the core of a wide range of computer graphics and computer vision applications, especially in image retrieval (Douze et al., 2011). However, it is a particularly challenging task as it reflects how humans perceive images, a task that cannot be inferred by low-level analysis. Supervised learning is a common means of studying such semantic problem, for which, the ground truth of how humans perceive similarity among images is critical.
However, the semantic context of images is often ambiguous as images that can be perceived with emphasis on different attributes (see Fig. 1). One example out of many is the separation of categorization and style (e.g., color, light, scene type, etc..). One can claim that two images are similar due to their categorization and another may find two images of similar categorization different due to their style. Similarities between the images may be measured in multiple attributes, which can be contradictory to each other.
Humans cannot state a consistent meaningful measure of semantic similarity for a large batch of images. Therefore, annotations about semantic similarity collected by crowd queries are qualitative in nature. In addition, they only contain a partial view of a whole dataset. To consolidate the partial and possibly conflicting views, the collected qualitative data is often embedded into a common Euclidean space that hosts all the images, such that the quantitative metric distances among images reflect the aggregate of the qualitative measures as much as possible Kleiman et al. (2016), Tamuz et al. (2011). However, since semantic similarity may reflect various attributes, one embedding space cannot represent well multiple distances among images. A few existing works address such contradictions by disentangling similarities in multiple attribute spaces, based on assuming latent embedding distributions (Amid and Ukkonen, 2015), explicitly specifying attributes (Veit et al., 2017), or learning similarities along with worker and context information (ho Kim et al., 2018).Download : Download high-res image (760KB)Download : Download full-size imageFig. 1. Often the semantics of images are ambiguous. It is unclear whether images in the second row are more similar to the top or bottom row. Images in the top row have similar category to the second row, while images in the bottom row are similar in style (e.g., color, pose, rendering and light).
In this paper, we present an unsupervised method for multi-attribute embedding, which disentangles similarity annotations based on their latent attributes and embeds them into multiple corresponding embedding spaces. The distances in each embedding space well represent object similarity under the corresponding attribute. The task is challenging since it encapsulates a two-fold problem. First, the semantic similarity among images has no clear quantitative measure and thus, it must be deduced from qualitative measures. Second, the attribute that each crowd member relies on is unknown.
To addressed the challenges, we collect qualitative semantic similarities from crowdsourced clustering queries, and embed images into multiple spaces by optimizing an objective function that evaluates the embedded distances with respect to the qualitative similarities. A critical issue in the optimization is to infer which attribute is used in answering a particular query. Thus, each query is associated with an additional variable on top of the unknown coordinates of the embedded elements. The key idea of our approach is to collect and embed qualitative measures in groups. The grouped measures necessarily share the same attributes, which significantly reduces the number of unknown variables.
More specifically, the task we use is designed as classifying a collection of images into clusters (see Fig. 2). This necessarily leads the user to use a single attribute in providing a series of qualitative measures on the collection of images. Each clustering annotation is then converted into a group of T(i,j,θ)-like tuples, where θ indicates whether image Oi is similar to image Oj (θ=1) or not (θ=0), and fed into our embedding optimization. As we shall show, the optimization with tuple groups requires less variables, leading to higher quality embeddings.Download : Download high-res image (960KB)Download : Download full-size imageFig. 2. A collection of images are classified into clusters by crowd users. Different clusterings reflect similarity in different attributes, which may be contradictory to each other. For example, cars may be clustered by either considering the rendering style shown in the leftmost column, or car type as shown in the rightmost column.
Besides, we further explore the usage of multi-attributeembedding in image retrieval by leveraging recent progress in the field of Deep Neural Networks (Krizhevsky et al., 2012). A CNN model is presented to map an image into the multi-attribute embeddings, so that it lies near those of other images containing similar objects in different attribute spaces.
We evaluate our multi-attribute embedding approach on various synthetic and crowdsourcing datasets, and show that it outperforms the state-of-the-art multi-embedding approaches. We also show that our method can support intuitive image retrieval by turning different attributes on and off .
