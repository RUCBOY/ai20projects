In the last few decades, large galaxy surveys have become one of the main research tools in astronomy, in particular, for the study of cosmology. The need for increasing statistical samples and depths have encouraged the design and construction of deeper, wider, and more sensitive surveys. These projects are generating vast amounts of data, bringing astronomy into the realm of “big data”, which increases the challenges associated with cosmological analyses.
One of these projects is the Dark Energy Survey (DES, Flaugher, 2005, DES et al., 2016), a 5-year program to carry out two distinct surveys. The wide-angle survey covers 5000 deg2 of the southern sky in five (grizY) filters to a nominal magnitude limit of ∼24 in most bands. Also, there is a deep survey (i∼26) of about 30 deg2 in four filters (griz) with a well-defined cadence to search for type-Ia Supernovae (SNe Ia) (Kessler et al., 2015). The primary goal of DES is to constrain the nature of dark energy through the combination of four observational probes, namely baryon acoustic oscillations, counts of galaxy clusters, weak gravitational lensing, and determination of distances of SNe. Besides, many other fields of astrophysics benefit from the large dataset generated by the survey, as detailed by DES et al. (2016).
The constraining power of cosmological results provided by DES will strongly depend on the ability to estimate reliable photometric redshifts (photo-z, e.g., Huterer et al., 2004, Ma et al., 2006, Lima and Hu, 2007, Ma and Bernstein, 2008, Hearin et al., 2010, Cunha et al., 2014, Georgakakis et al., 2014). In fact, the computation of accurate photo-zs has been one of the major concerns of the collaboration, which has spurred the implementation and testing of several algorithms. For instance, Sánchez et al. (2014) addressed the performance of several codes when applied to the DES science verification data (SVA1), while Banerji et al. (2015) discussed the effect of using infrared data. More recently, Bonnett et al. (2016) examined the impact of four photo-z algorithms on the conclusions of the first DES cosmological analysis based on weak lensing discussed by Abbott et al. (2016).
Photo-z estimation will only get more challenging for the next DES releases and future photometric surveys. The reason is that we are sampling magnitudes beyond the reach of most spectroscopic surveys and therefore, traditional photo-z validations are not realistic. This issue has inspired the implementation of new ideas in the collaboration, such as the calibration of photo-zs with cross-correlations Newman, 2008, Davis et al., 2017, Gatti et al., 2018, the training and validation of photo-z codes with simulations (data-augmentation) (Hoyle et al., 2015) and validation of photo-zs with multi-band photometric samples (Hoyle et al., 2017). Techniques for assignment and validation of photo-zs for DES are under continuous development.
There are a large number of methods and algorithms available in the literature to compute and validate photo-zs. Thus, it is useful to work in an integrated environment where one can perform repeated tests and compare the results, while keeping the history well documented. Such an environment should provide the necessary hardware and software infrastructure to make feasible the comparison of different methods applied to large datasets.
Besides dealing with big data, another remarkable aspect of current and near-future surveys is a large number of people working collaboratively. The computational methods are developed jointly by groups of people, commonly located in different countries. Therefore, it is useful to share a development environment that organizes software with version control, keeps the history, and ensures it is possible to reproduce results at any time.
Other web-basedinterfaces for astrophysical data mining and analysis are also being developed (e.g., the DAMEWERE environment by Brescia et al., 2014) aiming at the exploitation of large datasets.
The DES collaboration proposed, along with the Data Management system (DESDM,1 Mohr et al., 2012), the creation of a dedicated portal to solve some of the problems associated with the data processing. This concept became the DES Science Portal, hereafter “the Portal”.
During the early days of the DES project, the Portal was conceived as an “end-to-end” (E2E) process where the data flowed through a chain of tasks to prepare science-ready catalogs and perform scientific analyses. Since then the Portal has undergone several implementations for various scientific goals. The complexity of the system has been growing accordingly to accomplish the science demands. Now, there are instances of the Portal at Cerro Tololo Inter-American Observatory (CTIO), at the National Center for Supercomputing Applications (NCSA) and, at the Laboratório Interinstitucional de e-Astronomia (LIneA).2 In this paper, we refer to the instance at LIneA as “the Portal”.
The Portal provides the infrastructure necessary to handle large amounts of data, a common demand in extragalactic astronomy, but also attacks specific needs of the DES science, for instance, creating and applying systematic maps, computing zero-point corrections, performing star–galaxy classification, computing photo-zs and galaxy properties. The Portal generates galaxy samples in the form of pruned lightweight catalogs containing only the columns required by specific science analysis, which may also be integrated into workflows (Fausti Neto et al., 2018).
In this paper, we present, in particular, the capabilities of the Portal to produce photo-zs. It provides an integrated environment where all the steps necessary to compute photo-zs can be carried out in a controlled and consistent way. The automatic provenance, configuration management, and the computing facilities that sustain the Portal allow for a selection of many photo-z algorithms or settings, which would be highly time-consuming without infrastructure such as this. The need for the Portal capabilities will increase as the DES databases grow, and more generally, as we enter an era of big data astronomy.
In Cavuoti et al. (2015), the authors of the PhotoRApToR algorithm discussed the advantage of linking automatically different steps of photo-z calculation. The Portal surpasses PhotoRApToR in the sense that it is “method agnostic”: any photo-z algorithm can be incorporated into the portal framework, which becomes especially interesting when the investigation aims to compare results using different methods.
We present a sequence of tasks that include the preparation of a spectroscopic sample by combining data from different redshift surveys, the creation of training sets, the training and validation procedures for several algorithms, and the computation of photo-zs for large datasets. To show these examples, we used the DES first year data release, referred as Y1A1 Drlica-Wagner et al., 2018, Abbott et al., 2018.
The outline of this paper is as follows. In Section 2 we present the general technical aspects of the Portal. In Section 3 we go deeper in details of the processes related to the production of photo-zs. Still in Section 3 we present a use case of how the Portal can aid to determine reliable photo-zs through examples of runs using data from DES. The data is described in Appendix. Finally, our conclusions and a summary of the paper are presented in Section 4.
Also, we present, attached to this text, a list3 of five videos (V0 to V4), showing examples of live runs, in a guided tour through the photo-z production on the Portal.
