Face-to-face conversations are very rich contexts, in which not only verbal meaning is conveyed but also non-verbal cues are communicated [1], that is, information that is conveyed by gestures, postures, intonation or speech rhythm. These aspects are essential for an efficient process of verbal communication between speaker and listener, and allow one to perceive the emotional state, intentions or personality traits of the other. The ability to convey and to accurately and rapidly decode emotions is fundamental for the success of communication and social interactions [2].
This paper introduces an innovative instrument to assess auditory emotional recognition that can be used both in research and clinical settings, focused on a Tablet. The user interacts with a mobile application to provide feedback about the auditory stimuli. To do so, the participant selects which one of several emotion words (arranged in buttons and set by the expert when defining the study) best characterizes the emotion conveyed by the voice. The participant also classifies the valence, authenticity and intensity of the emotion that was expressed. While developed specifically for the field of auditory emotion recognition, the system can be easily adapted to other domains. Compared with more traditional assessments, this application aims to provide a faster and more dynamic way of assessing vocal emotional recognition in healthy subjects as well as in clinical populations. Moreover, this application incorporates concepts from Context-aware Computing [3], Ambient Intelligence [4] and Behavioral Biometrics [5], providing an innovative and interesting plethora of new variables that will significantly enrich these studies.
1.1. Related workThis multidisciplinary work brings together research from different fields, including computer science (namely human–computer interaction) and psychology. In this section we review some of the related work in these fields. While there are many works in the field of auditory emotion recognition in the field of psychology, these are all from a purely psychological perspective. Similarly, many researchers have studied human–computer interaction (although not so many have studied it with older users). However, this literature review shows that these two fields have never been brought together in the past.From a psychological perspective, auditory emotion recognition refers to the capacity of a listener to infer emotions from sounds in the environment, including the voice. Studies in the last decades have consistently demonstrated differences in the processing of neutral vs. emotional cues. For example, compared to neutral cues, emotional vocal stimuli tend to capture more attention resources (e.g. [[6], [7]]) to be associated with faster responses (e.g. [8]), and to increased arousal ratings (e.g. [[9], [10]]. It is worth noting that neutral and emotional vocal cues are distinguished very rapidly in the brain, with some studies indicating differences already within 50 ms after stimulus onset (e.g. [11]). Further, there is also evidence demonstrating that vocal emotions are more accurately decoded by female than by male listeners (e.g. [[9], [10]]), particularly negative vocal sounds [10], which highlights the need to account for individual differences in emotion perception.Moreover, the existing evidence indicates that emotions conveyed by the voice are perceived categorically (e.g. [12]), and suggests that some emotional categories are more easily identified than others (e.g. [[9], [10], [13]]). For example, vocal surprise and fear are typically associated with low accuracy rates and confused with each other due to their similar acoustic profile (e.g. [10]).When studying auditory emotion recognition, the standard perception paradigm is to instruct listeners to choose which one of several emotion words best characterizes semantically neutral utterances or nonverbal vocalizations uttered by actors portraying different types of emotions [[13], [14]]. In addition, listeners may be asked to classify the sound in several affective dimensions, such as its valence (a continuum ranging from unpleasant to pleasant), arousal (from calm to aroused), and dominance (from controlled to in control) [[5]]. Other important dimensions include the intensity of the emotion that was communicated, as well as its authenticity (genuine vs. acted emotions are processed differently [15]). Common approaches in emotion research involve setting up the experimental trials, as well as controlling for stimulus presentation and timing through software such as Presentation (Neurobehavioral Systems, Inc., Albany, CA, USA) or Superlab (Cedrus, San Pedro, CA). The measures that are often the focus of those studies (e.g. accuracy rates, reaction time) are usually obtained by recording the participants’ responses directly via the software, or by using a paper-and-pencil approach. This typical approach is often time-consuming, prone to errors (e.g. when the results are transferred from the paper to the computer), and dependent on the availability of the software and equipment in the context where behavioral data are required.The other field that supports this research is that of computer science, including disciplines such as Human–Computer Interaction [16]. Human–Computer Interaction seeks to study the relationship between humans and technological devices, albeit the focus of this work on mobile devices with tactile screens. Nonetheless, this discipline aims at the design of interactive computer systems that are efficient and easy to use, to which contribute (among others) task complexity, cognitive skills of the user (especially the temporal aspects of interaction) and physical skills (namely psycho-motor performance).These factors are even more relevant when the users are older people, who are generally more prone to have diminished cognitive and physical skills. When designing ICT for older users, one key issue is to understand the impact of their abilities and restrictions, as the ageing process causes important changes in perceptual and motor skill capabilities. However, the inclusion of older people within the design cycle for information technology is until now limited to aspects such as usability or the graphical aspects of user interface.Some authors investigated how touchscreen devices have affected the usability of interactive consumer products by older adults [17]. This work was conducted with older adults to explore their perceptions of touchscreen interfaces and to understand existing usability issues and barriers to their adoption. The research was conducted with only four participants and each was required to carry out common tasks on mobile phones which they were unfamiliar with. The main conclusion is that older adults find it easier to use touchscreens than more traditional interfaces. In [18], on the other hand, the authors focus on the design implications when developing devices for older people, with a focus on the exponential growth of the elderly population that suffers from age-related disabilities. In their work, the authors provide a set of guidelines in order to achieve accessibility in mobile interfaces for older people.In [19], the authors survey the needs and wishes of the elderly regarding mobile applications and tablets, using a questionnaire. They summarize different methods integrated into a user-centered design approach to develop design concepts for a tablet computer. In [20] the authors also address Human–Computer Interaction considerations in applications and hardware in the domain of smart living for elderly.Most of the research in existing literature adopts fairly similar approaches: they focus on the design needs or on how user experience must be adapted for the elderly. Which is, undoubtedly, also a necessary effort. However, and as this literature review shows, little is known about the psychomotor performance of technological devices for older adults [21]. Focus must be moved from the devices/applications and their development to the elderly users. That is, how does interaction change with age? How is it affected by specific cognitive or physical conditions? These aspects need to be further studied by such multidisciplinary projects.Hence, the current work represents an opportunity to understand the impact of these changes and to characterize (in terms of HCI) the population that will constitute, in the near future, the majority of technology users. To this end, we will follow an approach that this research team has developed to model, in the past, the interaction of people with technological devices. Specifically, we have shown how stress [22] and emotions [23] can be quantified from our interaction with handheld devices. This will allow for the development of the first model describing the older adult’s interaction with technological devices.Summarizing, the following conclusions can be drawn from the literature review conducted:

•
Memory remains plastic even in an older age and can be improved through cognitive training strategies;
•
Some of the existing methods for memory training are non-invasive but are intrusive, and cannot be broadly used;
•
Methods focused on the use of mnemonics may improve memory in specific tasks, but have limited influence on daily activities;
•
Memory training techniques should be personalized and engaging to improve the resulting outcomes;
•
Little is known about the psycho-motor performance of technological devices for older adults.

