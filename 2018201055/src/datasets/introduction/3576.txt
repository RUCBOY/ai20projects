Software is critical to research (Hannay, MacLeod, Singer, Langtangen, Pfahl, Wilson, 2009, Hettrick, Howison, Bullard, 2015, Howison, Deelman, McLennan, Ferreira da Silva, Herbsleb, 2015, Ince, Hatton, Graham-Cumming, 2012, Katz, Choi, Niemeyer, Hetherington, Löffler, Gunter, Idaszak, Brandt, Miller, Gesing, Jones, Weber, Marru, Allen, Penzenstadler, Venters, Davis, Hwang, Todorov, Patra, de Val-Borro, 2016, Katz, Ramnath, 2015, Morin, Urban, Adams, Foster, Sali, Baker, Sliz, 2012, Stewart, Wernert, Wernert, Barnett, Welch, 2013, Wilson, 2006), yet finding software suitable for a given purpose remains surprisingly difficult (Howison, Bullard, 2015, Cannata, Merelli, Altman, 2005, Bourne, White, Dhar, Bonazzi, Couch, Wellington). Few resources exist to help users discover available options or understand the differences between them (White et al., 2014). A recent study (Bauer et al., 2014) of developers at the Internet search company Google underscored the depth of the problem: the authors found the factor “most disruptive to the [software] reuse process” was “difficulties in finding artifacts.” In other words, even developers at Google have difficulty finding software.
Searching the Internet with a general-purpose search engine has previously been reported to be one of the most popular approaches (Samadi, Almaeh, Wolfe, Olding, Isaac, 2004, Umarji, Sim, Lopes, 2008). Despite its popularity, this approach suffers from demonstrable problems. It requires devising appropriate search terms, which can be challenging for someone not already familiar with a given topic or who is not a native English speaker. Web searches also can yield dozens of viable candidates with little direct information about each, requiring the user to follow links and examine individual candidates—a time-consuming and tedious task. Finally, some questions cannot be answered through Web searches without substantial additional effort, such as what are the differences between candidate software tools. Other approaches to finding software, such as looking in the literature or asking on social media, suffer from still other problems such as the potential for incomplete or biased answers. The difficulty of finding software and the lack of better resources brings the potential for duplication of work, reduced scientific reproducibility, and poor return on investment by funding agencies (Cannata, Merelli, Altman, 2005, Council, 2003, Crook, Davison, Plesser, 2013, Poisot, 2015, White, Dhar, Bonazzi, Couch, Wellington, Niemeyer, Smith, Katz, 2016).
One of the first steps to providing more effective resources for finding software is to understand factors that influence how users locate and select software today. However, most prior work on this topic has focused on software developers searching for source code; few studies included nondevelopers or asked how people look for ready-to-run software rather than source code. In addition, prior work has examined the use of search systems to find software, but not other options such as the use of catalogs. A variety of software catalogs exist today (e.g., National Aeronautics and Space Administration, 2016; Allen, Teuben, Nemiroff, Shamir, 2012, Noy, Shah, Whetzel, Dai, Dorf, Griffith, Jonquet, Rubin, Storey, Chute, Musen, 2009; Black Duck Software, Inc., 2016) and automated catalog generation has been explored (e.g., Tian et al., 2009; Kawaguchi, Garg, Matsushita, Inoue, 2004, Ugurel, Krovetz, Giles, 2002, Linares-Vásquez, McMillan, Poshyvanyk, Grechanik, 2014, Linstead, Bajracharya, Ngo, Rigor, Lopes, Baldi, 2009, Ossher, Bajracharya, Linstead, Baldi, Lopes, 2009; Yang and Tu, 2012), but there appears to be no direct study of users’ preferences and use of software catalogs.
In an effort to understand these and other aspects of how people—specifically those in scientific and engineering disciplines—find software, in late 2015 we conducted a survey involving members of numerous mailing lists primarily in the fields of astronomy and systems biology. In this article, we report on five of the research questions addressed by our survey:

RQ1: How do scientists and engineers look for ready-to-run software?RQ2: What criteria do scientists and engineers use when choosing ready-to-run software?RQ3: What information would scientists and engineers like to find in a catalog of software?RQ4: How do software developers in science and engineering look for source code?RQ5: What can prevent software developers in science and engineering from finding suitable source code?
This survey contributes to the body of research on discovery, search and reuse of software by people working in scientific and engineering disciplines. Here, “reuse” is meant broadly and encompasses application reuse, component library reuse or source code reuse (Stolee, Elbaum, Dobos, 2014, Sim, Clarke, Holt, 1998, Gerard, Downs, Marshall, Wolfe, 2007, Frakes, Fox, 1995). The survey results provide insights into people’s current practices and experiences when searching for software in two different situations: looking for ready-to-run application software and looking for software source code. The outcomes also reveal the current role of catalogs in these situations, as well as people’s preferences for information to include in catalogs. Overall, the survey results can inform the future development of improved resources to aid software discovery.
The remainder of this article is divided as follows. In Section 2, we overview related work. In Section 3, we describe our survey design and research methods, while in Section 4, we report our results. We discuss the results, implications, and limitations in Section 5, and conclude with Section 6.
