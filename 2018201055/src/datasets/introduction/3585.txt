Neural-symbolic integration attempts to bridge the gap between two prominent paradigms in artificial intelligence. Symbolic AI, the first of the two, encompasses explicit knowledge representation, logic programming and search-based problem solving techniques which have been responsible for many of the early successes in artificial intelligence such as game playing, automated theorem proving and natural language processing (Hsu, 2002, Robinson and Voronkov, 2001, Winograd, 1972). While the paradigm is still very much alive in expert systems managing and reasoning over vast quantities of symbolic data, it is also at times referred to as “good old-fashioned AI” or GOFAI (Haugeland, 1985), having lost some of its appeal as its limitations have become apparent. Learning from, and finding structure in sets of noisy data is something symbolic AI largely fails at. Unfortunately this means that whole classes of problems which are integral to a common conception of intelligence, such as image and voice recognition, on a general scale currently can hardly be addressed using symbolic AI.1 Also, while (mostly non-monotonic) logic-based cognitive modelling is still being pursued with valuable results, the brittleness of the corresponding models together with their necessary restriction to high-level cognition (leaving out the bigger part of the actual representation and processing apparatus of human cognizers), are clear drawbacks when compared to connectionist or statistical approaches.
The second paradigm is that of machine learning. As the name suggests, it refers to a variety of methods for learning from data, artificial neural networks (ANN) being one prominent group of these methods. Aided by a leap in processing power and available data, machine learning has been credited with most of the more recent accomplishments in AI, from the now commonplace feat of handwriting recognition to self-driving cars and the fully autonomous learning of computer games (Berger and Rumpe, 2012, Mnih et al., 2015, Plamondon and Srihari, 2000). Promising as the paradigm may be, there are areas in which, on its own, it performs very poorly. While the learning of simple logical dependencies from data is achieved with relative ease, the process becomes increasingly difficult when higher order concepts are involved (Garcez et al., 2015). Examples for the latter impasse are numerous, including connectionist systems’ problems with high-level visual analysis taking into account partial occlusion, light source identification, or shadow prediction, or with higher-level inference such as the recognition of intentions of depicted agents. Also, as knowledge is represented in connectionist systems in a distributed fashion that is hard to interpret from an outside perspective, it is usually difficult to provide background knowledge in a format which the machine learning algorithm can use, or to extract learned features from a network for instance for verification purposes. All of these are problems that often become trivial when tackled with a symbolic system.
Much stands to be gained from a unification of the two paradigms that could cancel out their respective weak spots and highlight their strengths. Neural-symbolic integration (Garcez, Broda, & Gabbay, 2002) offers some ideas in how this may be achieved, centering around the concept of the neural-symbolic cycle (see Fig. 1). The cycle contains two reasoning systems. One is symbol-based, utilizing available expert knowledge, and the other is a connectionist system or ANN, which learns from data. The challenge of interfacing these systems is twofold. Coming from the symbolic side, the first task is to find a way of translating the existing symbolic knowledge into the connectionist system, finding a representation that is appropriate for the network. Secondly, one needs to devise methods for extracting the information gained by the connectionist system through learning and convert it back into a clean symbolic format. Equipped with these processes of representation and extraction the system as a whole is capable of incorporating both background knowledge and training data as either become available.Download : Download high-res image (52KB)Download : Download full-size imageFig. 1. A conceptual overview of the neural-symbolic cycle (as introduced in Bader and Hitzler (2005)).
When asked about the feasibility of integrating both paradigms, the human brain and mind serve as prime examples and proof of concept. The brain has a neural structure which operates on the basis of low-level processing of perceptual signals, but cognition also exhibits the capability to efficiently perform abstract reasoning and symbol processing; in fact, processes of the latter type are taken to provide the foundations for thinking, decision-making, and other mental activities (Fodor & Pylyshyn, 1988). It is precisely this seamless coupling between learning and reasoning which is commonly considered the basis for intelligence in humans—see also, e.g., Valiant, 2013, p. 163: “While I do not regard intelligence as a unitary phenomenon, I do believe that the problem of reasoning from learned data is a central aspect of it.”—and, in close analogy, quite plausibly also for the (re-) creation of cognitive capacities up to human-level intelligence in artificial systems.
Returning to the neural-symbolic cycle discussed above, it should be made clear, that the task of constructing such a cycle rapidly increases in difficulty when raising the expressive capacities of the involved systems. There are approaches for fragments of first order logic (Bader et al., 2007, Gust et al., 2007), but most results focus on various propositional logics. Furthermore, extraction algorithms for connectionist systems tend to be intractable. So while the general method of the field can be described in a few pages, the underlying problems are hard and there is still a long way to go before neural-symbolic integration may be applied to state-of-the-art methods of either paradigm.
As one of the currently most prominent and best understood methods, Hölldobler’s and Kalinke’s core method (Hölldobler & Kalinke, 1994) has since been developed as a neural-symbolic system for, among others, propositional modal (d’Avila Garcez, Lamb, & Gabbay, 2007) and covered first order logic programs (Bader et al., 2007). It provides a way of translating logic programs into a type of multilayer perceptron (MLP) which, embedded in the core architecture, computes least models of these programs. In Hölldobler and Kencana Ramli (2009), a variant of the core method for three valued Łukasiewicz logic is presented, and it is suggested to apply the resulting approach to cognitive modelling tasks (see, e.g., Dietz, Hölldobler, & Ragni, 2012 for a subsequent application to Byrne’s suppression task Byrne, 1989). In the discussion of their work, the authors make the claim that the architecture they have used can be modified in such a way, as to allow training via the backpropagation algorithm (Rumelhart, Hinton, & Williams, 1986). If this is in fact the case, when additionally equipped with a rule extraction method, the resulting architecture should allow for a basic form of closure of the neural symbolic cycle.
Expanding upon work started by Harder and Besold in Harder and Besold (in press), in the following we put these ideas into practice by providing a modified core suitable for supervised learning, implementing and executing supervised learning with the backpropagation algorithm and, finally, constructing a rule extraction method. Section 2 gives an overview of the theory and methods underlying this work, after which Section 3 is used for an in-depth documentation of the implemented approach to learning cores and extracting learned rules. We present the empirical results of the corresponding computational experiments in Section 4, followed by a closing discussion and look ahead at future work in Section 5. The proofs corresponding to the theoretical results, together with the pseudo codes of the extraction algorithm, have been relegated to the appendix.
