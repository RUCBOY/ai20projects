A fundamental step for many image processing tasks is the separation of foreground and background. In this paper, we propose a novel framework (depicted in Fig. 1) where several new approaches are presented for pursuing more robust and accurate background subtraction. Because the quality of reference data seriously affects the accuracy of outcomes, we design an ingenious data structure for each pixel1, called Mino Vector (MV), which has a fixed length of 263 and it is functionally divided into two parts: The first part contains subscripts from 1 to 256 for storing the number of corresponding intensity values (0 to 255), and the second part contains statistical information, such as spans, maximum and minimum intensity values, main locations these intensity values located, along with other parameters. We note that MV keeps purified pixel values for each pixel in background scene over time, which is substantially different from conventional methods which preserve a fixed length (i.e., for a time window) of reference data.Download : Download high-res image (360KB)Download : Download full-size imageFig. 1. Diagram of the whole framework for background subtraction modeling. Samples for initialization are firstly gathered into MVs for all pixels. Then the scene is described by DN and DR. With the structured data in MV, optimal bandwidth and adaptive thresholds are calculated. As classifying an incident pixel, if the update condition is met, TUS will be triggered, otherwise classification is performed.
Based on the MV, we for the first time define a dynamic nature (DN) which is referred to the span of the intensity value expanding in each pixel of a scene during a period of time, and then associate each pixel with DN as an attribute. Generally, scenes regarded as dynamic are associated with a certain motion, such as swaying trees, rippling water, etc. In most cases, dynamic areas are only part of the scene while the remaining areas are static. From this point of view, the difference between a dynamic scene and a static scene is that whether there exists at least one dynamic area. With consideration of DN in a dynamic scene, dynamic areas must be composed of pixels with large values of DN, while static areas include pixels with small values of DN (and the same is true for a static scene). This is a process of quantizing the criterion of the subjective judgment and certainly this quantized criterion is still subjective. It implies that the boundaries between dynamic and static areas are relative. For further weakening the effect of the subjective factor, we rank DN values of a scene and generate a second-order attribute, called dynamic rank (DR), which indicates ranks of those DNs. In this way, an area with relatively large DN values in a static scene may own a DR that is the same as a dynamic area in a dynamic scene. Therefore, we assert that there is no need to distinguish between the dynamic scene and the static scene. By this assertion, we model the background scene, whatever dynamic or static, in a unified way in terms of DN and DR.
When samples are categorized into MVs, we use them to study the optimal bandwidth. Too small a value of bandwidth will cause under-smoothing problems such as an arising of spurious features, but too large a selection of the bandwidth may lead to over-smoothing with a certain loss of underlying structures. The best choice is to let the data appeared before determine the value. Those data with a high frequency of appearance should own more weight. The varying kernel density estimation (VKDE) is a method associated with the objective rules which are embodied in the use of sample-point estimator. In this estimator, bandwidth is a function of each data point which is better than a function of estimated data point used in the balloon estimator. In this framework, we choose the sample-point estimator with a Gaussian kernel and resort to a normal distribution for the pilot estimate. When compared with other methods like median absolute deviation (MAD) used by Elgammal et al. [1], our approach significantly improves the accuracy in estimating the probability density so as to the detection result. Furthermore, by using MVs, we can avoid redundant repetitive computation for a higher efficiency.
In practice, choosing thresholds is a key step which determines final results in the background subtraction model. At present, there are two kinds of thresholds: one is in the global-level, and the other in the pixel-level, according to the scope of the action. Elgammal et al. [1] set a global threshold over all images and empirically adjust it to satisfy a certain false positive ratio. It may be a good choice for a stable scene, but evidently it is less flexible. Wang and Dudek [2] report a pixel-level scheme where threshold is bounded within a fixed range. The threshold fluctuates by the activity level of each pixel where the fluctuation is a measure of the noise level. Their work is similar to ours in terms of adaptation, but our method does not involve the use of extra parameters. We firstly calculate the densities of the observed data points which are used to get the optimal bandwidth. Then, these densities are sorted in an increasing order without repetition in every pixel to get an array of thresholds. The initial threshold of each pixel is determined by its DR (as a subscript in the array) which is already calculated before. Note that the length of thresholds is always larger than DR, so that the threshold can be tuned upon thresholds for matching an appropriate one over time.
Besides detection or classification by KDE, another necessary step for pursuing continuous outstanding performance for all background subtraction algorithms is the update process. We draw the inspiration from the popular game Tetris, in which, by rules of the game, the full bottom rows will be cleared and the remaining Minos will drop down to the ground by the gravity. The data structure, MV, filled with reference data is similar to the fallen Minos and we take another strategy to press them into the ground to eliminate the bottom rows. The effect of this operation also reduces the number of normal intensity values but does not reduce their capacities of accepting a new comer which belongs to them. Our method is robust to noises which always appear randomly within the intensity range (0 to 255). Benefiting by the idea of preserving reference data mentioned above, we can achieve a good speed in recalculating bandwidths and thresholds after the update procedure is performed.
In summary, this novel framework mainly includes four contributions: (1) we first propose the definition of dynamic nature (DN) and its sub-attribute dynamic rank (DR) for filling up the gap between dynamic and static scenes. As a result, we can describe and model all scenes in a unified way in terms of DN and DR. Then, we design the Mino Vector (MV) to serve the whole framework; (2) we adopt a varying KDE with a Gaussian kernel and choose normal distribution as the pilot estimate. This approach yields improvements in estimation accuracy; (3) we present a pixel-wise threshold which has a high adaptability, controlled by each pixel's own DR; (4) we put forward a novel update mechanism called Tetris update scheme (TUS), which results in performance improvements in suppressing the noise and enhancing the robustness. Our experimental results demonstrate that this novel framework achieves competitive results by comparing with the existing state-of-the-art approaches.
