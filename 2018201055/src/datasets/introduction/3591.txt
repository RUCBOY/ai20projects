Extreme-scale scientific simulations and experiments can generate much more data than can be stored and analyzed efficiently at a single site. For example, a single trillion-particle simulation with the Hardware/Hybrid Accelerated Cosmology Code (HACC) [1] generates 20 PiB of raw data (500 snapshots, each 40 TiB), which is more than petascale systems such as the Mira system at the Argonne Leadership Computing Facility (ALCF) and the Blue Waters system at the National Center for Supercomputing Applications (NCSA) can store in their file systems. Moreover, as scientific instruments are optimized for specific objectives, both the computational infrastructure and the codes become more specialized as we reach the end of Moore’s law. For example, one version of the HACC is optimized for the Mira supercomputer, on which it can scale to millions of cores, while the Blue Waters supercomputer is an excellent system for data analysis, because of its large memory (1.5 PiB) and 4000+ GPU accelerators. To demonstrate how we overcame the storage limitations and enabled the coordinated use of these two specialized systems, we conducted a pipelined remote analysis of HACC simulation data, as shown in Fig. 1.
Our demonstration at the NCSA booth of SC16 (the International Conference for High Performance Computing, Networking, Storage, and Analysis in 2016), a state-of-the-art, 29-billion-particle cosmology simulation combining high spatial and temporal resolution in a large cosmological volume was performed on Mira at ALCF. As this simulation ran, the Globus [2] transfer service was used to transmit simulation data to NCSA each of 500 temporal snapshots as it was produced [3]. In total, this workflow moved 1 PiB in 24 h from the ALCF to NCSA, requiring an average end-to-end rate of ∼93 Gb/s. In this paper, we describe how we achieved this feat, including the experiments performed to gather insights on tuning parameters, data organization and the lessons we learned from the demonstration.Download : Download high-res image (313KB)Download : Download full-size imageFig. 1. Pipelined execution of a cosmology workflow that involves a mix of streaming scenarios, ranging from sustained ∼100 Gb/s over a 24-hour period to high-bandwidth bursts during interactive analysis sessions.
As snapshots arrived at NCSA, a first level of data analysis and visualization was performed using the GPU partition of Blue Waters. We note that the analysis tasks have to be carried out sequentially: information from the previous time snapshot is captured for the analysis of the next time snapshot in order to enable detailed tracking of the evolution of structures. The workflow system therefore was carefully designed to resubmit any unsuccessful analysis job and to wait for an analysis job to finish before starting the next one. The output data (half the size of the input data) was then sent to the NCSA booth at SC16 to allow access to and sharing of the resulting data from remote sites. The whole experiment was orchestrated by the Swift parallel scripting language [4]. In previous simulations, scientists were able to analyze only ∼100 snapshots because of infrastructure limitations.
This experiment achieved two objectives never accomplished before: (1) running a state-of-the-art cosmology simulation and analyzing all snapshots (currently only one in every five or 10 snapshots is stored or communicated); and (2) combining two different types of systems (simulation on Mira and data analytics on Blue Waters) that are geographically distributed and belong to different administrative domains to run an extreme-scale simulation and analyze the output in a pipelined fashion.
The work presented here is also unique in two other respects. First, while many previous studies have varied transfer parameters such as concurrency and parallelism in order to improve data transfer performance [[5], [6], [7]], we also demonstrate the value of varying the file size used for data transfer, which provides additional flexibility for optimization. Second, we demonstrate these methods in the context of dedicated data transfer nodes and a 100 Gb/s network circuit.
The rest of the paper is organized as follows. In Section 2 we introduce the science case and the environment in which we performed these transfers. In Section 3 we describe the tests to find the optimal transfer parameters. In Section 4 we summarize the performance of the transfers during the pipelined simulation and analysis experiments, and we describe our experiences with checksum-enabled transfers. Based on the demo at SC16, we propose in Section 5 an analytical model to identify the optimal file size and show that it can help improve the performance of checksum-enabled transfers significantly. In Section 6 we review related work, and in Section 7 we summarize our work.
