As computers increasingly rely on various forms of parallelism to obtain high performance, the design and performance optimization of large-scale numerical algorithms must focus on the efficient exploitation of parallel architectures. Unfortunately, simple algorithms frequently become remarkably complex when implemented in parallel and the most efficient parallel algorithm may not be the one that is most intuitive.
In this paper, we investigate a non-intuitive approach to solving the sparse triangular systems that arise when using incomplete factorization preconditioners with an iterative method for solving sparse linear systems. The conventional method of solving triangular systems is to use forward or backward substitution and this can be parallelized using level scheduling [[2], [21], [26], [33], [38]]. Here we investigate using an iterative method—in particular, Jacobi relaxation. The use of an iterative method is feasible when an approximate solution is acceptable, as in the case of preconditioning. The approach is less applicable to the case of solving with factors from a direct (not incomplete) factorization where the factors are much less sparse and typically an exact solve is desired.
In recent previous work, this approach demonstrated significant reductions in total solution time for the preconditioned conjugate gradient (PCG) method on highly parallel architectures such as Intel Xeon Phi co-processors and graphics processing units (GPUs) [[3], [10]], even though additional PCG iterations may be required to achieve the requested accuracy. The improved speed for each triangular solve is because, for some problems, particularly with high levels of fill in the triangular factors, level scheduling is unable to reveal sufficient parallelism to fully exploit the GPU hardware. On the other hand, Jacobi relaxation, which primarily relies on the sparse matrix vector product (SpMV) operation to compute a residual vector, is highly parallel and can exploit the substantial efforts that have been invested in optimizing SpMV on various parallel architectures.
Although the iterative triangular solve approach can result in significant speedups on highly parallel architectures, the approach does not work on all problems. It is possible for the iterations on the triangular systems to converge too slowly and thus be uncompetitive with the conventional level-scheduled approach. The goal of this paper is to investigate the range of applicability of using iterative triangular solves for incomplete factorization preconditioning by testing it with a large set of sparse symmetric positive definite linear systems. Further, we introduce the idea of using block Jacobi iterations,1 which improves the robustness of the iterative approach. Our hypothesis is that for matrices that model many types of physical systems, especially partial differential equations, an iterative approach to solving the systems involving its triangular factors can be effective. In particular, although these matrices may not be diagonally dominant and may be ill-conditioned, many have relatively large diagonal entries compared to the off-diagonal entries. If the incomplete factorizations of these matrices are stable, they are also likely to have relatively large diagonal entries. Systems with such matrices can typically be solved efficiently by iterative methods. If convergence is poor, a block diagonal scaling can improve diagonal dominance.
Alternatives to sparse triangular solves for incomplete factorization preconditioning have been proposed before. One major alternative is to compute and use sparse approximate inverses of the incomplete factors, so that preconditioning reduces to SpMV operations [[9], [37]]. Related to this is representing the inverse of a sparse triangular matrix as the product of sparse triangular factors [[1], [30]]. Another possible approach is to use Neumann series approximations to the inverse of the incomplete factors [[9], [36]]. Preconditioning is again reduced to a sequence of SpMV operations. This approach was found to be potentially competitive with other techniques for nonsymmetric problems but it lacks robustness, while for symmetric problems a number of more efficient alternatives are available [9]. The Neumann series technique is the same as the Jacobi relaxation approach investigated here if a diagonal scaling is first applied to the system in the former. Recently, based on the encouraging results in [10] for using Jacobi iterations to solve triangular systems, Huckle and coauthors [5] have suggested the use of stationary iterations for solving sparse triangular systems based on sparse approximate inverses.
In the symmetric case, the amount of parallelism in level-scheduled sparse triangular solves can be increased by using multicolor reordering of the rows and columns of the original matrix and computing the incomplete Cholesky factors of this reordered matrix [24]. However, multicolor reorderings generally give poorer PCG convergence results compared to other orderings such as the band reducing RCM ordering [12] or profile reducing Sloan ordering [34] (see, for example, [[8], [14], [15], [17], [18], [29]]). For some “easy” problems, the convergence rate may be degraded by as much as 60–100% but this can be compensated for by the additional parallelism in the solves. For harder problems, however, multicolor reorderings may result in no convergence.
The rest of this paper is organized as follows. In Section 2, we describe the use of Jacobi and block Jacobi relaxation for the iterative solution of triangular systems. In particular, various blocking strategies are reviewed. Section 3 presents our experimental study and demonstrates the potential effectiveness of Jacobi and block Jacobi solves using a large set of test problems. Concluding remarks are made in Section 4.
