Learning how to pronounce Mandarin properly presents a number of challenges. Non-native Mandarin learners must practice sounds and tones that do not exist in their own language or are typically different from “European/Western” languages. Proper instruction is needed for Mandarin learning. Traditional second language learning methods depend largely upon printed text, audio and video materials (Kim and Gilman, 2008). Recent developments in teaching Mandarin visual speech through animated talking heads provide an appropriate means of facilitating language learning (Chen, Massaro, 2011, Liu, Yan, Wang, Wu, Ng, 2013).
Animated talking heads have been applied to computer aided language learning (CALL) as carriers of audio-visual speech (Hazan, Sennema, Iba, Faulkner, 2005, Wang, Chen, Li, Meng, 2012), and they have been expected to enhance the computer aided language learning system by instructing the learners with visualized pronunciation animations. In particular, both internal and external articulator movements have been studied in a three-dimensional talking head to refine instruction for hearing-loss children or second-language learners (Badin, Tarabalka, Elisei, Bailly, 2010, Gibert, Leung, Stevens, 2015, Grauwinkel, Dewitt, Fagel, 2007, Wang, Chen, Li, Meng, 2012).
Multimodal presentation conditions promote effective learning because humans process information through both visual and verbal channels (Mayer, 2009, Sweller, van Merriënboer, Paas, 1998). It is promising that a three-dimensional talking head-embedded computer-aided language learning system could promote learning through multimodal interaction. The three-dimensional talking head provides language learners audio-visual and face-to-face instruction. Although several studies have focused on the implementation of a three-dimensional talking head, few experiments have been performed to evaluate the effectiveness of the talking head on pronunciation learning (Theobald et al., 2008). There is a lack of comprehensive understanding of whether a three-dimensional talking head can efficiently enhance language learning and effectively instruct language learners.
1.1. Articulation in MandarinMandarin is the standard Chinese spoken across most of northern and southwestern China. There are 900 million Chinese native speakers, which is greater than the number of native speakers of any other language in the world (Lewis et al., 2015). Each Mandarin character is spoken as one syllable, consisting of an initial and a final, encoded by a tone (Chen et al., 2013). Mandarin has a total of 21 initials and 39 finals that can be combined together to create more than 400 sounds. Most existing studies on visual speech have been performed on “European/Western” languages, particularly English (Chen and Massaro, 2011). Compared with English, Mandarin has some typical characteristics. For example, some Mandarin sounds do not typically appear in English, including the initials of blade-palatal and lingua-palatal, and the finals of close-mouth and round-mouth (Chen and Massaro, 2011). Moreover, the Mandarin supradental, blade-alveolar, and lingua-palatal are produced primarily with the tongue tip in constrast with those English sounds produced primarily with the tongue blade (Lee and Zee, 2003). Moreover, the perception and production of lexical tones are particularly difficult when learning Mandarin (Chen, Shivakumar, Harikumar, Ma, Li, 2013, Chiu, Lia, Külls, Mixdorff, Chen, 2009). Chen and Massaro (2011) suggested that studying and applying Mandarin visual speech information presents contributions to segmental and tonal aspects of visual speech.Many studies on Mandarin visual speech have been conducted from the perspectives of computer science and computer engineering (Chen, v. Spinko, Shi, 2005, Pei, Zha, 2006, Pei, Zha, 2007, Wang, Cai, Ai, Wu, Zhang, Cai, Meng, 2006, Zhou, Wang, 2007). However, very few evaluation works that specifically attempt to study the language-learning effectiveness of Mandarin visual speech have been published. In a study that evaluated synthetic and natural Mandarin visual speech, Chen and Massaro (2011) compared participants’ visual speech perception responses and then improved the quality of the synthetic Mandarin consonants, vowels, and whole syllables conveyed by an animated talking head. To date, there remains a lack of evaluation of language learners’ production of Mandarin initials, finals and tones after learning with a three-dimensional talking head.
1.2. Language learning with talking headsAudio-visual articulatory instructions conveyed by talking heads are beneficial to language learning (Engwall, 2008, Fagel, Madany, 2008). Fagel and Madany (2008) used a 3-D virtual talking head with visualized articulators to train the German pronunciations of /s/ and /z/ for eight children with speech disorders, in which the children’s pronunciations were recorded and scored manually. The results showed that six children could significantly enhance their speech production of the /s,z/ sound. Engwall (2008) used an animated virtual teacher to teach seven French subjects to pronounce nine Swedish words in a 5–10 min training program in which the subjects’ acoustic and articulatory data were collected by an ultrasound scanner and an electromagnetic tracking system. The results showed that the subjects’ pronunciation improvement was achieved through mimicking the articulations indicated by the virtual teacher (Engwall, 2008).Massaro et al. (2008) used a between-subjects design in which a talking head was shown to the subjects with different presentation conditions, including audio, audio-visual, frontal and inside views of the vocal tract. It was demonstrated that visible speech contributed positively to the acquisition of new speech distinctions. In recent work of Wang et al. (2014), two groups of Mandarin speakers were trained to learn 9 single vowels using either an auditory or audiovisual talking head. The experiment showed that the audio-visual group outperformed the auditory group in the task of immediate repetition of vowels. Liu et al. (2007) conducted an online experiment to compare language learners’ performance with different conditions of a talking head, human face and voice only, showing that learners in the talking head condition outperformed those in the voice only condition with respect to improvement on Mandarin finals, whereas no significant training condition effect was found on Mandarin initials. Hamdan et al. (2015) evaluated the effects of the realism level of talking-head characters on students’ pronunciation training. Four groups of students learned 20 English words from different characters, showing that the group of students learning with the 3D non-realistic animation character obtained the best performance in the pronunciation tests, followed by learning with the actual human character, the 2-D animation character and the 3D realistic animation character.Many factors can affect users’ language learning performance when using talking heads, such as imprecise articulatory movements, over-realistic appearance, limited language training materials and a short language training period, along with a lack of commonly accepted evaluation criteria or evaluation methods until now. To evaluate talking heads on Mandarin pronunciation learning, there is a need for a well-designed talking head and a proper between-subjects design. In our study, we evaluate a three-dimensional talking head on Mandarin pronunciation learning. The talking head exhibits both the external and internal articulatory movements of speaking and instructs Mandarin learners’ pronunciations. We developed an auto language tutor (ALT) configured with audio only (AU), human face video (HF) and audio-visual animation of a three-dimensional talking head (3-D). Sixty-nine non-native speakers were recruited to learn 60 Mandarin syllables under three conditions (AU, HF and 3-D). Comparative results under these conditions were collected and analyzed to provide a clear insight into 3-D talking head effects on Mandarin pronunciation learning.
1.3. Evaluation methodsSubjective evaluation is required to assess synthesized talking heads in terms of both visual speech synthesis intelligibility and naturalness in the LIPS2008 Visual Speech Synthesis Challenge (Theobald et al., 2008). Mattheyses et al. (2009) obtained participant ratings of visual speech naturalness and synchrony between audio and visual tracks using the LIPS2008 visual speech synthesis challenge database. The subjective ratings of preference and humor between the synthetic and natural talkers were collected in the study of Stevens et al. (2013) to evaluate modality effects on speech understanding and cognitive load. The subjective ratings of likeability with respect to different talking faces (a standard face, a texture mapped face and a sampled-based face) have also been used to evaluate synthetic talking faces for a simple interactive real-time system that provides information about theater shows (Pandzic et al., 1999).Objective evaluation is usually applied when subjects’ language learning performance is quantitatively measured. Word selecting accuracy (Fagel, Madany, 2008, Massaro, Light, 2004), pronunciation repeating accuracy (Calka, 2011), reaction time (Bailly, 2003, Stevens, Gibert, Leung, Zhang, 2013) and pronunciation naming accuracy (Ali et al., 2015) are the common quantitative measures employed to assess pronunciation learning performance. Ali et al. (2015) examined the effects of three different multimedia presentations on 3-D talking head Mobile-Assisted-Language-Learning (MALL). The objective pre-test and post-test pronunciation naming scores were utilized to determine participants’ overall performance. The results showed that the participants in the 3-D talking head with spoken text and on-screen text MALL outperformed those in the 3-D talking head with spoken text alone MALL and those with spoken text with on-screen text MALL (Ali et al., 2015).Converging methods based on both subjective and objective data have been conducted by Stevens et al. (2013), in which a dual-task paradigm was used to investigate the relative cognitive demand of perceiving Audio-only versus Audio-Visual speech produced by a talking head. They collected the objective measures of reaction time, shadowing accuracy and latency data, along with subjective ratings of quality, enjoyment and engagement. The results showed that the Audio-Visual modality had the advantage in speech understanding but created great cognitive load. D’Mello et al. (2010) demonstrated that spoken tutorial dialogues increased learning more than typed dialogues did in a human–computer tutorial dialogue system. They recorded the objective measures of content coverage and learning gains along with subjective ratings of user satisfaction. In the study of Yuen et al. (2011), an online CAPT system with a pronunciation learning cycle of “listen-record-check-learn” was designed wherein an animated talking head was used to provide visual articulatory feedback for Chinese learners to learn English. Both objective measures of mispronunciation detection and subjective ratings of user satisfaction were collected and analyzed. The results showed that the online CAPT system had the capability of mispronunciation detection and that the subjects were satisfied with the system.The ultimate goal of our work is to augment personalized pronunciation training for non-native Mandarin language learners. Both subjective and objective measures were collected in two independent experiments with well-designed training procedures to obtain a more comprehensive evaluation. In the first experiment, users’ acceptance of the 3-D talking head and users’ preferences for the three presentation conditions (AU, HF and 3-D) were evaluated. In the second experiment, users’ pronunciation learning improvements were measured and compared.
1.4. Research questionsThe goal of our study is to assess the degree of pronunciation learning using the auto language tutor (ALT) in three presentation conditions: audio only (AU), human face video (HF) and audio-visual animation of a three-dimensional talking head (3-D). Three research questions are investigated:RQ1: How do language learners accept the ALT with the 3-D talking head?RQ2: What are language learners’ impressions of the three presentation conditions of the ALT?RQ3: How do language learners perform using the ALT in the three presentation conditions?1.4.1. How do language learners accept the ALT with the 3-D talking head?Different systems have different talking heads to present speech. Many studies have made great efforts with graphics-based approaches to make the talking head appear human-like (Mattheyses, Latacz, Verhelst, 2009, Stevens, Gibert, Leung, Zhang, 2013, Theobald, Fagel, Bailly, Elisei, 2008, Wang, Han, Soong, 2012). However, the uncanny valley effect (Mori et al., 2012) refers to the idea that as robots appear more human-like, our sense of familiarity first increases but then declines sharply into a valley. Butler and Joschko (2009) suggested that a character that is over-realistic or almost resembles a human would eventually cause users to feel fearful and horrified. Our auto language tutor (ALT) system with the 3-D talking head was designed for computer-aided pronunciation training. Whether the appearance of the 3-D talking head is acceptable should be evaluated. Moreover, the 3-D talking head presents visualized pronunciation movements of external and internal articulators. It is unclear whether the learners would feel uncomfortable when the 3-D talking head speaks, particularly with transparent internal articulatory movements. In our study, we suppose that a high level of learners’ acceptance would lead to efficient pronunciation learning. Therefore, language learners’ acceptance of the ALT with the 3-D talking head was assessed by collecting subjective ratings at the beginning of the study. The subjective evaluation method that we adopted in our study has been widely used in other talking head system evaluations (Kühnel et al., 2008; Mattheyses, Latacz, Verhelst, 2009, Theobald, Fagel, Bailly, Elisei, 2008, Weiss, Kühnel, Wechsung, Fagel, Möller, 2010).1.4.2. What are language learners’ impressions of the three presentation conditions of the ALT?Many works have been conducted on analyzing language perceptual ability between the audio-only and audio-visual presentations of a talking head (Badin, Tarabalka, Bailly, 2008, Engwall, 2008, Navarra, Soto-Faraco, 2007, Wang, Hueber, Badin, 2014, Wik, Engwall, 2008). However, little is known about language learners’ comparative impressions of an animated talking head, a real human instructor and an audio-only condition. How do the learners think about the quality of these presentation conditions and which condition would they choose for future learning? Theobald et al. (2008) suggested that the synthesized visual speech involved in any applications must undergo an evaluation of its perceived quality, which can be judged by asking viewers to rate the visual speech, because the viewers are often very sensitive to the errors. Hamdan and Ali (2015) evaluated the functionality, future use and many other aspects of a non-realistic three-dimensional talking-head animation by collecting users’ subjective opinions. In our study, to obtain language learners’ preference for 3-D, HF and AU, learners’ interest, their opinions on the functionality (indicated by the perceived quality of the pronunciation instruction) and future use were evaluated by asking participants to rank the three conditions in order of their preference.1.4.3. How do language learners perform using the ALT in the three presentation conditions?Sweller et al. (1998) and Mayer (2009) in their study of multimedia learning proposed that multimodal presentation conditions promote effective learning because humans process information through both visual and verbal channels. Several studies have demonstrated that language learners’ performance of speech perception and speech production improved after learning with a talking head with internal articulatory movements (Massaro, Bigler, Chen, Perlman, Ouni, 2008, Wang, Chen, Li, Meng, 2012, Wang, Hueber, Badin, 2014). Badin et al. (2010) studied whether subjects can understand speech from seeing the tongue in an augmented speech condition. Four presentation conditions, AU (audio signal alone), AVJ (audio signal + cutaway view of the virtual head without tongue), AVT (audio signal + cutaway view of the virtual head with tongue) and AVF (audio signal + complete face with skin texture) were provided. The results showed that the subjects’ speech comprehension score was ranked as AVF  >  AVT  >  AVJ  >  AU, indicating that seeing the tongue benefited speech understanding. Badin et al. (2010) speculated that AVJ might show redundant information of the jaw, lips and cheeks and AVF might be more natural than AVT and AVJ.In our study, it is appealing to compare learners’ language performance using the ALT with the three presentation conditions (AU, HF and 3-D). Furthermore, to discover in which cases learning with 3-D is better than learning with AU or HF, we introduce a task to collect participants’ pronunciation learning performance in AU, HF and 3-D and analyze the results in Mandarin segmental and tonal aspects.
1.5. Research planA number of factors affect learners’ second language performance, such as the learner’s first language, educational level, chronological age, and the necessity to learn the target language (Piske et al., 2001). These factors vary from person to person. In our study, two groups of first year foreign Ph.D. students were recruited as language learners. All of them have the same educational level, a similar age and the same necessity to learn Mandarin. One group of 33 students participated in experiment 1, and another group of 36 students participated in experiment 2. The learning materials are 60 Mandarin syllables (see Appendix A) with clear phonological structures. Because minimal pairs are commonly used to distinguish similar phonemes of the target language (Wang et al., 2012a), all of the Mandarin syllables in the ALT were shown in terms of minimal pairs. Doing so made it easy for participants to observe and mimic.Both subjective and objective methods were adopted for investigating the three research questions. In experiment 1, RQ1 and RQ2 were assessed subjectively by conducting questionnaire surveys. Questionnaire I was designed to investigate RQ1, in which eight rating questions were included and each statement was rated by the participants on a five-level Likert scale from strongly agree to strongly disagree (Allen, Seaman, 2007, Seferoĝlu, 2005). Questionnaire II was designed to investigate RQ2, in which six ranking questions were included and each question asked the participants to rank the three conditions (AU, HF and 3-D) in order of their preference. In experiment 2, RQ3 was studied by comparing the participants’ learning performance through a between-subjects design with three groups for the three conditions (AU, HF and 3-D). The participants’ learning performance refers to pronunciation learning improvement, which is indicated by the pronunciation naming post–pre scores.The remainder of the paper is organized as follows. In Section 2, the implementation of the 3-D talking head is briefly described, and the interfaces with the auto language tutor (ALT) are shown. Details of the subjective and objective evaluations are presented in Sections 3 and 4. The conclusion and future work are provided in the last section.
