In scientific and engineering fields, many problems can be formulated as optimization problems (OPs) [1], [2], [3], [4], [5]. Linear optimization problem (LOP, i.e., both the objective function and constraint functions are linear) is an important kind of problems in the optimization field. However, strict LOPs do not exist and most of the optimization problems in practical situation are usually nonlinear. That is to say, the studies of the nonlinear optimization problem (NOP) are more significant than that of the LOP. The NOP (if at least one of the objective function and the constraint functions is nonlinear) is to minimize the nonlinear objective function subject to nonlinear equality or inequality constraints. Among the NOPs, nonlinear convex optimization problem (NCOP) is a kind of problems with convex objective function and convex constraints. Otherwise, the NOPs will turn to nonlinear nonconvex optimization problems (NNOPs). This paper focuses on the NOP solving problems, especially the NNOP solving problems.
In the past three decades, various numerical methods have been proposed and developed to solve optimization problems. Since the development of the digital computer, numerical algorithms have been widely studied and applied for solving the optimization problem, e.g., line search method [6], [7], [8], [9], trust region method [10], [11], [12], primal-dual method [13], [14], [15], Lagrangian method [16], [17], [18] and so on. However, most of the researches by using the numerical methods focused on non-real-time optimization problems. Furthermore, most of the numerical methods should iterate incrementally by digital computer for obtaining the optimal solutions to the optimization problem, which have to take a long time, especially when solving a large-scale optimization problem.
However, real-time solutions to the optimization problem are usually necessary in some applications and real-time optimization methods which can solve the nonlinear optimization problem are attracting increasing number of researchers [19], [20], [21], [22]. The neural network method can be applied to continuous system for obtaining the real-time solutions [23], which the numerical algorithm can not. However, real-time optimization problems exist in some applications and the corresponding continuous solutions to real-time optimization problems are necessary. With electronic circuit components, the real-time continuous solutions to optimization problems can be obtained. Compared with the numerical optimization methods, the recurrent neural network method possess its own advantages for obtaining the real-time solutions and the authors list as follows [24].

1.By using the optical and VLSI (very large scale integration) technologies, the recurrent neural network can be realized effectively and efficiently.2.Global convergence of the recurrent neural networks for solving the optimization problem with nonlinear constraint can be realized with weaker conditions, e.g., no Lipschitz condition.
Hopfield neural network, as one kind of classic RNNs, was firstly presented to solve linear programming problems by Tank and Hopfield [25]. An early nonlinear optimization problem was considered by Kennedy and Chua, who proposed an RNN with a penalty function [26]. Promoted by these pioneering researches, RNN methods have achieved rapid development in the past decades. Wang et al. proposed a deterministic annealing neural network for convex programming problems [19]. Xia et al. studied several classes of RNNs for convex optimization with nonlinear inequality constraints [27], [28]. Zhang and Constantinides proposed a Lagrange neural network to solve nonlinear programming problems subject to equality constraints [29]. Furthermore, by using Lagrange multiplier, the optimization problem can be converted into an equation solving problem, and thus some traditional recurrent neural network methods for solving the equation solving problem (such as the gradient-based recurrent neural network, i.e., GNN [30], [31], and the finite-time zeroing neural network, i.e., FT-ZNN [30], [32]) can be used. Liu and Wang proposed the simplified dual network based on the Karush-Kuhn-Tucker optimality conditions to solve convex optimization problems [20]. All of the above recurrent neural networks are presented to solve convex problems. However, in engineer applications, many optimization problems are nonconvex. It is urgent to consider the nonlinear and nonconvex optimization problems, and some researches (very few) started to consider this problem by using neural networks in recent years [21], [22]. It is worth pointing out that the existing neural networks are linear or exponential convergence. This implies that the solutions converging to theoretical solutions needs infinite long time. In practical application, a finite time and faster convergence is quite needed. Therefore, some neural networks with finite-time convergence are proposed [33], [34]. Inspired by the previous work, in this paper, a novel finite-time varying-parameter convergent-differential neural network (FT-VP-CDNN) is proposed to solve nonlinear optimization problems in nonconvex set.
Different from the previous methods, the proposed methods have the following advantages. First, the convergence performance of the traditional methods is limited due to the impossibility of enlarging the design parameter. As Ref. [1] shows that the previous methods (such as zeroing neural network, gradient-based neural network) need very large design parameter if better convergence performance is needed. In fact, this design parameter ε is related to the inductance parameters and the reciprocals of capacitive parameters, which are limited by practical physical device performance. That is to say, it is impossible to enlarge the design parameter, which is one limitation of the traditional approaches.
Second, it is more reasonable that the design parameter is time-varying but not fixed as a constant because the new design approach matches the actual situations more. As is known that the electrical parameters (i.e., inductance parameters and capacitive parameters) are usually time-varying in practical system. For satisfying the needs of the solving process and obtaining the best convergence performance, the design parameters of the neural network should be analyzed in advance and chosen repeatedly (especially the nonlinear optimization problem with perturbation), which is unpractical and difficult to apply. Inspired by the time-varying property of the electrical parameters of the practical system, a novel recurrent neural network with time-varying convergence parameter function and finite-time activation function, termed as finite-time varying-parameter convergent-differential neural network, is proposed for improving the practicability and convergence of the neural network method.
Third, theoretical analysis and lots of simulation experiments indicate that the computational efficiency and convergence performance of the proposed FP-VP-CDNN is much better than that of the traditional methods.
Furthermore, the varying parameter function will stop increasing when the error function of the solving problem is lower than the error criterion (e.g., 1% of the initial error). The differences and similarities between the proposed method and the previous method in Ref. [1] is summarized in Table 1.Table 1. Comparisons between the Proposed FT-VP-CDNN in the Paper and the Previous FT-FP-CDNN Method in Ref. [1].ItemFT-VP-CDNNFT-FP-CDNNMethodNeural dynamic modelNeural dynamic modelActivation functionFinite-timeFinite-timeDesign parameterTime-varyingFixedConvergence speedSuper-linearlinearConvergence timeFinite-timeFinite-timeRobustnessStrong robustnessWeak robustnessResidual error with perturbationConverges to 0Oscillate
The remainder of this paper is organized as follows. In Section 2, we proposed a novel FT-VP-CDNN for solving nonlinear nonconvex problem. The finite-time convergence property is proved in Section 3. In Section 4, computer simulations verify the superiority of the proposed FT-VP-CDNN. In Section 5, conclusion remarks are presented. The main contributions of this paper can be summarized as follows.

•A novel finite-time varying-parameter convergent-differential neural network is proposed to solve nonlinear and nonconvex optimization problems. To the best of the authors’ knowledge, it is the first time to propose such a recurrent neural network with finite-time and super-exponential convergence rate. It is the fastest neural network compared with the state-of-the-art neural networks when solving the nonlinear and nonconvex optimization problems.•The finite-time convergence property of FT-VP-CDNN is proved theoretically and the finite convergence time TVP is derived and computed.•Numerical simulations and comparisons between FT-VP-CDNN and FT-FP-CDNN for solving nonlinear and nonconvex optimization problems verify the super exponential convergence and strong robustness of the FT-VP-CDNN.
