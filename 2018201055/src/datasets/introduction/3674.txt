Procedural textures have been widely used in computer games, animation and many other graphics applications for efficient rendering of natural elements, such as wood, marble, rocks, clouds and other materials [1]. They are typically created by procedural models, which are essentially mathematical functions and implemented using computer algorithms. The advantage of using procedural textures is that they require little storage and computation, and can be generated in real time. However, tuning the parameters of procedural models to produce desired textures is a difficult task even for experienced users. Unless one has a good knowledge of procedural texture models, it is difficult to predict which model can produce what types of textures. In addition, parameters of one model will produce overlapping effects on the output texture appearance. Therefore, it is hard to evaluate the influence of each parameter on the output texture.
For artists, designing a “new” texture that can be used in games or animations normally starts with an example of texture, e.g. a rock surface image downloaded from the internet or generated using procedural models. However, the example texture might not meet user expectations; changes in one or more of its perceptual properties are often required (e.g. the artist might wish the rock surface to look rougher). Fig. 1 illustrates the designing process. Currently, to our best knowledge, no software can provide direct solutions to this user requirement. Most packages only provide functions for manual editing, which is complicated and time-consuming. Even though some powerful texture generators (e.g. Genetica, FilterForge) can create high-quality textures or animated textures from given images, they are not able to make direct modifications to perceptual attributes, e.g. modifying roughness, directionality or regularity of the input texture. It is also the case for finding proper procedural textures, i.e. a “new” procedural texture whose perceptual characteristics are different from the example. This is indeed an even more difficult task, because generation of a procedural texture with different perceptual properties involves finding both proper models and parameter settings. With only an example procedural texture as input, current commercial software cannot determine its generation models and corresponding parameters.Download : Download high-res image (317KB)Download : Download full-size imageFig. 1. The procedure of designing a “new” texture based on an example. The rock surface on the left is the example, and the surfaces on the right are the outputs that are a visually similar but look rougher than the example.
In this paper, we propose a novel approach for generating a new texture with different perceptual properties yet sharing certain similarity to the example image. Fig. 2 shows the framework. The input to the system is an example texture, and the system can automatically find a procedural model and determine the parameters to output a new texture. In the proposed approach, users are allowed to adjust one or more perceptual properties of the input texture. Thus, the new texture bears resemblance to the example, while certain perceptual features can be perceived differently from the example. The left block of Fig. 2 shows the training process. First, we use a training dataset introduced in the previous work [2]; the procedural textures in this data set are generated by 23 procedural models and annotated with 9-point Likert scales for twelve perceptual features. We call these annotated values as perceptual scales in this paper, assessing to what extent the features are perceived by subjects. The similarity matrix derived from the grouping experiment is used to construct the perceptual texture space (PTS) [2], while the perceptual scales of the training samples from the rating experiment are used to train regression models.Download : Download high-res image (878KB)Download : Download full-size imageFig. 2. The framework of the proposed approach. The left part shows the training process and the right part shows the process for generating a new texture.
The right part of Fig. 2 shows the process for generating a new texture. For an input texture, computational features are extracted using deep networks and the perceptual scales are predicted by employing a pre-trained SVM model. Meanwhile, the procedural model that can generate the example is also predicted. Then, the perceptual scales of the input texture can be mapped to a point in the PTS based on regressing the coordinates of the PTS. They can also be adjusted by the user and further mapped to a point in the PTS. Next, we perform similarity measurement in the PTS by finding the nearest neighbor to the point representing the texture we wish to generate. Since each point in the PTS represents a texture with known procedural models and corresponding parameters, we can determine the model and corresponding parameters to generate the new texture similar to the input or with desired perceptual scales.
The main contributions of this paper are twofold. First, we propose a method for generating procedural textures that are visually similar to the example or with user-defined perceptual properties. The proposed scheme supports a variety of procedural models. In addition, the perceptual features of the texture can be determined accurately, which are consistent with human perception. Second, a PCA-based Convolutional Network (PCN) is proposed for texture feature extraction. We would like to design a simple yet effective deep learning method. Ideally the network should be simple for training and able to learn features that can adapt to different datasets, even a dataset with a small number of samples. Compared to existing deep convolutional networks, the proposed PCN is composed of unsupervised pre-training stages. It does not involve regularized parameters and does not require numerical optimization solver either; these make the training procedural more efficient and require less time to obtain PCA filters. Moreover, it can learn effective features even with a small training dataset. The PCN also achieved state-of-the-art performance on several tasks based on publicly available datasets, including hand-written digital recognition, face recognition and texture classification.
The rest of the paper is organized as follows. Section 2 reviews relevant research to our work. Section 3 introduces the PCA-based Convolutional Network (PCN) for extracting texture features. Section 4 presents details of the proposed framework. Section 5 reports our experimental results. Finally, we conclude the paper in Section 6.
