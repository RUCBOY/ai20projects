High-performance computing platforms, large-scale data centers, and cloud computing facilities are among the largest consumers of energy. According to a report issued by the Natural Resources Defense Council [1], the electricity consumption of U.S. data centers in 2013 is estimated to have reached 91 billion kilowatt-hours. The total energy demand of data centers has been projected to increase from 1.3% of worldwide electricity supplies in 2010 to 8% in 2020 [2]. By then, their carbon footprint will exceed that of the airline industry.
Power consumption has thus become an important decision variable for scale and performance for systems and applications. Understanding energy use of the applications and programs running on the high-performance computing platforms and data centers is a critical step in optimizing power management, resource allocation, and scheduling of program execution to reduce the systems’ overall power consumption.
Existing approaches to power profiling include physical measurements, either by using special hardware instruments to measure power consumption at various computer components (e.g., [3], [4], [5]), or through embedded power meters and on-board sensors (e.g., [6], [7], [8]). Physical measurements can provide accurate power monitoring and power measurement in real time (subject to certain time granularity). In particular, with a programming interface, an embedded power meter can be of great help to designers of power-aware systems and applications. However, physical measurements report only the power consumption of a specific component or an entire system. It is difficult to translate measurements to the energy use of individual application programs. Furthermore, physical measurements alone cannot be used for prediction of future power consumptions as they report only the current state.
Power profiling also includes analytical approaches. In this case, simulation can be used to estimate power consumption of various components at the microarchitectural level [9], [10], [11], [12]. While simulation may provide a cycle-accurate estimate of power consumption from program execution, it can also be extremely time-consuming. Besides, simulation oftentimes considers only simple scenarios and ignores more complex effects, such as those from real operating systems (I/O, multi-tasking, and so on). Another type of analytical approach is to estimate power consumption by establishing correlations from the program's hardware performance events (e.g., [13], [14]). These approaches can provide power profiling more efficiently but require off-line analysis of logs.
In this article, we present a fast and effective program power profiling technique based on program phases. We observe that applications exhibit distinct behaviors during execution, which often fall into repeating patterns, called program phases [15]. These phases have several important features:
•A program may consist of many program phases, which alternate and oftentimes repeat throughout the program's execution.•Program phases occur at large time scales (typically consisted of execution of hundreds of millions to tens of billions of instructions).•Within each program phase, the performance metrics of the program, such as the cache miss ratio, branch misprediction rate, execution speed (measured in instructions per cycle), as well as power consumption, maintain relatively stable.•Between consecutive program phases, the program's performance metrics change significantly (sometimes by several orders of magnitude). They change synchronously, although not necessarily proportionally.
Based on these observations, we propose to correlate program phase detection with power prediction. A program's execution is divided into intervals, each of which consists of a fixed number of instructions of execution (we fixed on 100 million instructions for all our experiments). A program phase can be defined as a set of intervals with similar behavior [15]. Note that the intervals belonging to the same program phase may not be adjacent to one another in time.
To detect program phases, we use an offline program analysis tool, called SimPoint [16], which clusters the intervals according to the ratios in which different regions of the program (basic blocks) are being executed over time. We note that such classification can also be achieved online through dynamic branch profiling [15]. However, in this study, we limit to only off-line analysis.
For each program phase found, we choose one representative interval (say, at the centroid of the cluster) and measure its power consumption. This can be achieved using physical power measurement tools. In our study, we use the Running Average Power Limit (RAPL), a toolkit designed for microarchitectures with embedded sensors (such as Intel's Sandy Bridge) [6]. To assess the power consumption of other intervals, we simply calculate the distance between the intervals regarding the executed instructions, which we call executed instruction vector (EIV), and use a similarity metric to estimate its power consumption based on multi-variable linear regression.
EIV is a vector with elements each representing the number of times a certain type of instruction has been executed with an interval. For instance, if an interval contains 2 mov instructions and 3 add instructions, the values of the elements in the interval's EIV representing the two instructions will be 2 and 3, respectively. In doing so, we can easily define the distance (which is the inverse of similarity) between two arbitrary intervals using an algebraic distance between the two EIVs. Note that the EIV only represents the mixture of the instructions being executed within an interval without considering the order in which they are executed. We show that this first-order approximation in most cases is sufficient to identify different power regimes within a program. Also note that, in practice, one does not need to maintain a high dimension vector for each interval, since usually, only a subset of instruction types is present within an interval. Therefore, we can simply use a dictionary that maps from the instruction type to the number of occurrences as a succinct way of representing EIV. The size of an interval is an empirical value defined by us (100 million instructions), and it determines the granularity we analyze the program. The problem is that if we choose proper size of the interval, most of the distinct phase could be discovered through our method. Different size of the interval has little effect on the prediction for the power register updates around every 1 ms, we have to assure that there are sufficient power sample points within an interval to get accurate power value.
The above technique has also been extended for predicting power consumption of parallel programs. We also use SimPoint to cluster the intervals, but in this case, accounting for all processes or threads in a parallel program according to their execution profile (i.e., the total number of visits to each of basic blocks by all processes or threads). Note that in a parallel program, however, the same interval may belong to different phases at different processes or threads. To handle this situation, we apply weighted averaging to determine the power consumption of a phase, and then use the multi-variable linear regression method to obtain the average power of an interval across all processes or threads.
The major contributions of this article are two-fold. First, we show that using program phase behaviors we can quickly establish the baseline for the program's power consumption. Second, we show that using a simple similarity metric (by comparing EIVs between the intervals) one can predict power consumption of the entire program's execution with good accuracy. Using this method, we can effectively predict the power consumption of long-running programs based on measurements of only a small set of intervals. Presumably, our approach can also be extended for online power prediction if one can incorporate our method with effective online classification techniques for program phase detection and using online power measurement tools for power estimation. In this article, we focus only on static power profiling and defer the online methods for future work.
The rest of the article is organized as follows. In Section 2, we discuss the background and related work. We first provide an overview of our power profiling method in Section 3, followed by a detailed discussion of the specific techniques in Sections 4 and 5. We present validation experiments and the results for sequential programs in Section 7. We discuss the extended method for parallel programs in Section 8. We present validation experiments for parallel programs, particularly OpenMP programs, in Section 9. We discuss possible ramifications of our approach in Section 10 and finally conclude our article in Section 11.
