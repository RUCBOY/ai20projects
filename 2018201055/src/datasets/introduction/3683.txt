Human activity analysis has long been an important computer vision tasks [1], [2], [3], [4], while egocentric activity recognition [5] has recently emerged as a popular topic of activity analysis in egocentric videos [6], [7], [8] due to the large availability of wearable cameras allowing users to capture both planned and spontaneous moments from unique perspectives. Indeed, the consumer segment continues to grow beyond its core base of sports and adventure enthusiasts and into lifelogging and capturing photos and videos from life moments and special events. There is an increasing use of wearable cameras for recording daily routines. For instance, Los Angeles police departments are planning to outfit policemen with wearable cameras that tape what officers see as they do their jobs, providing records in the aftermath of incidents.
While recording egocentric video is on the rise, there is an important demand for mechanisms indexing the long unstructured video content in an efficient way (i.e., locate the relevant frames based on activities) [9]. In addition, video-based human activity is driven by a wide variety of applications in many high-impact societal needs including smart surveillance, web-video search and retrieval [10], [11], [12], [13], quality-of-life devices for elderly people, and human–computer interfaces [14], [15].
In the past decade, there has been a large amount of progress in video-based human activity recognition [15], [16], [17]. However, most of these previous works focused on activity recognition from a third-person perspective [18], [19], [20]. For example, Wang and Schmid [18] propose a dense trajectory-based feature to track human action over video sequences. They find that combining trajectory feature with other conventional local features (e.g., spatial-temporal interest point, temporal histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary histogram (MBH)) can improve the performance of activity recognition for third-person videos.
Egocentric activity recognition poses new challenges compared to conventional third-person activity recognition. To deal with egocentric videos, a straightforward solution is to borrow the ideas from the hand-crafted visual features which are successful for third-person video analysis [7], [14], [21], [22]. For example, Spriggs et al. [21] compute a 512 dimensional GIST feature of each frame at 4 scales and 8 orientations, discretized into 4 × 4 blocks, for recognizing first-person actions using objects. In [7], a gaze prediction is proposed based on head motion and hand location, while Bambach et al. [14], [22] use hand features for understanding first-person videos. Poleg et al. [8] propose a robust temporal segmentation of egocentric videos into a hierarchy of motion classes using a new cumulative displacement curves feature. However, since different users care about distinct aspects of the activities, hand-crafted visual features computed from specific scene or camera motion (e.g., bag-of-features; dense trajectories and motion-based local descriptors) have large intra-class variation, resulting to suboptimal recognition results. Therefore, the visual hand-crafted features in egocentric video activity analysis may not be useful as in third-person videos.
On the other hand, Convolutional Neural Networks (CNNs) have been shown to learn powerful and interpretable features for understanding image content and large-scale video classification {CITEDBLP:journals/pami/JiXYY13,karpathy2014large,simonyan2014two. Encouraged by the success of CNNs, few attempts {CITEDBLP:journals/corr/PolegEP015,poleg2014temporal have started to adopt deep learning features (e.g., CNN features) for recognizing long-term activities and gained significant performance improvement. However, there are two factors imped application of CNNs for activity recognition in egocentric video: (1) training deep CNNs requires large scale dataset. Compared to ImageNet dataset [27] for image classification, current existing egocentric video datasets [6], [8] for activity recognition are relative small both in quantity and class diversity. And, (2) Wearer’s activities are partly visible. CNN features computed from supervised learning are translate-invariant. However, partly visible activities have large intra-class variations. This will confuse activity feature learning. All these issues lead to limited generality of CNN features in egocentric videos.
To tackle these issues, we propose deep appearance and motion (DAML) to automatically learn feature representations within unsupervised fashion for egocentric activity recognition. The learned feature by the proposed DAML can be readily integrated into any supervised learning models for activity recognition. It is worthy to highlight the following aspects of our proposed DAML:
To leverage the recent advancement of deep neural networks, we propose deep appearance and motion (DAML) to automatically learn feature representations for egocentric activity recognition. The learned feature by the proposed DAML can be readily integrated into any supervised learning models for activity recognition. The highlights of the proposed DAML can be summarized in the following three major aspects:

•To the best of knowledge, this is the first work to jointly extract appearance and motion features by deep learning for egocentric activity recognition, and we propose an effective way to fuse these features, which achieves rich and informative activity representation. The DAML are generic features and can be integrated into any supervised learning models for activity recognition.•We explore different settings of DAML in terms of both regularizers and noisy level to fully investigate the capacity of DAML in feature learning. Empirical results provide a reliable guide for constructing a robust learning model in egocentric activity recognition.•We have conducted extensive experiments on both short- and long-term activity recognition tasks. Experimental results demonstrate that the DAML produces comparable or even better performance than the state-of-the-art counterparts.
The rest of this paper is organized as follows. We present related work in Section 2. Our proposed approach is detailed in Section 3, followed by our experiments evaluating the capability of our representations in Section 4. Final conclusion will be given in Section 5.
