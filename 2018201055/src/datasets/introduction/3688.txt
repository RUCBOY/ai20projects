The end of two well-established theories that have traditionally guided the development of CMOS-based computing devices cannot be prevented. First, Moore’s law that enables computer architects to exploit more transistors per unit area will be shortly limited by physical constraints (sub-nanometer technology nodes seem to be unfeasible [49]). Moreover, Dennard scaling [12], which states that power dissipation per unit area decreases as transistor density increases, has been recently transgressed [20]. As a result, we are witnessing the green computing era,1 where traditional homogeneous computing platforms are shifting to heterogeneous systems which are capable of sustaining the desired increment in performance, but yielding high energy efficiency.
Heterogeneous architectures are equipped with specialized cores (CPU’s co-processors and GPUs) in order to acceleratedata-parallel algorithms (e.g., 3D graphics rendering, hashing,encryption) to obtain better overall system performance and energy efficiency [23]. The reason is that the design of these specialized cores integrates heavy vectorization or Single-Instruction Multiple-Data (SIMD) capabilities, that maximize performance per watt, particularly in data-intensive applications [8]. For instance, vectorization is available in most microprocessors designed and introduced in different market segments [5], such as integrated devices (ARM NEON), servers and desktop processors (AVX, SSE and SVE) [1], and also accelerators like NVIDIA GPUs [38] or Intel Xeon Phi [24]. Indeed, accelerators are the most popular option nowadays for accelerating massively parallel and data-intensive workloads.
This new landscape of computation forces programmers to redesign and even rethink their algorithms to satisfy energy and performance requirements, as run-time systems are still immature to accomplish both requirements at the same time. Some of the most representative exponents are population-based algorithms, such as PSO (Particle Swarm Optimization) [19], genetic algorithms [21], and ACO (Ant Colony Optimization) [[15], [16], [18]], as they are massively parallel by its mathematical definition, but their straight-forward implementation on GPUs architectures may not constitute the best possible solution on current architectures.
Of particular interest to us is enhancing the ACO algorithm. ACO mimics the observed behavior of ant colonies. The ACO method uses artificial ants to try to get the process of traversing the graph. A complete tour represents a solution. Trajectories are evaluated according to the quality of the solution represented by these paths. And then, these artificial ants make a deposit of “pheromones” accordingly (the better the solution, the more pheromone deposit they drop). To solve a problem by means of ACO algorithm, there are two main phases: tour construction, where ants run in parallel looking for solutions; and pheromone deposition, where ants communicate to each other.
Parallel versions of ACO have been developed so far [[11], [35], [43], [46], [54]]. In particular, in one of our previous work [6], we offer a GPU-ACO version that parallelizes the main phases of the ACO algorithm (i.e., pheromone deposition and tour construction), giving more emphasis on data parallelism. However, to the best of our knowledge, all the existing parallel implementations developed so far in these architectures are not able to take full advantage of the underlying hardware resources, since they propose a task-based parallelism, or they are computationally demanding to avoid serialization [[6], [7]].
In this paper, we rethink contemporary parallelization strategies for the two main phases of the ACO algorithm (i.e., pheromone deposition and tour construction), in order to optimize performance when running on NVIDIA GPU platforms such as Fermi, Kepler and Maxwell.
The main contributions of the paper include the following:

1.We propose an agnostic vectorization scheme specifically geared towards massively parallel architectures for ACO’s main stage, named tour construction. This proposal establishes one ant to both 32-width vector (identifying one ant as a CUDA-warp) and 64-width vector. To implement the latter, we use partial synchronization and different communication schemes based on shuffle instructions combined with shared memory.2.We introduce a novel parallel implementation that mimics the behavior of the classical roulette selection procedure. This new implementation is called SS-Roulette, in reference to the patterns used in its implementation (i.e., the patterns named Scan and Stencil), and is presented as a new implementation that improves GPU data parallelism.3.A complete review of the main phases of the ACO algorithm tested against different instances of the TSP (Traveling Salesman Problem) is provided. We tune different GPU parameters up to offer a 3× factor for the tour construction phase, comparing our contribution against the best GPU development published. The usage of atomic instructions is analyzed on the pheromone update stage to conclude that in new generations of NVIDIA GPUs, the use of atomic instructions over global memory increases performance for disperse memory accesses. Moreover, we propose a joint execution of the two main ACO phases (i.e., pheromone deposition and tour construction) in just one kernel.
The article is structured as follows. Section 2 contains a revision of ACO and the CUDA architecture. Section 3 shows the parallelization techniques we use to improve the execution of ACO in NVIDIA GPUs. Section 4 describes the hardware and software environments before an in-deep analysis is carried out in Section 5. Section 6 summarizes related works that are relevant to this topic, and finally the paper ends with Section 7, highlighting main conclusions and several proposals for future work.
