How do we turn passive data into actionable knowledge or something compelling that improves wastewater treatment operation or supports decision-making? The aim of this paper is to describe the state-of-the art computer-based techniques for data analysis as applied in the context of wastewater treatment operation. This critical review targets method developers (mostly within the research community) by discussing the evolution of a selection of methods and identifying limitations of method development and selection, as well as plant managers and software developers by identifying barriers that limit bringing methods into practice.
This paper is structured as follows: first, we briefly define the driving forces within the wastewater treatment field that pushed for the development of computer-based techniques for data analysis. Second, we describe the variety of available techniques that enable the transformation of data into information and, beyond that, into knowledge by means of a review of the techniques applied thus far in wastewater treatment plants (WWTPs). In parallel, a critical analysis of the maturity and temporal evolution of each technique is given. Finally, a discussion is provided on the limitations in this field.
1.1. Driving forcesWWTPs treat wastewater collected from households and industries before being discharged to a receiving water body. WWTPs are complex systems, which have to maintain high performance at all times, despite suffering from hourly, daily, and seasonal dynamics. WWTP operations have the particular feature that any “raw material”, i.e. wastewater, must be accepted while the product, i.e. treated effluent, must adhere to its standards at all times. Furthermore, WWTPs have to adapt to new challenges posed by the society such as the removal of emerging pollutants, the minimization of greenhouse gases emissions, etc. (Hadjimichael et al., 2016). Overall, large amounts of data from WWTPs are being generated which need to be properly transformed into knowledge for enhancing their operation. Such knowledge can then be encapsulated into controllers or Environmental Decision Support Systems (EDSS) that allow maintaining high performance (and low emissions) at all times. During the last two decades, several driving forces that have intensified the development of computer-based techniques to transform data into knowledge in the wastewater treatment field.The first driving force was control implementation, to increase the stability of the process ensuring good performance at all times, and to optimize the usage of resources (e.g. energy and chemicals). Control stimulated developments since the early 1970s (Olsson, 2012) on data cleaning, selection and transformation, which renders the data interpretable and useful for human inspection and automatic feedback control. Today, many sensors, such as those used to monitor dissolved oxygen, several nutrients, suspended solids and organic matter, have undergone important transformations, rendering them reliable and affordable (see Vanrolleghem and Lee, 2003; see manufacturers Hach®, Endress + Hauser, S::CAN, etc.). The development of such sensors itself required the usage of data treatment methods (e.g. regression applied to information gathered from UV-VIS sensors). However, we realized that the installation of sensors and their maintenance efforts is insufficient to guarantee data quality and hence, methods were incorporated to allow for fast detection and diagnosis of faults. Also we incorporated methods to verify process normalcy and to create useful knowledge concerning plant malfunctioning and how to either improve plant performance or return it to normal operation. Hence, this stimulated the development of methods dealing with mass balances and data reconciliation for basic information extraction. Control development evolved from unit process control to sophisticated optimization and automation software packages, including rule-based systems and expert systems (Åmand et al., 2013, Ingildsen, 2002).The second driving force was the transformation of data graveyards into data mines. It is evident that the incorporation of new challenges (from aeration control to system-wide control) and the increased levels of monitoring, control and supervision have led to the need for the handling of a large number of signals. Our current experience suggests that small WWTPs (∼20,000 Population Equivalents, PE) can generate up to 500 signals (including online and offline signals), whereas larger ones (0.8–3 million PE) register analogical and digital signals exceeding 30,000 in number (Olsson et al., 2014, Freixó, 2016). As has been recognized, however, data-rich is all too often equivalent to information-poor (Nopens et al., 2007, Poynter, 2013). Indeed, vast amounts of data are languishing in databases, which are at best described as data graveyards and can certainly not be considered data mines. Indeed, current practice is arranged such that plant operators have an overwhelming stream of data at their hands, which is very difficult to process and analyse in a timely enough fashion to allow for better understanding or proper decision-making. As the effort to analyse data is costly because of a lack of trusted analytic data tools, potentially valuable information remains unavailable and unexploited (Yoo et al., 2008). Hence, methods appeared for Advanced information extraction to facilitate the interpretation of large datasets with multiple variables, i.e. multivariate methods such as principal component analysis (PCA), independent component analysis (ICA), and clustering. In addition, the large amounts of data stimulated the development of black-box models (such as artificial neural networks –ANN- or support vector machines –SVM-) which could be used for process optimization. Finally, other methods appeared for Human-interpretable information extraction, within the field of knowledge discovery (rule induction, decision trees, etc.) and management (ontologies). Taking advantage of increasing computing capacity, innovative knowledge-based systems have evolved to make use of both numerical models and heuristic knowledge in tandem with classical and innovative knowledge-acquisition techniques in EDSS.The current data-rich, information-poor condition is a general problem that is not unique to the wastewater treatment industry. Indeed, many tools have been developed already and are popular within the chemical processing and paper and pulp sectors. Wastewater treatment operations are unique, however, for the following reasons. First, material inputs (i.e., wastewater) (1) cannot be stored in large quantities if the supply exceeds the process capacity (e.g., storm water), (2) cannot be discarded and ignored if they are of low quality (i.e., all discharged waters are accounted for in performance evaluations) and (3) are characterized by high temporal variability in both volume and quality. Therefore, borrowing methods from other engineering fields is not sufficient to guarantee the successful transformation of data into knowledge. The field of wastewater treatment requires specific adaptation of the methods to account for the uniqueness of the wastewater treatment process. The IWA (International Water Association) Instrumentation, control and automation conferences in particular have provided an excellent platform for such adaptations. A summary is found in Olsson (2012).
