In the past several years, deep learning based face verification methods have achieved great success [1], [2], [3]. These methods try to learn good facial representations which maximize the between-class scatter and minimize the within-class scatter. The distances in the space of these representations are good facial similarity metrics for face verification. Another related research topic is the deep learning based Facial Expression Recognition (FER) [4], [5]. Most FER methods are discriminative models that map the facial crops to emotional labels. The identification and expression are two orthogonal properties of faces. But, few studies considered the two properties together in a unified framework. There are two unsolved problems for such a challenging task. The one is what kinds of network should we used for modeling the multi-properties of faces? The other one is how to get a large-scale facial dataset labeled with detailed properties?
One of the trends in deep learning is using large-scale datasets. There are some large-scale facial datasets presented recently such as CASIA WebFace Database [6] and MegaFace [7]. An alternative way is to synthesize more images using Generative Adversarial Nets (GANs) [8], [9], [10]. The GANs are very promising methods for solving the data size limitation problem. Rendering images using computer graphics techniques is another solution [11], [12] for building large-scale datasets. Based on the existing knowledge of computer graphics, rendered faces look real. Rendered facial expressions can strictly follow the definition of Facial Action Coding System (FACS) [13] and Emotional Facial Action Coding System (EMFACS) [14]. The synthesized datasets may be powerful tools for analyzing the real-world data.
The main contributions of this work include:

•A pair of 18-layered Convolutional Deconvolutional Networks (Conv-Deconv) is proposed to learn a bidirectional mapping between the emotional expressions and the neutral expression. From the view of facial representation, one network extracts the complementary facial representations from emotional faces. The other network reconstructs the original faces from the extracted representations. They are mutually inverse functions. The extracted complementary facial representations are used to reconstruct the original faces, generate new faces and interpolate new faces. The facial representations can also be used for facial expression recognition and face verification.•Based on computer graphics techniques, a new facial expression dataset called Large-scale Synthesized Facial Expression Dataset (LSFED) is presented. The dataset contains 105,000 emotional faces of 15,000 subjects. All the faces are synthesized using computer graphics techniques. To increase the difficulty and mimic real-world conditions, we create a distorted version of the LSFED, and name it as LSFED-D.•Good experiment results are obtained after evaluating our method on the synthesized clean LSFED dataset, the synthesized distorted LSFED-D dataset, and the real-world RaFD dataset.
The rest of the paper is organized as follows. Section 2 reviews the related work. In Section 3, the main method is proposed. The experiments and results are presented in Section 4. Section 5 gives the conclusions.
