A constrained learning model allows one to incorporate domain-specific knowledge as constraints to balance the learned model using the implicit structure of the data [7]. Even if a lower dimensional subspace is a common model used in machine learning [25], data often lies on specific manifolds [20], [38]. This is more relevant when one has to identify well-defined object categories. To this end, any methods taking advantage of the implicit structure of the data can obtain improved results. As another intuition, assuming that a solution of the optimization problem always belongs to the data of a related domain, the constraints derived from the data structure can bring robustness to variations on the testing data [5], [13], [34]. Consequently, it is advantageous to incorporate the structure prior, particularly manifold constraints, in the learning procedure. Nevertheless, the main issue is to efficiently embed such manifold constraints in any optimization methods.
In particular, the advantage of linearly constrained models in the form:
Y=AX,where Y, A and X represent the data, a matrix and the variable respectively, is that they can be solved in orders of magnitude faster than non-linear models. Moreover such models have been used in several state-of-the-art learning methods as shown in Table 1. However, it is worth noting that, such advantage is reduced whenever the data entails a non-linear structure that reasonably appears in most learning schemes with visual data. Of course, the clear advantage would be to embed such non-linear data structure while maintaining a linear inference paradigm.Table 1. Linearly constrained models in the form Y=AX are present in different computer vision and learning problems. Even if data and unknowns refers to different variables, a linear constrained problem is often the common link among all the approaches.ProblemLinearly constrained modelYAXPropertiesSparse representation classifier [24]Hr=HdZHrHdZZ associates the test samples Hr to HdLow rank decomposition [28]Vm=Vr+EVm–VrVr is a low rank matrixKernel Ridge Regression [35]yi−wTϕ(xi)=ξiyiwϕ(xi)yi is a classification outputExtreme learning machine [35]T=HβTHβFeedforward neural network with linear relationTotal variation [37], [41]y=LTx+uyLxx is calculated from y with operator LLogitBoost [42]Zi=yiHiwZiHiwZi is linearly dependent on Hi and w
This work shows that the non-linear constraints, raising from the fact that the data Y is lying on a manifold, can be included in a problem with linear constraints through an alternating direction method of multipliers (ADMM) [55]. To reach such conclusion, we first prove that, if the data Y admits a neighbour-preserving embedding (e.g. Local Linear Embedding [20]), the constraints given by the embedding in Y can be transferred to the unknowns X of the problem. The type of unknowns differs given the problem at hand but after such transfer X is still linearly related with the input data Y. Now, given the new problem resulting from the transfer, optimization on X is carried forward with an iterative ADMM approach easily solving for each step and imposing manifold constraints with a simpler matrix projection. This procedure allows efficient implementation without using non-linear optimization.
To sum up, the contributions of this paper are threefold, and the framework is shown in Fig. 1: (i) given a linear relation between the optimized variables and the data from a manifold, we present a novel and general theory to transfer the manifold constraints from the data to the variables; (ii) we present a manifold based ADMM (MADMM) framework and prove its convergence; (iii) we validate our approach in several problems including visual tracking and recognition.
The rest of the paper is organized as follows. In Section 2, we introduce related work. Section 3 details how manifold constraints can be efficiently embedded in a MADMM optimization framework, and Section 4 shows the theoretical insights of our approach. We apply our theory on kernelized correlation filter (KCF) [36] and sparse representation classifier (SRC) [25] in Sections 5. Experiments on relevant datasets are discussed in Section 6 and conclusions are drawn in Section 7.
