1.1. MotivationAge and gender classification play a very important role in our social lives, by which we can find whether the persons we contact are “sir” or “madam” and young or old. These behaviors are heavily dependent on our ability to estimate these individual traits: age and gender, which are from facial appearances [1]. These attributes are important in our lives while the ability to estimate them accurately and reliably from facial appearance is still far from satisfying the needs of commercial applications [2].In order to enhance the ability to estimate or classify these attributes from face images, many methods have been put forward in the past years. Based on cranio-facial changes in feature-position rotation and on skin wrinkle analysis, these attributes have been classified from facial images [3] while a methodology is proposed to classify age and gender automatically from facial images through feature extraction including primary and secondary features [4]. However, these approaches mentioned above have been designed particularly for processing constrained age or gender tasks which are not suitable for practical applications including unconstrained image classification tasks.The accuracy of age and gender classification depends on two aspects: feature extraction and classification, while feature extraction is a crucial factor for the success of classification. It not only demands the features having the most differentiable characteristics among different classes, but also retains unaltered characteristics within the same class. In recent years, due to its good feature extraction ability, CNN has been highlighted in machine learning and pattern recognition fields. It has achieved state-of-the-art performance in image recognition and can automatically extract the features.With full consideration of what mentioned above, CNN has been introduced to classify unconstrained age and gender tasks automatically and significant performance has been obtained [2]. More importantly, the unconstrained images are without prior manual filtering, which are as true as real-world applications. CNN has shown great advantages in image recognition while it is the first time to use CNN to process these unconstrained tasks so that we can further improve the accuracy of classification through the fine tuning of its structure or its parameters.With more discriminative features and more powerful classifier, higher recognition rate will be obtained. In a plain CNN, the full-connection layers are as same as a general single hidden layer feedforward neural network (SLFN) and trained through back-propagation (BP) algorithm. On the one hand, BP algorithm is sensitive to local minima of training errors. On the other hand, SLFN is likely to be over-trained leading to degradation of its generalization performance when it performs BP algorithm [5]. Therefore, the generalization performance of the fully connection layers in the network is probably sub-optimal and they cannot make full use of discriminative features extracted by convolutional layers.In order to deal with the problems, it is urgent to find a new classifier which owns the similar ability as the full-connection layers or softmax classifier, while it can make full use of the discriminative features. Niu and Suen [6] proposed a hybrid model which integrated the synergy of two superior classifiers including CNN and Support Vector Machine (SVM), and got a better results compared with a plain CNN. In general, the design of SVM is so complicated that is important to find other classifiers with least needing tuning parameters, good classification performance, and high generalization ability to process the same tasks mentioned above. To the best of our knowledge, SVM, Naive Bayes [7], and Extreme Learning Machine (ELM) [8] are three important classification algorithms at present while ELM has been proved to be an efficient and fast classification algorithm because of its good generalization performance, fast training speed, and little human intervene [9]. What’s more, ELM and improved ELM, including mixing with other methods, have been widely used to process pattern recognition tasks and obtain a good performance [10].
1.2. Our contributionsIn order to make full use of the advantages of CNN and ELM, we propose a hybrid recognition architecture, called CNN–ELM, which is used to process age and gender classification tasks. It not only sufficiently exploits the excellent feature extraction ability of CNN and the outstanding classification property of ELM, but also is used to classify the popular human facial image datasets. At the same time, different effective approaches are adopted to reduce overfitting. With lower time complexity, the hybrid architecture gets a better performance compared with a plain CNN structure which contains the identical convolutional layers. The major contributions of this paper are summarized as follows:
•We propose a new hybrid CNN–ELM method to process age and gender classification aiming at image tasks. It combines Convolutional Neural Networks and Extreme Learning Machine in a hierarchical fashion which is sufficient in applying the advantages of CNN and ELM.•We present the process of integrating the synergy of hybrid structure in detail, including the design of the layers in CNN, the selection of parameters in hybrid structure, the realization of back-propagation process in this hybrid model, and so on.•Finally, two popular datasets, such as MORPH-II and Adience Benchmark, are used to verify our hybrid structure. Experiments show that our hybrid structure gets better performance compared with other studies on the same image datasets and also can fulfill the requirements of many real-world application.The remainder of this paper is organized as follows. Section 2 reviews the related work. Section 3 gives preliminary information. Section 4 discusses architecture of our hybrid CNN–ELM model. Section 5 describes merits of hybrid CNN-ELM model. We also analyze the time complexity of hybrid classification in Section 6. The experiments and results are illustrated in Section 7. Finally, we make a conclusion in Section 8.
