Scientific literacy involves both knowing what scientists know and knowing how scientists know what they know. Recent science education reform efforts capture these two essential components (i.e., the what and the how of scientific knowledge) in a three-dimensional learning framework that intertwines scientific practices, crosscutting concepts, and disciplinary core ideas (National Research Council, 2012, NGSS Lead States, 2013). In this framework, evaluative processes act as a central hub linking the scientific activities of empirical inquiry and constructing explanations. Although the framework embeds reasoning throughout, evaluation as argument, critique, and analysis is central to scientific thinking and knowledge construction (NRC, 2012).
Evaluation often follows a dynamic and iterative cycle in the scientific enterprise. For example, some climatologists construct explanatory and predictive models representing Earth’s atmosphere, and then collect empirical data to calibrate these models. Evaluation of connections between lines of evidence (e.g., sea surface temperatures) and scientific explanations (e.g., the interdependence of oceans and atmosphere) could lead to subsequent model refinements and validation with additional empirical data. Conant (1951) describes this dynamic and evaluative process as the speculative enterprise of science, where scientific knowledge construction is complex and requires mature, evaluative thinking (Kuhn & Pearsall, 2000). But such thinking may be difficult for students to learn and for teachers to teach (see, for example, Erduran and Dagher, 2014, Klopfer, 1969). Because of this difficulty, instructional scaffolds may be required to help students learn how to critically evaluate connections between evidence and explanations (Greene et al., 2012, Li et al., 2016) and construct scientifically accurate knowledge (Duschl, 2008, Sandoval and Reiser, 2004).
Our recent classroom-based research project focused on developing and testing instructional scaffolds—called Model-Evidence Link (MEL) diagrams1—that facilitate students’ evaluations and judgments during knowledge construction (Fig. 1). Our project concentrated on the Earth science domain, which includes many topics that are challenging for students because: (a) the underlying scientific principles are complex, (b) the processes frequently occur over very long time and large spatial scales, and (c) students have difficulty understanding how scientifically accurate explanations are constructed. Furthermore, some topics in Earth science are particularly salient because they concern issues of great local, regional, and global importance (e.g., climate change; see, for example, Sadler, Klosterman, & Topcu, 2011). Therefore, investigating scaffolds (e.g., the MEL) that help students think more scientifically about Earth science—specifically within a classroom context—may be both relevant and useful to systematically understand contemporary learning environments (Barab & Squire, 2004). Our project, and specifically the present study, comparatively examines MELs in authentic secondary classroom settings with the goal of gauging how Earth science students can deepen their knowledge about natural phenomena through scientific evaluations and judgments.Download : Download high-res image (717KB)Download : Download full-size imageFig. 1. A student example of the fracking Model-Evidence Link (MEL) diagram.
The present study examines the MEL in comparison to two other scaffolds. Lombardi, Nussbaum, and Sinatra (2016) argued that evaluative comparisons of alternative explanations could facilitate students’ knowledge construction through increased cognitive engagement. Therefore, we specifically compared the MEL, where students evaluate connections between lines of evidence and two alternative explanations (i.e., the scientific alternative vs. another alternative), to the Mono-MEL, where students evaluate connections between lines of evidence and only one explanation (i.e., the scientific alternative). We also compared the MEL, where students evaluate connections diagrammatically, to the Model-Evidence Link Table (MET), where students evaluate connections using tables and letter codes. In subsequent sections, we elaborate further on our justifications for comparing these three scaffolds.
We built our project on a theoretical perspective that posits the following: learners may construct scientifically accurate knowledge through a process of generating explicit evaluations about scientific evidence and reappraising their plausibility judgments about explanations (Lombardi, Nussbaum et al., 2016). This perspective has both philosophical foundations (Rescher, 2009, Salmon, 1994), and empirical bases in educational, developmental, and cognitive psychology (Chinn and Brewer, 2001, Collins and Michalski, 1989, Connell and Keane, 2006, Dole and Sinatra, 1998, Kuhn and Pearsall, 2000, Nussbaum, 2011) and science education research (Braaten and Windschitl, 2011, Chi, 2005, Chinn and Brewer, 1993). Our discussion below highlights the extant literature supporting this theoretical perspective, as well as recent empirical work examining ways to promote scientific thinking and knowledge construction through more critical evaluations and judgments, and situates this perspective within the context of the present study.
1.1. Evaluation, plausibility, and knowledgeAll scientific practices emerge from “processes of perpetual evaluation and critique that support progress in explaining nature” (Ford, 2015, p. 1043), and recent science education reform efforts call for students to engage in the scientific practices to help them achieve college- and career-readiness (NRC, 2012). To effectively participate in these practices, students should cognitively evaluate scientific evidence and “plausible explanation[s] for an observed phenomenon that can predict what will happen in a given situation” (NRC, 2012, p. 67). The scientific community also compares the plausibility of alternative explanations when constructing scientific models and theories. Yet within the context of certain Earth science phenomena (e.g., climate change and hydraulic fracturing, aka “fracking”), scientists may generate explanations that seem implausible to students. In contrast, alternative lay explanations about such phenomena – such as the notion that increasing amounts of energy received from the Sun are the cause of current climate change – may seem more plausible than scientific ones. Students may consider this lay explanation more plausible than the scientific explanation that human activities are the cause of current climate change. This difference in judgments about what explains a phenomenon is what Lombardi, Sinatra, and Nussbaum (2013) call a “plausibility gap.”Plausibility judgments may be associated with critical and scientific thinking. For example, Beyer (1995) says that questioning the plausibility of explanations is one characteristic of skepticism, a disposition of critical thinkers. Differentiating between evidence that supports the truthfulness of a claim, and theory that supports the plausibility of a claim, is also a characteristic of those who are developing scientific thinking skills (Kuhn, 1999). By examining a theory’s potential truthfulness, plausibility judgments used in a critical mode may be evaluative. Such critical evaluations about the plausibility of explanations are also fundamentally linked to an individual’s knowledge (Willingham, 2008), based on the presupposition that plausibility judgments are tentative in nature and may contribute to knowledge construction (Lombardi, Nussbaum et al., 2016). Although explicit and critical evaluations of novel explanations may influence appraisals of plausibility, people’s implicit perceptions and biases may activate cognitive processes that are not reflective and purposeful. Lombardi, Nussbaum et al. (2016) speculate that plausibility judgments often form implicitly and without much thought, thereby necessitating that students be more critically evaluative when judging the plausibility of scientific explanations.Plausibility judgments have also long been theoretically implicated as one of many important factors in the process of science learning (see, for example, Chinn and Brewer, 1993, Dole and Sinatra, 1998, Kapon and diSessa, 2012, Posner et al., 1982), but until recently, almost no empirical research has validated the importance of plausibility in knowledge construction and reconstruction (see Lombardi, Nussbaum et al., 2016, for a detailed philosophical, empirical, and theoretical review). Although recent research (see, for example, Lombardi, Bickel, Bailey, & Burrell, 2018) shows that importance of plausibility judgments in knowledge construction, Lombardi, Nussbaum et al. (2016) state that students are only most “likely to engage with ideas that are perceived to have [both] high plausibility and cognitive utility” (p. 49). Indeed, other factors, such as commitment-based social group membership, could override increased plausibility (Dole & Sinatra, 1998), which makes the process of constructing conceptions consistent with scientific understanding difficult (Chi, 2005). Lombardi, Nussbaum et al. (2016) recently proposed a theoretical model that posits initial plausibility judgments might be reappraised through the process of being critically evaluative (i.e., plausibility reappraisal may elevate initial plausibility judgments from regimes of low/implicit evaluation to high/explicit evaluation). Reappraisal, in turn, may be a component of constructing scientifically accurate knowledge, but only if the plausibility judgment is now considered greater than the plausibility of preexisting and/or alternative conceptions, and only if other factors, such as personal stake in the outcome, do not strongly override the plausibility judgment.
1.2. Scientific thinking through evaluationStudents may be naturally curious about scientific topics, but they are not necessarily evaluative as they consider hypotheses and theories constructed by scientists. The process of evaluation can involve judgments about the relationship between evidence and alternative explanations of a particular phenomenon (McNeill, Lizotte, Krajcik, & Marx, 2006). Therefore, when students are more critical in their evaluation of scientific knowledge they will seek to weigh the strengths and weaknesses in the connection between evidence and explanations, and gauge how well evidence potentially supports both an explanation (e.g., an argument, a scientific model) and its plausible alternatives (e.g., a counterargument, a contrary hypothesis). Students’ evaluations should also reflect on the process of knowledge construction to promote deeper understanding (Mason, Ariasi, & Boldrin, 2011). When students model practices used by scientific experts they may cognitively reflect and evaluate in a manner similar to scientists (Duschl, Schweingruber, & Shouse, 2007). Students who engage in reflective evaluation understand that scientific knowledge emerges from collaborations that are constructive, critical, and open to revision (Nussbaum, 2008). Because students may not be critically reflective when engaging in collaborative knowledge construction, they may need instructional scaffolds to evaluate the quality of explanations (Gijlers and de Jong, 2009, Kyza, 2009, Metz, 2004, Nussbaum and Edwards, 2011). Instructional scaffolds that facilitate students’ coordination of lines evidence with alternative explanations (e.g., the MEL) hold some promise for deepening students’ science learning (Chinn and Buckland, 2012, Lombardi et al., 2016). Such scaffolds may be particularly useful because the process of weighing the connections between lines of evidence and more than one explanation may help students become more scientific and critical in their evaluations, which in turn could promote plausibility reappraisal and deeper knowledge construction (Lombardi et al., 2018, Lombardi et al., 2013, Lombardi et al., 2016).
1.3. Relating evaluation and plausibilityResearchers have implicated plausibility judgments in facilitating co-construction of knowledge in discourse associated with collaborative argumentation (Duschl et al., 2007, Nussbaum, 2011). Researchers have also proposed that plausibility may be an important judgment involved in construction of scientifically accurate knowledge (Dole and Sinatra, 1998, Pintrich et al., 1993, Posner et al., 1982). Lombardi, Nussbaum et al.’s (2016) theoretical model describes how plausibility judgments often may be formed through automatic cognitive processes (i.e., cognitive activities that require very little attentional capacity; LaBerge and Samuels, 1974, Stanovich, 1990). However, explicit prompting (i.e., via instruction) may facilitate reappraisal of these implicit plausibility judgments toward a more scientific stance. Such instruction may be particularly relevant for complex and abstract scientific topics (e.g., climate change), where a gap exists between what students and scientists find plausible.Empirical research has revealed that a plausibility gap exists for the topic of global climate change among middle school students (Lombardi et al., 2013), undergraduate students (Lombardi and Sinatra, 2012, Lombardi et al., 2016), and elementary and secondary science teachers (Lombardi & Sinatra, 2013). To address this gap, Lombardi et al. (2013) developed a MEL for the topic of climate change. Grade 7 students who used this MEL experienced significant shifts in both plausibility and knowledge toward the scientifically accepted model of human-induced climate change. These students also retained their knowledge gains six months after instruction. In comparison, grade 7 students at the same school and taught by the same teachers did not experience plausibility or knowledge shifts when experiencing another instructional activity designed to promote scientific inquiry and deeper understanding of climate change (Smith, Southard, & Mably, 2002). This comparison activity asked students to construct their own explanations based on evidence, rather than weigh evidence between two competing models of climate change (i.e., as the treatment activity did). Lombardi et al. (2013) speculated that the students’ plausibility reappraisal—a skill that is important for understanding the development of scientific knowledge (Duschl et al., 2007, Hogan and Maglienti, 2001)—was related to the MEL’s ability to facilitate students’ critical evaluation. Plausibility reappraisal, in turn, may have promoted the students’ enduring knowledge gains (Erduran & Dagher, 2014).
1.4. Linking scientific practices to evaluation and plausibility reappraisalRecent empirical research closely examining student work on the MEL activities shows that students engage in various levels of evaluation when considering alternative explanations about Earth and space science phenomena and that these evaluation levels are significantly related to plausibility appraisals and knowledge about the phenomena (Lombardi et al., 2016, Lombardi et al., 2018). Specifically, high school students shifted plausibility toward scientifically accepted explanations and increased their knowledge about relevant Earth science topics after participating in MEL activities. Greater levels of evaluation were related to plausibility shifts and knowledge increases, as shown by structural equation modeling. Effect sizes were small to large, depending upon topic and instructional context. For example, on one hand, there was a large effect size where combined knowledge scores increased over time. On the other hand, individual classroom settings revealed large effect sizes for some topics (e.g., the connections between fracking and earthquakes) and small effect sizes for others (e.g., the importance of wetland resources). These findings support the idea that MEL activities moved students to cognitively engage in practices that helped them think more scientifically. Specifically, students learned “that alternative interpretations of scientific evidence can occur, that such interpretations must be carefully scrutinized, that the plausibility of the supporting evidence must be considered, [and] …that predictions or explanations can be revised on the basis of seeing new evidence or of developing a new model that accounts for the existing evidence better than previous models did” (NRC, 2012, pp. 251–252). However, we still wondered if the MEL was particularly effective at promoting evaluation and plausibility reappraisal, or if students would perform as well, or better, when engaging in other, similar tasks. This general question motivated the present study.
1.5. The present studyThe present study represents the culmination of a three-year classroom-based research project. This project’s overall purpose was to design and test instructional scaffolds, based on Lombardi, Nussbaum et al.’s (2016) theoretical perspective, that (a) facilitate students’ scientific evaluations about the connections between lines of evidence and alternative explanations, (b) promote shifts in their plausibility judgments about scientific explanations, and (c) deepen students’ scientifically accurate knowledge about Earth science phenomena. The project was a collaboration between master teachers and researchers per Anderson and Shattuck’s (2012) guidance. Specifically, we partnered with teachers in identifying the initial problem, designing and constructing the interventions, and creating publications for fellow practitioners. In the project’s first year, the team designed three MEL diagrams and associated materials, covering the topics of (a) fracking and earthquakes, (b) wetlands and land use, and (c) formation of Earth’s Moon.We added these three to the existing climate change MEL developed by Lombardi et al. (2013). We chose these four topics (causes of current climate change, relations between fracking and earthquakes, use of wetlands, and formation of Earth’s Moon) because each has multiple plausible explanatory models (a scientifically accepted and an alternative) that students could evaluate. Furthermore, these four represent a wide range of topics that might be covered in a typical high school Earth science scope and sequence. In authentic classroom settings, our team conducted pilot testing of these four instructional scaffolds during the initial year. We revised the scaffolds at the end of the first year based on feedback from the teachers. During the second year of the project, we again tested the full suite of MELs in four school settings (see Lombardi et al., 2018 for details on the second-year testing and study) and made final revisions to all materials in preparation for the project’s third year.The present study is associated with a quasi-experimental design phase of the project (third year). In this phase, we examined the finalized MEL materials with two other comparison activities: the MET, which used a table format in place of a diagram, and the Mono-MEL diagram, which used a diagram format similar to the MEL but with only one model, the scientific explanation. We conducted the present study in authentic school settings that had not been previously involved in the project, and specifically examined two research questions:1.How do instructional scaffolds promoting evaluation of alternatives (i.e., the MEL and MET) compare to one that does not (i.e., the Mono-MEL), specifically in the shifting plausibility judgments and changing knowledge toward scientifically accurate understanding?2.What are the relations between evaluation, plausibility, and knowledge, and how do instructional scaffolds promoting evaluation of the connections between lines of evidence and alternative explanations facilitate plausibility reappraisal and knowledge construction over the course of a school year?The novel aspect of the present study was our use of comparison activities that provided a robust test of the potential effectiveness of scaffolds designed to promote more critical evaluations (e.g., the MEL). Whereas one previous study has measured the effectiveness of the MEL against more traditional science instruction (i.e., generating evidence-based explanations; Lombardi et al., 2013), the current study compared the MEL, where students evaluated connections between lines of evidence and two alternative models, to the Mono-MEL, where students evaluated lines of evidence to only one model (i.e., the scientific explanation). Lombardi et al. (2013) suggested that the appreciable instructional advantage of the MEL (i.e., over the more traditional activity) was due to the MEL’s structure, where students evaluated connections between lines of evidence and two alternative explanations. In short, evaluations involving alternative explanations may be deeper and more critical than evaluations considering only one explanation. Therefore, we hypothesized that the MEL would be more effective than the Mono-MEL in both facilitating plausibility reappraisal and deepening knowledge. Because we were curious about the graphical nature of the MEL diagram, the present study also compared the MEL to the MET, which also asked students to evaluate connections between lines of evidence and two alternative models but did so in a tabular format. We hypothesized that MEL may have slightly more favorable outcomes (i.e., in terms of plausibility reappraisal and knowledge changes) because the directionality of the arrows might reinforce the causal relationship between lines of evidence and the explanatory models. In the MET, there is no visual cue that the evidence may be causally linked to the model (i.e., the code is only a pairing), and the lack of such relationship could influence evaluations and plausibility judgments (Chinn & Brewer, 2001).
