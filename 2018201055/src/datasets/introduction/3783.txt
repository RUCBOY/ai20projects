In recent years, artificial intelligence (AI) has been hitting the headlines with impressive achievements at matching or even beating humans in complex cognitive tasks (playing go or video games: Mnih et al., 2015, Silver et al., 2016; processing speech and natural language: Amodei et al., 2016, Ferrucci, 2012; recognizing objects and faces: He et al., 2015, Lu and Tang, 2014) and promising a revolution in manufacturing processes and human society at large. These successes show that with statistical learning techniques, powerful computers and large amounts of data, it is possible to mimic important components of human cognition. Shockingly, some of these achievements have been reached by throwing out some of the classical theories in linguistics and psychology, and by training relatively unstructured neural network systems on large amounts of data. What does it tell us about the underlying psychological and/or neural processes that are used by humans to solve these tasks? Can AI provide us with scientific insights about human learning and processing?
Here, we argue that developmental psychology and in particular, the study of language acquisition is one area where, indeed, AI and machine learning advances can be transformational, provided that the involved fields make significant adjustments in their practices in order to adopt what we call the reverse engineering approach. Specifically:
The reverse engineering approach to the study of infant language acquisition consists in constructing scalable computational systems that can, when fed with realistic input data, mimic language acquisition as it is observed in infants.
The three italicised terms will be discussed at length in subsequent sections of the paper. For now, only an intuitive understanding of these terms will suffice. The idea of using machine learning or AI techniques as a means to study childâ€™s language learning is actually not new (to name a few: Anderson, 1975, Berwick, 1985, Kelley, 1967, Langley and Carbonell, 1987, Rumelhart and McClelland, 1987) although relatively few studies have concentrated on the early phases of language learning (see Brent, 1996b, for a pioneering collection of essays). What is new, however, is that whereas previous AI approaches were limited to proofs of principle on toy or miniature languages, modern AI techniques have scaled up so much that end-to-end language processing systems working with real inputs are now deployed commercially. This paper examines whether and how such unprecedented change in scale could be put to use to address lingering scientific questions in the field of language development.
The structure of the paper is as follows: In Section 2, we present two deep scientific puzzles that large scale modeling approaches could in principle address: solving the bootstrapping problem, accounting for developmental trajectories. In Section 3, we review past theoretical and modeling work, showing that these puzzles have not, so far, received an adequate answer. In Section 4, we argue that to answer them with reverse engineering, three requirements have to be addressed: (1) modeling should be computationally scalable, (2) it should be done on realistic data, (3) model performance should be compared with that of humans. In Section 5, recent progress in AI is reviewed in light of these three requirements. In Section 6, we assess the feasibility of the reverse-engineer approach and lay out the road map that has to be followed to reach its objectives, and we conclude in Section 7.
