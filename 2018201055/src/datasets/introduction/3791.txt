When facing real-world problems, two major issues become crucial: to use problem-domain knowledge to adapt the problem solver to the task at hand, and to exploit available computational resources in the best way. These two issues can be regarded in an abstract sense as fighting the same dragon: complexity. This is very clear, even from a classical standpoint, in connection to the inherent difficulty of the problem, be it because of its size, the intricacy of the data and/or objectives or its potentially dynamic nature. The fact that they are facing complexity also becomes evident in relation to the latter aspect, that is, the use of computational resources, if we consider the increasing prevalence of interconnected techno-social systems composed of heterogeneous layers of resources with complex dynamics, ultimately driven by human and social factors. These can for example take the shape of non-conventional computational platforms such as volunteer-computing environments, peer-to-peer networks or pool-based systems, just to name a few.
Referring to these non-conventional environments, the notion of Ephemeral Computing (Eph-C) [[1], [2]] has been defined as “the use and exploitation of computing resources whose availability is ephemeral (i.e., transitory and short-lived) in order to carry out complex and possibly lengthy computational tasks”. The main goal in Eph-C is thus making an effective use of heterogeneous resources with low availability and high volatility whose computational power (which –if harnessed – can be collectively enormous) would be otherwise wasted or under-exploited; for instance, networked smartphones, tablets and, lately, wearables and sensors [3] –not to mention more classical devices such as desktop computers [4] – whose computational capabilities are often not fully exploited. There are some obvious connections to ideas from ubiquitous computing, volunteer computing and distributed computing, although Eph-C focuses more on the dynamics of the nodes and network and on the redesign of traditional algorithms to make them better suited to this kind of environment.
The effective exploitation of these platforms for problem-solving, as well as the effective resolution of complex problems requires the use of flexible, robust and adaptive algorithmic methods, capable of coping with the highly dynamic and volatile computational landscape and the massive complexity of the problem. Bioinspired algorithms are particularly well suited to this endeavor, thanks to some of the features they inherit from their biological sources of inspiration, namely decentralized functioning, intrinsic parallelism, resilience, and adaptiveness. In particular, they are prone to augmentation with self-control on their own functioning and/or structure. They are also readily adaptable to different optimization targets by embedding suitable problem-aware algorithmic components. This is crucial in order to tackle the study, analysis and optimization of techno-social systems, whose massive complexity often calls for a bottom-up perspective, understanding the behavior of the system as emergent properties of the interaction of its constituents.
Some bioinspired algorithms such as evolutionary algorithms (EAs) fit nicely into this scenario. However, few works have previously considered the interest of adapting evolutionary algorithms by adding the capability for coping with transient behaviors in the underlying computer systems. Moreover, Big Data [[5], [6]] has nowadays become a standard issue in many initiatives, in which large amounts of computational resources are required for storing, processing, and learning from huge amounts of data. This underpins the need for managing (often heterogeneous) computing resources widely distributed along the world, a task for which new methods and algorithms are very much welcome.Download : Download high-res image (335KB)Download : Download full-size imageFig. 1. Road map for the use of deep bioinspired algorithms for tackling problems of massive complexity and/or deployment on ephemeral environments.
In this context, and much like deep learning algorithms feature multiple processing layers to learn representations of data with multiple levels of abstraction [7], we can think of deep bioinspired algorithms (Deep-Bio) exhibiting multiple interconnected layers contributing the desired characteristics by encapsulating the tools required to tackle the different aspects of the complexity of the problem and the intricacy of the computational substrate, and whose interaction optimize the solving process. Throughout this paper we will elaborate on the need of these techniques, the major issues they have to face, their appropriateness for this purpose, and some of the most distinguished application areas for them, following for this purpose the road map depicted in Fig. 1 in which the major themes in this area and their interrelations are shown. As an entry point, we shall begin with an overview of Eph-C in next section.
