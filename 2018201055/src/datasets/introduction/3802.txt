Technology continues to play an increasingly important role in industrial-organizational (I-O) psychology and human resource management (HRM), particularly in the context of personnel selection and assessment (Stone, Deadrick, Lukaszewski, & Johnson, 2015). However, it would seem that the rapid rate of technological change has resulted in practice outpacing science (Arthur, Doverspike, Kinney, & O'Connell, 2017; Morelli, Potosky, Arthur, & Tippins, 2017). This is particularly salient in the realm of unproctored Internet-based testing (UIT) where the associated advantages, specifically, the ability of test-takers to test anywhere and at anytime, have resulted in a burgeoning increase in the use of mobile devices to complete these assessments (Arthur, Doverspike, Muñoz, Taylor, & Carr, 2014; McClure Johnson & Boyce, 2015) because UIT also by definition, in principle, gives test-takers the opportunity to use the device of their choice. Thus, mobile devices untether test-takers from the wall in terms of internet access,1 giving them more degrees of freedom in terms of where they can complete employment-related tests and assessments. However, in spite, or maybe because of these rapid changes, there is limited theoretically and empirically based guidance on the effects of technology on employment-related test and assessment outcomes (Arthur, Keiser, & Doverspike, 2017). Consequently, there has been a call for theoretical and conceptual models and frameworks that formally speak to when one should or should not expect technology-based effects on measurement-related outcomes of interest (Morelli et al., 2017). Furthermore, although most work in this area has focused on personnel selection and employment-related assessment, the issues of interest are potentially relevant to all researchers and practitioners who use UIT assessments in their research and practice.
So, in an effort to address the dearth of theoretical work in this domain, with an emphasis on the technology-mediated devices used to complete UITs, Arthur, Keiser et al. (2017) built on two dimensions—the structural characteristics of UIT devices, and the associated information processing variables that they engender—to develop a framework, the Structural Characteristics/Information Processing (SCIP) framework, as means for psychologically conceptualizing the effect of UIT device-type on test and assessment scores. Arthur et al. (2017) defined a UIT device as “any device that a test-taker can use to complete an unproctored Internet test or assessment where by definition, the test-taker also decides when and where to complete the assessment or test. Thus, a ‘UIT device’ is not synonymous with a smartphone or other mobile devices. A smartphone or other mobile devices are just one example of a UIT device; so is a desktop computer.” (p. 1). Subsequently, they argued that the technological characteristic of being unplugged from the wall (“mobile”) versus being plugged into the wall (“nonmobile”) failed to provide any conceptual basis or psychologically grounded explanations for why the use of ‘mobile’ versus ‘nonmobile’ UIT devices should or should not have an effect on test scores. Thus, the SCIP framework focuses on the psychological aspects of UIT devices that influence test scores.
The basic treatise of the SCIP framework is that differences in specified structural characteristics engender concomitant associated information processing demands, resulting in additional construct-irrelevant cognitive load which interacts with the device type, resulting in differential outcomes as a function of the construct (cognitive versus noncognitive) assessed. Hence, Arthur et al. (2017) advanced the SCIP framework as a means of psychologically conceptualizing the effect of UIT devices on assessment and test scores, and subsequently, also as a means for classifying UIT device-types on a continuum of construct-irrelevant cognitive load. Based on a review and integration of the literature, Arthur et al. also demonstrated that the SCIP framework conceptually explains and accounts for the measurement outcome-related findings (e.g., measurement equivalence, mean score differences, criterion-related validity, test-taker reactions and preferences) observed in the literature. They also advanced several testable propositions pertaining to when one might and might not obtain UIT device-type effects on employment-related assessment and test scores. However, there have not yet been any empirical tests of the tenets of the model to date. Consequently, the present study tests one major component of the model, namely, propositions related to working memory.
