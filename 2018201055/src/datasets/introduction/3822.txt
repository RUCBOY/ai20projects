The accurate and rapid classification of wild images plays a critical role in computer vision related applications. In practice, deep learning methods typically require thousands of well-labeled training samples to learn new concepts. Specifically, different categories often involve unbalanced samples, which means the sample number of each class is not equal. The instinct of unbalance problem is the unequal difficulties to obtain the labeled images of each class [4], [5], [30], [56], [60]. For example, the samples of automobiles and planes can be more easily obtained from the internet than those of ships and rockets, and the samples of cats and dogs can be more easily obtained than those of any other rare animals. Furthermore, even if the numbers of the samples are equal, the class-specific accuracy is affected by the diversity of the class samples. Thus, the balance of training dataset is vital for classification tasks.
Deep learning methods can achieve satisfactory performance with sufficient well labeled data sets [50], [54], but still face the unbalanced challenges. It should be noted that the performance gains achieved from existing deep networks can be attributed to the increase of network depth and large-scale well-labeled training dataset. For example, some works [16], [50], [54], [55] employ the blocks mode to increase the depth of deep convolutional architecture, and rank tops in recognition and detection tasks for Imagenet [46] challenges. However, deep models increase the dependence of the well-labeled training dataset at the same time [38], [39]. Unfortunately, the annotation of the dataset is expensive and time-consuming. Imagenet dataset needs tens of thousands of people to manually annotate 1500-class objects over fifteen million images in a crowdsourcing way, which costs about five years. It is hard to guarantee the balanced number of each class. However, the balance of sub-datasets is not essential for human’s complex neurons system, while only a few examples are sufficient for human to understand a new category and further make meaningful reasoning about other similar instances. Therefore, the learning method is to be improved in the sense of extracting more information on the unbalanced dataset. Thus, we aim to improve the learning ability of CNNs over the unbalanced dataset.
To solve the unbalance problem in deep learning field, previous works mainly focus on the re-sampling over numbers and distributions in-between categories [4], [5], [8], [30], [31], [51], [56] and the cost-sensitive loss [18]. The traditional methods can benefit to the “shallow” learning methods [13], however, it is not the most effective way to deal with unbalanced data in the context of deep learning. Moreover, such works commonly have inherent limitations [18]. For instance, over-sampling [36] can easily introduce undesirable noise, which gives rise to overfitting risks. Such limitations are also negative to deep learning methods [18]. These methods do not consider the essential problems during the unbalanced learning process, which can be summarized as three main challenges below. The first one is how to properly leverage practical unbalanced dataset with complex variances in scales, qualities, and acquisition difficulties. With this method, overfitting problem can be avoided as much as possible. The second one is how to adaptively enrich the information of scale-limited training dataset. With this method, unbalanced dataset is trained sufficiently. The third challenge is how to effectively accommodate the inexhaustible feature changes involved in the same-class instances. With this method, the generalization ability can be guaranteed.
Furthermore, the unbalance problem defined by traditional classifiers is limited to the number of each class. According to the previous works [2], the balanced dataset should satisfy two principles: (1) each of the involved sub-dataset has equal number of training samples; (2) the sub-datasets corresponding to each class should play an equal role in the training phase. This number-based definition makes the previous works focus on expanding number of samples, which can not expand the information for minority classes. When trained in an information-insufficient way, the CNNs easily produce an overfitting problem. Besides, the two principles are more or less ignored in the training process, which depends on the difficulty degrees of each class, and it is the instinct factor for training datasets of CNNs. Therefore, we extend the number-based unbalanced dataset to the accuracy-based unbalanced dataset for CNNs. On that basis, current CNNs-related methods typically suffer from unbalanced training datasets (such as MNIST, CIFAR-10, CIFAR-100, and SVHN datasets). In addition, the classification accuracy on the number-based unbalanced datasets, such as, CIFAR-10, CIFAR-100 (we re-organize them as number-based unbalanced dataset on purpose) can also be improved when adopting proper unbalanced learning schemes in the training phase. Therefore, the accuracy-based unbalance principles are more general than number-based ones (unbalance refers to accuracy-based unbalance in this paper unless otherwise specified). However, unbalanced learning will inevitably give rise to great challenges in CNNs, as it becomes very hard to learn from the samples of the minority classes [4], [5], [8], [30], [31], [51].
Bayesian based techniques, which are able to learn the distribution based on the training datasets, show potential over the small-scale dataset [49]. And the techniques can avoid the overfitting in large-scale datasets. For instance, Bayesian Program Learning (BPL) [26] is capable of learning a large class of visual concepts from a single example and generalizes in ways that are mostly indistinguishable from people. One-shot learning [48] developed a deep generative model to combine the representational power of deep learning with the inferential power of Bayesian reasoning. In fact, such methods are hard to represent high diversity in visual features. Motivated by both of the learning methods, we propose to learn the features via deep learning and further resort to the Bayesian network to augment the distribution, so as to solve the unbalance problem during CNNs training. Our method is based on the observation that the inter-class objects may have similar sample distribution, while the intra-class objects may have variations.
To tackle the aforementioned challenges, we propose to integrate the advantages of probability graph method built on the inter-class and intra-class distributions based on Bayesian hierarchical model to embed into CNNs. It is firstly trained on the CNNs to extract features, the features are clustered into groups via hyper-graph. The groups are further clustered into three subspaces based on the degree of data overlapping among different classes. Then based on the inter-class and intra-class distributions, new virtual samples are generated to augment the training datasets to solve the unbalance problem. Specifically, the salient contributions of this paper can be summarized as follows:

(1)We pioneer a generic deep variance network (DVN) by integrating three subspaces as the prior in the Bayesian network to iteratively back propagate into the powerful CNN framework, which can greatly improve the CNN performance for unbalanced training datasets.(2)We propose a hierarchical Bayesian model for the transfer learning of intra-class heterogeneity and inter-class homogeneity over CNN-produced feature space, which can intrinsically transfer the joint feature distribution from certain complete training dataset to other incomplete datasets, and expedite the training convergence of CNN.(3)We propose a virtual example generation method based on Gaussian kernel density estimation, which conduces to complete the unbalanced training dataset via passing the information from feature level to image level in a top-down way.(4)We verify our DVN by using it to furnish several state-of-the-art CNN networks, and conduct extensive experiments and comprehensive evaluations over CIFAR-10, CIFAR-100, MNIST and SVHN benchmarks, which demonstrate its superiorities in effectiveness and universality.
The rest of the paper is organized as follows. Section 2 firstly analyzes the unbalance problem in detail, and then briefly reviews the deep neural networks from three different perspectives. 3 Method overview, 4 Heterogeneity and homogeneity analysis on clustered CNN features and 5 detail our DVN framework. Section 6 evaluates our DVN over four datasets under accuracy-based and number-based unbalanced settings. Section 7 discusses the theoretic boundary of the proposed DVN. Finally, Section 8 makes conclusions and conducts discussions for future work.
