This paper adopts an “item explanatory” approach, where the focus is on investigating whether certain item features can explain differences in item difficulties by applying an extension of the linear logistic test model (LLTM; [6] to a middle school science assessment that was designed to follow the Next Generation Science Standards (NGSS; [15]. Explanatory item response models (EIRMs; [5] have the potential to provide explanations for the item responses, unlike descriptive models where item responses are merely described by the estimated parameters. While more traditional models output a list of estimated item difficulties, an “item explanatory” approach results in a list of estimated difficulties for item features. These item features must be selected a priori and, if content-related features are chosen, have the potential to provide strong content validity support for an assessment. Specifically, this paper explores the effects of five features on item difficulty: type, context, format, graphics, and academic vocabulary.
1.1. The Next Generation Science Standards (NGSS)The NGSS is a U.S. initiative designed to increase understanding of science, create common standards for teaching across the U.S., and develop more interest in science in school-age students in the hopes that more of them will major in a science-related area of study in college. The NGSS provides performance expectations to reflect a reform in science education that includes three dimensions: (1) developing disciplinary core ideas (DCI), (2) linking these core ideas across disciplines or crosscutting concepts, and (3) engaging students in scientific and engineering practices—based on contemporary ideas about what scientists and engineers do. The emphasis, in particular, is on integrating these three dimensions so that core ideas are not taught in isolation, but connect to larger ideas that also involve real-world applications. Rather than learn a wide breadth of disconnected content topics, the goal is to develop a deeper understanding of a few core ideas that set a strong foundation for all students after high school. The Learning Progressions in Middle School Science Instruction and Assessment (LPS) project, described in the next section, examined two of these three dimensions and designed an assessment to reflect their integration.
1.2. Item features for the Learning Progressions in Middle School Science Instruction and Assessment (LPS) projectOne of the main research goals for the Learning Progressions in Middle School Science Instruction and Assessment (LPS) project,1 was to explore the relationship between science content knowledge, a DCI, and scientific argumentation, a scientific practice. To further explore this relationship, the assessment was divided into three “complex tasks”, which consist of three item types: (1) argumentation items assessing argumentation competency in a specific scientific context (e.g. two students arguing over what happens to gas particles placed in a container), (2) content science items embedded within the same scientific context (e.g. what happens when you insert gas particles into a sealed container), and (3) content science items assessing knowledge of other concepts in the same science domain but not so closely associated with the context (e.g. compare the movement of liquid water molecules with the movement of ice molecules). In this paper, these three item types are referred to as ‘argumentation’, ‘embedded content’, and ‘content.’ Examples are provided in Fig. 1, Fig. 2, Fig. 3.Download : Download high-res image (143KB)Download : Download full-size imageFig. 1. An argumentation item from the Onions complex task.Download : Download high-res image (38KB)Download : Download full-size imageFig. 2. An embedded content item from the Onions complex task.Download : Download high-res image (48KB)Download : Download full-size imageFig. 3. A content item from the Onions complex task.The ‘complex tasks’ are each set within a context—that is all the items within a complex task share a common setting. These contexts are what happens when someone (a) chops onions, (b) inserts gas particles into a container, and (c) mixes sugar into a glass of water. These contexts will be referred to as ‘Onions’, ‘Gases’, and ‘Sugar’ for easier reference. The embedded content and argumentation items were presented in these contexts, while the content items were related (i.e., they were more generalized but about the same concepts). Note that while the context of the content items are more generalized, they were still designated into a context by the test developers.The remaining three item features explored in this paper are often tested in psychometric studies to examine whether they have an unintended effect on the item difficulties. For instance, the format refers to whether an item is open-ended or forced-choice (e.g., multiple-choice). Previous studies have suggested that multiple-choice items are easier for students than open-ended ones [10], [11]. Because the assessment includes a combination of both, this feature is investigated to see if this finding holds true for the LPS data.The graphics feature includes three categories: schematic representations, pictorial representations, and no graphics. Schematic representations are defined as abstract pictures whose “schematic meaning is provided by the symbolic/visual representation in the item” [13]. An example would include an image of the movement of gas particles. This contrasts with pictorial representations, which are concrete images that simply illustrate the details of objects described in an item.Lastly, whether an item contains academic words is explored. Academic vocabulary words are those that are not among the 2000 most common words and occur most often in academic texts [4]. Unlike technical vocabulary—which are the specialized words specific to a discipline, academic vocabulary words are more generalized and span across many content areas [19]. This distinction is important for many studies investigating the language effects of content assessments because while technical vocabulary is deemed to be construct-relevant, academic vocabulary is often seen as construct-independent and, subsequently, may interfere with the interpretations of student scores on assessments [2], [9], [23]. Coxhead’s [4] academic word list (AWL) is used here to identify academic words on the assessment2. Note that the word “evidence” is on the AWL, but will not be counted as an academic word in this paper because “evidence” is central to the argumentation construct. Thus, “evidence” is deemed to be construct-relevant, whereas other words on the AWL may be considered construct-independent.
1.3. Research questionsThis paper explores the effect of each item feature on the overall item difficulty. Specifically, the research questions are:RQ1. Which of the following, if any, item features—type, context, format, inclusion of graphics, and inclusion of vocabulary from the Academic Word List (AWL)—contribute to the explanation of item difficulty?RQ2. Does the item type feature interact with any of the other features to have a statistically significant effect on the item difficulties?Because of the added complexities in interpreting interactions, only one feature was explored in detail for RQ2. Item type was chosen because it directly relates to the content of the items and is also related to the main research goal of LPS—exploring the relationship between content knowledge and argumentation.
