Breast cancer is one of the most common invasive diseases among women worldwide. In 2016, there were more than 2.8 million women with a history of breast cancer in the U.S and this includes women currently being treated and women who have finished treatment. In 2017, 1,688,780 new cases of breast cancer are expected to be diagnosed and 600,920 cancer deaths are projected to occur, though death rates have been decreasing since 1989. These decreases are thought to be the result of treatment advances, increased awareness and earlier detection through screening [1]. Mammography is the recommended imaging modality for breast cancer screening [2], it is more useful as an early detection tool before the appearance of the physical symptoms. Early diagnosis of the disease via mammography screening increases the chances of recovery dramatically [2]. However, the accuracy of the diagnosis can be affected by the image quality or the radiologist's expertise prone to errors. The average error rate among radiologists is around 30%, according to some studies [3], [4]. In some recent surveys [5], error in diagnosis was the most common cause of litigation against radiologists. The majority of such cases arose from failure to diagnose breast cancer on mammography [5]. To reduce the rate of false-negative diagnoses, lesions with a 2% chance being malignant are recommended for a biopsy [6]. However, only 15–30% of the biopsies are found to be malignant [6]. As a result, the unnecessary biopsies end up costing so much in terms of time, money or even discomforts that can occur for some patients due to anxiety or panic attacks. It is therefore substantial to improve the accuracy of the radiologic diagnosis to increase the positive predictive value of mammography.
Computer-aided Diagnosis (CAD) systems aim at giving a second objective opinion to assist the radiologist medical image interpretation and diagnosis. CAD systems are especially used as applications that perform the labeling or differentiation between benign and malignant lesions. In the last few years, deep learning [7], [8], [9], [10] especially through Convolutional Neural Networks (CNNs) [11] has been proved to work very well in vision tasks. Some of the recently proposed CAD systems adopted the renowned deep learning techniques and obtained promising results [12], [13], [14]. The deep learning CADs were introduced to different medical domains, for example we can mention pulmonary Peri-fissural nodule classification [12] or Interstitial lung disease and Thoraco-abdominal lymph node classification [14] and many others. We were particularly interested in the works on breast lesions [15], [16], [17], [18]. Most of the proposed methods involved CNNs but in a traditional way, where they use only the extracted CNN features or combine them with some other hand-crafted descriptors to carry out the classification task [17], [18]. However, the most interesting aspect of using CNNs is the end-to-end supervised learning process which does not rely on complex engineered descriptors and instead uses the whole raw image [11].
Convolutional Neural Networks learn discriminative features automatically, their architecture is particularly adapted to take advantage of the 2D structure of the input image, but more importantly one of their most impressive characteristic is that they generalize surprisingly well to other recognition tasks [14], [16]. In order to train deep CNNs we need large annotated datasets which are lacking in the medical domain especially for breast cancer. Moreover, training a CNN from scratch requires high computational power, large memory resources and time, and with the little data provided we can easily start overfitting. One way to overcome this is to use transfer learning [19] from natural images (for example ImageNet which has more than 1.2 million images categorized under 1000 classes) and perform a fine-tuning as proposed in [14].
Transfer learning is commonly used in deep learning applications. It has been very effective in the medical domain [13], [14] when the amount of data is normally limited. Using transfer learning from natural images to breast cancer mammography images, has not yet been fully explored in the literature. And, as far as we know the only work [16] which uses transfer learning to classify breast lesions, employs small sized datasets and the deep Convolutional Neural Network CNN-F [20] as a model. We propose to perform the learning on different other datasets using some of the recent, well-engineered and deepest CNN architectures.
In this paper we exploit three of the most impressive CNN models recently proposed VGG16, ResNet50 and Inception v3 [21], [22], [23] trained on ImageNet [24]. We investigate the importance of transfer learning instead of random initialization for each model, and explore the impact of the number of fine-tuned layers on the final results. Our primary aim is to make use of these state-of-the-art CNNs, and perform a transfer learning from natural images to mammography images, in order to build a powerful mass lesion classification tool which can assist the radiologist by giving him a “second-opinion” and help him make more accurate diagnoses.
The remainder of this paper is organized as follows: Section 2 describes the proposed approach to classify breast abnormalities into benign or malignant, in Section 3 we give details of the experimentations lead to evaluate the proposed approach and we show the results. Section 4 gives a brief discussion and finally Section 5 concludes the paper.
