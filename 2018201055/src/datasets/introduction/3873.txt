Semantic scene completion is one of the significant computer vision tasks in the 3D field. It is helpful for a robot in obstacle avoidance and grasping, and to infer the complete 3D shape of objects from partial observations. A robot walks into a bedroom, for example, recognizes a doll which is almost completely hidden behind the sofa. Then it might want to go around the bed, walk over, and pick up the doll. These tasks require parsing the scene into different objects and surfaces. In addition, few tasks require understanding the interactions of scene elements. While this might be easy for humans, it is not easy for robots. In recent years, due to the popularity of Microsoft Kinect and other depth capturing devices, interest in 3D scene understanding has renewed. For this reason, some indoor RGB-D datasets, such as NYU-Depth v2 [1], SUN3D [2], SUN RGB-D [3], ScanNet [4], etc. [5], have appeared to support the 3D computer vision research for indoor scenes. Recently, SUNCG [6] has been introduced as a large-scale synthetic scene dataset. However, it only consists of synthetic depth maps and volumetric ground truth. In [6], the end-to-end 3D Convolutional Neural Network (CNN) known as SSCNet uses the single depth image to infer the occupancy and semantic labels for all voxels in the grid that are constructed for the scene. Our work is mainly inspired by Song et al. [6] but differs in that we combine the output of the network with dense Conditional Random Field (CRF) to carry out our work. Although CNNs have dominated the field in many high-level computer vision tasks most recently, we find that CRF is still a useful segmentation approach, either as a post-processing step [7], [8], [9] or as a part of the deep neural network itself [10], [11]. Recently, the influential work in [12] has popularized the use of dense CRFs. We extend the dense CRF model to 3D which can utilize the features of the volume data and aims at improving the depth maps. In this work, we call our proposed model as VD-CRF. The structure of our approach is illustrated in Fig. 1, where we leverage the dense CRF to improve the semantic scene completion from an indoor depth image. Our goal is to combine the output probability map of the SSCNet and the processed depth image, to make up the VD-CRF model and choose an effective inference algorithm to obtain a better result. We use some different data processing methods and various inference algorithms to achieve maximum a posterior and compare the effects. The main contributions of our work are as follows:
(1)We encode the depth map into Truncated Signed Distance Function (TSDF) or flipped TSDF, and obtain a better semantic scene completion performance by using these forms of data to build the CRF potentials.(2)We adopt different down-sampling styles for high-resolution 3D grid TSDF data and verify their effectiveness for the CRF.(3)We employ more than six different CRF inference algorithms, such as the mean-field (MF), the negative semi-definite specific difference of convex relaxation (DCneg), the proximal minimization of linear programming (PROX-LP), etc., and compare the performance of these inference algorithms based on a variety of parameter settings for the semantic scene completion task.(4)We experimentally compare our improvement with the SSCNet on three different datasets: SUNCG, NYU, and NYUCAD. We verify that our VD-CRF model is able to tackle these completion tasks on both man-made and real-world datasets.Download : Download high-res image (383KB)Download : Download full-size imageFig. 1. The structure of our proposed approach. First, we represent the input depth image using a TSDF or flipped TSDF to voxelize the space. We then combine the output probability map from the SSCNet (which is fed with the processed depth map) and the down-sampling volumetric data to build the potential functions of our VD-CRF model. Finally, a reliable inference algorithm is chosen to reason out the elevating prediction. Details about these procedures are given in Section 3.
The rest of the paper is structured as follows. Section 2 gives an overview of the related work. In Section 3, we discuss our proposed method. Experimental results are given in Section 4. Finally, Section 5 concludes the whole work.
