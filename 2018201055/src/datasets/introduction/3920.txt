Stirred tanks are widely used in chemical and process industries for the mixing of immiscible or miscible fluids, crystallization or chemical reactions. Stirred tanks are involved in manufacturing nearly half of all chemicals worldwide by value (Bashiri et al., 2016). A stirred tank is characterized by a vessel equipped with rotational impellers. The fluid flow generated by impellers is generally turbulent and the mixing is therefore enhanced. One critical step in the design and optimization of stirred tanks is to estimate their performances in the development phase. Compared to the experimental studies, Computational Fluid Dynamics (CFD) is expected to serve as an alternative tool to accelerate process development. The ultimate requirements for CFD simulation from industry are the fast and reliable predictions with easily accessible to computational resources, e.g., desktop computers or workstations, though supercomputers are becoming more favorable.
Over the past three decades, CFD simulation of stirred tanks has made a mighty advance. A large number of modeling techniques for stirred tanks were developed such as the black-box method (Harvey and Greaves, 1982), the inner-outer iterative method (Brucato et al., 1998), the multiple reference frame method (MRF) (Luo et al., 1994), the Snapshot method (Ranade and Dommeti, 1996) and sliding mesh method (Bakker et al., 1997). These methods can be classified into two categories. The former four represent the steady-state methods in which impellers keep motionless. By contrast, the impeller motion with respect to static walls is directly simulated in the sliding mesh method, which was reported to be more accurate than others (Yeoh et al., 2005), and yet both the transient simulation and the dynamic reconstruction of a body-fitted mesh are time-consuming. In these CFD solvers, many iterations are required in solving the Poisson equations of a huge matrix in the whole flow domain. In the sliding mesh method, the grid system is dynamically generated, and the computation data needs to be interpolated onto the updated grid system, which further increases the computational time. These intrinsic properties make most of the CFD solver structures, algorithms and multiphase models not compatible with the carrier of the computation, i.e., the high-performance computational architecture of Graphics Processing Unit (GPU) (Zhao, 2008). Moreover, the manually body-fitted mesh generation for stirred tanks might take about 80% of the analysis time (Zhang, 2012).
Lattice Boltzmann Method (LBM) is a novel approach with high parallelism in nature and explicit time-marching properties. Over the past two decades, LBM has been used as a fluid flow solver for the direct numerical simulation of immiscible multiphase flow such as bubble dynamics (Shu and Yang, 2013), high Reynolds number (up to 10 million) turbulent flows (Shu and Yang, 2018) and the simulation of stirred tanks. There are several pioneering studies about the parallel computation of LBM simulation of stirred tanks based on many CPUs (Central Processing Unit) (Derksen and Van den Akker, 1999, Derksen, 2003, Derksen, 2011, Derksen, 2012, Gillissen and Van den Akker, 2012, Hartmann et al., 2004a, Hartmann et al., 2004b, Hartmann et al., 2006a, Hartmann et al., 2006b). A large number of CPUs are needed to accelerate the LBM simulation. Moreover, several hours or even about one day were required to simulate only one impeller revolution of flow for laboratory-scale (Eggels, 1996, Tyagiet al., 2007) or industrial-scale (Derksen et al., 2007) stirred tanks. The newly booming GPU is of high-performance computation and lower power consumption, becoming widely used in scientific computation. The theoretical peak performance of a single latest NVIDIA Tesla V100 GPU graphics card is about 15 TeraFLOPS in single precision, which is faster than that of the world’s fastest supercomputer system based on CPU in TOP 500 list of supercomputers in 2001. The continuously emerging GPU enables a desktop computer to be powerful enough to boost large-scale simulations without resort to supercomputers. LBM is well suited to the GPU architecture and transient flow simulation due to its explicit time-marching characteristics or localized calculations. The compatibility of GPU architecture and the numerical algorithm of LBM could lead to more efficient simulations.
Although GPU-accelerated simulations have been used in solving LBM (Janßen and Krafczyk, 2011, Tolke and Krafczyk, 2008, Xian and Takayuki, 2011, Xionget al., 2012), the applications of GPU technique for stirred tank simulations are seldom reported. It is therefore of practical significance to utilize LBM and GPU architecture to accelerate or upgrade the stirred tank simulation for industrial applications. Moreover, the Immersed Boundaries Method (IBM) (Peskin, 2003), which explicitly treats complex or moving boundaries, can greatly save the time of manual mesh generation. The complex or curved boundaries can be represented by a couple of Lagrangian marker points and the boundary conditions are implemented by imposing additional forces or source terms to the equations in the Eulerian meshes adjacent to the boundaries. Therefore, there is no need to generate body-fitted grids in IBM. Instead, uniform cubic grids can always be used for flow solvers and the Lagrange points on boundary surfaces can be easily generated by Computer Aided Design (CAD) software. Implementations of IBM in stirred tank simulations have been conducted in previous works (Derksen and Van den Akker, 1999, Derksen, 2003, Derksen, 2011, Derksen, 2012, Gillissen and Van den Akker, 2012, Hartmann et al., 2004a, Hartmann et al., 2004b, Hartmann et al., 2006a, Hartmann et al., 2006b).
Large Eddy Simulation (LES) has been widely used and proved to be more accurate and promising (Hartmann et al., 2004a, Min and Gao, 2006). It is generally acknowledged that the grid resolution in LES has to be fine enough to resolve more than 80% of turbulent kinetic energy (TKE) (Pope, 2001). This may hinder their industrial applications, especially for GPU-accelerated computation. The GPU memory capacity is still limited and even the latest NVIDIA V100 card has only 16 Gigabyte memory since fast memory is expensive, though GPU-accelerated LBM simulation enables the highly temporal resolution or long-time simulation of stirred tanks. Therefore, more advanced LES models with reasonable grid resolution is required. An index based on the resolved TKE has been developed to estimate the grid resolution quality for bulk flow, and therefore more resolved TKE was equivalent to better grid resolution (Celik et al., 2009). This suggests that the grid resolution could be lower to resolve more TKE when an advanced LES model is used. Hence, it is of great significance to assess the various LES models by the resolved TKE with the GPU-accelerated computation.
In this paper, we attempt to evaluate the possibility or potential to accelerate the LBM-LES simulation of stirred tanks with a desktop computer of a single GPU graphics card. The Multiple-Relaxation-Times (MRT) LBM (D'Humieres et al., 2002, Lallemand and Luo, 2000) is used as a fluid flow solver, together with the IBM for modeling complex moving boundaries and structured meshes to discretize the flow domain of stirred tanks. The parallel computation code was developed and implemented on a desktop computer of a single graphics card. The computational efficiency (performance) of the GPU-accelerated simulation was estimated in terms of MLUPS (the million lattice units updates per second) (Xian and Takayuki, 2011) when using the maximum number of grid nodes that a graphics card can reach. It gives a reference of the capability of the desktop computation of a graphics card. With the GPU-accelerated LBM solver, we are able to estimate the macro-instability in stirred tanks with thousands of revolutions’ simulation. We also try to assess the grid resolution requirement of two different LES models with the percentage of the resolved TKE, and the accuracy of two different LES models by the predictions of time-averaged properties.
