Researchers have recently observed a significant increase in data that are encoded as symmetric positive definite (SPD) matrices - because they provide an easy platform for compactly fusing multiple features. Examples of SPD matrices in computer vision applications include diffusion tensors, structure tensors and region covariance descriptors. The diffusion tensor stems from medical imaging, where the tensors represent the covariance in a Brownian motion model of water diffusion [1], [2], [3]. In DTI, water diffusion is represented by diffusion tensors that characterize the anisotropy in the tissues. Structure tensors are low-dimensional feature – calculated from the spatial derivatives of the image; they are extensively employed in optical flow estimation and motion segmentation [4], [5], [6]. Region covariance descriptors are often employed to encode important image features [7], [8]. The region covariance matrix offers an efficient method for fusing multiple low-level features such as illumination, color, and gradient, etc, into a compact feature representation for computer vision-related applications.
Compared with vectors, SPD matrices offer a new method for capturing intrinsic geometric structures. Benefiting from an additional structure, matrices (also known as tensors) are often more informative and have been determined to be more empirically effective feature representations [1], [5], [6], [9], [10], [11], [12], [13]. However, SPD matrices lie on a Riemannian manifold that constitutes a convex half-cone in the vector space of matrices [14]. The space of SPD matrices, although a subset of vector space, is not a vector space, e.g., the negation of a positive definite matrix is not positive definite. Due to a lack of usual vector operations such as subtraction, addition and the mean, direct adoption of traditional algorithms often yields unsatisfactory performance [5], [15], [16], [17], [18]. The demand for a rigorous framework to address SPD matrices is urgent.
In this paper, we investigate the clustering of SPD matrices - a fundamental operations for the SPD matrix, by grouping intrinsically similar SPD matrices into the same cluster. This is to be distinguished from the well-known topic of manifold learning [19], where the data are assumed to be sampled from certain manifold embedded in a usually much higher dimensional Euclidean space and one is supposed to extract intrinsic geometric properties of the manifold from observations.
Drawing inspiration from the principle of competitive learning in the Euclidean space, we have developed a new framework, termed Riemannian Competitive Learning (RCL), for SPD matrices clustering. Different from traditional competitive learning, the new framework RCL considered the intrinsic geometry of the Riemannian manifold of SPD matrices by operating along the geodesics on the manifold. Similar to other winner-take-all clustering algorithms, RCL is sensitive to the initial states. To overcome this drawbacks, we further developed rFSCL by improving RCL with a conscious mechanism. Experiments demonstrate the effectiveness of the rFSCL algorithm.
Compared with existing state-of-the-art SPD matrices clustering algorithms, the new framework has several distinctive advantages. First, rFSCL inherits the online nature of competitive learning, which makes it very effective for handling very large data sets with moderate computer memory resources. Conversely, existing methods are batch algorithms and the need to compute and store the similarity matrix or kernel matrix of the samples hinders their ability to cope with large data sets. Second, rFSCL inherits the advantage of conscious competitive learning, which indicates that it is less sensitive to the initial values of the cluster centers and all clusters are fully utilized without the “dead unit” problem associated with many clustering algorithms. Third, as an intrinsic Riemannian clustering method, rFSCL operates along the geodesic on the manifold in a closed-form analytic manner and is completely independent of the choice of local coordinate systems. Extensive experimental results reveal that the clustering performance of rFSCL is superior to state-of-the-art clustering methods for SPD matrices.
The paper is organized as follows: In Section 2, we provide a brief introduction to the Riemannian manifold and the geometry of SPD manifold. In Section 3, the explanation of Euclidean simple competitive learning is given. In Section 4, we present a Riemannian competitive learning framework for SPD matrices clustering. Extensive experiments on simulated data sets are given in the Section 6. In Section 7, we describe the experiments using real applications and conclude our paper in Section 8.
Related work: In recent years, several SPD matrices clustering methods that consider the Riemannian geometry have appeared in the literature. Using a matrix logarithm, the authors in [20] mapped the manifold onto the tangent space, which is Euclidean at the mean tensor, and then applied standard techniques for image segmentation. Using the same strategy, the authors in [21] developed a semi-supervised framework for clustering covariance descriptors. These mappings approximate the manifold by Euclidean space but a mapping that could globally preserve the manifold structure does not theoretically exist.
In [6], the authors applied affine-invariant Riemannian metric (AIRM) [22], [23] as a similarity measure to implement SPD matrices K-means for human activity analysis. The authors in [10] employed Jensen-Bregman LogDet Divergence (JBLD) as a similarity measure for clustering covariance matrices. In K-means type Riemannian clustering methods, the need to compute the Karcher mean [16], [24], which usually incurs heavy computational overheads (no closed form solution exists). To decrease the computation time of the K-means type Riemannian clustering algorithm, the authors in [18] presented a recursive estimation of the Stein metric [25] center of SPD matrices. However, this method will sacrifice the accuracy to some extent.
In [17], the authors proposed mapping SPD matrices to a high dimensional Reproducing Kernel Hilbert Space (RKHS) and then extended the kernel-based algorithm developed for Euclidean space to a Riemannian manifold of SPD matrices. In [26], the authors first implemented random projections for manifold points via kernel space, and then applied K-means to cluster the manifold-valued data. These methods usually incur randomness and sometimes bad results. The authors in [15] clustered submanifolds of Riemannian space using basic concepts from Riemannian geometry and nonlinear dimension reduction. The clustering is performed in a low dimensional space after dimension reduction, which doesn’t always preserve all the information in original data. Similar to the kernel method [17] which requires calculation of a kernel matrix, this method requires computation of a similarity matrix (or affinity matrix), which hinders the method’s application to problems with large sample sizes.
An EM algorithm for clustering SPD matrices using a mixture of Wishart distribution is suggested in [27]. To apply the clustering algorithm to large scale real-world problems, the authors in [28] presented the Dirichlet Process Mixture Model (DPMM) framework for clustering SPD matrices. The DPMM can dynamically update the number of clusters according to the complexity of the data. However, DPMM is slow and sometimes fails to converge.
