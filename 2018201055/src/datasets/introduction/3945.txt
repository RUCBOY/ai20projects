The rapid evolution of mobile devices has drastically changed the way people conceive of the notion of mobility. For decades, the general public was accustomed to accessing information through immobile computers. However, the advent of laptops has lead the public to reassess their expectations in terms of mobility. Recently, the growing ubiquity of mobile devices has allowed users to overcome this limitation. For example, Facebook reported 1.44 billion active monthly users and 936 million active daily users globally; of those monthly active users, 87% use Facebook on their mobile devices.
This new paradigm of accessing information on mobile devices has created new conflicting requirements between performance and responsiveness. Users expect desktop-level performance on mobile devices. In addition, although the improved performance of computing and information application services is a common goal for mobile and desktop applications, the different expectations with respect to responsiveness suggest that the criteria for user satisfaction are device-specific.
To satisfy the increasingly demanding requirements in performance and responsiveness, mobile devices have been borrowing hardware components that are traditionally present on computers. Recently, the popularity of mobile gaming has driven the industry to incorporate graphic processing units (GPU) into mobile devices to handle heavy graphics computation. Nowadays, mobile devices are able to handle many complex computations heretofore reserved to desktop computers. This rapid development of mobile technology has led the public to gradually shift from desktop computers to mobile devices as their primary computing device.
Mobile CPUs and GPUs have gradually integrated desktop software techniques. Nowadays, the general-purpose computation on GPU, or GPGPU, paradigm has been ported to mobile devices and can be used to accelerate different types of applications by parallel computing on the GPU. Considerable research has been conducted to develop a new programming model that could leverage the computing capabilities of GPU. The Open Computing Language (OpenCL) has recently emerged as an open standard that supports parallel execution across heterogeneous platforms consisting of multiple types of computing devices, such as CPUs and GPUs. In addition, while the CPU is designed for latency, the focus of the GPU is throughput and performance per watt. Because parallel application execution results in increased performance within the same power envelope and thus better end-user satisfaction with mobile devices, programmers are now able to take advantage of task-based and data-based parallelisms in their application and provide users desktop-level performance on mobile devices.
Despite the benefits of adopting GPU into mobile devices, the power consumption and cooling of GPU impose severe limitations; the full utilization of hardware resources such as CPU and GPU can lead to excessive power consumption to the extent that mobile devices are not able to cool down. Consequently, the computing resources of mobile devices are generally not able to operate under full load or consume their maximum thermal design power budget. This constraint can impact user satisfaction negatively.
The number of available mobile applications is increasing, and their resource requirements are ever more demanding. Despite the technological developments, e.g., parallel computation on GPU, the hardware capabilities of mobile devices have fallen far behind the resource requirements of these applications. Hence, the execution of resource-intensive tasks is likely to impede mobile phone responsiveness and reduce user satisfaction.
Presently, industry and academia see in machine learning an opportunity to improve user experience by giving computers the ability to learn from experience. Mobile devices and their interaction with the Internet Of Things are a massive source of data on user behavior. Machine learning algorithms can analyze these data and extract information on users that can be used to personalize or enhance the each user’s experience. As a result, several researchers have attempted to bring the machine learning paradigm into the mobile computing world. Recently, Google patented a concept of native machine-learning services on mobile devices [10].
Despite their sophistication, mobile devices are not able to efficiently perform resource-intensive tasks such as machine learning. Much research has been devoted to leverage cloud resources to augment mobile devices [[3], [2], [18], [20], [26], [50], [51], [55]]. This kind of approach has been used by “SIRI”, “Cortana”, “Google Now”, and recently by “M”, the new Facebook virtual assistant. The rise of the Mobile Cloud Computing research area has created an opportunity to offload the mobile device’s resource-intensive workload to the server. Although offloading computation to the server improves the end-user experience, the mobile application becomes highly network-dependent. As a result, a degradation in network quality could seriously hurt the performance gained by offloading. Therefore, highly network-dependent applications do not fit into the mobile cloud computing paradigm.
Despite the obvious performance benefits of offloading techniques, executing a machine learning algorithm on the server side could also present an ethical challenge for privacy. Indeed, the public is reluctant to offload their private data. In addition, Big Data and the Internet of Things work in conjunction, and consequently, the training data produced by mobile devices tend to increase, which is an obstacle for an offloading approach. In light of these issues, a new machine learning strategy that would perform the training and the prediction process on the local device needs to be developed.
Principal Component Analysis (PCA) is a widely used unsupervised dimension reduction technique that performs an orthogonal transformation of a set of possibly correlated features to a possibly smaller set of linearly uncorrelated components. In this paper, we implement PCA on a mobile device. This choice is motivated by three reasons. First, it has been proven that unsupervised dimension reduction is closely related to unsupervised learning. The principal components extracted by PCA are the continuous solution of the discrete K-mean cluster membership problem [24]. In fact, PCA aims to determine the most discriminative cluster subspace. Consequently, PCA could be used in order to make an efficient predictive model on mobile devices. Second, while supervised algorithms depend upon training data label, unsupervised approaches such as PCA offer the advantage of not requiring any labeled data. With regards to mobile devices, it is impractical to require the user to label all training data by himself. Finally, in a mobile computing context, where the memory and the limitation in computation capability are particularly serious issues, an efficient dimension reduction algorithm such as PCA plays a crucial role in the development of a fast, accurate, and energy-efficient machine learning strategy.
In order to perform a machine learning task with desktop-level performance on a mobile device, it is mandatory to exploit its heterogeneous architecture. GPU is particularly suitable to a vectorized implementation in that it speeds up this kind of parallel processing application and reduces its power consumption. Despite these merits, mobile GPUs suffer from a lack of precision that can lead to round-off error. The multiple linear algebra operations inherent in a machine learning application may cause errors to accumulate and lead to a biased overall output. On the other hand, modern mobile CPUs are provided with several CPU cores that can perform parallel tasks with high precision. However, due to the CPU’s general purpose architecture, computation on the CPU cannot always be streamlined efficiently, and the utilization of all of the cores on a CPU could lead to a significant rise in power consumption.
Our research aims to design a hybrid machine learning system that takes the best of both worlds: the massive throughput provided by the GPU and the excellent precision provided by the CPU. Our research exploits the shared memory architecture of mobile devices’ System-on-Chip design in order to make the CPU and the GPU collaborate on a specific task without any data copying. In addition, we also consider the processing speed and the energy efficiency of our computation. Finally, in this paper we attempt to define a clear boundary regarding the restrictions of utilizing the mobile device’s GPU as an accelerator.
The rest of this paper is organized as follows: in Section 2, we give an overview of the working environment and detail the main issues in such an environment. In Section 3, we describe a collaborative CPU–GPU implementation of PCA that efficiently exploits its inherent parallelism. In Section 4, we investigate the role of a collaborative CPU–GPU approach in achieving good performance and energy efficiency on a mobile heterogeneous platform. We also give experimental evidence via a concrete utilization of PCA for image recognition purposes. Section 5 develops the analysis further and discusses several concerns in implementation. In this section, we will also provide recommendations to extend this approach to other parallel computations in a heterogeneous mobile computing environment. Finally, in Section 7 we summarize our work.
