The basic idea of matrix completion is to find a complete low rank matrix that is consistent with original matrix on known entries, which has been studied widely in many fields such as recommendation systems [1], [2], background subtraction [3], [4], Photometric Stereo [5], and structure from motion [6].
Given a matrix X=[x1,x2,⋯…,xn]∈Rm×n, where n is the number of samples and m is the dimensionality of sample. The matrix X is stacked by n column vectors, hence the matrix completion can be naturally seen as an accumulation of n vector completion problems. As described in Fig. 1, given a complete vector m=(x,y,z), one can exactly determinate its location in original space. Nevertheless, the vector (point) m is confirmed only up to the line l when the value of z is absent. In practice, however, if the low dimensional subspace S where m lies is known in advance, one can confirm the value of z by intersecting line l and subspace S [7]. Hence, the vector completion problem can be further translated as a subspace learning problem.Download : Download high-res image (61KB)Download : Download full-size imageFig. 1. A sample instance for vector completion. The point m is confirmed only up to the line l when its coordinate on z-axis is absent. However, the locations of m can be determined when the subspace S where m lies is known.
Principal Component Analysis [8] is a pioneer for subspace learning. Over the past decades, a number of algorithms have been proposed to improve its robustness and practicability. Such as ℓ1-PCA [9], which is developed for handling sparse noise, PCE [10], a method that can automatically estimate the feature dimension of subspace, LRR [11], [12] and its variants [13], two representative methods for coping with the setting that samples are drawn from a union of multiple subspaces. Nevertheless, both of them are based on the assumption that the input data is complete. Recently, research work focus on the problem that learning a subspace from highly incomplete information. Generally, most existing methods can be divided into two categories: batch methods [14], [15], [16], [17], [18], [19], [20] and online methods [21], [22], [23]. Compared with the latter, the former has received more attention in past ten years due to its conciseness and theoretical guarantee.
Batch methods. The batch methods are generally referred to robust matrix completion. Given a matrix X ∈ Rm × n, the batch algorithms aim at solving the following problem:(1)minLrank(L)s.t.PΩ(L)=PΩ(X).Since the problem (1) is NP-hard, many research works were devoted to solve its variants. Depending on the way of relaxing the rank minimization problem, all batch methods can be grouped into two categories: Bilinear Factorization (BF) [17], [18], [24] and Nuclear Norm Minimization (NNM) [25], [26], [27], [28]. The most attractive advantage of NNM is that it can automatically determine the dimensionality of subspace. While the high time consuming [28] caused by SVD per iteration limits its application on large scale data. Comparing with the NNM, the BF based methods generally performs better in terms of computational efficiency and storage consumption. By utilizing the full rank decomposition when the rank r of desired matrix is known, most BF based methods reformulated the problem (1) as:(2)minU,V∥PΩ(X−UV)∥ℓq,where U ∈ Rm × r and V ∈ Rr × n are two low rank matrices, ℓq denotes the norm used to measure loss function. It is obvious that the low dimensional subspace U is fixed when matrix L=UV is determined.
For problem (2), the ℓ2-norm based loss function is widely used due to its convexity and smoothness. Although ℓ2-norm based matrix completion algorithm returns a good estimation of the subspace when the known entries are damaged by Gaussian noise, it breaks down under sparse corruptions, even if that only a fraction of known entries are corrupted.
This undesirable issue has motivated the research of recovering a low rank matrix L from a corrupted and incomplete matrix PΩ(X)=PΩ(L+E), where E represents a sparse matrix. To emphasize the sparseness of E, many algorithms [29], [30], [31], [32] utilize ℓ1-norm to build the loss function. Here ℓ1-norm is a convex relaxation of the ℓ0-quasi-norm. Unfortunately, the solution provided by ℓ1-norm may deviate from the original solution when the magnitude or number of corruptions is very large [33]. Recently, considering that some columns (samples) may be contaminated completely,  Chen et al. [25] developed a robust method which emphasizes the sparseness of E by ℓ2, 1-norm. However, its performance on real data is limited by the fragility to sparse corruptions. To reduce the ambiguity of notations, in the rest of this paper the entries corrupted by sparse noise are named as corruptions, and the columns (samples) contaminated completely by unknown noise are named as outliers. The [34] has considered the problem of recovering a low dimensional subspace from a matrix contaminated by sparse corruptions and outliers simultaneously, but it can not deal with the setting that the matrix is incomplete.
In addition to the susceptibility to mixed noise, a significant drawback of existing batch methods is the low efficiency for dealing with the out-of-sample problem. Here, the out-of-sample denotes a fresh vector falling outside the training data. For instance, the new frame in video data. Actually, to estimate the missing values of a new sample the batch methods have to re-implement the algorithms on the entire new dataset, which is unacceptable for high dimensional data. The out-of-sample problem has been considered in [35], while it is developed for subspace clustering and can not be directly generalized to matrix completion.
Online methods. For the purpose of real-time in some practical applications, several online methods have been proposed. Compared with batch algorithms, the online methods have received little research attention, because it is susceptible to noise. Most online methods share the common program: first, parting the matrix X as a series of column vectors {xi}; second, initializing a low rank matrix and then updating it by solving the following problem:(3)minU,vi∥PΩi(xi−Uvi)∥ℓq.where vi denotes the coordinate vector that projecting xi onto the current subspace. Finally, estimating the missing values of xi by Uvi, where the columns of U are the basis of subspace. The main differences between existing methods are the way of updating U and the norm used to measure the residual error, i.e., the value of q. Incremental Singular Value Decomposition (ISVD) [36] is a prior work for online learning subspace from incomplete information, which utilizes the SVD of Xn=UnSnVnT to estimate the SVD of Xn+1=Un+1Sn+1Vn+1T, where Xn+1=[Xnxn+1]. Then an online manifold learning algorithm GROUSE was proposed in [21], which conducts gradient descent on the Grassmannian manifold to update U. The equivalency between ISVD and GROUSE has been proved in [37]. Subsequently, considering that the ℓ2-norm based objective functions are very sensitive to sparse corruptions, He et al. [22] substituted the ℓ2-norm by ℓ1-norm in GRASTA. However, this method is still deficient due to the gap between ℓ1-norm and ℓ0-norm.
In addition to the approaches mentioned above, some online methods [23], [38] have been developed, but most of them are vulnerable to outliers due to the way of updating subspace. As presented in (3), the online methods update the subspace by processing the data matrix, one column at a time, which makes them can estimate the missing values of one column vector (training data or out-of-sample) efficiently. However, a clear drawback is that the recovered subspace may deviate the intrinsic subspace significantly when some columns of training data are contaminated completely. Besides, although the existing online methods can be used to cope with the out-of-sample problem, none of them give a reasonable interpretation w.r.t this strategy. A natural problem is that why the coordinate vector v learned from the known entries can be used to calculate the missing values. In Section 2, we will fill this blank via a sample formulation.
Paper innovations: As mentioned above, whether out-of-sample or matrix completion, each can be solved by vector completion under the condition that the intrinsic subspace is known. Hence, learning an optimal subspace from an incomplete matrix with simultaneous presence of sparse corruptions and outliers is the core of our method. To achieve this goal, we make the following innovations:
•First, we provide a formulation interpretation for Fig. 1 (a general case that x contains multiple missing values), which has been omitted or ignored by many online methods. Significantly, the interpretation provides a guarantee for utilizing the subspace to solve the out-of-sample problem.•Second, to learn the intrinsic subspace where clean samples lie we develop a robust batch method based on matrix Bilinear Factorization (BF). Particularly, we use the non-convex ℓp-norm rather than convex ℓ1-norm to built the objective function, which improves the robustness of algorithm to sparse corruptions. Note that the proposed non-convex optimization problem can be solved efficiently via an Inexact Augmented Lagrange Multiplier (IALM) method.•Third, to remove the obstacle caused by outliers, we introduce a potential outliers tracking and removing strategy. More exactly, we learn the subspace by recovering only a subset of columns of original matrix. And, these columns have been determined as inliers. The experiment results present the motivation of this strategy and verify the advantage of it.•Fourth, we propose a robust vector completion model which can utilize the subspace learned from inliers to estimate the missing values of out-of-sample accurately, even when the out-of-sample are contaminated by a great many of sparse corruptions.
Organization. The rest of the paper is organized as follows. We begin by describing the proposed method in Section 2. The optimization processes of proposed batch method and robust vector completion model are presented in Section 3. The performance evaluation and relevant experiment results are reported in Section 4. Finally, we conclude this paper with directions on potential improvements for future work in Section 5. The code of our method can be downloaded from in https://github.com/sudalvxin/Matrix-completion.git.
