Visualization is a fundamental task in the scientific discovery process, although it was not until the late 80s when it was properly established as a research area [1]. Leigh et al. stressed that visualization serves at least three important purposes [2]: (i) quick verification of simulation models; (ii) quick delivery of simulation results; and (iii) easier communication to a layman. In Data Science, visualization is particularly valuable in the exploration and presentation of the results stages; that is, at the beginning and at the end of the data analysis process [3]. These are two phases which require a transfer of knowledge between data scientists and domain experts. Indeed, data scientists need to validate their assumptions and findings with the experts in order to redesign, repeat, or conclude the process.
Visual data exploration is particularly useful when little is known about the source data and the analysis goals are vague. In this context, visual data exploration can be viewed as an evolving hypothesis-generation process [4], during which hypotheses can be validated or rejected on a visual basis, and new ones can be introduced. According to Shneiderman [5], visual data exploration is a three-step procedure encompassing data overview, facet zoom and filter, and on-demand detail request.
Knowledge graphs – and more specifically, Semantic Web ontologies and linked data – are an increasingly important source of primary and contextual data in Data Science [6]. The rationale is that Semantic Web technologies provide a suitable infrastructure for publishing, storing, retrieving, reusing, integrating, and analyzing data [7]. Linked data languages are based on 3-tuple representations; specifically, RDF (Resource Description Framework) uses 〈subject,predicate,object〉 tuples [8], and OWL (Ontology Web Language) add formal semantics to RDF triples, allowing the definition of more complex hierarchies and networks [9]. Therefore, they have favored the development of graph-based visualizations. Other visualizations, such as topic maps [10], geographic maps [11], and statistical charts [12] have also been used when the semantics of the underlying data is appropriate.
The growth of available data is a common issue in Data Science, Semantic Web and Data Visualization, and so the necessity of frameworks capable of dealing with this overabundance. Regarding visualization, hardware and software tools are being created to deal with large-scale datasets and to visualize them on large screens and video-walls. Visual spaces, supported by distributed data processing clusters, are fostering speed, accuracy, comprehension and confidence on the data analysis. Early works in this topic were developed by the Electronic Visualization Laboratory at the University of Illinois at Chicago, resulting in the CAVE [13] and CAVE2 [14] environments. Other examples of visualization environments have been described in the literature [[2], [15], [16]]. Such proposals have been shown effective in domains in which large amounts of intensively-interrelated data are generated. However, the increasing potential of new Big Data technologies for distributed data collection and processing calls for new initiatives.
Recently, our research group proved that graph visualizations, backed by a large-scale visualization environment, can be very helpful to understand the dynamics of different phenomena; e.g. Bitcoin transactions [17] and online discussions [18]. In these works, we showed how filtering, grouping and highlighting specific patterns help to increase the usability of large graph visualizations. Nevertheless, we also experienced that the scalability of the implemented platforms is limited. Furthermore, until recently we lacked a clear characterization of the efficiency of the graph generation process, and a wider analysis of the Big Data tools that can help to mitigate performance issues.
This paper presents the results of evaluating the computational performance of large-scale graph processing technologies for different visualization tasks. Such research work is an open challenge in this area [[19], [20]]. Our analysis focuses on the performance of the data preparation, calculation of graph metrics, and graph layout stages for different graph types. For the experiments, we developed a software package implementing these tasks based on GraphX [21], the API included in Apache Spark [22] for graphs and graph-parallel computation.
The main contributions of the paper are the following:



•
We describe a Big Data architecture for the generation of customized knowledge graph visualizations.
•
We perform several experiments with different freely available datasets, in order to assess the efficiency of the graph visualization process.
•
We conclude that the performance of the graph building and the graph layout stages significantly improves in the Big Data setup, allowing us to process graphs with more than one billion edges.
