In the era of data explosion, it becomes very difficult for people to acquire truly desired information from billions of bytes. In such a context, recommender systems (RSs) have attracted great interests since they perform efficient information filtering. In an RS, a user-item rating matrix is commonly considered as the fundamental data source [3], [4], [5] that describes a user’s preference on an item according to his/her historical experiences. Meanwhile, due to the exploding user and item counts, it is impossible to obtain the full interactions between users and items. Under such circumstances, a high-dimensional and sparse (HiDS) matrix [1], [2], [3] is frequently adopted to describe their connections. For instance, LibimSeTi collects the Dating Agency matrix [45], which contains 17,359,346 observed interactions between 135,359 users and 168,791 profiles, whose dimension comes to 135,359 × 168,791 while data density is 0.076% only.
In spite of its high sparsity, an HiDS matrix contains various desired knowledge like users’ potential favorites [4], [5], [6], [20], [21], [22], [23], [24]. Hence, many models are proposed for acquiring knowledge from an HiDS matrix [8], [11], [14], [46], [47], [48]. Considering existing models, a latent factor analysis (LFA)-based model is highly popular owing to its high scalability and efficiency [4], [5], [9], [15], [16], [17], [18], [19]. The essentially principle of an LFA model is to map the row and column entities into the same low-dimensional LF space, and build a learning objective on an HiDS matrix’s observed data related to desired LFs. This objective is then minimized with a learning algorithm to build these LFs for correctly representing an HiDS matrix. So far, many sophisticated LFA models have been proposed, like a probabilistic LFA model [8], a biased combination LFA model [9], a neighbor correction LFA model [10], a fast nonparametric LFA model [7], [11], and an inherently non-negative LFA model [13], [14].
An LFA model often adopts a standard stochastic gradient descent (SGD) algorithm as its learning algorithm to optimize its non-convex learning objective with desired LFs [49,59]. However, such learning algorithm updates an LF relying on the stochastic gradient defined on the instant loss only, without considering information described by historical updates. Hence, an SGD-based LFA model commonly takes many iterations to converge [4], [8], [9], [12], [37], [38], which results in enormous time cost on large-scale datasets.
On the other hand, a proportional-integral-derivative (PID) controller [30], [31], [32], [33], [34], which is often adopted in feedback control system [34], [35], [36], [50], exploits the past, current and future information of prediction error to control a feedback system. Owing to such design, a PID controller has achieved great success in industrial applications [50], [51], [52], [56], [57], [58]. Is it possible to incorporate the principle of PID into an SGD-based LFA model, thereby making it achieve high performance on an HiDS matrix? To answer this critical question, this paper presents a PID-incorporated SGD-based LFA (PSL) model. Its main idea is to rebuild the instant error on a single instance following the principle of PID, and then substitute this rebuilt error into an SGD algorithm for accelerating an LFA model’s convergence. The contributions of this paper are as follows:
a)A PSL model that incorporates the principle of PID into its SGD-based learning scheme, thereby successfully combines the information of past, current and future updates to achieve high performance;b)Detailed algorithm design and analysis for a PSL model.
Extensive experiments on six widely-accepted HiDS matrices indicate that compared with state-of-the-art LFA models, a PSL model achieves significantly higher computational efficiency with highly competitive prediction accuracy for missing data of an HiDS matrix.
Section 2 gives the preliminaries. Section 3 presents the methods. Section 4 reports the experimental results. And finally, Section 5 draws the conclusions.
