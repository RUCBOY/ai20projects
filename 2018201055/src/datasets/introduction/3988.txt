Complex-valued Hopfield neural networks (CHNNs) employing multistate activation functions can deal with multi-level information, and have often been applied to the storage of image data [1], [2], [3], [4], [5], [6]. Many extensions of CHNN have been proposed. Hyperbolic algebra is a 2-dimensional (2-D) number system, like a complex number field. Several models of hyperbolic Hopfield neural network (HHNN) have been proposed [7], [8], [9]. The HHNNs that employ directional multistate activation function improve noise tolerance [10]. Dual-numbered Hopfield neural networks have also been proposed [11]. Quaternion algebra is a 4-D number system, and includes a complex number field. Several models of quaternion Hopfield neural networks (QHNNs) have also been proposed [12], [13], [14], [15], [16]. Twin-multistate QHNNs can deal with multi-level information, and require only half the connection weight parameters of CHNNs [17]. The commutative quaternion is another 4-D number system [18]. The multiplication of quaternions is not commutative, unlike that of commutative quaternions. Commutative quaternions include both complex and hyperbolic number systems, and are helpful for processing non-Euclidean space. Commutative quaternions are easy to calculate. However, it is difficult to define Hopfield neural networks using commutative quaternions, since commutative quaternions include zero divisors like hyperbolic numbers. Isokawa et al. attempted to define commutative quaternion Hopfield neural networks (CQHNNs) [19]. As they used polar representation, their models were complicated to handle. In fact, the learning algorithms and computer simulations have never been provided. Here, we define CQHNN as an analogy of twin-multistate QHNN.
The projection rule is an excellent learning algorithm, as it is a one-shot learning algorithm with large storage capacity [20]. However, it requires fully connected networks. This problem is severe under the restricted memory resource. The projection rule for CQHNNs has been provided, and requires only half the connection-weight parameters of CHNNs, similar to QHNNs. Thus, the storage capacities of QHNNs and CQHNN are half of that of CHNNs. Computer simulations were performed to compare the noise tolerance of QHNNs and CQHNNs. CQHNNs underperformed QHNNs. We discuss the reasons why from the perspective of rotational invariance and self loops.
The rest of this paper is organized as follows. Section 2 describes quaternions and commutative quaternions. CHNNs and QHNNs are defined in Sections 3 and dummyTXdummy- 4, respectively. In Section 5, we propose the CQHNNs. Rotational invariance is described in Section 6, and computer simulations are conducted in Section 7. We discuss the simulation results in Section 8, and Section 9 concludes this paper.
