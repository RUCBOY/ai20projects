Learning systems are an important new class of systems designed to support the many phases of the Machine Learning (ML) lifecycle (see Fig. 1). Various learning systems have been developed to support model development [10], [24]; scalable training across thousands of cores and GPUs [4]; model publication and sharing [6]; and low-latency and high-throughput inference [22].
The rapid adoption of ML across science, for example to design and discover new materials and molecules [55], [56]; to detect and make cancer diagnoses [32] and to enhance patient care [48]; to act as surrogates for more expensive simulations [27], [47]; and to guide genome-editing capabilities [30], brings with it unique and urgent challenges. For example, there is a need to discover, reuse, and reproduce models published in the literature to validate and extend cutting-edge results; publish models with descriptive metadata and persistent identifiers for discovery and unambiguous citation; and to scalably and reliably execute models on the myriad resources available to researchers.Download : Download high-res image (129KB)Download : Download full-size imageFig. 1. ML lifecycle, adapted from Miao et al. [36].
In this paper, we present the Data and Learning Hub for science (DLHub) and outline initial experiences applying this learning system to science. While many learning systems focus on building and training ML models [1], [4], [29], DLHub is a unique learning system that is designed to support the publication and serving of ML models in science. DLHub is implemented as a cloud-hosted service that allows researchers to deposit and share models of various types, including TensorFlow [1], Keras [20], PyTorch [43], and Scikit-learn [44]. It defines common metadata schemas for describing these models and the parameters and other inputs used to invoke them. It also implements a rich access control model that allows users to publish their models privately, publicly, or with a select group of other users.
DLHub offers a unique model serving infrastructure that is capable of serving many different types of models on a range of distributed computing resources including clouds, clusters, and supercomputers. The serving infrastructure builds upon funcX [15], [17]—a distributed Function-as-a-Service platform developedspecifically to support remote and distributed execution of functions. DLHub implements a flexible pipeline that converts deposited models into servables—executable containers that implement a standard DLHub execution interface, irrespective of the model type, and includes the trained model, model components (e.g., training weights, hyperparameters), and dependencies (e.g., system or Python packages). DLHub registers these published functions with funcX which then allows the servables to be transferred and deployed to remote computing resources and invoked one or more times on different input arguments. funcX elastically provisions compute nodes (e.g., via cloud API or batch scheduler) in response to workload requirements, deploys special funcX worker agents in servable containers for fine-grain execution, and then manages the secure and reliable execution of inference tasks.
In this paper, we extend our previous work [16], [19] by outlining the new DLHub architecture that is able to serve models on arbitrary distributed resources using funcX. This architecture also allows researchers to use their own resources when invoking models published in DLHub.
We evaluate the performance of DLHub by showing that it can scale to hundreds of concurrent containers when deployed on different resources including a supercomputer, cluster, and Kubernetes cluster. and compare it against alternative learning systems. We show that DLHub performs comparably with other systems, such as TensorFlow Serving [41] and SageMaker [4], when using a Kubernetes cluster. Finally, we show that memoization and batching can significantly improve performance and that DLHub can serve models on remote computing resources in less than 75 ms.
The remainder of the paper is structured as follows. In Section 2, we outline the need for, and unique requirements of, learning systems in science. In Section 3, we survey a range of model repository and serving systems. In Section 4, we present the DLHub architecture and describe how it supports publication and serving. In Section 5, we evaluate DLHub by exploring the serving latency and scalability, performance optimizations, and comparing it against three related systems. Finally, in Section 6, we present case studies that highlight the benefits of DLHub in science, and we summarize our contributions in Section 7.
