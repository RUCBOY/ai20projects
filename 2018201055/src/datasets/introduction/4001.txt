The problem of assessing the quality of a publication venue has attracted significant attention in the literature. Especially for the case of scientific journals, a large number of bibliometric indicators is available, as noted by Setti (2013). The same is not true for conferences, however, hence this work focuses on the assessment of conference quality, and in particular on Computer Science/Computer Engineering conferences.
Existing work on assessing conference quality tends to use either too abstract criteria or to need the collection of data that is extremely difficult to gather, especially for large conferences. Section 7 of this paper discusses the related work.
A new metric, implicitly evaluating a researcher’s contribution through the quality of the conference venue where it is presented, was proposed by Nelakuditi, Gray, and Choudhury (2011). The authors argued for the validity of their metric, named Peer Reputation (PR), by claiming that the selectivity of a publication venue is a function of the reputations of the authors' affiliating institutions. For simplicity, they represented each paper by the affiliation of its first author. Hence, according to PR, a quick assessment of the quality of a conference can be made by using university rankings and checking the respective ranking of the affiliation of the first author of each paper. It is explained, by Nelakuditi et al., that although PR is not a perfect metric to assess the quality of a publication, it provides a coarse-grain measure of the selectivity of a conference or a journal, and can potentially be more helpful than the Acceptance Ratio (AR) of a conference (defined as the number of accepted papers divided with the number of submitted papers).
Despite its interesting basic idea, in our view PR is not adequate by itself in evaluating a researcher’s contribution. The reason is that it is too narrow in scope, as it only focuses on a researcher’s affiliation, as if this affiliation completely defines the researcher. This choice may serve the logic of a “snap judgement” which the authors wanted to propose (Nelakuditi et al., 2011), but as we discussed (Loizides & Koutsakis, 2013), a finer-grain approach needs to be used in order to make a thorough assessment of the quality of a conference.
Even so, we need to state that, in our view, the fact that a paper is accepted in a conference, as good as that conference might be, does not define the quality of the paper itself. Similar points have been made in the literature regarding journal publications (Nature, 2005; Nature Materials, 2013 editorials). Still, a publication in a high-quality conference can serve as an indication of the paper’s quality and potential impact. Therefore, defining the quality of a conference is of significant interest.
In this work, we propose the Conference Classification Approach (CCA), which is a finer-grain method for evaluating the quality of Computer Science/Computer Engineering conferences as it is indicated by a number of metrics which are easy to compute. Similarly to the case of the scientific impact of journals, which is known to be unrealistic to be captured by any single indicator (Setti, 2013) we argue that the use of more than one metric is necessary to judge the quality of a conference. These multiple metrics are incorporated into CCA, in order to lead to a unique classification for each conference venue.
