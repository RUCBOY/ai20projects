Computer experiments often use computer models (simulators) to simulate the behavior of a complex real system under consideration. These models are usually designed according to theories believed to govern the real system. They usually include calibration parameters, that is unknown parameters that regulate the behavior of the computer model; hence we wish to tune (calibrate) them in order for the computer model to represent the real system accurately. Often, calibration of a computer model is performed in the presence of experimental data in order to find optimal values for the unknown calibration parameters. In cases that the computer models are expensive to run, there is interest in building inexpensive predictive statistical models.
Kennedy and O'Hagan [1] proposed an effective Bayesian computer model calibration to address such cases. Briefly, the experimental observations are represented as a sum of three functional terms: the computer model output, a systematic discrepancy, and an observational error. These functional terms are modeled as Gaussian processes [1], [2], [3], [4], because computer models are often computationally expensive, and available training data are limited. Literature includes several variations of computer model calibration which can handle different issues; e.g. discontinuity/non-stationarity in the outputs [5], discrete inputs [6], calibration in the frequentest context [7], high-dimensional outputs [8], dynamic discrepancy [9], large number of inputs and outputs [10], etc. However, these works are restricted in cases where a single computer model is available. Nowadays, there is a plethora of computer models that aim at simulating the same real system. These models may differ either in precision (multi-fidelity case) of the solvers involved, or in the theories based on which they are designed (multi-physics case). Recently, Goh et al. [11] proposed a procedure to perform Bayesian calibration of computer models available at different levels of fidelity. It combines the models in a nested structure according to a given fidelity order. However, this approach is restricted to address only multi-fidelity cases where the fidelity order of the computer models is known.
Often, there are available several computer models, based on different theories, that represent the same real system. Each single computer model may have its own unique properties and predictive capabilities in representing the real system. Therefore, there is not a commonly acceptable way to order such models. Possible reasons for example can be: (i) incomplete knowledge of the complex real system, (ii) different computational capabilities of research groups, (iii) different scientific theories or perspectives describing the same real system, etc. In such cases, using only a single computer model may lead to misleading inferences and predictions and ignore the physics considered by other computer models only. Furthermore, traditional multi-fidelity calibration methods, such as [11], are not suitable to address such cases because the fidelity order of the models is not available a priori, or because nesting one model to another could possibly impose unrealistic relations among the models. Moreover, in the presence of moderately large number of models, the direct implementation of standard multi-fidelity calibration method becomes very expensive. Here, the question of interest is how to properly combine and calibrate such computer models in order to represent the real system output accurately.
In this study, the motivation for addressing the aforesaid problem raises from the Weather Research and Forecasting (WRF) regional climate model [12]. WRF allows for different configurations (sub-models), e.g. different parametrization suits, physics schemes, or resolutions, which in principle can constitute different models. Briefly, here the available computer models consist of different combinations of radiation schemes (the Rapid Radiative Transfer Model for General Circulation Models [13], and the Community Atmosphere Model 3.0 [14]) that describe different physics, and different resolutions (25 km and 50 km grid spacing) that describe different fidelity levels. It is uncertain which radiation scheme leads to better simulations. Moreover, higher grid spacing does not necessarily lead to more accurate simulations because WRF is sensitive to other physical parametrizations which is uncertain how they are affected by the grid spacing. Combination of physics variability is expected to result better predictions in climate models [15]; hence interest lies in combining suitably these computer models in order to integrate the associated physics and fidelity variations. WRF is employed with the Kain Fritsch (KF) convective parametrization scheme (CPS) [16]. For climate models, it is important to better understand and constrain the convective parametrization, and hence interest lies in quantifying and reducing the uncertainties regarding of those parameters. The computational cost of running WRF is prohibitively high, and an exhausted direct simulation study is not possible in practice; hence there is interest in a predictive model.
In this article, we propose the Bayesian calibration of computer model mixture method, as an extension to the traditional Bayesian (single) model calibration [1], [2]. Central to the proposed methodology is the idea of (i) representing the output function of the complex real system as a mixture of output functions of the available computer models with unknown input dependent weight functions, and (ii) specifying a fully Bayesian model to quantify the associated uncertainties. The proposed method allows one to build a predictive model (emulator) for the output of a real system by properly calibrating, weighting, and combining the available computer models in the Bayesian framework. Additionally, it allows the design of a calibrated mixture of computer models (simulators) by evaluating the associated weight functions and the calibration parameters. The resulting computer model mixture, as well as the predictive model, aims at representing the real system output more accurately than the single ones by aggregating the unique features of different models. We introduce the concept of shared calibration parameters that allows inference on calibration parameters to be based on multiple computer models (and hence different physics), however, the method allows different models to have different calibration parameters. The Bayesian computations are performed via Markov chain Monte Carlo methods. A computational highlight of the procedure is that it builds the unknown mixture weight functions via a stochastic bases selection from a pool of basis functions in a data-driven manner.
The method is suitable to address realistic problems that one model may be more accurate than the other at different (unspecified) input sub-regions. In particular, through the weight functions, it allows the determination of the input sub-region at which a individual computer model is more preferable to be used than the rest individual ones. The method is particularly suitable to address applications where the outputs of the available computer models tend to differ from the output of the real system at different directions. This is because the weight functions can adjust the outputs of contributing models in the mixture, in a manner that the overall discrepancy of the mixture will be less that the individual ones. Therefore, in such cases, the resulting calibrated computer model mixture is able to produce more accurate simulations than the single ones. This covers a large range of important real-world applications [15], such as the WRF one analyzed here.
The article is organized as follows. In Section 2, we present the proposed method. In Section 3.1, we validate the proposed method with that of Goh et al. [11] in a validation example. In Section 3.2, we assess the good performance of the method and compare it with that of Kennedy and O'Hagan [1] in a more challenging benchmark example. In Section 3.3, we implement the method on a real-world large-scale climate modeling application that involves the WRF with the KF CPS. In Section 4, we conclude and propose possible extensions. In Appendix B, we provide a technique that mitigates the computational overhead of the procedure which is caused by the consideration of multiple computer models.
