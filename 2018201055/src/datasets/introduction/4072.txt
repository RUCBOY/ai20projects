The PageRank algorithm by Brin and Page (1998) was intended to rank webpages by importance using the link structure of the web, but this recursive technique quickly gained popularity and found numerous other applications. Among other things, the citation networks of research papers were particularly well suited for the usage of methods based on PageRank because they could be easily modelled as directed graphs (Chen, Xie, Maslov, & Redner, 2007; Ma, Guan, & Zhao, 2008; Walker, Xie, Yan, & Maslov, 2007). From these graphs, however, further extended citation networks can be generated: those of authors (Fiala, 2012b; Fiala, Rousselot, & Ježek, 2008; Ding, 2011; Ding, Yan, Frazho, & Caverlee, 2009; Liu, Bollen, Nelson, & Van De Sompel, 2005; Yan and Ding, 2011), journals (Bergstrom, 2007; Bollen, Rodriguez, & Van De Sompel, 2006; González-Pereira, Guerrero-Bote, & Moya-Anegón, 2010), institutions (Yan, 2014), countries (Fiala, 2012a), etc. On the other hand, these “bibliographic” networks differ from the web graph in some aspects. For instance, the citation networks of papers do (usually) not contain loops, which are common on the web, because the citation direction is always heading towards the past, i.e. newer papers cite older ones. And even if this feature has been weakened in recent years due the existence of online “ahead of print” publications, which at its most extreme enables citations pointing from the past to the future, it is still a distinctive property of paper citations. The other distinctive characteristic of citation networks is that they never get smaller with vertices disappearing or edges being removed. Unlike the web graph, which frequently changes its structure and commonly loses nodes as well as links, once a citation is made in a bibliographic network, it remains there for good. Therefore, most of the current PageRank-related methods employed in bibliometrics rely on some properties unique to bibliographic networks and cannot be applied to the web again. As a result, because of the now widely recognized merits of PageRank, both of the two most eminent academic databases presently make use of PageRank-based metrics in the assessment of journal impact: Web of Science as Eigenfactor Score (Bergstrom, 2007) and Scopus as SJR indicator (González-Pereira et al., 2010).
When the whole structure of the citation network is unknown or ignored, the prominence of a researcher may simply be based on the number of incoming citations from other scholars which determine that researcher's popularity. By contrast, if the whole citation network topology is considered and citation weights depend on the importance of citing scientists like in PageRank, prestige is measured rather than popularity. The techniques that determine popularity are sometimes called first-order methods and those that calculate prestige are called higher-order methods. The computational costs of the procedures in the latter group are by definition much larger beyond any doubt, but their practical benefits for the detection of prominent scholars are less clear. Having said that, recently, there have been some contradicting results with respect to the performance of PageRank-related methods in the task of identifying salient researchers compared to simple citations. Whereas Fiala, Šubelj, Žitnik, and Bajec (2015) have reported that there is no evidence of PageRank outperforming citations, Nykl et al., 2014, Nykl et al., 2015 and more recently Panagopoulos et al. (2017) have found the opposite to be true. While the difference in the findings by Nykl et al. may be caused by their different approach to the construction of the citation network (first, PageRank is computed on the paper citation network and its scores are then distributed to individual authors instead of being calculated directly on the author citation network like in the analysis by Fiala et al.), the disagreement with the outcomes by Panagopoulos et al. is more obscure given the complex methodology used in their study. Moreover, Dunaiski et al. (2016) have concluded something in between: that citations work better in general, but high-impact research itself is more frequently detected by PageRank. All in all, the above mentioned discrepancies have motivated us to formulate the following research question: Can the prediction of award-winning researchers be used to show the superiority of PageRank-based methods over simple citations in the ranking of scientists by their impact? Therefore, the goal of the present analysis is to shed some more light on the performance of citations and PageRank in identifying influential researchers in the past as well as in the future, rather than to just rank researchers to predict prizes. And even though some evaluation methods independent of citations will be tested too, the direct and indirect impact of citations on the standing of scientists seems inevitable because of their irreplaceable role in scholarly communication.
Due to the lack of a baseline ranking, different ranking methods must be compared to each other and/or to a reference set of outstanding scholars. This reference set of important scientists may consist of the winners of prestigious awards (Sidiropoulos and Manolopoulos, 2005), programme committee members of renowned conferences (Liu et al., 2005), or editorial board members of high-impact journals (Fiala, Šubelj et al., 2015). In this study we will use the recipients of the ACM SIGMOD E. F. Codd Innovations Award1 in 1992–2016 and ACM A. M. Turing Award2 in 1966–2015 as our reference-set researchers and later also add two other prizes for verification. Two PageRank-based methods (standard PageRank and time-weighted PageRank) and two citations-based techniques (Citations and Indegree) will be employed to rank researchers by influence based on a large citation graph with almost 0.7 million nodes and over 26.4 million edges. Because the data set was generated from nearly two million computer science papers indexed in the Web of Science (WoS) database covering the period 1945–2014 and the four rankings were cumulatively produced for each of the last 25 years in that time range, the identification of the award-winning researchers often results in the prediction of future awardees. The research question is whether PageRank-based methods outperform citations-based procedures in the detection of award-winning researchers by assigning them better ranks and we will see that it can be answered positively when relative ranks are considered.
