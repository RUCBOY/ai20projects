In machine learning, we care about the generalization performance of our predictor over unseen data. Regularization is one of the fundamental methods to combat overfitting the training data, by preferring smaller, simpler, or smoother models. Traditional regularizers work by pushing model parameters to smaller values (e.g., L1/L2 regularizers), or increasing the model sparsity.
Deep neural networks composed of many layers with many units can overfit and a number of regularization methods have been proposed. One such method is the dropout in which each unit is randomly dropped (along with its connections) from the network with a certain probability [1]. Intuitively, this reduces the co-adaptation of units by forcing them not to rely too much on one another. This is equivalent to sampling from an exponential number of networks of different complexities at training time.
The dropout method has spurred research and several variants have been proposed, such as drop-connect where each connection can be separately dropped [2], fast-dropout that performs a Gaussian approximation to the implied objective [3], and word-dropout in natural language processing where each word in a given sentence is dropped out (or replaced with the unknown token) [4]. Later work grounded the approach in a stronger theoretical basis [5], [6].
The hierarchical mixture of experts (HMoE) is a meta-model that combines several models using a gating function that is defined with respect to a tree hierarchy [7]. When the gating function and the individual experts are differentiable with respect to their parameters, the whole tree can be trained using stochastic gradient-descent (SGD), just like the weights of a neural network given its structure. The hierarchical mixture of experts can be used in supervised learning for regression or classification, and we have recently shown how an autoencoder can be built using such models for unsupervised learning [8]. Shazeer et al. [9] used the (non–hierarchical) mixture of experts model to choose among thousands of neural network models. Their motivation is to decrease the overall complexity by choosing and using only the relevant model; we use much simpler (constant) models at the leaves and we have no such concern.
As we discuss in more detail shortly, the hierarchical mixture model defines a soft decision tree because of the soft gating function and this leads to a smoother output. Still, when the hierarchy has many levels, the number of gating models and experts increase exponentially with depth and we have observed experimentally that the HMoE is prone to overfitting. Our contribution in this work is to propose a dropout mechanism suitable for a tree structure to avoid such overfitting behavior. The original dropout method was proposed for the fully-connected layers of a feed-forward neural network; the novelty of our contribution is in adapting the dropout to a tree structure. Our experimental results show that our proposed dropout method works as a regularizer and indeed improves generalization.
The basic idea behind the original dropout as applied to feed-forward neural networks is that the final decision should be distributed over the whole model, because any part of the model can be dropped out. The same also holds for the decision tree. With a tree of many levels, every leaf or subtree is dedicated to some small region in the input space and such a tree is likely to learn the noise in the training data and overfit. However, if parts of the tree can be dropped out at random, to be able to still generate the correct output, the rest of the tree should learn to make up for the missing parts, and this forces the response to be distributed over the tree, hence generating a smoother output.
Another important point about the original dropout is that every time a subset of the units is dropped out at random, we get a different network having different complexity, and when we train them all, it is as if we are averaging over all of them, as opposed to “putting all our eggs in the same basket.” The same also holds when dropout is applied to a tree; every time some parts of the tree are dropped out, we get a different tree with a different complexity and when we train them all, we are averaging over trees of different complexities. It is this averaging effect that is another explanation for why dropout is a method for regularization, alleviating overfitting and improving generalization.
This paper is organized as follows: We review the basic HMoE model and the dropout method as applied to multilayer neural networks in Section 2. Our proposed dropout method for HMoE is discussed in Section 3. Our experimental results on three image data sets (one toy and two real-world) and one sentiment recognition data set are given in Section 4. We conclude in Section 5.
