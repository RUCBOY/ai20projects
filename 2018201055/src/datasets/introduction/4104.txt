The field of Learning Design has emerged during this century. It seeks to develop a conceptual and descriptive framework of teaching practices, making use of technology, in order to make learning designs explicit, sharable, and reusable (Conole, 2012, Kirschner et al., 2004). Since assessment is a common element of learning design, it is of great importance to understand how assessment activities are configured, and to represent them in an explicit, sharable, and reusable manner. In terms of capturing the design of assessment activities, an increasing number of educators use computer-based assessment (CBA). CBA can be defined as assessment presented using digital means and submitted electronically. CBA offers many advantages over traditional forms of assessment as it is electronically built, and therefore generates user data on assessment activities.
CBA allows teachers to compare the designs of their assessment approaches across different tasks and modules. For example, Toetenel and Rienties (2016) compared 157 modules at The Open University (OU) and found that, on average, 21.50% of students' total workload was allocated for assessment, although substantial variation (SD = 14.58%, range 0%–78%) was found amongst these modules. By representing CBA in an explicit way, educators can learn from each other by comparing, reusing, and adapting peers' CBA designs in their own learning context.
The impact of CBA designs on the learning processes of students may be better understood with the support of learning analytics, which make use of the digital traces of learners' interactions in a virtual learning environment (VLE) that are preserved in log-files. Recent learning analytics research has found that the way in which teachers design tasks and assessments at a micro level (within one assessment or task: see for example Greiff, Wüstenberg, and Avvisati (2015)) and a macro level (across various assessments within or across modules: see for example Lockyer, Heathcote, and Dawson (2013)) can influence how students are engaging with CBA tasks and their academic performance (Koedinger et al., 2013, Rienties and Toetenel, 2016, Toetenel and Rienties, 2016).
For example, in a fine-grained log file study of one Programme for International Assessment (PISA) CBA task on climate control that compared complex process solving amongst 16,219 children in 44 countries, Greiff et al. (2015) found substantial differences in individual strategies. These strategies had a significant impact on individual performance and also highlighted differences in problem-solving strategies between countries. In a follow-up study of 1476 Finnish children completing nine complex problems, Greiff, Niepel, Scherer, and Martin (2016) found that there was an optimal level of effort spent on these tasks and consecutive performance, as well as a negative relation between the frequency of changes made in learning strategy and performance. These micro-level CBA studies provide rich and complex understandings of how students engage with specific assessment tasks (Koedinger et al., 2013, Vandewaetere et al., 2011), and may help teachers to make decisions about intervention both on individual tasks and on a broader, module-based level.
Similarly, but at a more macro-level of analysis, Rienties and Toetenel (2016) studied the activity of 111,256 students on 151 modules at the OU using multiple regression models. The study found that learning designs strongly predicted VLE behaviour and performance of students. In follow-up research, Rogaten, Rienties, and Whitelock (2016) compared the learning gains of 17,700 students on 110 Science and Social Science modules and found individual differences in learning gains as well as significant differences in assessment practices within and across OU modules. Module characteristics accounted for 6%–33% of variance in students' initial achievements and 19%–26% of variance in subsequent learning gains. In other words, recent preliminary findings suggest that how teachers design CBA influences how students learn over time on both a micro and a macro level.
By aligning the designs of CBA with fine-grained data relating to students' engagements with the VLE, together with their satisfaction and performance, educators may be equipped with valuable insights as to how their students are “reacting” to the design of CBA (Rienties and Toetenel, 2016, Toetenel and Rienties, 2016, Koedinger et al., 2013). While, to the best of our knowledge, the study by Rienties and Toetenel (2016) was the first to link aggregate learning design data of CBA and other activities on a large scale with actual student behaviour and cognition across a large number of modules, the study did not deal with how educators use CBA on a week-by-week basis. Aligning how educators balance CBA on a weekly basis with what students actually do in the VLE can not only advance our insights into CBA, but may also help to strengthen the links between learning analytics and CBA.
Therefore, firstly we aim to understand how educators design and implement CBA on a weekly basis. Specifically, we will investigate how educators have allocated time for seven types of learning activity (assessment, assimilative, finding information, communication, productive, interactive, and experiential) at both module level (for 74 modules) and weekly level (for 37 modules).
Secondly, by building on previous research (Rienties and Toetenel, 2016, Toetenel and Rienties, 2016), we will investigate the impact of CBA designs on students' behaviours in the VLE using log-files data, satisfaction levels, and module pass rates. This element of the study is of particular interest as a large body of assessment literature has indicated that assessment drives learning (Bearman et al., 2016, Segers et al., 2003), but limited empirical evidence on a macro level is available to confirm this claim. Our analysis will use a combination of visualisation techniques and fixed effect models on 72,377 registered students studying 74 modules across different disciplines.
Finally, while learning analytics may highlight key correlations and even causation in relation to learning design, CBA, student engagement and learning, it is important to be aware that learning always takes place in a context. Therefore, our third and final aim is to unpack how six teachers designed their modules and understand how their weekly design decisions influenced student learning, using a case-study approach.
