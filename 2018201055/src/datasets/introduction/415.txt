The aim of visual saliency analysis is to computationally evaluate which object in an image will be the focus of human attention. Due to the availability of depth images, salient object detection in colour and depth images has attracted considerable interest, and it is beneficial for many intelligent tasks, such as object detection and object tracking in RGB-D images [1], [2], [3]. Many approaches have been proposed for detecting salient objects in RGB-D images and videos  [4], [5], [6].
Previous methods generally extract hand-crafted features from RGB-D images and fuse them to detect salient objects [7], [8]. Although such hand-crafted features and fusion schemes are useful in some scenes, these methods may not be ideal in robustness for the lack of high-level semantic representation. Recently, CNNs have achieved promising successes in computer vision tasks, such as image classification and object detection [9], [10], [11]. To build discriminative features from low-level to high-level for salient object detection in RGB-D images, the methods based on CNNs has been developed [12], [13], and also such kind of methods tend to learn feature fusion using convolutional layer.
A general pipeline of CNNs-based model for RGB-D salient object detection, as shown in Fig. 1(a), typically consists of extracting hierarchical features of different modalities using CNNs, combining these features through convolution, and then up-sampling the feature maps through deconvolution. In CNNs, convolutional layers are used to obtain local features with local filters, and pooling layers are used to reduce the size of the feature maps [9]; both of these operations are performed on local image regions. During the combination, concatenation is generally used for feature fusion. In the up-sampling process, deconvolution tends to be applied to local regions. Because context dependencies are quite important for object detection [14], due to the operations for the fusion and deconvolution, the current models for RGB-D saliency detection are limited in exploring contextual information of multi-modalities and deconvolution layers.Download : Download high-res image (246KB)Download : Download full-size imageFig. 1. Two pipelines for saliency detection. (a) A general structure for saliency detection, in which convolution is used to fuse features and deconvolution is used to up-sample saliency cues. (b) The pipeline of our proposed method (CAN), in which features are combined via multi-modal context-aware fusion and saliency cues are up-sampled via context-dependent deconvolution.
Due to its advantages in modelling sequential context dependencies [15], long short-term memory (LSTM) has been successfully applied for detecting salient objects in one single image [16], [17]. Notably, it is therefore reasonable to model the contextual relationship by LSTM for saliency computation in single depth image [18]. As the contexts of colour and depth are cooperative and strong correlated in the same location, analogously, their joint contexts can further reflect information hints in sequential patterns. In other words, the joint contextual cues implying sequential hints can be conveyed by LSTM. In brief, the context information of two modalities can serve complementary purposes for characterizing an object, and context dependencies also particularly exist within a single modality. However, in the existing models, multi-modal features computed using CNNs are fused through concatenation, ignoring these implicitly complex contextual relations. Consequently, the context modeling in RGB-D images, especially the strong sequential correlations between colour and depth information, cannot be effectively captured. This implies that it is necessary to perceive the contextual interaction between two modalities besides the context in each modality, when fusing high-level context features extracted from RGB and depth images.
The contextual information is useful for feature representations, when one saliency map is generated in a coarse to fine manner. In  [19], the context is spread through LSTM within one deconvolution layer. As a saliency map is generated by performing deconvolution on local image regions with restricted receptive fields, a single salient object is represented at multiple different scales. In other words, the output of a preceding deconvolution could, in turn, guide the operation of the subsequent deconvolution layer, which means that dependencies also exist between contexts at higher and lower scales. However, this scale-based context dependency is not extensively considered in the deconvolution processes of the existing models for RGB-D saliency detection. Hence, we make efforts to model the context dependencies between layers for powerful feature representations.
To address aforementioned problems, in this paper, two types of context-aware LSTM (a context-aware fusion unit based on LSTM, a hierarchical LSTM) are proposed. The first one is to explore context fusion and spread cues of features which are computed from colour and disparity images, respectively. The second one is used for modelling context dependencies between layers. Based on these two units mentioned above, we propose a context-aware network (CAN) that can learn discriminative feature representations in a context-aware manner for saliency detection in RGB-D images. In our CAN, high-level context information from two modalities are fused by considering the interactions of the multi-modal contexts. Additionally, the context dependencies between deconvolution layers is encoded among multiple scales.
The schema of the proposed CAN model is shown in Fig. 1(b). During feature extraction, two CNNs are first exploited to separately extract high-level features from RGB and depth images. Secondly, LSTM-based context-aware fusion is deployed using high-level cues from the two modalities as input to generate fused context information, while the inter- and intra-modality contexts are both memorized. Therefore, contextual interactions both between and within modalities are considered. Thirdly, the high-layer composite features are up-sampled using deconvolutions with different receptive filters to produce saliency maps at multiple scales. Specifically, a hierarchical LSTM is further proposed to model the scale dependencies among image locations based on saliency cues at different scales. By feeding the context from each deconvolution layer into the successive layers, complementary context information from different scales is learned to progressively refine the contexts of the saliency cues. And the multi-scale saliency cues are combined to produce a final saliency prediction. By integrating the discriminative features extracted by CNNs with the context dependencies modelled by an LSTM, our proposed CAN is able to learn robust feature representations for RGB-D salient objects.
The main contributions of our work are summarized as follows:
•We propose the CAN architecture to learn discriminative feature representations for saliency detection in RGB-D images, by modeling multi-modal and multi-scale context dependencies within the context-aware fusion and context-dependent deconvolution. It is demonstrated that the proposed end-to-end CAN can achieve favourable performance compared with state-of-the-art methods.•A context-aware fusion unit based on the LSTM architecture (MCFLSTM) is developed to learn complementary contexts from two modalities. The positive effect of this fusion approach is demonstrated experimentally.•A hierarchical LSTM structure called HSCLSTM is proposed to progressively refine saliency cues by modelling the context dependencies among different scales. Its effectiveness is also verified by experimental results.
