The widespread popularity of Machine Learning (ML) in modern technology and its rapid advancements in recent years have paved the way for its adoption in a variety of application areas. ML techniques empower a range of diverse applications, including malware detection (Gibert et al., 2020), self-driving cars, network intrusion detection, natural language processing, behavioral analytics, speech recognition, and so on. Nevertheless, researchers have demonstrated that ML models are vulnerable to adversarial examples in its inputs. The purpose of these adversarial techniques is to employ malicious inputs to fool a ML model into producing erroneous outputs (Hu et al., 2019; Papernot et al., 2017). Consequently, this has given rise to a field of research known as adversarial machine learning (Biggio and Roli, 2018).
There are a number of different adversarial attacks that can be deployed against ML techniques, for instance, an adversary can poison a dataset by perturbing samples in the training data (Biggio and Roli, 2018). Over the years, researchers have examined and demonstrated the effectiveness of such poisoning attacks (Biggio et al., 2012; Rubinstein et al., 2009; Xiao et al., 2015). Adversarial attacks are a serious threat to the success of ML in practice, as small and subtle perturbations in its inputs can mislead a ML model into outputting incorrect predictions, thereby negating the usefulness of the ML model. For ML in computer vision, such perturbations made to images in the ML dataset are often imperceptible to the human visual system (Akhtar and Mian, 2018).
This paper focuses on methods of protecting image-based datasets by verifying the visual fidelity of the data. The effectiveness of ML techniques is affected by the availability of training data. In general, ML models require large amounts of training data to be effective. In light of this necessity for significant amounts of training data, many ML models are trained using public datasets that are freely available online. These public datasets are often copied and distributed without any mechanism for protecting the integrity of the data. This makes these datasets vulnerable to alterations by an adversary.
To address this problem, the research presented here investigates two methods of verifying the visual fidelity of image-based datasets by detecting perturbations in the data using QR codes. The advantage of the proposed methods is that a copy of the original dataset does not have to be stored or used for verification. In the first method, a verification string is generated for each image in a dataset using a QR code. The size of a verification string is much smaller than the original image, and it can be used to verify the visual fidelity of the image. To verify image fidelity, a verification process is used to recover a QR code for each image. If the resulting QR code is noisy or cannot be recovered through this verification process, this indicates that the dataset has been altered.
However, since this method requires a verification string for each image in a dataset, the storage requirement increases linearly with the number of images in the dataset. While this is fine if storage space is not an issue, it may not be an attractive solution for applications with limited storage capacity, e.g., for mobile computing and pervasive social networking that may have storage restrictions (Yan et al., 2017). Therefore, a second method is proposed where only a single verification string is required, and can be used to verify the fidelity of images in an entire dataset. The limitation of the second method is that if the dataset has been altered, one cannot determine which image has been altered, only that the dataset is not intact as it is different from the original.
1.1. ContributionThe research presented in this paper investigates the problem of protecting image-based ML datasets against alteration by an adversary. The proposed methods, which were first presented in Chow et al. (2019), attempt to provide mechanisms for verifying the visual fidelity of images in a dataset without the need to store and use the original dataset for this purpose. To do this, verification strings are generated from the visually important content of the images and these are associated with QR codes. The reason for using QR codes for this purpose is due to its inherent data capacity and error correction properties, which are inbuilt in the QR code structure. Two methods for creating verification strings to verify the fidelity of an image dataset are presented. The advantage of the first method, i.e. the Linear Verification String (LVS) method, is that the fidelity of each image can be verified individually. However, this comes at the cost of higher storage requirements. The advantage of the second method, i.e. the Aggregate Verification String (AVS) method, is that only a single verification string is required to determine whether an entire dataset is intact. Nevertheless, it does not allow for one to determine the visual fidelity of individual images.
