The process of plant identification is an important component of typical workflows in plant ecological research. Speeding this task up and making it accessible for non-experts would be highly beneficial, even more so in view of the many threats to and general irreversible decline of plant biodiversity, which can only be addressed by continued concerted efforts in plant biodiversity research and conservation biology. Even for researchers with expert knowledge on given plant taxa, the process of manual plant species identification may be difficult to scale to high-throughput demands, while for non-experts, it may be prohibitively time consuming and error-prone. Given ubiquitous resource limitations (time, money, experts), it may turn out as a bottleneck in many projects.
When faced with an unknown plant, manual identification typically involves navigating a key that consists of a series of prescribed identification steps (usually printed single-access keys, rarely more modern digital interactive keys, see e.g. Stevenson et al., 2003 or Farnsworth et al., 2013). At each identification step, a question about some of the plant's characters needs to be answered. The selected answer out of two (dichotomous key) or more (polytomous key) selectable answers determines the next identification step. Unfortunately, choosing the appropriate answer may not be trivial, when either (i) the key is of suboptimal design (ambiguous questions, contrasting of overlapping ranges, etc.), (ii) the botanical sample lacks relevant characters, (iii) expert knowledge is necessary but not available in order to correctly answer the question. In consequence, the result from a manual identification may suffer from being less accurate or less reproducible. (See e.g. MacLeod et al., 2010 and references therein for a survey of human error in taxon identification.)
In recent years, DNA barcoding has gained momentum and started to unnecessitate some of the traditional identification needs. However, barcoding initiatives are still lacking or reference databases still under construction in most countries. Even more recently, researchers started to more systematically address the issue by providing plant identification tools that employ image recognition technologies (e.g. Cope et al., 2012, Goëau et al., 2016, Joly et al., 2014, Kadir et al., 2011). In contrast to DNA Barcoding techniques that require DNA extractions from harvested tissue, using computer vision methods for high-throughput identification purposes generally can be non-destructive or less invasive. Also, such methods hold the potential to be easier applicable to herbarium specimens where DNA quality has degraded and, thus, DNA Barcoding is no option.
1.1. Related workVariations on leaf characteristics are preferably employed in automated plant identification systems using computer vision methods because of leaves being easier observable, accessible and describable compared to other plant organs. Kadir et al., 2011 as well as Cope et al., 2012 and Ahmed et al., 2016, give comprehensive surveys on methods for automated plant identification. However, plant identification is still considered to be a challenging and unsolved problem since all classical computer vision employ hand-crafted methods that are dependent on chosen features given in a natural given extreme diversity of botanical data. For example, Jin et al., 2015, employ a classical image processing chain of image binarization to separate background and the leaf, detection of contours and contour corners, and geometrical derivations of leaf tooth features. The approach was evaluated on eight species and Jin et al., 2015 reported in species specific identification accuracies between 72.8% and 79.3%. But obviously, this approach solely cannot deal with species showing no significant appearances of leaf teeth.To overcome the naturally given limitations of such model-based approaches, model-free approaches and machine learning methods have been introduced. Wilf et al., 2016, employ the model-free Scale-invariant feature transform (SIFT) approach of Lowe, 2004, to detect and describe in a training set of leaf images the visual appearances of so-called interest points which are assembled in a so-called codebook. A standard approach to supervised learning (i.e., the Support Vector Machine (SVM)) is then trained on the coefficients for all codebook elements and the associated taxonomic labels (families or orders). The learned classification function is then applied to predict familial and ordinal identifications of novel images. The SIFT approach detects and describes interest points based on the appearances of significant local image gradients, i.e., contours, corners, lines, etc. and thus does not model certain features explicitly (model-free approach). Wilf et al., 2016, apply their approach on the vein features of the leaf images and report a classification accuracy of 72.14% for 19 families. However, the images are taken from cleared specimens that are prepared in a laborious way and there is still a limitation to a restricted class of features.Additionally, machine learning approaches to active learning are employed to meet the challenges of data acquisition, data labelling (i.e., taxonomic identification of training data), and availability of experts. Methods of active learning implement the concept of the user-in-the-loop (UIL) to improve system performance by engaging the human users (or some other information source). In this context, two prominent approaches to mobile participatory sensing and citizen science working on social image data are presented by Kumar et al., 2012, and by Joly et al., 2014 and both use a classical computer vision pipeline.Kumar et al., 2012, propose a mobile app, called LeafSnap, to enable users to identify trees from photographs of their leaves. LeafSnap achieves a top-1 recognition rate of about 73% and a top-5 recognition rate of 96.8% for 184 tree species. Joly et al., 2014, propose Pl@ntNet as a citizen science approach to speed up the collection and integration of observed botanical image data. The web interface of Pl@ntNet offers to employ images of different plant organs (i.e., leaves, fruits, barks, flowers) and of the complete plant to identify a plant. Joly et al., 2014, evaluated Pl@ntNet on about half of the plant species of France (2200 species), showing top-5 identification rates of up to 69% for single images. Again, LeafSnap and Pl@ntNet, are designed with dependencies on the chosen sets of hand-crafted features that had been selected a priori to measure similarities between novel plant images and plant organ images and stored images of known species.The competition within the ImageCLEF initiative1 was based on the Pl@ntView dataset which focuses on 250 herb and tree species from France area and contains 26,077 pictures showing plant organs and the entire plants Goëau et al., 2013. The task was more related to a retrieval task instead of a pure classification task in order to consider a ranked list of retrieved species rather than a single determination. Yanikoglu et al., 2014 describe the approach of their team from Sabanci Univ. and Okan Univ. (both from Istanbul, Turkey) that had been the winner for images of the so-called “SheetAsBackground” category with an average score of 0.607.The last step towards model-free approaches to plant identification is to get rid of hand-crafted features. In the last years, deep learning convolutional neural networks (CNNs) have seen a significant breakthrough in computer vision, especially in the field of visual object categorization Krizhevsky et al., 2012, due to the rise of efficient general-purpose computing on graphics processing units (GPGPU provides high degrees of parallelization) and the availability of large-scale image data (in publicly available datasets, in the internet, in social media, (specialized) social networks, etc.) that provide the data amount necessary for training deep CNNs with thousands of parameters. An essential advantage of deep CNNs is the automatic learning of task-specific representations of the input data which replace traditional feature-based representations using hand-crafted features.The deployment of deep CNNs has especially led to a breakthrough in fine-grained visual categorization (FGVC). Fine-grained or so-called subordinate categories are often challenging due to high intra-class variance and low inter-class variance since some categories only differ in detail that only experts notice.The categorization of plant organs definitely belongs to the FGVC field, since plant organs can show very high variance of visual appearance within the same genus or the same species as well as high similarities across different genera or species.Lee et al., 2015 presented a CNN approach to taxon identification based on leaf images and reported an average accuracy of 99.7% on a dataset covering 44 species. Zhang et al., 2015 used CNN to classify the Flavia dataset with and obtain an accuracy of 94,69%. Goëau et al., 2016 report on the plant identification task PlantCLEF 2016 that was organized within the ImageCLEF initiative2 dedicated to the system-oriented evaluation of visual based plant identification. The competition based on the PlantCLEF 2015 dataset3 which consists out of 113,205 pictures depicting plant organs and entire plants covers 1000 woody and herbaceous species from France and neighbouring countries. Only 8 of 94 research groups succeeded in submitting runs and all employed CNNs. The winning team of the KDE lab of Toyohashi Univ. of Technology, Japan, reached a classification mean average precision of 74.2% (cf. Hang et al., 2016). Aside from academic prototypes and competitions CNN-based approaches to visual categorization and especially to fine-grained visual categorization are on the way to become practical computer vision systems employable by practitioners as depicted by Lu et al., 2016 who apply a CNN-based approach to maize cultivar identification in agriculture.
1.2. ContributionsWe present an approach to automated plant identification based on scans and smartphone pictures taken from the leaves of the plants. Our approach employs the technology of convolutional neural networks and show superior identification results compared with state-of-the-art systems.
