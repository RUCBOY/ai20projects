Many computer vision algorithms for image classification rely on the detection and extraction of local characteristics in images; as a result, much of the computer vision literature is focused on discovering, understanding, characterizing, and improving features that can be extracted from images. A large number of features reported in the literature have been manually designed, or “handcrafted,” with an eye for overcoming specific issues like occlusions and variations in scale and illumination. The design of handcrafted features often involves finding the right trade-off between accuracy and computational efficiency. The powerful Scale Invariant Feature Transform (SIFT) [1], for example, is well-known for its robustness to object rotation and scale variations, but this robustness comes at a high computational cost. The most widely used features often generate an ensemble of variants to tackle the inherent shortcomings in the speed and/or accuracy of the original versions. For example, some fast variants of SIFT [2] have been proposed to be used in real-time applications and on devices with low computational power.
The computation of handcrafted features, such as SIFT, is normally a two-step process [3]. First, a keypoint detector locates characteristic regions of an image (e.g., corners), which are then characterized by calculating a descriptor that is capable of distinguishing each particular keypoint from the others. This process is outlined in detail in [4]. The set of cues considered for building the descriptor depends on the specific feature being used. At a lower level, a descriptor is a vector of measurements that can be used to train a classifier, such as a Support Vector Machine (SVM) [5].
The deep learning paradigm [6] enables the creation of complex networks for solving (among the others) the problem of image classification, usually tackled by means of CNNs, where deep layers in these complex networks act as a set of feature extractors that are often quite generic and, to some extent, independent of any specific classification task [7]. This means that deep learning obtains a set of features learned directly from observations of the input images [8], possibly preprocessed using a pyramidal approach [9]. The idea behind this approach is to discover multiple levels of representation so that higher level features can represent the semantics of the data, which in turn can provide greater robustness to intra-class variability [10]. Of interest here are the characteristics of the different layers in many deep neural networks that have been trained on images: first layer features resemble either Gabor filters or color blobs and tend to be generalizable, i.e. transferable to many different datasets and tasks [11]. Therefore, it is possible to consider the deep layers of a CNN as a feature extractor, much like SIFT, the major difference being that the features extracted by a CNN are learned using the data in contrast to hand-crafted features that are designed beforehand by human experts to extract a given set of chosen characteristics.
Because the features extracted by the lower levels of a CNN strongly depend on the training set, special care must be taken in the selection of the dataset. The choice of a representative training dataset is usually performed by generating or selecting huge training sets, in the order of millions of images [12]. This solution, however, is far from ideal, as it requires considerable human effort in the selection of images suitable for building the training set and substantial computational power during the training phase. Both these drawbacks can partially be overcome by exploiting semi-supervised and unsupervised techniques [13] and parallel computing to reduce the human labor and the computational costs involved in the training.
In Table 1, we provide a brief descriptive summary of several recent deep transfer learning methods, including the approach proposed here for comparison purposes.Table 1. Descriptive Summary of other deep transfer learning approaches.PaperFeatures extracted using CNNhereUsing deep layers, shallow layers, and the scores obtained by CNN[14]Representation of images as strings of the top layers of pretrained CNNs[15]The convolutional layers of a CNN are used as a filter bank[16]Representation of images using the seventh fully connected layer of a CNN on ImageNet[17]Representation of images using the last convolutional layer of a CNN[18]Representation of images using five convolutional layers and two fully connected layers[19]Lower-layer features are used[20]Features are extracted from the first few layers of a pre-trained CNN model[21]Top-layer activations are used as features
In this paper, we are interested in exploiting features extracted by the deep layers of a network to build a general, or General-Purpose (GP), ensemble capable of handling a broad range of image classification problems. An ideal GP system is capable of providing results comparable with state-of-the-art systems by working out-of-the-box with little or no fine-tuning. Some recent papers have investigated how it is possible to build heterogeneous GP ensembles [22], [23]. For example, in [22], a GP ensemble performed competitively compared with other state-of-the-art methods across sixteen benchmark datasets representing very different problems (for instance, numerous medical problems, many image classification problems, a vowel dataset, and a credit card dataset). However, no single ensemble investigated in [22] worked consistently well across all sixteen datasets. Nonetheless, one GP ensemble worked well across all the image datasets—on some datasets performing even better than an SVM whose parameters had been fine-tuned for that specific dataset.
Several papers have also investigated GP ensembles that exploit the different types of information available using different feature extraction methods and representations of the data. In [23], for example, the goal was to discover a GP ensemble for protein classification that combined different types of protein representations and descriptors and that was capable of performing well across fourteen protein classification datasets representing several different classification problems. Again, no single ensemble was discovered that obtained top performance across all fourteen datasets; however, it was shown that it is always possible to find a more limited GP ensemble that performs consistently well across each type of dataset. Finally, in [24], a GP heterogeneous ensemble combining many handcrafted and non-handcrafted features, including features taken from a specific layer of a deep neural network trained on a specific dataset, was able to perform competitively well across twenty-five datasets (fourteen image datasets and eleven UCI data mining datasets). In some cases, it outperformed an SVM classifier for which both the kernel and parameters selection were carefully fine-tuned for the datasets.
Our goal in this paper is to build a better performing GP ensemble by comparing and combining state-of-the-art handcrafted features with non-handcrafted features. The handcrafted approaches considered in this paper include Local Ternary Patterns, Local Phase Quantization, Rotation Invariant Co-occurrence Local Binary Patterns, Completed Local Binary Patterns, Rotated Local Binary Pattern Image, Globally Rotation Invariant Multi-scale Co-occurrence Local Binary Pattern, as well as several others. The non-handcrafted features examined here include two recently proposed methods, the Principal Component Analysis Network (PCAN) [10] and the Compact Binary Descriptor (CBD) [25]), as well as different methods of deep transfer learning. The deep transfer learning methods are based on Convolutional Neural Networks (CNN) trained to perform classification on the ImageNet ILSVRC challenge dataset. The approach chosen for the deep learners exploits the features provided by the last layer of the CNN along with the feature set obtained from an internal layer. Two feature dimensionality reduction methods (discrete cosine transform and principal component analysis) are used for reducing these feature sets since they often have high dimensionality. The features extracted from the CNN are then used to train a Support Vector Machine (SVM), which implements the deep transfer learning approach.
The contributions of this paper are twofold. The first is our proposal of combining features extracted from the deep layers of CNNs with powerful handcrafted approaches to solve a classification problem: in this way we are able to verify that each of the two feature extraction paradigms is capable of extracting information that is neglected by the other paradigm. In order to reduce the extremely large feature vectors that are extracted from deep layers (which could result in the curse of dimensionality [26], [27]), two reduction methods are employed: Discrete Cosine Transform (DCT) and Principal Component Analysis (PCA). The second contribution is our wide and thorough evaluation of the possible combinations of handcrafted and non-handcrafted features that are needed to build a generic computer vision classification system. This can be seen as a detailed guideline for synthesizing the GP computer vision system concept outlined above. Such a guideline is of general interest because the system performance of our approach was evaluated over a mix of very different computer vision classification tasks in order to avoid any polarization of the proposed architecture.
The different handcrafted and non-handcrafted features were compared and combined in this work, with their independence examined using the Q-statistic [28]. The proposed GP ensemble obtains state-of-the-art results on a large set of different datasets. Performance differences among the methods are evaluated using the Wilcoxon signed rank test. The MATLAB source code to replicate our experiments will be available at (https://www.dei.unipd.it/node/2357+Pattern Recognition and Ensemble Classifiers).
