Scientists generally build statistical models for two main reasons; prediction and/or explanation. Within the social sciences, statistical models are almost exclusively used for explanation and to better understand the world around us and the underlying causal mechanisms driving human behaviour and/or attitudes. Such understanding is necessary in order to affect change and improve society, for example, through policy interventions.
Commonly, statistical modelling in the social sciences is done using observed data (often from social surveys) and regression analyses to test a priori theoretically-derived causal hypotheses about the relationship between a set of explanatory variables, x, and an outcome, y. This involves a researcher-led process of defining theoretical constructs, deriving variables to measure these underlying constructs, running statistical models including these observed variables, evaluating model results based on effect sizes and goodness of fit statistics, leading to conclusions and/or model refinement and retesting [1].
In the absence of well-defined theory and/or in the presence of large, new or complex data sets containing multiple variables which cannot easily be reduced to a series of testable hypotheses, such an approach to statistical modelling may not always be feasible however. Social scientists may turn to prediction via data mining or machine learning techniques in order to better understand the phenomena of interest [2]. Such an approach is likely to become increasingly common as social scientists move beyond a traditional reliance on social survey data as the basis of statistical modelling and embrace new forms of data [3]. The primary objective of such models nevertheless remains explanation; evidence from predictive modelling should be used to refine the underlying theory and develop hypotheses for subsequent testing. The relationship between observed data and underlying theoretical constructs therefore remains of fundamental importance. If they are to be of use, it must be possible to relate the results of data-driven models back to sociological theory and the domain knowledge held by researchers and policy makers.
The eventual aim of this study is to use interactive visualisation to bridge the gap between data-driven machine-learning techniques and theoretically-driven researcher-led modelling; enabling social scientists to manage increasingly complex datasets containing large numbers of potential variables and run multiple exploratory and/or predictive analyses but to do so in a way that ensures models remain rooted in theory and informed by domain knowledge. Integrating computational methods within interactive visualisation approaches – one of the core mechanisms of the field of visual analytics – has already shown great potential [4], [5], [6] and our work contributes to this body of methods. In this paper, we describe our collaborative study in which we have designed and implemented interactive visualisation techniques that enhance an existing workflow for constructing models. We introduce a number of techniques to: (a) assist in exploring statistical summaries of hundreds of variables, (b) facilitate comparison between the alternative models that are iteratively built and (c) help keep track of the modelling process and decision made. We describe and discuss our initial ideas on designs and functionalities. While developing these techniques, we derive general roles for visualisation in supporting such involved model-building processes and use these roles as guidance to inform our designs.
