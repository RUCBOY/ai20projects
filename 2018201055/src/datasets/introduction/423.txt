Subspace clustering (SC) is a hot research topic in computer vision and data mining community due to its promising performance in most practical applications, such as person reidentification [1], multi-task classification [2], and metric learning [3], to name a few. The foundation of SC is rooted in the truth that most real-world data always has intrinsically multiple structures drawn from a union of subspaces. Thus, the main goal of SC is to segment the given points into their exclusive clusters for certain high-level applications. The key ingredient is to construct a desirable affinity matrix (or affinity graph) [4], based on which clustering can be implemented via typical spectral clustering methods such as normalized cut (NCut) [5].
Most SC methods for learning the affinity graph follow the self-expressive framework, which assumes that data points with multiple subspaces can be expressed as a linear combination of the points themselves, i.e., X=XZ, where X ∈ ℜm × n is the dictionary matrix with each of its columns corresponding to a sample and Z ∈ ℜn × n is the representation matrix (or coding matrix). In order to tackle the corrupted data, the self-expressive formulation is always relaxed to X=XZ+E with E ∈ ℜm × n being the error matrix, then the SC problem can be uniformly written as follows [6] :(1)minZf(X−XZ)+λ1g(Z)+λ2l(Z),where g and l are the global and local structural regularization (GSR and LSR), respectively; f is the loss function, λ1 and λ2 are two balance parameters. The detailed selection of loss function f has been intensively studied to deal with heterogeneous error distribution [7], [8]. Thus, the main difference among the pending work lies in the choice of g and l for specific properties on Z.
With respect to the GSR, different regularizers including l1-norm [9], Frobenius norm [10], [11], nuclear norm (NN, ‖ · ‖*) [12], and their mixtures, e.g., l1+F [13], l1+∥·∥* [14], have been extensively studied. Among them, NN has received wide attention due to its close relationship to the intrinsically low-rank property of most real-world data. According to [15], NN is the tightest convex surrogate of the low-rank constraint, which performs well under broad conditions. However, it is still a loose approximation to the natural rank constraint. This motivates to surrogate the rank norm by nonconvex functions for closer approximation. Iteratively reweighted nonconvex norm (IRNN) [15] is a typical representative along with this line, which presents a general framework containing many well-known nonconvex functions, e.g., Schatten p [16] and Logarithm [17], etc. Although these nonconvex attempts achieve better performance than the convex surrogates, IRNN suffers from slow convergence and local optimum due to the nonconvex nature of its cost function. Lanza et al. [17] propose to construct and optimize convex functional containing nonconvex terms for the global optimum. However, it promotes property of sparsity rather than low-rank, and the resultant optimization problem remains challenging.
In terms of LSR, most researchers focus on exploiting the spatial information to generate a better coding matrix, under the assumption that when two samples distribute closely in their original space, then their coefficients in the subspace will be also close to each other [18]. Many works attempt to incorporate a spatial Laplacian term into (1) for explicitly taking into account the local structure of input data [14], [19]. Several other works [12], [20] further impose a rank function on the Laplacian term for a direct block-diagonal constraint. However, with the notable exception of temporal SC (TSC) [21], [22] and ordered SC (OSC) [23], there are few algorithms focusing on the data with local sequential properties. In reality, samples collected in sequence can be found everywhere, such as video and motion data, which implies that the neighbor points are from the same subspace in a large probability. Liu et al. [6] have empirically verified that LSR is more crucial than GSR for ordered data clustering. Nevertheless, both TSC and OSC shed a light on the sequential SC problem, yet they do not go deeper to explore more intrinsic properties or prior information embedded in data. OSC with block-diagonal prior (BDOSC) [24] claims better performance by simultaneously taking into account of order constraint and spatial structure. However, it runs slowly due to the inevitability of the Sylvester equation, which costs O(n3) time complexity at each iteration.
In this paper, a new TSC based method, sketched in Fig. 1, is proposed by utilizing the enhanced low-rank property and temporal predictability. We also propose a computationally efficient algorithm to optimize the representation matrix Z for better application feasibility. Specifically, our main contributions can be highlighted as follows:1.Under the general self-expressive framework, a new SC method for sequential data named as temporal plus low-rank SC (TLRSC) is proposed by simultaneously considering the intrinsic rank property and temporal information for a well-behaved affinity matrix.2.A strongly convex majorizer is presented for our cost function even with the regularization term being nonconvex, which theoretically guarantees the unique solution of subsequent update subproblems. Moreover, a general assumption for the nonconvex regularization is also provided.3.Based on IRNN, a more efficient iteratively reweighted algorithm is developed by the introduction of extrapolation technique and rank progression operation. Furthermore, an automatic stop condition via a block comparison of singular values and thresholds is employed for fast implementation.Download : Download high-res image (684KB)Download : Download full-size imageFig. 1. The flowchart of our method. Given a sequential data, we learn the coding matrix through self-expressive framework with the aid of both GSR and LSR.
