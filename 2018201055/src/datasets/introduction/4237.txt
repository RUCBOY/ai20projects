1.1. BackgroundRecent years have witnessed an increasing emphasis on integrating computer science into K-12 settings. This boom is being driven by many factors including economic and technological demands for a future workforce with necessary computer skills. In the United States, there has been a governmental level push in computer science education (Smith, 2016). Starting computer science education in early grades contributes critically to this initiative since children's early experiences likely factor into their persistence in the domain and future career choices (Margolis et al., 2010, Yardi and Bruckman, 2007). Unfortunately, there is limited research targeting elementary students in this area.The Transformative Robotics Experience for Elementary Students project is one of the many initiatives in bringing robotics and computer science curriculum to elementary students. The project utilizes the humanoid robot platform NAO by Aldebaran Robotics1, as it contains many of the sophisticated tools such as voice/speech and face recognition, which is reflective of the current status of professional robotics and aligned with our goal of exposing students to robotics platforms that are similar to what is used by professionals. More importantly, this platform has a visual programming platform (i.e., drag-drop) that is appropriate for elementary age students. Furthermore, the NAO platform includes a robotics simulator, thus allowing the students to run their code quickly on the simulator while sharing the physical robot resulting in the cost being relatively affordable.This paper focuses on the assessment aspect of the project. As in many other similar projects (e.g. Weintrop, Beheshti, Horn, Orton, Trouille & Jona, 2014), we are deeply concerned with what students learn in this curriculum. One thing is certain, that if the only thing students take away from this curriculum is a set of specific commands that ask the robot to move forward, turn left, say “hello”, etc. (which is important), this project could hardly be recognized as a success. Computer science is such a fast-changing domain that those who possess a single set of skills tied to specific software or hardware may quickly lag behind. What we deem more important is to assist our students to acquire certain skills and thinking patterns that are readily transferable, and thus conducive, to their future learning and problem solving in computing related subjects or even everyday reasoning. In order to prepare themselves for future learning and transferring (Bransford & Schwartz, 1999), students need to be equipped with a set of necessary skills to make sense of the learning context from multiple angles. In this sense computational thinking emerged as our central construct for our assessment effort.
1.2. Computational thinkingComputational Thinking (CT) was proposed by Wing (2006) as a general term that “involves solving problems, designing systems, and understanding human behavior, by drawing on the concepts fundamental to computer science.” It entails a whole set of mental tools that enable people to reduce difficult problems into readily solvable subtasks, represent problems appropriately, interpret data, compose algorithms that are executable by a machine, and take correctness, efficiency, and even aesthetics into consideration when solving a problem. Wing (2006) deemed CT as an essential skill that is established by the widespread application of computing and computers, just as the 3 R's (reading, writing, and arithmetic) that were facilitated by the advent of printing. Since CT has gained wide currency among scholars and practitioners, it has been interpreted in many different ways (Aho, 2012, Barr and Stephenson, 2011, Cuny et al., 2010, Grover and Pea, 2013, National Research Council (US), 2010). For instance, Cuny et al. (2010) argued, “CT is the thought processes involved in formulating problems and their solutions so that the solutions are represented in a form that can be effectively carried out by an information-processing agent.” This definition is further simplified by Aho (2012) as formulating problems in a way that could be solved by “computational steps and algorithms.” Barr and Stephenson (2011), aiming to develop a way to define CT in K-12 settings, argued that “CT is an approach to solving problems in a way that can be implemented with a computer.” CSTA (2011) proposed an operational definition of CT which dissects CT into six dimensions: (1) formulating problems in a way that machines can help to solve, (2) processing data in a logical way, (3) representing data abstractly, (4) algorithmizing the automated solutions, (5) solving problems in an efficient way, and (6) transferring knowledge and skills in solving other problems. Diverse as these proposed definitions are, CT in its essence entails at least thinking in a way that can be represented and processed by machines to formulate and solve problems. Furthermore, beyond the growing interest in CT in the research community, CT is also becoming popular among K-9 educators (Mannila, Dagiene, Demo, Grgurina, Mirolo & Rolandsson, 2014).
1.3. Assess computational thinkingCT is the central construct we used to conceptualize our assessment. Through the curriculum we teach students how to program a robot to complete certain tasks (e.g., problem solving in terms of moving a robot) and examine their CT abilities in this context. In our assessment we stress the transfer (Bransford and Schwartz, 1999, Bransford et al., 2000, Haskell, 2000) component of CSTA's definition: i.e., students need to demonstrate improved CT in solving problems in other contexts, such as commanding other machines to perform similar tasks, or reasoning about everyday scenarios. The focus on transfer, we believe, is more aligned with a broad interpretation of CT - a fundamental skill for everyone.Existing work has often focused on assessing student created artifacts for CT skills in a variety of settings. For instance, Koh, Basawapatna, Bennett, and Repenning (2010) developed a real-time CT assessment system that stresses semantic analysis of student-created games or simulations and visualizes students' learning in terms of CT patterns. With a common focus on games, Werner, Denner, Campe, and Kawamoto (2012) tested students' (10-14-year-olds) CT learning by implementing three challenges in a 3D gaming environment powered by Alice (http://www.alice.org/index.php) and examined several factors (parental education, mother languages, high school grades, etc.) and their relationships to students' CT performance. In the Scratch (https://scratch.mit.edu) platform, Seiter and Foreman (2013) proposed a CT assessment framework and demonstrated its efficacy by applying it to 150 Scratch projects done by students from grade one through six. Similar work was also done by Brennan and Resnick (2012), who developed a framework to analyze young Scratchers' CT development by looking into their artifacts. Finally, closest to the current project, Bers, Flannery, Kazakoff, and Sullivan (2014) evaluated children's (4.9–6.5-year-olds) written programs after each activity of their curriculum to determine the students' CT learning patterns.As shown in the above examples, the majority of existing CT assessments focus more on examining student products, after they have learned a particular programing platform. This limitation prevents such assessment method from being used as pre/post measure of a specific curriculum. Furthermore, given an interpretation of CT as a fundamental skill that can be transferred across platforms, we should aim for assessment tools that apply across platforms. This is especially important given the proliferation of many coding and robotics platforms for the elementary level (e.g. Programmable Bricks, Creative Hybrid Environment for Robotic Programming, VEX Robotics Design System, and Dash) and a need for assessment tools that cut across platforms.Given these goals, recent work by Tew and Guzdial (2011) on assessing introductory computer science concepts is readily applicable across platforms. Their work showed the possibility of adopting a pseudo-code approach in assessing programming and thus evaluating pedagogical effectiveness of different courses. Furthermore, their work introduced a viable process of developing and validating assessment items of its kind. However, their assessment was not designed for use in elementary school and did not address the broader construct of CT, which presents as a limitation when aiming to understand if students can transfer CT in different contexts – a critical component that gives CT much of its currency. Another increasingly popular tool to assess CT is Bebras, which pools multiple challenges composed of short tasks aiming to promote informatics and CT among students of various age groups (Dagienė and Stupuriene, 2016, Duncan and Bell, 2015). While Bebras shares our goal of assessing CT given students’ limited prior knowledge about computation, it does not have a robotics focus. Given these limitations, we developed a CT assessment instrument that targets elementary students and minimizes the requirement of being familiar with a specific computing platform.Based on the operational definition of CT proposed by CSTA (see section 1.2), which enjoys a wide popularity from CS teachers as well as researchers, we worked out a five-component framework (Table 1) that guided our assessment items development. We did not include the component on transfer from the CSTA framework because all of our items are designed for transfer - either in the context of solving a problem in a new coding environment that is not covered in the curriculum or in the context of reasoning about everyday scenarios. Furthermore, since fifth grade students have limited programming experience, we used a pseudo-code approach in the coding items with two forms of syntax, text-based and drag-drop.Table 1. Computational thinking components.SFormulating problems and solutions using machine recognizable syntaxDOrganizing and analyzing dataAConceptualizing and generating solutions through algorithms (a series of ordered steps)RRepresenting problems and solutions through multiple external means such as a model and a formulaEGenerating, revising, and evaluating solutions with the goal of achieving the most efficient and effective combination of steps and resourcesGiven our design rationale, we have the following research questions that guided this study: What are the psychometric properties of the instrument we develop assuming it measures one construct (i.e., CT)? How do students perform on the assessment in light of the CT dimensions in our framework, in the two transfer contexts, and in the text-based and drag-drop coding environments?
