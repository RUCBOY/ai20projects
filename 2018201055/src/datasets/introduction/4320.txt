Computational fluid dynamics (CFD) has been progressively adopted in the last decade for studying the role of blood flow on the development of arterial diseases (see, e.g.,  [1], [2]). Computational investigations–compared to more traditional in vitro and in vivo studies–are generally more flexible and cost-effective. In combination with appropriate image-processing techniques–see, e.g.,  [3], [4], [5], [6], [7], [8], [9] ​–CFD can be used in a patient-specific setting. This means that the morphological and functional conditions of a specific patient may be reproduced in mathematical terms and quantitative analyses can be performed by solving the corresponding partial differential equations describing the physical and constitutive laws behind the physiopathology. There are several uses for this kind of analysis, including a deeper understanding of the clinical conditions, performing virtual surgery or therapy for predicting outcomes, to a personalized optimization/customization of generic procedures  [10], [11], [12], [13], [14]. Recently, the concept of Computer Aided Clinical Trials (CACT) has been proposed to identify the systematic use of scientific computing within the frameworks of studies oriented to extract knowledge from clinical data [7]. Similarly, Surgical Planning (SP) is the name given to an ensemble of procedures aimed at the prediction of the outcomes of an operation by exploiting the intrinsic predictive potential of numerical models.
Adoption of CFD for CACT and SP is however still an open challenge. In fact, the time for obtaining results from computational studies can be too long for the fast-paced clinical environment — especially in emergency scenarios. Furthermore, the association of computed blood-flow patterns with outcome of clinical relevance is often not supported by large enough sample sizes. On the other hand, the computational analysis of large number of patients calls for significant computational resources  [7]. In short, we may say that computational hemodynamics is a field with great potential, but currently limited by time and cost constraints. As a matter of fact, differently than in proofs of concepts, in CACT and SP accuracy of computations is not the only priority — as far as numerical simulations obtain sufficient reliability to correctly support clinical practice. Efficiency and robustness of the numerical procedures must be properly considered as well.
Increasingly however, scientists and clinicians have access to several classes of available computing platforms which could alleviate the resource bottleneck. While local (owned) resources are faster and cheaper, overall system and operating expenses have led to resource sharing and leasing paradigms, i.e.,  grids and clouds, respectively. More specifically, hospitals and healthcare institutions are expected to outsource numerical simulations in a routine process more than hosting local computing facilities. But it is not trivial to identify the platform that best suits the problem to be solved in each situation. Overall performance depends on two interrelated factors: (1) the architecture of the physical resource and (2) its optimal exploitation for the specific problem to solve. In real production settings, performance must be judiciously balanced with cost.
As for point (1), traditionally performance of HPC applications has been measured by a single metric, i.e., time to completion for the particular application at hand, parameterized with respect to problem size and number of processing elements used. Nevertheless, with the advent of cloud computing, the viability of executing parallel applications on the cloud (either through self-assembly or renting a prebuilt cluster) and the actual dollar cost effectiveness of executing HPC applications on different target platforms have become relevant. On the other hand, communication is an issue of paramount concern in the matter of efficiency. On clouds, a great deal of attention has been devoted to data handling but there has been relatively little focus on interconnection network capabilities. For explicit message passing parallel programs, such as those which make use of MPI, data handling and interconnection network capabilities lead to substantial heterogeneity in communication, with significant impact on performance. In addition, it is worth stressing that most real-life applications we are interested in are not regular or symmetric and thus their MPI process communication graphs are intrinsically unevenly weighted. The most popular software packages for graph partitioning (see, e.g.,  [15]) employ algorithms that minimize the edge cut or the communication volume as to obtain load balance. Nevertheless, numerical analysis suggests that a suitable percentage of additional work on each processing unit benefits the overall performance, due to a faster convergence of the iterative solver. The interplay between the additional numerical costs on each processor, the advantages induced by the faster convergence of the iterative method and the total communication time (which in turn relates to the specific architecture employed) is not trivial in problems of practical interest.
In this work we tackle these issues by exploring two aspects related to work partitioning. 
(a)We re-map the effective topology of the application’s interconnection network by managing the allocation of MPI processes to processor cores before the execution of the application, so that highly coupled MPI processes are “close”, i.e., mapped on cores within a single node. In this way, the intra-node communication is maximized and the long-distance inter-node communication is reduced.(b)We consider well-established methods to associate mathematical formalism to the parallel solution of complex systems of partial differential equations (PDEs). In particular, we resort to domain decomposition techniques (DD) to detect the optimal splitting of the tasks that minimizes the computational time. This method was historically introduced–well before the advent of parallel computing–to compute manually the solution of PDEs by splitting the process over different subdomains of the region of interest to take advantage of simple geometries (e.g., a L-shape domain was split into rectangles Fig. 2) where simple methods were available. Nowadays, DD is a powerful approach to manage the solution over different computational resources either with or without overlapping of subdomains, depending on the specific problem of interest and the identification of optimal interfaces to minimize inter-node communications.
Our reference application is the solution of problems related to computational hemodynamics, blood flow and solutes like Oxygen. We aim at demonstrating the relevance of all these issues in a realistic context, when dealing with a patient-specific setting to be considered as one out of many similar–but different–cases to be routinely simulated. We use an object oriented C++ library for the solution of PDEs with the finite element method (FEM) called LiFEV (“Library for Finite Elements 5”)  [16], extensively adopted in several projects of practical interest — see, e.g., [14], [17], [18], [19], [20], [8].
After providing some background on the numerical setting and the formulation of the two classes of problems used to test the performance of IaaS grids and clouds as opposed to local clusters and to experimentally detect the optimal partitioning that guarantees the fastest convergence of the numerical solver and a brief summary of the packages used, in Section  2 we discuss our experiences with comparing cost and utility on three typical platform types: (a) Infrastructure as a Service (IaaS) clouds, (b) grids, and (c) on-premise local resources, with a particular focus on process-to-node mapping vis-a-vis efficiency.
In Section  3, we consider the work balance in terms of DD and interface handling. We present an automatic procedure to optimize the mapping of the sub-domains to the available processing units based on graph analysis. We first consider a nonoverlapping strategy, where each domain shares with the others only the interface (e.g., a surface cutting in our case the volume of the artery of interest). However, it is well known that this is not necessarily the best option. In fact, a faster convergence to the desired solution in the iterative-by-subdomain approach can be attained if we allow some overlapping.
In Section  4, we test this option in both idealized and real 3D geometries. We show that the detection of the optimal overlapping in real cases–albeit nontrivial–has the potential to significantly reduce the computational costs of the entire solution process.
1.1. The numerical problemComputational hemodynamics requires the study of incompressible fluids described by the Navier–Stokes equations (NSE) [21], [22]. From the computational viewpoint, these equations are very challenging, for intrinsic mathematical features (see, e.g.,  [22]). In our tests NSE–completed by appropriate initial and boundary conditions–are solved for computing blood velocity and pressure in an artery affected by a disease, called cerebral aneurysm. The latter consists of an abnormal sac in the artery, inducing non-physiological flow patterns that can lead eventually to rupture of the arterial wall and brain hemorrhage. The application of computational hemodynamics to the study of vascular diseases is time- and cost-sensitive, as it typically entails the generation of large data sets of simulations on patient populations, with the final goal of finding statistical correlations of flow patterns with outcome  [23], [14]. Here, in particular, we consider a benchmark problem proposed in the Inaugural CFD Challenge Workshop  [24], i.e. the study of blood flow inside a giant brain aneurysm in an internal carotid artery.The equations are approximated by the Finite Element Method (FEM) combined with backward difference formulas (BDF) to handle the time dependence. With FEM, the solution is approximated by piecewise polynomial functions over subdivisions of the artery, called elements. The collection of elements is called mesh. This step reduces the partial differential equations to a system of ordinary differential equations in time. The latter is finally solved in selected instants by a second order BDF approximation. At each time step, a large sparse (i.e. with the majority of entries of the associated matrix equal to 0) linear system needs to be solved. The more elements are introduced in the computational domain and the more instants are collocated for the numerical solution; the higher the computational costs of the procedure are, the more accurate the solution is. In particular, here we consider a mesh with 837,154 elements, such that the total number of unknowns in the linear system is 3,162,146. The equations are collocated in 100 instants within the cardiac cycle (i.e. the simulation time step is 0.01 s). A snapshot of the computed solution is shown in  Fig. 1. Although current HPC facilities handle larger problems, these numbers can be considered representative of the size of problems of interest in CACT and SP in computational hemodynamics — as a reasonable trade-off between the accuracy requested by clinical applications and the expected timeline.Download : Download full-size imageFig. 1. Solution of the problem, based on NSE, when t=0.28s. Streamlines of the velocity field, when the flow rate is maximum over the cardiac cycle.
1.2. Domain decomposition techniques for the solution of partial differential equationsDD techniques provide an important framework to associate mathematical formalism to the parallel solution of a complex PDEs system — see, e.g.,  [25], [26]. The PDE problem over a region of interest Ω is decomposed in subproblems to be iteratively solved by single processors or clusters up to the fulfillment of a convergence criterion stating that the solution found is equivalent to the one of the unsplit system. Each subproblem exchanges information with the neighborhood ones by means of interface conditions. In nonoverlapping splittings, these conditions need to be properly chosen to guarantee that the split-by-subdomain solution is equivalent to the global one. In overlapping partitions, less constraints are required since synchronization conditions for each subdomain are prescribed on different space locations. In fact, each subdomain has its own interfaces. Notice that with overlap the PDE problem is solved multiple times on the overlapping regions, with a potential computational duplication overhead. However, beyond the more freedom when selecting the interface conditions, the iterative solver requires in general a lower number of iterations to converge. We illustrate the difference between the two approaches for a simple problem in Fig. 2.Download : Download full-size imageFig. 2. Schematic representation of (a) nonoverlapping and (b) overlapping DD in a L-shaped domain Ω. In the first case, conditions on the interface Γ must fulfill compatibility constraints depending on the nature of the PDE for the split-by-subdomain solution to be equivalent to the unsplit one.The interplay of (i) additional numerical costs due to the overlap, (ii) efficiency advantages induced by the specific iterative methods and (iii) versatility of the selection of domain interfaces (and the associated conditions) for the communication time, is not trivial in problems of practical interest. Numerical analysis focuses typically on points (i) and (ii) in idealized or simple geometries, while in the present work we assess the performances when the geometry of Ω is nontrivial, following up previous works  [27], [28].To this aim, we consider the differential Advection–Diffusion–Reaction (ADR) problem (1)−∑i=13∂∂xi(μ∂u∂xi)+∑i=13βi∂u∂xi+σu=f, for (x1,x2,x3)∈Ω⊂R3 with μ>0 and σ coefficients for simplicity assumed to be constant. Here the unknown u may represent the density of a species in a region where it diffuses with diffusivity μ, it undergoes a chemical reaction with rate σ and it is convected in the domain by the vector field β=[β1β2β3]T that denotes the blood velocity and it is a function of the space coordinates x1,x2,x3. When available, it can be prescribed analytically, as we do in the tests in idealized geometries. More in general, it is retrieved by solving the NSE computed as in the previous sections. The forcing term f is a given function of space too. Hereafter it will be set to 0 for simplicity. We associate with the equations, the boundary conditions u(ΓD)=g(x1,x2,x3),∂u∂n(ΓN)=0, where ΓD and ΓN are two disjoint portions of the boundary of Ω such that ΓD∪ΓN=∂Ω. This is a simplified model of the dynamics of blood solutes like Oxygen in the arteries  [29]. Specifically, we do not consider time dependence, since it does not introduce significant changes for the focus of the present paper. The NSE solution is therefore retrieved in a particular instant of the heart beat, the so-called systolic peak, corresponding to the maximum opening of the ventricular valve.To take advantage of domain decomposition, we split the domain Ω into two overlapping subdomains Ω1 and Ω2, such that Ω1∩Ω2=Ωo and Ω1∪Ω2=Ω. Let us denote by Γj the interfaces between the two subdomains (j=1,2), that is the portion of the boundary of Ωj that is not also the boundary of Ω, in short Γj≡∂Ωj∖(∂Ωj∩∂Ω). The solution of the problem in each subdomain will be denoted by uj(x1,x2,x3). We reformulate the original problem in an iterative fashion. Given an initial guess uj(0) (typically =0), we solve on each subdomain for k=1,2,…(2)−∑i=13∂∂xi(μ∂uj(k)∂xi)+∑i=13βi∂uj(k)∂xi+σuj(k)=finΩj,j=1,2 with boundary conditions (3)uj(k)(ΓD∩∂Ωj)=g(x1,x2,x3),∂uj(k)∂n(ΓN∩∂Ωj)=0,uj(k)(Γj)=uȷ̂(k−1)(Γj), (where ȷ̂=2 for j=1 and ȷ̂=1 for j=2) up to the fulfillment of the convergence condition to check that the solution in the overlapping region is not changing significantly along the iterations.At each iteration, we solve two independent problems in each subdomain, while the communication by subdomain occurs in the latter of boundary conditions (3). The convergence of the iterative scheme depends in general on the size of the overlapping region. In fact, if the overlapping is 100% of Ω, convergence is trivially guaranteed as at the first iteration (2)–(3) we are solving (twice) the unsplit problem. On the other hand, if the overlapping reduces to a volume-zero region, convergence is not guaranteed, as in general the juxtaposition of the two problems does not coincide with the original problem (if the interface conditions are chosen properly).The one presented here is the so-called additive formulation of the overlapping DD method, where the two subdomain problems can be solved simultaneously — as opposed to the multiplicative version, where one subdomain can be solved only when the problem on the other subdomain is completed. In the multiplicative formulation a faster convergence is guaranteed in terms of number of iterations (about one half of the additive scheme), but the algorithm has an intrinsically sequential structure. From now on, we refer only to the additive algorithm.The selection of the interfaces Γj has the only constraint to guarantee a non-empty overlapping. The optimal selection is the result of the trade-off between the computational cost of each subproblem and the reduction of the communication between processors. This will be investigated in Section  4 — see also  [28].
1.3. Packages used by the numerical solverFor a more detailed description of the implementation of the numerical solver we refer to  [30], [28]. We report here the complete list of required packages: –LiFEV library  [16], for the formulation of the algebraic counterparts to differential problems; this library is the direct dependency for our solver application;–Third-party scientific libraries: (1) Trilinos  [31] for the solution of linear systems (data structures and algorithms); (2) ParMETIS  [15], used for mesh partitioning; ad hoc MATLAB scripts were prepared to add an overlapping region to an existing nonoverlapping partition. (3) SuiteSparse  [32], as a support library extending the capabilities of Trilinos; (4) BLAS/LAPACK libraries (generic or vendor-specific implementations); (5) NetGen  [33] for generating the mesh to partition.–General-purpose and communication libraries: (1) Boost C++ libraries  [34] 1.44 or above, mainly used for memory management (smart pointers); (2) HDF5  [35], for the storage of large data on file; (3) MPI libraries (e.g., Open MPI);–Compilers: C++ compiler (e.g., GCC version 4 or above); [optional] Fortran compiler, compatible with C++;–Deployment tools: (1) GNU make; (2) Autotools; (3) CMake (version 2.8 or above).
