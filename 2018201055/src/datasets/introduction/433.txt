Visual tracking has raised continuous attention in recent decades with many practical applications ranging from intelligent security, video analysis to human computer interaction. Given the annotated target state (bounding box) in the initial frame, the aim of visual tracking is to automatically predict the target states in the subsequent video frames. Though rapid development has been achieved in constructing advanced trackers, a variety of practical challenging factors including background clutter, scale variation, motion blur and occlusion, still increase the demand of more accurate and robust visual tracking approaches [1], [2].
Recently, tracking methods based on deep Convolutional Neural Networks (CNNs) [3], [4], [5] have obtained wide attention due to their representative power and generative capabilities. Among these deep trackers, Discriminative Correlation Filters (DCFs) and Siamese networks are two typical approaches to construct tracking models. Specifically, DCFs are expert in learning online updated classifiers [6], while Siamese methods employ deep structures to train a general similarity function in offline manner [7]. First, DCFs employ circulant matrix [8] to generate augmented samples to support discriminative online training, alleviating the issue of sample shortage in online visual tracking. With the structure of circulant samples, DCFs enable efficient closed-form solution in the frequency domain with only element-wise multiplication and division operators needed [9]. Therefore, the advantage of DCFs focuses on its online learning capacity to the tracking model under temporal appearance variations [10], [11], [12]. On the other hand, with the development of offline trained deep approaches, Siamese networks have become one of the most popular frameworks in visual tracking. Siamese structure constructs a shared network to achieve stable similarity learning [7], [13], [14]. Due to the diversity of visual appearance variations supported by large volume of labelled video datasets [15], [16], [17], Siamese approaches enable effective and hierarchical feature training. However, there exists limitations for the above two methods. In general, DCFs approaches pay more attention to the recent changing appearance in the learning framework, with a simple average moving strategy to balance the historical information [9], [18]. While Siamese networks rely heavily on the quality and adequacy of the training samples, without considering dynamic adaptation in the online tracking stage [14].
To mitigate the above issues, ATOM combines the DCFs and Siamese approaches together to simultaneously improve the tracking performance via offline training and online tracking [19]. Firstly, an offline trained target-specific IoU-Net [20] is proposed to automatically predict the bounding box. Specifically, given random generated proposal bounding boxes, the target-specific IoU-Net predicts the overlap between the object and the proposals, achieving bounding box refinement by maximizing the IoU. Second, in the online tracking stage, ATOM employs a two-layer convolutional neural network to achieve classification between target and background. This classifier is similar to the DCFs formulation, which is trained online from the first frame using the least square loss. Besides, to reduce the redundancy from multi-channel backbone features, ATOM designs a projection matrix layer to perform feature compression in the online tracking stage.
Despite its success in accurate bounding box estimation, the ATOM tracker suffers from several limitations in the online classification stage. First, multi-level features are not fully utilized. In the classification component of ATOM tracker, only middle level convolutional outputs are utilized as feature inputs to distinguish the target from background. However, single convolutional layer is impossible to express both fine-grained and semantic discriminative information. Such single layer feature representation may drift the predictions to background in complex tracking scenarios. For example, intra-class distractors and the demanded target always belong to the same category, with analogous semantic features, leading to potential mistakes in the final response. To this end, combining different levels of discriminative features should be more essential to distinguish the target from background. Second, the quality of online training samples can not be guaranteed. In the online training stage, feature representations generated by the projection matrix layer and their corresponding desired response maps are considered as training samples. However, the projection matrix is designed as a fixed network, which can not be adaptively updated under appearance variations [19], [21]. Therefore, the compressed features can be easily contaminated by the fixed projection matrix layer when encountering dramatic change of scene.
Motivated by the above observations, an adaptive feature fusion mechanism is proposed to explore the potential power of multi-level feature representations. First, features from different convolutional outputs are reprocessed to the same resolution with corresponding designed mapping layer. Here, representative power can be enriched by stacking multi-level features with improved diversity in granularities. Unfortunately, redundancy and noise are also inevitably introduced along with the fused feature maps, which may also generate sub-optimal results. In order to overcome the degeneration caused by the above issue, our proposed mechanism is designed to dynamically activate specific features and weaken the impact of less discriminative layers. This is achieved by introducing a layer-wise similarity measurement. Specifically, we compare the layer-wise similarity of the target features between template and instance samples. The measurement can be viewed as an activation possibility of the corresponding layer in the current scenario. In addition, with the use of a regularization term, the adaptive weights fitting the similarity measurement can be attributed to the corresponding layers for final fusion. As for enhancing the robustness of the projection matrix layer, we propose to jointly train the projection matrix layer and the correlation layer in both initialization and subsequent tracking stages, with adaptive fused features as training samples.
To summarize, the main contributions of this work are three-fold.
•We propose an adaptive feature fusion mechanism to provide both semantic and discriminative feature representations by automatically fusing multi-level convolutional layers.•We reformulate the update strategy. Through joint training the projection matrix layer and correlation layer, a more convincing target localization formulation can be achieved.•We validate our method on several benchmarking datasets with state-of-the-art methods. The experimental results and corresponding analysis demonstrate the merit of the proposed tracker.
