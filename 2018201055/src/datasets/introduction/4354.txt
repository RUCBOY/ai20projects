High Performance Computing (HPC) systems and architectures are evolving rapidly. Traditional single processor-based CPU clusters are moving towards multi-core/multi-threaded CPUs. At the same time new architectures based on many-core processors such as graphics processing units (GPUs) and Intel's Xeon Phi are emerging as important systems and further developments are expected with energy-efficient designs from ARM and IBM. According to the IT industry, such advances are expected to deliver compute hardware capable of exascale-performance (i.e. 1018 floating-point operations per second) by 2018 [1]. Yet many frameworks aimed at computational/numerical modelling are currently not ready to exploit such new and potentially disruptive technologies.
Traditional approaches to numerical model development involve the production of static, hand-written code to perform the numerical discretisation and solution of the governing equations. Normally this is written in a language such as C or Fortran that is considerably less abstract when compared to a near-mathematical domain specific language. Explicitly inserting the necessary calls to MPI or OpenMP libraries enables the execution of the code on multi-core or multi-thread hardware. However, should a user wish to run the code on alternative platforms such as GPUs, they would likely need to re-write large sections of the code, including calls to new libraries such as CUDA or OpenCL, and optimise it for that particular hardware backend [2]. As HPC hardware evolves, an increasing burden faced by computational scientists becomes apparent; in order to keep up with trends in HPC, not only must a model developer be a domain specialist in their area of study, but also an expert in numerical algorithms, software engineering, and parallel computing paradigms [3], [4].
One way to address this issue is to introduce a separation of concerns using high level abstractions, such as domain specific languages (DSLs) and active libraries [4], [5], [6], [7], [8]. This paradigm shift allows a domain specialist to describe their problem as a high-level, near-mathematical specification. The task of taking this specification and transforming it into executable computer code can then be handled in the subsequent abstraction layer; unlike the traditional approach of hand-writing the C/Fortran code that discretises the governing equations, this layer generates the code automatically from the problem specification. Finally, the generated code can be readily targetted towards a specific hardware platform through source-to-source translation. Hence, domain specialists focus on the equations they wish to solve and the setup of their problem, whilst the parallel computing experts can introduce support for new backends as they become available. At no point does the code have to undergo a fundamental re-write if the desired backend changes. Use of such strategies can have significant benefits for the productivity of both the user and developer, by removing the need to spend time re-writing code and/or the problem specification [5].
Given the motivation for the use of automated solution techniques, in this paper we present a new framework, OpenSBLI, for the automated derivation and parallel execution of finite difference-based models. This is an open-source release of the recent developments in the SBLI codebase developed at the University of Southampton, involving the replacement of SBLI's Fortran-based core with flexible Python-based code generation capabilities, and the coupling of SBLI to the OPS active library [9], [10], [11], [12] which targets the generated code towards a particular backend using source-to-source translation. Currently, OpenSBLI can generate OPS-compliant C code to discretise and solve the governing equations, using arbitrary-order central finite difference schemes and a choice of either the forward Euler scheme or a third-order Runge-Kutta time-stepping scheme. OpenSBLI then uses OPS to produce code targetted towards different backends. It is worth noting that backend APIs such as OpenMP (version 4.0 and above) are also capable of running on CPU, GPU and Intel Xeon Phi architectures, for example. However, currently OPS has no support for OpenMP version 4.0 and above. Moreover, codes that are written by hand in OpenMP would still potentially need to be re-written if different algorithms or equations were to be considered. Thus, the benefits of code generation still play a crucial role here, regardless of which backend is chosen.
The application of SBLI has so-far concentrated on problems in aeronautics and aeroacoustics, in particular looking at shock-boundary layer interactions (see e.g. [13], [14], [15], [16] and the references therein for more details). While such applications entail solving the 3D compressible Navier-Stokes equations, in principle other equations expressible in Einstein notation and solved using finite differences are also supported by the new code generation functionality, highlighting another advantage of such a flexible approach to numerical model development. Note also that while OpenSBLI does not yet feature shock-capturing schemes and Large Eddy Simulation models (unlike the legacy SBLI code), these will be implemented in the future as part of the project's roadmap. The main purposes of this initial release is the algorithmic changes to legacy SBLI's core.
Details the abstraction and design principles employed by OpenSBLI are given in Section 2. Section 3 details three verification and validation test cases that were used to check the correctness of the implementation. The paper finishes with some concluding remarks in Section 4.
