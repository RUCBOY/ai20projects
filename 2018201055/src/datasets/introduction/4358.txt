Probabilistic cognition is a natural fit to the kind of problems posed by the environment: people are faced with noisy and ambiguous observations about the world, yet need to make good decisions. Probabilistic models allow for uncertainty and ambiguity to be dealt with appropriately, because instead of incorrectly assuming that imperfect information is known perfectly, these models can find the best possible action given that imperfect information.
These models have had broad success in explaining human data, accounting for how people are aware of their perceptual uncertainty and combine it appropriately with prior knowledge (Körding and Wolpert, 2004, Tassinari et al., 2006), and explaining how people can learn to represent an ambiguous environment in cognitive tasks (Griffiths et al., 2007, Kemp and Tenenbaum, 2008). However despite these successes, probabilistic models have faced skepticism from two major sources: evidence of mismatches between human behavior and probabilistic cognition (Tversky & Kahneman, 1978), and the inherent computational complexity of these models. It just does not seem like we as humans can do the complex calculations necessary to arrive at the best answers, and so there must be shortcuts involved (Anderson, 1991, Simon, 1955, Van Rooij, 2008).
Fortunately the problem of working with complex probabilistic models in limited systems has received a lot of attention from computer scientists and statisticians. Researchers in these fields have developed algorithms that arrive at good solutions while minimizing computational and memory requirements. These algorithms then provide an interesting alternative to extant heuristics in psychology and neuroscience, and in cognitive science using these algorithms to explain behavior has been termed rational process models (Sanborn, Griffiths, & Navarro, 2010). The advantage of this approach is that when these algorithms are used in situations for which they are well-adapted, they make probabilistic cognition achievable, but when they are applied to situations for which they are poorly adapted, they can explain biases in behavior that cannot be explained by probabilistic models alone.
Computer scientists and statisticians have developed various types of approximations for probabilistic models, such as Laplace’s method, sampling algorithms, variational approximations, and expectation propagation (Bishop, 2006, Doucet et al., 2001, Minka, 2001, Neal, 1993, Wainwright and Jordan, 2008). Here I focus on the two types that have been applied to approximate probabilistic cognition: sampling and variational approximations. Sampling algorithms are stochastic, randomly drawing samples to represent a probability distribution as a collection of points. While sampling algorithms asymptotically provide the correct answer, they are less accurate and can show biases for small numbers of samples. In contrast, variational algorithms trade stochastic sampling for deterministic optimization. These algorithms can be very fast, but are asymptotically biased.
Researchers have used both sampling and variational algorithms as approximations to probabilistic cognition in behavior and the brain. However these investigations have tended to proceed separately, with little comparison between the work using the two types of algorithms. Below, I describe examples of both types of algorithms, how they can produce behavioral biases, and how they might be implemented in the brain. A comparison of the two types shows what each is good for, and how they could be profitably combined in future work.
