Several software architectures can be adopted to integrate workflow engines in the ecosystem of tools and services offered by science gateways, with important consequences for the development effort required and resulting system. This paper describes, illustrates and compares such architectures, based on system-independent representations of their main components and interactions. It is informed by our experience in the development and sustained operation of the CBRAIN  [1], NSG  [2], [3] and VIP  [4] science gateways during the past 7 years, as well as by lessons learned from several science gateway and workflow projects such as SHIWA1 and ER-flow.2
This analysis is intended for experts of science gateway and workflow engine design. It is an abstraction effort to identify and evaluate the fundamental architectural patterns that are encountered while integrating workflow engines and science gateways. In real systems, such patterns sometimes coexist due to the historical and technical context of software projects.
The remainder of this section provides background information and definitions of workflow engines, science gateways and infrastructures. In Section  2, we describe six architectures within a consistent framework that underlines the functional interactions between their main software components. We illustrate these architectures on real systems in Section  3. In Section  4, we introduce metrics that measure integration complexity, robustness, extensibility, scalability and functionality. The metrics are specifically designed to measure the ability of the architectures to address workflow-related issues commonly encountered in science gateways. We evaluate the architectures individually in Section  5, and we compare and discuss them in Section  6. Finally, in Section  7, we illustrate how our evaluation framework can be used to help design new systems. Related work is presented in Section  8.
1.1. Workflow enginesIn the last decade, the e-Science community has developed workflow systems to help application developers access distributed infrastructures such as clusters, grids, clouds and web services. These efforts resulted in tools among which Askalon  [5], Hyperflow  [6], MOTEUR  [7], Pegasus  [8], [9], Swift  [10], Taverna  [11], Triana  [12], VisTrails  [13], WS-PGRADE  [14] and WS-VLAM  [15]. Such workflow engines usually describe applications using a high-level language with specific data and control flow constructs, parallelization operators, visual edition tools, links with domain-specific application repositories, provenance recording and other features. An overview of workflow system capabilities is available in  [16].At the same time, toolboxes have been emerging in various scientific domains to facilitate the assembly of software components in consistent “pipelines”. In neuroimaging, our primary domain of interest, tools such as Nipype (Neuroimaging in Python, Pipelines and Interfaces  [17]), PSOM (Pipeline System for Octave and Matlab  [18]), PMP (Poor Man’s Pipeline  [19]), RPPL  [20], SPM (Statistical Parametric Mapping  [21]) and FSL (FMRIB Software Library  [22]) provide abstractions and functions to handle the data and computing flow between processes implemented in a variety of programming languages. Such tools were interfaced to computing infrastructures, in particular clusters, to execute tasks at a high throughput. Some of these tools also support advanced features such as provenance tracking or redundancy detection across analyses to avoid re-computation. A wide array of workflows have been implemented using these pipeline systems and are now shared across neuroimaging groups world-wide, which represents a tremendous opportunity for science gateways to leverage. Domain-specific engines nicely complement e-Science systems that are more oriented towards the exploitation of distributed computing infrastructures, in particular grids and clouds.In this paper, a workflow engine (also abbreviated engine) is a piece of software that coordinates and submits interdependent computing tasks to an infrastructure (local server, cluster, grid or cloud) based on a workflow description (a.k.a workflow), using input data that may consist of files, database entries or simple parameter values. Although simplistic, this definition covers both e-Science workflow engines and domain-specific pipeline systems. Some workflow engines, usually from the e-Science community, may transfer data across the infrastructure, and others, usually domain-specific ones, may leave this role to external processes. Workflows may be expressed in any language, including high-level XML or JSON dialects such as Scufl or Hyperflow, and low-level scripting languages such as Bash.
1.2. Science gatewaysScience gateways are used to share resources within a community and to provide increased performance and capacity through facilitated access to storage and computing power. They are often accessible through a web interface that helps users manage access rights, data transfers, task execution, and authentication on multiple computing and storage locations. Workflow engines are part of this ecosystem as core components to implement and execute applications.Various science gateways have been developed, including frameworks such as Apache Airavata  [23], the Catania Science Gateway Framework  [24] and WS-PGRADE/gUSE  [14]. Numerous science gateways were built using such frameworks  [25], [24] or as standalone systems  [1], [4]. Most of these systems include one or several workflow engines.Integration between workflow engines and science gateways varies across systems. Some science gateways are tailored to a particular engine, while others are more general and host applications executed by different types of engines.Extensibility is an important property of the integration. New workflows are added frequently, different types and versions of workflow engines may be integrated over time, and different kinds of infrastructure can be targeted.Scalability is also a crucial concern for such multi-user, high-throughput systems. For this purpose, science gateways may balance the load among different instances of the same engine, start new engines elastically using auto-scaling techniques such as the ones reviewed in  [26], and use advanced task scheduling policies on the infrastructure to improve performance, fault-tolerance and fairness among users.Robustness is highly desirable as well since it is key to a good user experience. Simple architectures facilitate the implementation effort towards robust interactions which, in turn, have a positive impact on characteristics such as gateway predictability, transparency, reliability, traceability and reproducibility.Other specific features may also be available, for instance data visualization and quality control, workflow edition, debugging instruments, or social tools among users.
1.3. InfrastructureAn infrastructure consists of the computing and storage resources involved in workflow execution, as well as the software services used to access these resources. The infrastructure can be composed of computing or file servers, databases, clusters, grids or clouds. Some workflow engines and science gateways may require specific characteristics, such as the presence of a shared file system between the computing nodes, the availability of a global task meta-scheduler, or the presence of a file catalog. In the analysis presented hereafter, such specific requirements are not discussed. Instead, we consider the infrastructure as an abstract system that can execute tasks and store data regardless of the enabling mechanisms.
