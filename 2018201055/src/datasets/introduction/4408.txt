The number of transistors in a microprocessor has been doubling approximately every two years and as a result, the performance of supercomputers measured in floating-point operations per second (FLOPS) has been following a similar increase. However, since increasing the clock frequencies of microprocessors to gain better performance is no longer feasible because of power constraints, this has led to a change in their architectures from single-core to multi-core.
While modern central processing units (CPUs) utilize more cores and wider SIMD units, they are designed to perform well in general tasks where low memory access latency is important. On the other hand, graphics processing units (GPUs) are specialized in solving data-parallel problems found in real-time computer graphics and as a result, house more parallel thread processors and use higher-bandwidth memory than CPUs. With the introduction of general-purpose programming frameworks, such as OpenCL and CUDA, GPUs can now also be programmed to do general purpose tasks using a C-like language instead of using a graphics application-programming interface (API), such as OpenGL. In addition, APIs such as OpenACC can be used to convert existing CPU programs to work on a GPU. For these reasons, GPUs offer an attractive platform for physical simulations which can be solved in a data-parallel fashion.
In this work we concentrate on investigating sixth-order central finite-difference scheme implementations on GPUs, suitable for multiphysics applications. The justification for the use of central differences with explicit time stepping, a configuration which is not ideal concerning its stability properties, comes from the fact that, even though some amount of diffusion is required for stability, they provide very good accuracy and are easy to implement (see, e.g. [1]). In addition, the various types of boundary conditions and grid geometries needed in multiphysics codes such as the Pencil Code1 are easy to implement with central schemes. Moreover, the problem has the potential to exhibit strong scaling with the number of parallel cores in the optimal case.
There are astrophysical hydro- and magnetohydrodynamic solvers already modified to take advantage of accelerator platforms (i.e. [2], [3], [4]), that most often use low-order discretization. As an example of a higher-order scheme for cosmological hydrodynamics, we refer to [5]. We also note that more theoretical than application-driven work on investigating higher-order stencils on GPU architecture exists in the literature, see e.g. [6]. There are many scientific problems, such as modeling hydromagnetic dynamos, where long integration times are required, either to reach a saturated state (see e.g. [7]), or to exhibit non-stationary phenomena and secular trends (see e.g. [8]). Therefore, it is highly desirable to find efficient ways to accelerate the methods, GPUs offering an ideal framework. The accelerated codes typically employ lower-order conservative schemes, in which case the halo region to be communicated to compute the differences is small, and does not pose the main challenge for the GPU implementation. High-order schemes of similar type as presented here exist for two-dimensional hydrodynamics (e.g. [9]); in this paper, we deal with a 3D implementation of a higher-order finite-difference solver. Such schemes are much less diffusive and they are more suitable for accurate modeling of turbulence, which is, on the other hand, crucial for e.g. investigating various types of instabilities in astrophysical settings. One mundane example, which is the solar dynamo, is responsible for all the activity phenomena on the Sun, driving the space weather and climate that affect life on Earth [10]. The accurate modeling of turbulence is also important in understanding such phenomena as the structure of interstellar medium [11] and star formation [12].
We make the following contributions in this work. First, we describe, implement and optimize two novel methods for simulating compressible fluids on GPUs using sixth-order finite differences and 19- and 55-point stencils. The current implementation is for simulations of isothermal fluid turbulence. The bigger picture is that it uses the same core methods as the Pencil Code. Thus the current code development works as a pilot project in the conversion of the Pencil Code to use GPUs.
Our implementations perform 1.7× and 3.6× faster than a state-of-the-art finite difference solver, Pencil Code, used for scientific computation on HPC-clusters. Second, we present an optimization technique called kernel decomposition, which can be used to improve the performance of latency-bound kernels. Currently our code, called Astaroth, supports isothermal compressible hydrodynamics, but it will be expanded in the future to include more complex physics, in the end supporting the full equations of magnetohydrodynamics (MHD).
In this paper, we present the physical motivation (Section 2) behind our implementations, and the technical justification and background (Section 3). The details of our implementations and the Astaroth code are presented in Section 4. In Section 5 we present the performance of our GPU implementations and compare the results with physical test cases in Section 6. Finally, in Section 7, we discuss our results and conclude the paper.
