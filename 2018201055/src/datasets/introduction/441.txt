Recently, with the explosion of deep learning (DL) technologies, artificial intelligence (AI) has made a huge leap forward. DL technologies have gone far beyond traditional machine learning (ML) methods in certain areas such as image classification [47], audio recognition [14], and natural language processing [59]. While DL has achieved excellent results in many applications, it poses potential risks due to its black box approach in learning. Unlike traditional ML methods, the training process of DL lacks transparency, making it difficult to understand what specific content DL has learned during the training process.
The black box approach in training makes the development of the DL field full of instability and limitations, this is especially true in sensitive areas such as autonomous driving, smart medical care, smart transportation, and security inspection. Ribeiro et al. [73] showed that a mistaken interference of the original input would cause the classifier to extract unnecessary associations during training, and this wrong association is difficult to be recognized by humans. Similarly, Hendrycks et al. [37] fully exposed the weaknesses of neural networks in accurately classifying images. They specifically explored the natural adversarial examples, and used these examples in nature to trick deep learning models into misclassification objects. Marcus [58] pointed out that the transparency of ML has not been solved, and it would be a key issue for applications that are represented by financial and medical diagnosis. As Cathy [68] pointed out, the opacity of ML will lead to a series of prejudices. After all, human needs a rerasonable explanation to the machineâ€™s decisions.
With an increasing demand on interpretation for DL models, many works in this field have been published. From a macro perspective, existing interpretable methods on DL models specifically DNNs are divided into two types [20], [39], [33], [62], [63] in general - global and local interpretation methods. Global interpretation methods[60], [107], [15], [12], [84], [31] are based on an overall understanding of DNN model features and each of the learned components such as weights and structures. This type of methods can systematically analyze the training process of a DNN model, but it is very difficult to completely transform the complex black box approach into a model with global attributes [63]. Existing global interpretation methods usually only target at specific simple DNN models due to a large amount of computations incurred. A local interpretation method instead checks individual predictions locally, which can be focused on a single instance trying to figure out how a DL model makes a decision [20], [63]. Generally speaking, the interpretation of a DNN does not necessarily require a global interpretation of the DNN model. Since local interpretation methods only analyze the individual cases locally, they have the advantages of easy implementation and low computation complexity compared with global interpretation methods.
As a result, the past few years have witnessed a rapid development of local interpretation methods. Different from existing surveys [20], [39], [33], [62], [63] which give a broad review on DL interpretation, this survey focuses on the category of local interpretation methods and provides a systematic analysis of the state-of-the-art works. Based on the ways that local interpretation methods work, we categorize them into data-driven and model-driven approaches. These two categories of approaches are distinct and elaborated in detail in the following paragraphs (see Fig. 1).Download : Download high-res image (133KB)Download : Download full-size imageFig. 1. A taxonomy of local interpretation methods for DNNs.
Data-driven interpretation on a DNN model employs the method of perturbing data to compare the prediction results generated by the same DNN model, and extract the content that needs to be explained in the DNN model. There are three types of data-driven interpretation methods: perturbation-based interpretation [100], [28], [17], [71], [82], [9], [35], [56], [19], [26], [97], [106], adversarial-based interpretation [92], [66], [24], [23] and concept-based interpretation [43], [110], [112], [27], [30], [11]. Perturbation-based interpretation refers to an interpretation that changes the original data by masking the part of input data and systematically analyzes the contrast result. Adversarial-based interpretation adds adversarial samples to the original data set, and implements interpretation by optimizing gradients or obscuring disturbances. By using different types of disturbances, it can improve the robustness of model interpretation, and support fine-grained with preserved characteristics of images such as edges and colors. Concept-based interpretation is based on a third-party concept set which is usually obtained by experts in the industry.
Model-driven interpretation performs to the corresponding sensitivity analysis of the internal components of a DNN model (such as neurons and weights) to assess the importance of the components, and select the data points with a higher explanatory power through the division of the threshold. We divide model-driven interpretation into three types: gradient-based interpretation [100], [98], [85], [88], [57], [87], [75], [21], [91], [89], [104], correlation-score interpretation [83], [79], [4], [7], [86] and class activation mapping[111], [72], [69], [80], [70], [22], [10]. Class activation mapping is a visualization method based on the accumulation of neurons and weights. Through the cumulative heating map provided, we can obtain the information learned by a DNN model during the training process. Gradient-based interpretation further consists of many types of methods, but the core idea is to map the set formed by the prediction results and the corresponding weights to the original input. Then, sensitivity analysis can be conducted on the original input data points based on the magnitude of the gradient value. Correlation-score interpretation no longer uses gradient calculation to achieve interpretation, instead it calculates the correlation score of each input pixel for interpretation.
In this survey, we summarize the representative works and the characteristics of the state-of-the-art methods. We particularly emphasize the highly cited papers and the latest papers published in high-impact conferences over the last three years. In addition, emerging methods on DNN interpretation such as context-aware second order(CASO) [86] will also be reviewed. It is worth noting that this survey also reproduces the results of a number of interpretation methods based on their open source plugins and further demonstrate the effects of these methods in interpretation of DNNs.
The structure of this survey is summarized as follows. In Section 2 and Section 3, we review data-driven and model-driven interpretation methods respectively. Section 4 reproduces the interpretation results of a number of methods based on their open source plugins and demonstrates their effects on interpretation. In Section 5 discusses challenges in interpretation and points out future research directions. Section 6 concludes the survey.
