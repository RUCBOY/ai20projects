Any engineering design process deals with the evaluation of a possible set of design solutions, each obtained as the combination of specific values of a set of design parameters. This evaluation is usually performed comparing a design score with a predetermined limit score, and often defining a safety factor to ensure compliance despite the uncertainty of the actual manufacturing process and operating conditions. Traditional applications of this methodology, such as the verification of static and fatigue criteria for beams and shafts, are based on simple analytic relationships between design and operation parameters (e.g. dimensions, material properties and loading) and design score (e.g. maximum von Mises equivalent tensile stress).
With a push for innovative design solutions, the designer is often following a time- and resource-intensive trial-and-error design process, including the proposal of creative new solutions and a long series of design score evaluations. In complex engineering systems (i.e. multiple design parameters and nonlinear functional relationships) the number of possible suitable solutions and the cost of computing each solution's design score increases dramatically. If the designer is unaware of the likely sets of design parameter combinations resulting in acceptable design scores, this process may become cumbersome.
Providing an estimate of the limit-score boundary separating acceptable and unacceptable design parameters (in terms of design score) would enable the designer to focus on areas of the solution space sufficiently far from the limit boundary. Moreover, once the solution is chosen, the assessment of the distance from the boundary would also allow evaluating its “safety margin”, i.e. robustness to variations of design parameters.
This study aims at obtaining an estimate of the limit-score boundary using a limited number of computationally expensive design-score evaluations. The boundary search consists of finding all the combinations of design parameters in the space resulting in a fixed value of the design score (limit score). Without loss of generality, this task is equivalent to a zero-finding problem for an unknown multi-input single-output function.
To achieve this result, this paper proposes to (i) represent the zero finding task as a classification problem and estimate the level set as a decision boundary; (ii) ensure sufficient exploration of the design parameter space by using a space filling algorithm to select candidate samples; (iii) reduce the number of design-score evaluations needed to estimate the boundary by iteratively evaluating only samples likely close to the boundary.
The proposed iterative procedure is based on:

1.a Sobol sequence for the generation of a space-uniform set of candidate design parameter combinations;2.a Support Vector Machine (SVM), applied to a small subset of the candidate combinations from 1 to obtain a decision boundary used as a proxy (or surrogate) of the limit-score boundary;3.the use of this computationally inexpensive SVM-surrogate to identify which of the design parameter combination of 1 is most likely close to the boundary (without running actual simulations);4.the iterative (points 3 and 4) generation of refined SVM-based representations of the boundary by running targeted numerical simulations only for samples identified in 3.
This procedure allows producing a refined representation of the boundary without the need for an exhaustive evaluation of the design score throughout the multidimensional space of design parameters. This is done by using at each step the partial information on the location of the boundary to select only few and high-value design parameter combinations to be evaluated. The proposed methodology thus uses an SVM surrogate (or “meta-model” ) to identify an explicit representation of the boundary. While other meta-modelling approaches are available, e.g. Artificial Neural Neworks (Sundar & Shields, 2016), Response Surface methods (Goswami et al., 2016, Roussouly et al., 2012), and Kriging Methods (Sun, Wang, Li, & Tong, 2017), SVMs have important advantages for pursuing boundary refinement (point 4) (Kremer, Steenstrup Pedersen, & Igel, 2014) and are therefore selected here.
The proposed estimation-and-refinement strategy (points 3 and 4) belongs to the framework of active learning, a branch of machine learning where training includes iteratively querying new training data points. Active learning is particularly beneficial when the labelling of new training points comes at a high computational cost. The idea is that by intelligently querying points, one can achieve a high-accuracy classifier with only a limited subset of training samples (Settles, 2009). Given a classifier trained on a (small) subset of the available data, the key question in active learning is how to select the most informative unlabelled samples. The most popular strategy is to select samples in regions where the classifier is the least confident, called uncertainty sampling or simple query strategies (Guyon et al., 2011, Ho et al., 2011, Kremer et al., 2014, Lewis and Gale, 1994, Settles, 2009). However, it is well known that this sampling approach can be problematic: it can over-emphasize regions of the feature space that are not representative of the data distribution and it assumes that the classifier accurately labels points that are far from the estimated decision boundary (Kremer et al., 2014). In other words, uncertainty sampling alone tends to stress “exploitation” while sacrificing “exploration” of new feature space regions (Guyon et al., 2011).
In active learning SVMs are commonly chosen as the algorithms to perform classification owing to their ability to clearly identify samples near the decision boundary (Kremer et al., 2014). SVMs have been traditionally used to classify experimental data in a wide variety of applications, including condition monitoring (Kim et al., 2012, Samanta, 2003, Widodo and Yang, 2007), face recognition (Huang, Shao, & Wechsler, 1998), medical diagnosis (Chen et al., 2011, Musselman and Djurdjanovic, 2012), and pattern recognition in control charts (Hachicha and Ghorbel, 2012, Lu et al., 2011). In the traditional active learning applications, as for condition monitoring and diagnostics, the main focus has been the result of the classification (accuracy, recall, etc.) rather than the classification boundary itself, whose intrinsic value is generally disregarded because not physically meaningful.
A more closely related line of work can be found in literature on structural reliability where surrogates are used to decrease the computational effort required for Monte Carlo simulations about a design point. Two main approaches exist for developing surrogates for structural reliability analysis:

1)regression methods (e.g. Response Surface Methods) where the limit score function itself (not just the boundary) is approximated, often with a surrogate regression model such as Artificial Neural Networks (ANNs) or Gaussian Processes (Dai et al., 2012, Roussouly et al., 2012, Sundar and Shields, 2016, Viana et al., 2012);2)classification methods that seek only to ascertain if a design is unacceptable or acceptable.
In structural reliability, regression approaches dominate, but a number of studies have been conducted in recent years using the classification approach (Alibrandi et al., 2015, Basudhar and Missoum, 2010, Bourinet et al., 2011, Gorissen et al., 2010; J.E. Hurtado and Alvarez, 2010, Lin et al., 2012, Song et al., 2013, Van Der Herten et al., 2016). Moreover, in a review of surrogate modelling tools, Hurtado (Hurtado, 2004) noted that classification methods are more naturally suited to identification of implicit limit score boundaries. The main justification is in the nature of the problem: one is only interested in the exceedance of score function limit and not in its exact value.
Most of the aforementioned classification studies employed SVMs as the surrogate classifier and some use different active learning strategies to refine boundary estimates. Alibrandi et al. (2015) developed a strategy that generated points in a cone between the nominal design values and the nearest point on the surface (found via a separate optimization problem). Bourinet et al. (2011) developed a subset sampling algorithm that employed an SVM-based active learning strategy which generated new samples close to the SVM boundary by clustering points that are close the boundary. Hurtado and Alvarez (2010) used Particle Swarm Optimisation to find local minima of the score function (i.e. samples that are close to the boundary).
Basudhar and Missoum developed an active learning strategy based on an SVM estimate of the decision function and an auxiliary optimization strategy (Basudhar & Missoum, 2010) aimed at selecting new samples that provide the highest refinement of the boundary. Song et al. (2013) made use of the score function values (not just the sign) and augmented Basudhar and Missoum's method to include “virtual samples” based on a local regression. This most recent line of work is based on the active learning strategy proposed by Basudhar and Missoum (2008) combined with different complex methodologies to alleviate the “locking” phenomenon which results from a strong focus on selecting samples close to the limit score boundary.
In this work, a new SVM-based active learning strategy is developed with the aim of avoiding complex and ad hoc anti-locking strategies (Basudhar and Missoum, 2008, Basudhar and Missoum, 2010, Bourinet et al., 2011, Song et al., 2013). To strike a balance in the exploitation-exploration trade-off, an innovative combination of a space-filling strategy and active learning is proposed. In addition to its anti-locking properties, the new methodology enables an approximate a priori specification of the resolution error when the designer possesses some modest knowledge of the properties of the boundary.
The general SVM theory will be introduced and described conceptually in the first section of the paper, in combination with a recursive and efficient surface refinement methodology aimed at obtaining the best definition of the limit hyper-surface with the minimum number of numerical simulation runs. The next two sections will present the practical implementation of the methodologies in an efficient algorithm for N-dimensional problems and the validation of the procedure with numerical tests on a-priori known functions, therefore allowing the exact quantification of the difference between actual limit surface and SVM surrogate. Subsequently, the new methodology will be compared to Basudhar and Missoum (2010) on a benchmark function, showing no loss in performance (on the contrary, slightly improved) despite the significantly simpler sampling strategy. Finally, the algorithm will be tested on a railway engineering application, using ADTRES software (Bruni, Collina, Diana, & Vanolo, 2000) for the simulation of train dynamics
