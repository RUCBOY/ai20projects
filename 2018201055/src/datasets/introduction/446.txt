Natural Language Processing (NLP) is the sub-field of Artificial Intelligence that focusses on understanding and generating natural language by machines (Khurana et al., 2017). Formally, NLP is defined as “a theoretically motivated range of computational techniques for studying and representing naturally occurring texts (of any mode or type) at one or more levels of linguistic analysis for the purpose of attaining language that is like a human-like language processing for a range of tasks or applications” (Liddy, 2001). NLP is an interdisciplinary field lying at the intersection of computing science, computational linguistics, artificial intelligence and cognitive science. NLP is concerned with research and development of novel applications for Human Computer Interaction (HCI), with human languages as a medium of communication. NLP applications include human language understanding, lexical analysis, machine translation, text summarization, speech recognition, sentiment analysis, expert systems, question answering and reasoning, intelligent tutoring systems and conversational interfaces.
Calculating the similarity between text snippets is an important task for many NLP applications. Similarity scoring schemes range from basic string-based metrics to more complex techniques that employ semantic analysis. Simple string-based metrics only apply in cases of exact word matching. They do not consider inflection, synonyms and sentence structure. To capture these text variations, more sophisticated text processing techniques, able to calculate text similarity on the basis of semantics, are needed. Latent Semantic Analysis (LSA) is one such technique, allowing to compute the “semantic” overlap between text snippets. Introduced as an information retrieval technique for query matching, LSA performed as well as humans on simple tasks (Deerwester et al., 1990). LSA’s abilities to handle complex tasks, such as modelling human conceptual knowledge, cognitive phenomena and morphology induction have been assessed on a variety of tasks consistently achieving promising results (Landauer et al., 1998, Landaueret al., 2007, Landauer and Dumais, 2008, Schone and Jurafsky, 2000). As its underlying principle, LSA considers the meaning of text in direct relationship with the occurrence of distinct words. Intuitively, LSA considers that words with similar meaning will occur in similar contexts. It has been used successfully in a diverse range of NLP applications (Landauer, 2002, Vrana et al., 2018, Wegba et al., 2018, Jirasatjanukul et al., 2019). For example, it has been extensively used as an approximation to human semantic knowledge and verbal intelligence in the context of Intelligent Tutoring Systems (ITS). LSA-based ITSs, such as AutoTutor and Write To Learn (Lenhard, 2008), allow learners to interact with the system using a natural language interface. Even though LSA provides promising results in a multitude of applications, its major shortcomings come from the fact that it completely ignores syntactic information during similarity computations. LSA suffers the following inherent problems:
1)LSA is based on the semantic relations between words and ignores the syntactic composition of sentences. Consequently, it may consider semantically similar sentences with very different or even opposite meaning (Cutrone & Chang, 2011).2)LSA does not consider the positions of subject and object of a verb as distinct, while comparing sentences. For example, LSA considers the sentences “The boy stepped on a spider” and “The spider stepped on a boy” as semantically identical, although they are semantically opposite to each other.3)LSA considers list of words as complete sentences, despite the lack of proper structure (Islam and Hoque, 2010, Braun et al., 2017). For example, “boy spider stepped” is considered equivalent to the sentences in (2), and LSA considers them as semantically identical.4)LSA does not consider negation. Consequently, it cannot differentiate between two semantically similar sentences, but one contains some negation. For example, “Christopher Columbus discovered America” and “Christopher Columbus did not discover America”. Negation inverts the sentence’s meaning. However, LSA assigns a similarity score of more than 90% to this pair of sentences.
In this paper, we explore ways to enrich LSA with syntactic information to enhance its accuracy when comparing short text snippets. We employ Parts-Of-Speech (PoS) tags and Sentence Dependency Structure (SDS) to enrich the input text with syntactic information. Current trends in NLP research focus on Deep Learning. Neural network-based architectures are employed to model complex human behaviors in natural language. These methods have achieved top performance levels for many semantic understanding tasks, arguably due to their ability to capture syntactic representations of text (Gulordava et al., 2018, Kuncoro et al., 2018, Linzen, Emmanuel, & Yoav, 2016, Hewitt and Manning, 2019). Lately, a variety of models that produce embeddings that capture the linguistic context of words have been proposed, ranging from Word2vec (Mikolov, 2013) to state-of-the-art transformer-based architectures, such as BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019). We evaluate our method against some of the current neural network-based methods, such as Universal Sentence Encoder (USE) (Cer, 2018), Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) and XLNet (Yang et al., 2019). The results show that xLSA performs consistently better than these techniques on short text snippets.
The rest of the paper is organized as follows: Section 2 provides an overview of text similarity approaches and describes research work on enriching the LSA model with syntactic information. Section 3 introduces xLSA and provides details about the proposed extension to LSA. Section 4 describes the experimental settings and summarizes the results of the comparative analysis. Section 5 concludes the findings and proposes directions for future work.
