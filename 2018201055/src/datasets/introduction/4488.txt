Reproducibility of results of scientific experiments is a cornerstone of the scientific method. Therefore, the scientific community has been encouraging researchers to publish their contributions in a verifiable and understandable way  [1], [2]. In computational science, or in-silico science, reproducibility often requires that researchers make code and data publicly available so that the data can be analyzed in a similar manner as in the original work described in the publication. Code must be made available, and data must be accessible in a readable format  [3].
Scientific workflows are a useful representation for managing the execution of large-scale computations. Many scientists now formulate their computational problems as scientific workflows running on distributed computing infrastructures such as campus Clusters, Clouds, and Grids  [4]. Researchers in bioinformatics have embraced workflows for a whole range of analyses, including protein folding  [5], DNA and RNA sequencing  [6], [7], and disease-related research  [8], [9], among others. The workflow representation not only facilitates the creation and management of the computation but also builds a foundation upon which results can be validated and shared.
Much research has focused on studying the reproducibility of scientific results in life sciences. Some studies clearly show the difficulties when trying to replicate experimental results in biology  [10]. The Reproducibility Project: Cancer Biology   [11] is an active project aiming to independently reproduce the experimental results of 50 high-impact cancer biology studies, evaluating the degree of reproducibility of those results and the main issues related to them.
Since workflows formally describe the sequence of computational and data management tasks, it is easy to trace the origin of the data produced. Many workflow systems capture provenance at runtime, what provides the lineage of data products and as such underpins scientific understanding and data reuse by providing the basis on which trust and understanding are built. A scientist would be able to look at the workflow and provenance data, retrace the steps, and arrive at the same data products. However, this information is not sufficient for achieving full reproducibility.
Reproducibility, replicability, and repeatability are often used as synonyms. Even when they pursue similar goals, there are several differences between them  [12]. In this work we consider them as separated concepts. While replicability can be defined as a strict recreation of the original experiment, using the same method, data and equipment, reproducibility implies that at least some changes have been introduced in the experiment, thus exposing different features. While being a less restrictive term, reproducibility is a key concept in science, as it allows incremental research by modifying, improving and repurposing the experimental methods and conditions.
In this work, we address the reproducibility of the execution environment for a scientific workflow, as we do not aim to obtain an exact incarnation of the original setup, but rather an environment that is able to support the required capabilities exposed by the former environment. In order to reproduce or replicate any digital artifact we need to properly handle its conservation. According to  [13], to achieve conservation one needs to guarantee that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author”. Hence, we address workflow conservation in order to attain its reproducibility.
In  [14], authors explain the problems they faced when they tried to reproduce an experiment  [15] for mapping all putative FDA and European drugs to protein receptors within the scope of a given proteome. For each identified problem, they enumerate a set of suggestions for addressing the related issues. In four out of the total six cases, execution environment problems are mentioned.
Currently, most of the approaches in computational science conservation, in particular for scientific workflow executions, have been focused on data, code, and the workflow description, but not on the underlying infrastructure—which is composed of a set of computational resources (e.g., execution nodes, storage devices, and networking) and software components. We identify two approaches for conserving the environment of an experiment: (1) physical conservation, where the real object is conserved due to its relevance and the difficulty in obtaining a counterpart; and (2) logical conservation, where objects are described in such a way that an equivalent one can be obtained in a future experiment.
The computational environment is often conserved by using the physical approach, where computational resources are made available to scientists over a sustained period of time. As a result, scientists are able to reproduce their experiments in the same environment. However, such infrastructures demand huge maintenance efforts, and there is no guarantee that it will not change or suffer from a natural decay process  [16]. Furthermore, the infrastructure may be subjected to organization policies, which restrict its access to a selective group of scientists, thereby limiting reproducibility to this restricted group. On the other hand, data, code, and workflow descriptions can be conserved by using a logical approach, which is not subjected to natural decay processes.
Accordingly, we propose a logical-oriented approach to conserve computational environments, where the capabilities of the resources (virtual machines (VM)) are described. From this description, any scientist, interested in reproducing an experiment, will be able to reconstruct the former infrastructure (or an equivalent one) in any Cloud computing infrastructure (either private or public). One may argue that it would be easier to keep and share VM images (or containers) with the community research through a common repository, however the high storage demand of VM images (in particular for Big Data applications) may be a challenging problem  [17], [18].
Inspired by the aforementioned ideas, exposed in  [13], we aim to define means for authors to share the relevant information about the execution environment of a given scientific workflow. We argue that by explicitly describing this knowledge we increase the degree of reproducibility of the environment and of the workflow.
Semantics have been proposed as a way of attaining curation and conservation of the digital assets related to scientific experiments (e.g., biomedical research  [19]). Our approach uses semantic-annotated workflow descriptions to generate lightweight scripts for an experiment management API that can reconstruct the required infrastructure. We propose to describe the resources involved in the execution of the experiment using a set of semantic vocabularies, and use those descriptions to define the infrastructure specification. This specification can then be used to derive the set of instructions that can be executed to obtain a new equivalent infrastructure.
We conduct a practical experimentation process in which we describe a set of workflows and their environments using a set of semantic models. Then, we use an experiment management tool to reproduce a workflow execution in different Cloud platforms. In this work, we study three real and representative production workflows, which have been disseminated in several scientific publications. Many scientific workflows allow their execution infrastructure to be escalated, depending on the processing needs of the application. In this work, we analyze the reproduction of workflows which were formerly configured to run into single-node infrastructures.
The first iteration of the semantic models for representing computational resources, and some of the reproducibility tools were introduced and evaluated in  [20] for a single astronomy workflow. In this work, we extend our previous work by introducing (1) a set of new features to our framework based on the result of our previous work, including a new version of our models and features in our algorithmic process; (2) a study of two new life sciences workflows based on genomic processing applications and their related challenges with respect to previous cases; and (3) a practical evaluation of the framework with the new features for the astronomy workflow as well as to the new life science workflows.
The paper is organized as follows. Section  2 describes our semantic approach for documenting computational infrastructures. Section  3 presents the description of the tools used to implement the semantic models and manage the experiment. Section  4 describes the experimentation process. Section  5 presents the related work, and Section  6 summarizes our results and identifies future works.
