Recently, deep Convolutional Neural Network (CNN) has achieved the state-of-the-art performance in image classification task [1], [2], [3], [4]. With the rebirth of CNN, a series of feature extractors stacked from low-level to high-level can be automatically learned from large-scale training data in an end-to-end manner. A number of works have shown that these learned feature extractors can be successfully transferred to other computer vision tasks [5], [6], [7]. As an active research topic, content-based image retrieval (CBIR) also can utilize CNN activations as universal representation for image. In this paper, we attempt to achieve better retrieval performance based on the complementarity of CNN activation maps of different layers.
CBIR always relies on the descriptor's ability of representing image. The powerful hand-crafted descriptors can capture local characteristics of object, such as SIFT [8]. Most existing approaches encode these gradient-based features to overcome the semantic gap, such as Bog-of-Words (BoW) [9], Fisher Vectors (FV) [10] or Vector Locally Aggregated Descriptors (VLAD) [11] and their variants [12], [13], [14]. However, these encoding methods are still far from capturing high-level semantic information.
When the pre-trained CNN is applied to CBIR, the feature maps of higher layers are selected to present whole image [15]. In recent findings, feature maps of lower layers can achieve better results in instance-level image retrieval [16]. Actually, feature maps of different layers extract information of different levels from input image. Fig. 1 illustrates some patches corresponding to top activations on some filters learned by CNN.1 The dimensions of low-layer features tends to response the patches with similar simple patterns and with more ambiguity. The feature maps of higher layers care more about semantic information but less detail information about image, since higher layers are closer to the last layer with category labels. The feature maps of lower layers contain more structural information about image, but suffer from the problem of background clutter and semantic ambiguity. Although the feature maps of lower layers share similar semantic gap as hand-crafted features, the feature extractors of lower layer can capture local patterns for describing instance-level detail.Download : Download high-res image (1MB)Download : Download full-size imageFig. 1. The patches with top activations on sampled dimensions of CNN features from different convolutional layers. Each row shows the patches with largest activations in the corresponding dimension. All patches are sampled from Oxford 5K dataset.
High-layer feature is used to measure semantic similarity and low-layer feature is used to measure fine-grained similarity. Giving an easy-to-understand example, when query image is a building, high-layer similarity captures the images contains a building and low-layer similarity captures the building with same subordinate-class even same instance. Obviously, the complementarity of low-layer and high-layer features can improve the similarity measuring between query image and other candidate images. Some existing methods attempt to utilize multi-scale orderless pooling for CNN activations. For instance, CNN features are extracted and encoded from different layers respectively, then these aggregated features of different layers are concatenated to measure images [17]. However, direct concatenation can not use the complementary of high-layer and low-layer features adequately. High-layer feature can search a collection of candidate images with similar semantic for query image, yet it is not powerful enough to describe the fine-grained detail. Thus, high-layer similarity will weaken the effectiveness of low-layer similarity, when fine-grained distinctions are distinguished between the nearest neighbors with similar semantic.
In this paper, we propose to exploit more complementary strengths of CNN features of different layers in a simple but effective way. Our method attempt to highlight the effectiveness of low-layer similarity, when measuring fine-grained similarity between query image and its nearest neighbors with similar semantic. In other words, low-layer feature is used to refine the ranking result of high-layer feature, rather than concatenating multiple layers directly. As shown in Fig. 2, high-layer feature is not powerful enough to describe the detail information, while low-layer feature suffers from background clutter and semantic ambiguity. In the manner of direct concatenation, low-layer similarity can not play a vital role in distinguishing fine-grained distinction due to the influence of high-layer similarity. Using a mapping function, our method takes more advantage of the low-layer feature measuring fine-grained similarity between query image and its nearest neighbors with same semantic. In experiments, we demonstrate that our method performs better than single-layer feature, concatenation of multiple layers and other hand-crafted features based methods.Download : Download high-res image (913KB)Download : Download full-size imageFig. 2. The ranking results searched by different CNN features. From top to bottom, the rows show the ranking results of high-layer feature, low-layer feature, direct concatenation and our method. High-layer feature is extracted from fc1 layer, low-layer feature is extracted form conv4 layer, direct concatenation and our method use these two layers. The green border denotes positive candidate image, while red border denotes false candidate image. Both query image and candidate images are sampled from Oxford 5K dataset. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
