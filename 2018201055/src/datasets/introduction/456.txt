Deep neural networks (DNNs) [1], especially convolutional neural network (CNN) [2], have shown great performance advantages over conventional machine learning methods in pattern recognition tasks, e.g. [3], [4], [5]. Although it works well in terms of accuracy, the algorithm has no biological root. DNNs learn different features of target tasks by supervised learning. The supervised learning requires a large number of labels to ensure the success of learning tasks. However, there are not enough labels in the real world. In particular, in the brain, learning is usually performed through unsupervised learning. Therefore, the spiking neural network (SNN) [6], [7] is a satisfactory model to simulate the brain. The basic unit of SNN is an spiking neuron. Spiking neurons simulate the process in which nerve cells receive stimulation, generate action potentials and emit spikes. Neurons communicate with each other by propagating discrete spikes. As the nonlinear, discontinuous and not differentiable characteristics of spiking neurons, it is difficult to train the SNN. The most frequently used method is to use different data representation between training and processing, i.e. training a conventional artificial neural network (ANN) and developing conversion algorithms that transfer the weights into equivalent SNN [8], [9], [10]. However, these approaches limit synaptic weights in a linear range so that the ANN could be converted directly into a linear SNN classifier. SpikeProp algorithm [11] is another manner which makes the equivalent exchange from spatial information in ANN to the temporal information in SNN. This manner successfully converts the spatial backpropagation in ANN into the temporal backpropagation in SNN. However, the problem of non-differential characteristics of SNN still exists, which has limited further improvements of SNNs on the performance of both accuracy and efficiency [12]. An spiking deep convolutional neural network is proposed in [13]. This network consists of three convolutional and three pooling layers. Differential of Gaussians (DoG) filters are used for image preprocessing in the first layer of the network. The DoG filters detect the contrasts and emit a spike. And its performance depended to some extent on DoG filters. An random contrastive Hebbian learning is proposed in [14]. It uses random matrices to transform the feedback signals during the clamped phase, and the neural dynamics are described by first order non-linear differential equations. A highly effective and robust membrane potential-driven supervised learning (MemPo-Learn) method is proposed in [15]. The MemPo-Learn method employs an error function based on the difference between the output neuron membrane potential and its firing threshold. [16] trains spiking neurons with a dynamic firing threshold named noise-threshold. With a combination of noise-threshold, the anti-noise capability of the existing supervised learning methods of SNNs have improved significantly, and the trained neuron is precisely and more reliably to reproduce target firing patterns even under high level noise. [17] proposes an architecture for unsupervised ensemble learning in a population of spiking neural network classifiers. The main aim of [17] is to explore a hypothesized role for ITDP in the coordination of ensemble learning, and in so doing present a bio-inspired architecture, with attendant mechanisms, capable of producing unsupervised ensemble learning in a population of spiking neural networks. This has been achieved through the development of an MoE type architecture built around SEM networks whose outputs are combined via an ITDP based mechanism.
[18] develops a deep spiking neural network that can learn from few training trials. It uses the back-propagation based learning rule to a train the network to detect basic features of different digits. By using synaptic pruning and a predefined threshold, some of the synapses through the training are deleted. Therefore information channels are constructed that are highly specific for each digit as matrix of synaptic connections between two layers of spiking neural networks.
In this paper, an ensemble SNN via unsupervised training manner based on spike-timing-dependent plasticity (STDP) [13] rule is proposed. The network consists of excitatory and inhibitory neurons, and the convolution and pooling layers are embedded in the SNN. Lateral inhibition [19], [20] mechanism is used during training. The purpose of integrating individual SNNs with the same structure and different parameters is to improve the accuracy of the network. In order to avoid repetitive training of the network when performing different tasks, several layers of the trained network are fixed in this work. The fixed network layers do not participate in the training of new tasks. Results will show that the ensemble SNN via transfer learning under different object recognition tasks can gain satisfactory performance.
The main contributions of this paper are as follows.
1)A convolutional SNN architecture using unsupervised STDP learning rule is proposed. The excitatory and inhibitory neurons are integrated together in an interactive way to make the network in balanced states [12], [21].2)Different patterns emit different values of action potentials of neurons. If all patterns set the same threshold values, some patterns overshadowing others will be observed. To avoid confusion in different patterns, a strategy of a variable threshold is used in our work.3)To improve the SNN performance, an ensemble SNN architecture and transfer learning are proposed in this paper. The ensemble SNN via voting manner can improve the classification accuracy. Transfer learning is used to avoid re-training our network. Results demonstrate the performance of the ensemble SNN in different object recognition tasks via transfer learning manner.
The rest of the paper is organized as follows. Section 2 provides the related works. Section 3 presents convolutional SNN models, the input encoding method, the variable threshold neuron model, the network structure, and the STDP learning rule. Section 4 provides the ensemble SNN architecture and transfer manner. Section 5 provides the experimental results that demonstrate the performance of the ensemble SNN under different object recognition tasks. Section 6 concludes and gives insight into the potential directions for future work.
