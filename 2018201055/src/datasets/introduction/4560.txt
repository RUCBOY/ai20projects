Academic journals differ widely in their reputation and impact (Bradford, 1934, Nisonger, 2008, Seglen, 1992). The differences between the highest- and lowest-impact journals can be striking. In 2015, for instance, the top 32 chemistry journals were cited more than the next 790 combined (SCImago Research Group, 2017). The evaluation of journals is therefore central to the work of faculty and librarians in their roles as scholars, authors, and collection managers. Journal rankings have been used for nearly a century to identify the foremost journals in each subject field, to evaluate the differences between journals, and to track changes in reputation and impact over time (Nisonger, 1999, Nisonger, 2004).1
Two broad types of journal rankings have been identified: revealed preference rankings and stated preference rankings (Tahai & Meyer, 1999). Revealed preference rankings generally focus on scholarly impact, and most are based on citation metrics such as the impact factor (IF) and the h index. They can be defined more broadly, however, as any rankings that reflect the actual behavior of scholars, librarians, or readers—rankings based on publishing productivity, for instance, or on the extent to which journal articles are included in dissertation bibliographies or course reading lists (Esteibar and Lancaster, 1993, Sugimoto, 2011, Tjoumas, 1994). In contrast, stated preference rankings are subjective assessments based on the opinions of authors, faculty, or other subject experts. Surveys, interviews, or focus groups are used to elicit respondents' ratings of journals based on criteria such as scholarly impact, reputation, prestige, utility for research, utility for teaching, or importance in tenure and promotion decisions (Walters, 2017b).
Within the field of library and information science (LIS), both kinds of metrics are readily available. Many LIS journals are covered by citation databases such as Web of Science and Scopus, and at least nine studies have presented rankings of LIS journals based on surveys of faculty, deans, and practitioners (Blake, 1991, Blake, 1994, Blake, 1996, Kohl and Davis, 1985, Manzari, 2013, Nisonger and Davis, 2005, Nkereuwem, 1997, Tjoumas, 1991, Tjoumas and Blake, 1992). Different ranking methods can lead to different results, however. Citation-based rankings are not always consistent with stated preference rankings, and the ratings assigned by library directors do not always match those assigned by the deans of MLIS programs (Kim, 1991, Kohl and Davis, 1985, Nisonger and Davis, 2005, Walters, 2017a).
This study presents a factor analysis of journal ranking metrics based on data for 55 LIS journals. Specifically, it (1) identifies the factors (dimensions) underlying a set of six citation metrics and five stated preference metrics, and (2) generates a set of composite rankings that represent the 11 metrics in a more parsimonious way. Two questions are central to this research. First, can the 11 metrics be represented fully by a single factor? If one factor is sufficient to represent all 11 metrics, we can conclude that they all measure the same underlying construct. A second question—What are the underlying factors represented by the set of 11 metrics?—comes into play if more than one factor emerges. For instance, we might plausibly identify two factors, one representing citation impact and another representing the various subjective journal rankings. Alternatively, we might discover that the opinions of LIS faculty are closely aligned with the citation metrics but that an additional factor is needed to account for the journal ratings assigned by academic library directors.
Because factor analysis, in this context, is an exploratory technique, we cannot know in advance which factors will emerge. For instance, the results may draw attention to the distinctions between objective citation impact and subjective reputation, LIS faculty and library practitioners, information science and library science, importance to readers (as an information resource) and importance to authors (as a publication outlet), international and local emphasis, orthodox and heterodox approaches to LIS research, older and more recently established journals, or earlier and more recent journal rankings. If the results reveal the existence of two or more factors along any of these dimensions (or a combination of them), we can conclude that multiple dimensions of perceived quality or impact ought to be considered by those who seek to fully understand each journal's place within the discipline.
