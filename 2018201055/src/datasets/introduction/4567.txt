Advances in CT technology have improved image quality and increased the total number of CT examinations. However, this has raised concerns about radiation exposure and the potential cancer risk induced by it [1]. To limit the radiation dose, low-dose CT (LDCT) was performed in clinical situations. For example, National Lung Screening Trial confirmed a 20% reduction in lung cancer mortality among subjects allocated to a LDCT screening group [2].
Although LDCT has proven useful for screening of lung cancer, the cumulative radiation dose associated with LDCT is a major problem. McCunney et al. showed that the cumulative radiation dose from LDCT screening could exceed that received by nuclear workers or atomic bomb survivors if LDCT lung cancer screening was conducted over a 20â€“30-year period [3]. They also showed that such a radiation dose could independently increase the risk of lung cancer beyond that associated with cigarette smoking.
To overcome this issue, ultra-low-dose CT (ULDCT) has been studied intensively [4, 5, 6]. Because images obtained by LDCT or ULDCT are severely affected by noise (e.g., streak artifact) and differentiation between normal/abnormal findings on the noisy CT images is difficult, image-processing techniques have been utilized to improve image quality of LDCT or ULDCT [4, 5, 6, 7, 8, 9, 10]. While many methods have been proposed in previous studies, image-processing techniques of LDCT or ULDCT can be roughly divided into two categories: raw-data-based techniques [4, 5, 6] and post-processing techniques [7, 8, 9, 10]. In the present study, we focused on post-processing techniques. Although the major strength of post-processing techniques is that it can be applied directly to CT images, it is often difficult to differentiate noise and artifacts from the actual signal on the noisy images. For example, Chen et al. showed that although one type of large-scale nonlocal mean (LNLM) [7, 11] was useful for denoising abdominal LDCT images, the LNLM method was not effective in suppressing the non-stationary streak artifacts in thoracic CT images [8].
One previous study showed that it was possible to achieve state-of-the-art image-denoising performance with plain multilayer perceptron that maps noisy image patches onto noise-free ones [12]. The performance of this perceptron could rival that of block-matching and 3D filtering (BM3D) [13], a well-engineered image-denoising algorithm. In line with this trend, the present study utilized neural network for image denoising of ULDCT images as a post-processing technique. One approach to use neural network as image denoising is denoising auto-encoder (DAE), a special type of neural network. DAE takes a pair of original input and noisy input, maps the noisy input to the latent representation, and uses the latent representation to reconstruct the output [14]. DAE trains its parameter such that the loss between the original input and its reconstruction is reduced. While noise was artificially added to the original input in the previous study of DAE, pairs of standard-dose CT (SDCT) and ULDCT image patches were used to train DAE in the present study. In addition, we utilized convolutional auto-encoder (CAE) [15] to improve image denoising. If CAE is successfully trained with pairs of SDCT and ULDCT image patches, CAE would output the noise-free image patch of ULDCT.
The objectives of the present study were: i) to validate a patch-based, neural-network-trained image-denoising method for ULDCT images; ii) to train the neural network with CAE and pairs of SDCT and ULDCT image patches; and iii) to investigate the performance of our proposed method using a chest phantom. In addition, our proposed method was compared with the method proposed by Chen et al. [16].
