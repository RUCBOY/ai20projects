The emergence of wearable video devices such as action cameras, smart glasses and low-temporal life-logging cameras has detonated a recent trend in computer science known as First Person Vision (FPV) or Egovision. The 90’s idea of a wearable device with autonomous processing capabilities is nowadays possible and is considered one of the most relevant technological trends of the recent years  [1]. The ubiquitous and personal nature of these devices opens the door to critical applications such as Activity Recognition  [2], [3], User–Machine Interaction  [4], Ambient Assisting Living  [5], [6], [7] Augmented Memory  [8], [9] and Blind Navigation  [10], among others.
One of the key features of wearable cameras is their capability to move across different locations and record exactly what the user is looking at. This is an unrestricted video perspective that requires existent methods to perform good in the unknown number of locations and the changing light conditions implied by this video perspective. A common way to deal with this problem is to predefine a particular application or location and bound the algorithms based on this. This is the case of gesture recognition for virtual museums proposed in  [4] or the activity recognition methods based on the kitchen dataset  [5], [11]. Another way to alleviate the large number of recorded locations is by using exhaustive video labeling of the recorded locations and objects as is done in  [6] to detect daily activities. The authors in  [12] use global histograms of color to reduce the effect of light changes in a color-based hand-segmenter.
The approach of  [12] shows that contextual information, such as light conditions, are valuable sources of information that can be used to improve the performance and applicability of current FPV methods. This idea is also applicable to other FPV related functionalities such as activity recognition, on which a device that can understand user’s location can easily reduce the number of possible activities and take more accurate decisions. Pervasive computing refers to the devices that can modify their behavior based on contextual variables as context-aware devices  [13], and its benefits are widely explored for example in assisted living  [14] and anomaly detection  [15].
This paper is motivated by the potential impact of contextual information, such as light conditions and location, on different FPV methods. The strategy presented, is a first step toward our envision of a device that can understand the environment of the user and modify its behavior accordingly. The proposed approach understands the contextual information on which the user is involved as a set of different characteristics that can point to previously recorded conditions, and not as a scene classification problem based on manual labels assigned to particular locations (e.g., kitchen, office, street). In this way, this study devises an unsupervised procedure for wearable cameras to switch between different models or search spaces according to the light conditions or location on which the user is involved. Fig. 1 summarizes our approach.Download : Download high-res image (408KB)Download : Download full-size imageFig. 1. Unsupervised strategy to extract contextual information about light and location using global features.
From Fig. 1 it is clear that the transition from the global features to the unsupervised layer can be seen as a dimensional reduction from the global feature space (high dimensional space) to a simplified low dimensional space (intrinsic dimension). The latter provides an unsupervised location map to be used later to switch between different behaviors at different hierarchical levels. These dimensional reductions are known as manifold methods, and their capabilities to capture complex patterns are defined by their algorithmic and/or theoretic formulation  [16].
Regarding the global features to be used, relevant information can be obtained from recent advances in FPV  [1] and scene recognition  [17], [18]. Given the restricted computational resources of wearable devices, we use computationally efficient features such as color histograms and GIST descriptors. However, the proposed approach can be extended with more complex data such as deep features  [15]. In that case three important issues must be considered: (i) the computational cost will restrict the applicability in wearable devices; (ii) it will require large amounts of training videos and manual labels; (iii) the use of existent “pre-trained” neural architectures compromises the unsupervised nature of our approach.
The novelties of this paper are three folded: (i) It evaluates the capability of different linear and non-linear manifold methods, namely Principal Component Analysis (PCA), Isometric Mapping (Isomaps), Self Organizing Maps (SOM) and Growing Neural Gas (GNG), to capture light/location patterns from different global features without using manual labels. (ii) It analyzes, following a feature selection procedure, the most discriminative components of the selected global features, (iii) As an application case, the proposed unsupervised strategy is used to improve the hand-detection problem in FPV. The hand-detection problem is used as an example, because of its impact on context-aware devices in hand-based methods, and because it allows us to illustrate the role of the unsupervised layer and its contribution to the final hand-detection performance. The use of the same strategy at higher inference levels such as hand-segmentation or hand-tracking is left as future research.
The remainder of this paper is organized as follows: Section  2 summarizes some recent strategies to understand automatically contextual information. Later, Section  3 introduces our methodological approach, summing up the selected features, different manifold methods and some common unsupervised evaluation procedures. In Section  4 the manifold methods are trained, and their capability to capture light/location patterns is evaluated in a post-learning strategy using the manual labels of two public FPV datasets. Section  5 illustrates the use of the best performing manifold method to improve the hand-detection rate in FPV. Finally, Section  6 concludes and provides some future research lines.
