Deep convolutional neural networks (DCNNs) have achieved tremendous success in a wide range of visual tasks [1], [2], [3]. However, such success has so far relied on large-scale fully annotated training datasets, like ImageNet [4], PASCAL VOC 2012 [5] and MS COCO [6]. In particular, for tasks such as semantic segmentation, performance heavily relies on the availability of a large number of pixel-wise labels, whose generation requires dramatically more time than image-level labels. This bottleneck prompted researchers to investigate for less time-consuming and expensive approaches. One potential attractive solution is weakly supervised learning, which seeks to mimic humans’ ability to discover whole objects from one small clue. Weakly supervised learning aims to use weak labels to create strong predictive models. For the task of semantic segmentation, weakly supervised learning has relied on bounding boxes [7], [8], scribbles [9], points [10] and image-level labels [11], [12], [13], [14], [15], [16], [17], which have proven to be the most popular and cost-effective as they are simple, and easy to collect.
Studies on weakly supervised semantic segmentation have focused on how to use weak labels (e.g., image-level labels in this paper) to effectively supervise the training of segmentation networks. More specifically, they have focused on how to use image-level labels to alleviate the need for pixel-level labels that are used in fully supervised learning. The relationship between image-level labels and pixel-level labels is established by image classification networks through their implicit object localization ability. Techniques such as class activation mapping (CAM) [18] or Grad-CAM [19], allow us to visualize class predictions on any given image, highlighting the discriminative object parts detected by the CNNs. The detection results constitute object localization maps (also known as object attention maps), which can be used to produce pseudo/proxy ground-truth for the task of weakly supervised segmentation. However, the major problem is that the generated object localization maps are usually small and sparse, resulting in deficient supervision for the training of the segmentation networks.
There are two key reasons. First, for image classification, it is usually sufficient to make correct classification prediction from small or local object regions. Therefore, the CNNs only detect or pay attention to the small object regions. To address this limitation, a heuristic solution is to iteratively erase the detected object regions as used in [12], [15], [16]. It thus drives the networks to not choose the common patterns to classify a class, but instead to rely on new patterns for a specific class. However, it is hard to control the degree of changes. Overly-changed inputs will mislead the network, making the convergence harder to achieve. Second, we believe that the problem of incomplete object localization maps is due to the lack of context information. CAM or Grad-CAM can only produce useful object attention maps at the last convolutional layer, but produce unmeaningful saliency maps at the earlier layers [20]. However, in image classification CNNs, there are repeated pooling and down-sampling layers in order to achieve the image transformation invariance and to increase the ability of a hierarchical abstraction of data. As a result, feature maps from the higher layers contain limited context information. So far, however, there has been little discussion on how to address this limitation in previous works. This paper is a contribution towards producing better object attention maps by exploiting multi-scale context representations.
Recently, atrous convolution (also known as dilated convolution) has shown popularity in semantic segmentation, as it enlarges the receptive fields by inserting “holes” in the convolutional kernels, thus eliminating the need for the down-sampling operations such as max-pooling or strided convolution [21]. It also maintains the resolution of feature maps, and the same computational cost as the conventional convolution. In this work, in order to generate better pseudo segmentation labels with image-level annotations, we propose an atrous convolutional feature network (ACFN) for multi-label image classification. The proposed ACFN contains both cascaded atrous convolutions and pyramidal atrous convolutions. Specifically, the cascaded atrous convolutions are used in the middle layers to retain more spatial details. The pyramid structure (e.g., pyramidal image [22], pyramidal feature [23], [24] and pyramidal attention [25]), has shown its effectiveness in addressing scale-related problems in various vision tasks. Inspired by these prior works, we propose to exploit pyramidal atrous convolutions in the last convolutional layers to capture multi-scale context information for the subsequent extraction of object attention maps. Wei et al.’s MDC [13] also used atrous convolutions for weakly supervised semantic segmentation. Our method differs from MDC in the following aspects: (i) it performs early feature-level fusion on features from different atrous convolutions before performing classification. In contrast, MDC uses late fusion to combine the outputs of classifiers trained on different features. As each individual classifier operates on the discriminative regions of feature maps at a given scale, MDC’s late fusion may not capture correlation information in the multi-scale feature space. In comparison, early fusion yields true and more informative multi-scale features because it integrates the features from the start. (ii) To aggregate multi-scale features, we propose an attention mechanism, which allows the network to learn the relative importance of different features, thereby enabling the adaptive selection of useful features and the suppression of redundant features.
Our contributions are summarized as follows:
•We propose an atrous convolutional feature network that includes two important modules to generate object attention maps, i.e., an atrous convolution cascade module and an atrous convolution pyramid module. We improve object attention maps by enhancing the context representation of image classification networks. This is a new perspective to the weakly supervised semantic segmentation.•For the aggregation of multi-scale context information, we propose an attentive fusion strategy to adaptively learn the relative importance of features at different scales and compare the performance with five other aggregation methods. Experimental results demonstrate the effectiveness of our method in generating object attention maps.•Our method achieves state-of-the-art performance for the task of weakly supervised semantic segmentation on both the PASCAL VOC 2012 and MS COCO datasets.
The rest of this paper is organized as follows: We review related work in Section 2, and describe our proposed approach in Section 3. Section 4 provides a description of experiments and discusses the reported performance evaluation and our ablation study results. Finally, we conclude this paper in Section 5.
