Citizen science, also known as “public participation in scientific research” (Hand, 2010), can be described as research conducted, in whole or in part, by amateur or nonprofessional participants often through crowdsourcing techniques. Extant citizen science projects require the participant to either act as a sensor and collect data, typically ‘in the wild’ with an array of mobile technologies, or analyse previously collected data through internet-based Virtual Citizen Science (VCS) platforms (Reed et al., 2012). Launched in 2009, the Zooniverse (www.zooniverse.org) is home to some of the internet's most popular VCS projects, which contribute to a wide range of research, with volunteers asked to, for example, classify different types of galaxies from photographs taken by telescopes (www.galaxyzoo.org), transcribe historical ships logs and weather readings (www.oldweather.org), or mark craters found on images of planetary surfaces (www.moonzoo.org).
As a relatively new form of activity, online citizen science research has tended to be driven by concerns around the core science rather than being considered as something that can be designed to suit its user population (with some exceptions, e.g., Prestopnik and Crowston, 2012). This is perhaps ironic given the importance of the ‘citizen’ to the endeavour, especially as the effectiveness of a citizen science venture is related to its ability to attract and retain engaged users, both to analyse the large amount of data required, and to ensure the quality of the data collected (Prather et al., 2013). Current VCS platforms tend to require the user to carry out tasks in a very repetitious manner, the design of which are arguably driven more by the ‘science case’ (analogous to a ‘business case’ in industry) rather than any consideration of the experience of the citizen scientist (Cox et al., 2015). In the study reported here we make a first step in considering how VCS platforms can be designed to better meet the needs of the citizen scientists by exploring whether the influence of manipulating task flow predicted with similar systems would affect the rate and number of features indicated, as well as user ratings on difficulty and usability issues. We also investigate how these factors affect the (volunteered) data's volume and accuracy by comparing it with expert judgements.
Some studies have considered motivation amongst citizen science volunteers (Reed et al., 2013, Eveleigh et al., 2014), but not considered the form of work activity itself in any depth. This may be considered remiss since forty years of research have identified a relationship between motivation, satisfaction and work design (Hackman and Oldham, 1975, Oldham and Hackman, 2010) and in recent times has been directly applied to online crowdwork (Kittur et al., 2013). Factors such as task variety, complexity and autonomy were identified as important influences on motivation and productivity, all of which can be influenced by VCS design.
We begin with a review of relevant literature on the interplay between motivation, performance and task design in the areas of Citizen Science, work design and HCI. We then introduce Planet Four: Craters – a Zooniverse citizen science project that consists of three separate interfaces that vary in task workflow design (TWD) for the marking of craters on the surface of Mars, and present a laboratory study that directly compares participants’ performance and experience across the three interfaces. Finally the impact of TWD on these results, and the implications for VCS platforms and other online mechanisms, are discussed.
