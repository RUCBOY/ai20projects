As one of the most fundamental and important topics in computer vision, stereo vision attracts many researchers and has been widely researched since there was the publicly available performance testing such as the Middlebury [1] and KITTI stereo benchmark [2], which allow researchers to compare their algorithms against all the state-of-the-art algorithms.
Different from the feature matching [3], [4], which matches sparse feature points in two images, the stereo matching can densely match the pixels. Scharstein and Szeliski [1] summarized four steps for a typical stereo algorithm, i.e., matching cost computation, cost aggregation, optimization, and disparity refinement, respectively. Local stereo methods focused on the first two steps [5], [6] often fail in challenging scenarios of weakly textured, saturated or reflective regions. Many global methods based on the research on steps (3) and (4) are thus researched and perform well on Middlebury benchmark, such as graph cuts [7], [8], [9] and belief propagation (BP) [10], [11], [12], [13], [14]. The stereo is achieved in these global algorithms, essentially, by solving a Markov Random Field (MRF) model, including the assumptions of photo-consistency and smoothness. However, stereo for the complex outdoor scenarios is still a challenging issue [15].
Recently, Zbontar and LeCun [16] trained a convolutional neural network (CNN) to compute the matching cost. Their method outperformed on KITTI benchmark but need about 67 s for calculating a single image pair, where the majority of time during prediction is spent in the forward pass of the CNN with a Nvidia GeForce GTX Titan GPU.
Our method is based on learning a Euclidean embedding using a convolutional neural network. Different with [16] where the matching cost is computed by five full connected layers, the network in this work is trained such that the squared L2 distances in the embedding space directly correspond to matching cost between a pair of patches. Thus our method takes only about 5 s for predicting a single image pair, where the computing of convolutional neural networks only needs 1.2 s with a Nvidia GeForce GTX 880 GPU or 2 s with a Intel i7 CPU.
1.1. Related workMany learning-based stereo algorithms [17], [18], [19], [20], [21], [22], [23] have been proposed since the introduction of large stereo datasets [21], [24].A class of training methods aim to compute or refine matching cost besies Zbontar and LeCun’s convolutional neural network [16]. For example, Kong and Tao [17], [18] initialized the matching cost with sum of squared distances, and using a trained model to predict the probability that whether the initial disparity is correct. The initial matching cost was refined based on the predicted probabilities. Some other works [22], [23] focused on estimating the confidence of the computed matching cost.Similarly to our training strategy, Schroff et al. [25] trained a deep convolutional neural network called FaceNet for face recognition and clustering. They used triplet loss to separate the positive pair from the negative by a distance margin. Similar work proposed by Liu et al. [26] used similar framework for face recognition and achieved the state-of-the-art accuracy of Labeled Faces in the Wild (LFW)1 database. The main difference is that FaceNet only trained one network, whereas this work trained two networks.Except the matching cost, smoothness constraint is an important factor as well, where encouraging self-similar pixels to be assigned to the same label is an effective strategy. A typical way of taking advantage of self-similarity is performing a color segmentation on the image and regarding the pixels within each segment as self-similar pixels. Some segmentation-based algorithms use a hard constraint that the pixels within a single segment must be assigned to the same plane [10], [27], [28], and some others use a soft constraint that the neighboring two pixels sharing the same segment are only encouraged to lie on the same plane [29], [30]. The scale of segmentation is important for these algorithms. A large scale of segmentation over-constrains the small objects and a small scale of segmentation is hard to constrain the large objects. Bleyer et al. [31] use a soft segmentation term to encourage the stereo result to consistent with a precomputed segmentation, but optimizing these higher-order cliques is difficult and time consuming.
1.2. ContributionsA typical stereo matching algorithm involves the matching cost and smoothness constraint. Both aspects are studied in this paper, which are in correspondence to two main contributions of this work that are summarized as follows, respectively.Firstly, we proposed a method of learning a Euclidean embedding using a convolutional neural network. Different with [16] where the matching cost is computed by five full connected layers, the network in this work is trained such that the squared L2 distances in the embedding space directly correspond to matching cost between a pair of patches. Thus our method takes less than 1 s for computing the matching cost for a single image pair.Secondly, as mentioned before, the positive effect of image segmentation has been shown in many stereo matching algorithms. However in the methods of hard constraint, an appropriate scale of segmentation is hard to find for different scenarios. In the methods of soft constraint, optimizing the higher-order cliques is difficult and time consuming. To make a tradeoff, we adopt an adaptive smoothness penalty for each pair of neighboring pixels that depends on multi-scale segmentations. If two neighboring pixels are always in same segment with different segmentation scales, they are naturally expected to be assigned as a same label.
