A brain–computer interface (BCI) allows a person to communicate with or control a computer or other devices without using peripheral nerves and muscles. In the paper, we focus on P300 speller BCI, which is based on oddball paradigm. A rare target stimulus, in particular, a target character flashing, is presented to the user among usual non-target stimuli. The P300 wave can be elicited around 300 ms after the target stimulus appears [1]. It is a positive displacement of electroencephalography (EEG) amplitude. Its most famous application in BCI is P300 speller.
Unlike image or text classification task, EEG signals show strong non-stationary characteristic. A pre-trained classifier often has limited generalization performance when applied to a different subject or session. Hence, a long calibration time is needed for each use, which makes it quite inconvenient for BCI users. There are diverse researchs to solve the problem, e.g., transfer learning finds a way to transfer useful information to target domain. Another way is semi-supervised learning that could make use of unlabeled data to improve the classifier. For example, Li et al. proposed a self-training support vector machine (SVM) algorithm for P300 speller in [2], which used small labeled training data and large unlabeled data to train a strong classifier with spelling accuracy above 95%. Other semi-supervised method for BCI could also be used, e.g., EM with native Bayes classifier, co-training, etc. But these methods assume that all the unlabeled data are available, which makes the algorithms only suitable for offline analysis. Gu et al. proposed a sequential update self-training least squares SVM (SUST-LSSVM) method for online semi-supervised learning in [3]. However, the computational complexity still increases non-linearly with respect to the number of data.
The extreme learning machine (ELM) algorithm is known for its extremely fast learning speed. Hence, we consider the self-training regularized weighted online sequential version of ELM in this paper to reduce the computational complexity. The original ELM algorithm has been proposed by Huang et al. [4], which was designed to train a single-hidden layer feedforward neural network (SLFN). Unlike the gradient-based algorithm, the input weights and bias of the network are fixed and randomly chosen from a continuous distribution. Then, least squares (LS) method is applied to obtain the output weights. This makes ELM extremely fast in learning speed, and the empirical studies show that ELM also has a similar generalization performance compared with the classical SVM and LS-SVM [5]. The regularized ELM has been proposed to avoid over-fitting problem in [6], [7] . In [8], Liang et al. presented an online sequential ELM (OS-ELM). Weighting strategy was also discussed in [9], [10]. Actually, the work of Zong in [9] contained regularization procedure, which is referred to as regularized weighted ELM (RW-ELM) in this paper. The weighted OS-ELM (WOS-ELM) has been proposed in [11].
In this paper, a self-training regularized WOS-ELM (ST-RWOS-ELM) is employed to train a robust P300 classifier online with low computational complexity. Specifically, after a short supervised training, the speller system is switched into an input mode where unlabeled data are collected. At the same time, an SLFN is updated using the ST-RWOS-ELM algorithm with these unlabeled data after every character input. In our experiment, the spelling accuracy could be quickly improved to around 90% after a few character input. In Section 2, we will give a brief description of the RWOS-ELM algorithm. In Section 3, we introduce the ST-RWOS-ELM and the details of how we apply it in our P300 speller. The experimental results are shown in Section 4. Besides, we make a comprehensive comparison between ST-RWOS-ELM and SUST-LSSVM. Section 5 gives a conclusion of this paper.
