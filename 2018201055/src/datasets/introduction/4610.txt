Node centrality [8], [22], [24] captures the structural importance of a node in a network (or says a graph), and is a fundamental concept in analyzing complex networks, e.g., rank Web pages in the WWW [39], measure the influence of a user in an online social network (OSN) [10], analyze the load of a server in a communication network [47], and so forth. Many centrality measures such as degree [2], closeness [14], [37], [45], betweenness [43], [50], and their variants [42], [47], have been proposed to measure the importance of an individual node in a network. While these measures apply usefully in finding the K most important (top-K) nodes in a network, they are actually not suitable for finding a set of nodes of size K such that these K nodes form the most important node group in the network.
Indeed, such a problem widely exists in scores of applications. For example, in an OSN, product retailers may want to locate K people to promote their products so as to maximize the number of potentially influenced customers [25]; however, due to the overlap of people’s friend circles, simply returning the top-K most influential people (say, measured by degree) in the network is unlikely to be optimal. As another example, considering the fast development of electric vehicles in recent years, it urges urban planers to build charging stations at the proper places of a city to facilitate drivers to find a nearest one [31]. If we consider this problem as finding K nodes in the urban traffic network to minimize the average distance that other nodes can reach one of these K nodes, then choosing the top-K most central nodes (say, measured by closeness) in the network is obviously not satisfactory. Many data summarization tasks in machine learning (e.g., extractive summarization of documents, image collections, videos etc. [48]) can also be abstracted as identifying the most important node group from some properly defined network (e.g., a network formed by words and their correlations [1], [3]).
These examples motivate researchers to develop new metrics that are suitable for measuring the importance of a group of nodes rather than an individual node in a network. To this end, Everett and Borgatti, in their seminal work [18], extended the idea of individual node centrality to group centrality. They conceptually illustrated the definitions of group degree, group closeness and so on (which we will elaborate in Section 2) for two graphs containing 14 and 20 nodes respectively. Despite its conceptual novelty and potential usefulness in addressing practical problems, group centrality lacks efficient calculation methods that can scale to large graphs which contain billions of edges such as Twitter and Facebook. Such large graphs call for efficient group centrality calculation methods and tools.
Designing scalable algorithms for handling large real-world graphs is challenging. As graphs grow larger in scale and more complex in their structure, they easily outgrow the memory capacity of a single computer and complicate the system design of distributed computing [33]. In some situations, graph compression techniques can be applied to fit a graph into a computer’s main memory. For example, Boldi and Vigna [7] reported that Web graphs can be compressed by using about 3 bits per edge; however, social networks are far less compressible than Web graphs [12]. Furthermore, decompression may lead to overhead in CPU time and harm computational efficiency. On the other hand, distributed graph computing systems such as Spark and GraphLab [32] can handle billion-scale graphs; however, the cost of having and maintaining a large cluster is prohibitory for most users. In addition, it is also a difficult problem to break a large graph into smaller balanced subgraphs in order to process them across cluster nodes and minimize the communication between cluster nodes [30]. In recent years, disk-based graph computation has undergone an explosive development, such as GraphChi [28], X-Stream [44], TurboGraph [23] and VENUS [11]. Such systems use clever graph storage methods and do not need to load a complete graph into main memory, and can process large graphs on just a single PC with computational efficiency as competitive as distributed computing systems. However, these existing systems do not yet support calculating group centrality.
Present workIn this work, we study how to efficiently calculate group centrality (more specifically, the H-group closeness centrality) over large disk-resident graphs on a single PC equipped with moderate sized main memory, e.g., 4 ∼ 8GB. Our main result is a disk-based group centrality computation system/tool that can handle billion-scale graphs within hours of time. For example, finding a node group to (approximately) maximize the group closeness centrality on a Twitter graph of 1.4 billion edges, our method needs less than 2 h, while a baseline method that relies on sequentially scanning the data on the disk multiply times needs several days.We first introduce the group degree/closeness centrality defined by Everett and Borgatti in Section 2, and then show that they can be extended to a H-group closeness centrality, which captures the features of both group degree and group closeness. We thus focus on calculating the H-group closeness in a large disk-resident graph. Here, the calculation of H-group closeness includes solving two basic problems: (1) given a node group, how to efficiently compute its H-group closeness value, i.e., the computation problem; and (2) how to find a node group containing at most K nodes to maximize the H-group closeness, i.e., the maximization problem. These two problems arise from two typical real-world applications. For example, given two user groups of an OSN, which one is more influential and hence more suitable for advertising targeting? Given two charging station placement plans, which one should the urban planer choose? These questions can be answered by solving the computation problem. The solution to the maximization problem can be applied to find optimal solutions, e.g., find a user group with the maximum influence for advertising targeting; find out the best charging station placement strategy for a city. Throughout the paper, the proposition of H-group closeness centrality and the solutions to these two problems constitute our main contributions.
ChallengesFor large graphs that cannot entirely fit in a computer’s main memory, both the computation problem and the maximization problem become challenging.
•In the computation problem, we need to solve the single-source shortest path (SSSP) problem for a group of nodes, which is known to be a high computational complexity task in a large graph (either use a breadth-first-search (BFS) method or Dijkstra’s algorithm).•In the maximization problem, we need to solve the all-pairwise shortest path (APSP) problem, which has even larger time complexity than solving the SSSP problem. Even worse, the maximization problem is NP-hard. Although a greedy algorithm can find an approximate solution, it generates too many random accesses to disk when handling disk-resident graphs which results in poor computational efficiency.
SolutionsWe develop efficient and novel algorithms to solve these problems in this work.
•We propose an efficient method based on a probabilistic counting method to estimate H-group closeness centrality with high accuracy, rather than exhaustively computing it in an exact fashion. The proposed method has time complexity O(m) using O(nlog log n) extra storage, where n and m are the number of nodes and number of edges respectively.•We design a novel I/O-efficient greedy algorithm, that exploits properties of the H-group closeness centrality to handle a disk-resident graph efficiently, and improves the computational efficiency a lot. For example, our method is about 300 times faster than a baseline method on the Twitter graph with 1.4 billion edges.We validate the efficiency of our techniques by conducting extensive experiments in both synthetic and real-world graphs, which cover a large variety of networks, including citation networks, collaboration networks, social networks, etc. The empirical results demonstrate that our solution performs well, both in computational accuracy and efficiency.The remainder of the paper will proceed as follows. In Section 2, we give the definitions of group degree, group closeness, and H-group closeness. In Section 3, we formulate the group closeness computation/maximization problem. We then elaborate the methodologies for solving these two problems in Section 4, and validate our proposed methods in Section 5. We also show some interesting observations in Section 6. Finally, Section 7 summarizes some related work, and Section 8 concludes. Proofs of main results can be found in Appendix.
