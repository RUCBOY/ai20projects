The massive amounts of data that the world produces every day pose new challenges to modern societies in terms of how to leverage their inherent value. Social networks, instant messaging, video, smart devices and scientific missions are just mere examples of the vast number of sources generating data every second. As the world becomes more and more digitalized, new needs arise for organizing, archiving, sharing, analysing,visualizing and protecting the ever-increasing data sets, so that we can truly develop into a data-driven economy that reduces inefficiencies and increases sustainability, creating new business opportunities on the way (The European Commission, 2014).
Traditional approaches for harnessing data are not suitable any more as they lack the means for scaling to the larger volumes today available in a timely and cost efficient manner. This has somehow changed with the advent of Internet companies like Google or Facebook, which have devised new ways of tackling these issues. However, the variety and complexity of the value chains in the private sector as well as the increasing demands and constraints in which the public one operates, needs an ongoing research that can yield newer strategies for dealing with data, facilitate the integration of providers and consumers of information, and guarantee a smooth and prompt transition when adopting these cutting-edge technological advances.
Scientific data output is no exception to this data deluge and is currently increasing at 30% every year (Pryor, 2012). Some studies (Vines et al., 2014) conclude that the usage of existing scientific data sets decline 17% per year, with 80% of them being simply unavailable after 20 years. Given that a lot of research endeavours are nowadays publicly funded, more and more pressure is being allocated to them in order to get an optimum return on investment, not only from the specific project outcome perspective, but also from the potential synergies produced when leveraging the results of other existing undertakings (and those to come). This is particularly the case in astronomy and astrophysics, where the exponential growth of data collected by both ground and space based instruments has fostered the appearance of new disciplines, i.e. astroinformatics (Accomazzi et al., 2013) and astrostatistics (Sarro et al., 2014), as a way of seamlessly integrating the different skillsets found in disciplines like astronomy/astrophysics, computer science and statistics, to be able to efficiently combine the ever-increasing sources of data in the field. The successful integration of skills and data sets will be the key to increase research output both in volume and in quality.
The European Space Agency’s Gaia mission (Gaia Collaboration et al., 2016b) is a good example of a scientific venture whose results will likely be the astronomical data resource for decades thereafter, representing a tremendous discovery potential. It will create the largest and most precise three dimensional chart of our galaxy (the Milky Way), by surveying more than one billion stars. Furthermore, it will also provide unprecedented position, parallax and proper motion measurements as per the mission science performance constraints (de Bruijne, 2012). The resulting catalogue, along with the raw data collected by the mission’s instruments, will be made available to the scientific community, and will be analysed in many different ways for many different purposes. In this scenario, the identification and availability of the proper analytical tools and frameworks will become crucial to ease the process taken by scientists when building models or conducting any other research.
To this extent, the Gaia Science Enabling Applications work package in general, and the Data Mining sub-work package in particular, have been established. They will aim at a wide variety of requirements and functionality that will be provided to the community (see Section 3). These include:



•
The provision of enriched and high level data access tools.
•
Capabilities for cross-matching the final catalogue with other scientific surveys.
•
A framework to perform complex queries (normally with some analytical workload).
•
A set of models that can be utilized in a straightforward and seamless way, allowing composability for more complex use cases and configurability for different approaches.
•
The means for managing the variety of workloads coming through (and their security constraints).
•
Visualization of complex relations and high dimensional data sets.
