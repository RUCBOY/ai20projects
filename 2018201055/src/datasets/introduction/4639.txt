A standard formulation of supervised learning starts with a parametrized class of mappings, a training set of desired input–output pairs, and a loss function measuring deviation of actual output from desired output. The goal of learning is to minimize the average loss over the training set. A popular minimization method is stochastic gradient descent. For each randomly chosen input, the parameters of the mapping are updated in minus the direction of the gradient of the loss with respect to the parameters. Here we are concerned with a class of mappings known as convolutional networks (ConvNets).
Significant effort has been put into parallelizing ConvNet learning on GPUs, as in the popular software packages Caffe  [14], Torch  [6] and Theano  [1]. ConvNet learning has also been distributed over multiple machines  [7]. However, there has been relatively little work on parallelizing ConvNet learning for single shared memory CPU machines.
Here we introduce a software package called ZNN, which implements a novel parallel algorithm for ConvNet learning on multi-core and many-core CPU machines. ZNN implements 3D ConvNets, with 2D as a special case. ZNN can employ either direct or FFT convolution, and chooses between the two methods by autotuning each layer of the network. FFT convolution was previously applied to 2D ConvNets running on GPUs  [22], [31], and is even more advantageous for 3D ConvNets on CPUs.
As far as we know, ZNN is the first publicly available software that supports efficient training of sliding window max-pooling ConvNets, which have been studied by  [21], [8], [26].
There is related work on using Xeon Phi™ for supervised deep learning  [32], [19] and unsupervised deep learning  [15]. A comparison of multi-core and GPU parallelization has been studied by  [11]. In recent effort, Intel has been working on improving the speed of 2D direct convolutions through its MKL libraries  [12]. FFT implementations can speedup serial 3D FFTs by nearly 5×   [33] for inference-only networks. However, this optimization is not suitable for training.
What all these methods have in common is that they utilize the fork-join model provided by OpenMP to run multiple instances of the standard backpropagation algorithm in parallel, or to parallelize certain parts of the algorithm. This model resembles the SIMD parallelism of the GPU in the sense that each thread executes the same code on different data; however the execution is not necessarily done at the same time.
In contrast, ZNN introduces a novel task parallelization model optimized specifically for ConvNets. This model can achieve higher utilization of the available CPUs, and has lower memory overhead; thus it can train much larger models efficiently.
