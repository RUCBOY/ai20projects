Multi-label learning, handling instance associated with multiple labels, has attracted lots of attention due to its widespread applicability in diverse fields such as image annotations [1], music classification [2], multi-topic text categorization [3], etc. During the past decades, a large number of methods have been proposed and achieved good performance for multi-label learning. According to [4], these methods can be roughly divided into two categories: algorithm adaptation and problem transformation. Algorithm adaption methods attempt to adapt popular learning techniques to handle multi-label learning problems directly. Some notable examples include ML-kNN [5], ML-DT [6] and Rank-SVM [7]. While problem transformation methods tackle the problem by transforming it to other well-established learning scenarios. Binary Relevance [1], Classifier Chains [8], Calibrated Label Ranking [9] and Random k-labelsets [10] fall into this category.
The aforementioned methods generally assume that the labels of training instances are complete. Unfortunately, in real-world applications, some labels tend to miss from the training set, consequently forming a kind of weakly supervised learning problem [11]. Label missing can generally be due to that human labelers may sometimes ignore labels they do not know or of little interest, or following the guide by some algorithms to reduce labeling costs [12], [13]. Therefore, these methods will fail in this situation.
To solve this problem, a simple solution is to discard all samples with missing labels, though at the expense of potentially losing a significant amount of label information. Another is to recover the missing labels by exploiting label structure, and of which the low-rank of label matrix is the most commonly used label structure due to theoretical support, namely, for a low-rank matrix M∈Rn×m of rank r, it can be perfectly recovered from O(r(n+m)ln2(n+m)) observed entries when the observed entries are uniformly sampled from the M. Besides, when additional label structures are incorporated into learning of low-rank models, they can achieve better or even the state-of-the-art classification performance. For example, [14] incorporates structured semantic correlations into low-rank model learning and gets improved performance.
In multi-label learning, asymmetric co-occurrence relation is one of the most useful label structures. To be more specific, if a sample is labelled λ1, then it must be labelled λ2, but the opposite is not necessarily true. If all the samples labelled λ1 are used to form a new sample submatrix, then the corresponding label submatrix has smaller rank than the original label matrix. An example of this phenomenon is shown in Fig. 1. Based on this, we call asymmetric co-occurrence label structure as local low-rank label structure, while the low-rank property of whole label matrix as global low-rank label structure.Download : Download high-res image (98KB)Download : Download full-size imageFig. 1. An example of local low-rank label structure. The label matrix in the figure is taken from a real multi-label dataset “Corel16k”. As we can see, if a sample is labelled as “snake”, then this sample must be labelled as “reptile”, but the opposite is not true. As a result, although the rank of the original label matrix (inside the red box) is 4, the rank of the label submatrix labelled as “snake” (inside the blue box) is 3. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
Another is label discriminant information. Since label matrix has missing entries, it is quite difficult to obtain the true rank of the label matrix in advance. If the rank is set too small, it will inevitably lose useful label discriminant information, hence affecting unfavorably the classification performance. An example of this phenomenon is shown in Fig. 2. Therefore, only if the rank of the label matrix is large enough, can the recovered label matrix provide more discriminant information.Download : Download high-res image (71KB)Download : Download full-size imageFig. 2. An example of discriminant label information with missing labels. If we set the rank of the label matrix to 3 or 4, then the missing value of the label matrix will be correspondingly set to 1 or −1. Obviously, the label matrix with larger rank provides more discriminant information.
Nevertheless, to the best of our knowledge, current methods have not considered incorporating local low-rank label structure and label discriminant information into a single low-rank model to improve performance. Therefore, how to embed both label structures into the low-rank model elegantly and effectively is still very challenging at present. To achieve this, we develop a Discriminative Multi-Label Learning (DM2L) model for multi-label learning with missing labels. Our model is not only simple and effective, but also capable of jointly capturing global low-rank label structure, local low-rank label structure and label discrimination information in the original label space, which is supported by theoretical analysis. Besides, we provide a nonlinear extension of DM2L by kernel trick to enhance its ability and develop a concave-convex programming to solve these optimization objectives. Compared to the other low-rank based methods, our method involves the fewest assumptions and only one hyper-parameter. Even so, our method still outperforms the state-of-the-art methods as shown in the experiments.
The rest of the paper is organized as follows. In Section 2, we review some related works. In Section 3, the proposed framework is presented. Experimental results are presented in Section 4. Finally, Section 5 gives some concluding remarks.
