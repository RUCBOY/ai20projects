In the recent past, teaching and learning of parallel programming has become increasingly important due to the ubiquity of parallel processors in portable devices, workstations, and compute clusters. Furthermore, stagnating single core performance of modern CPUs requires future computer scientists and engineers to write highly parallelized code in order to fully utilize the compute capabilities of current hardware architectures. The design of parallel algorithms can be a stressful challenge especially for inexperienced students due to common pitfalls such as race conditions when concurrently accessing shared resources, defective communication patterns causing deadlocks, or the non-trivial task of efficiently scaling an application over the whole number of available compute units. Hence, acquiring parallel programming skills is nowadays an important part of many undergraduate and graduate curricula. More importantly, education of concurrent concepts is not limited to the field of High Performance Computing (HPC). The emergence of deep learning and big data lectures requires teachers and students to adopt HPC as an integral part of their corresponding knowledge domain.
Although theoretical education is indispensable for acquiring a deep understanding of fundamental parallelization techniques, a major objective of HPC lectures is the writing of correct yet efficient source code. There can be many reasons for defective code during the practical implementation of parallel algorithms including incorrect indexing of multi-dimensional arrays during transformation of associated index domains, implicit serialization caused by erroneous synchronization calls, or inefficient memory access patterns resulting in excessive cache invalidation. Thus, short feedback loops between students and instructors are advisable in order to point out common mistakes as soon as possible to avoid unnecessary frustration. Traditional workflows often lack this interactive feedback since the time until evaluation is determined by technical constraints.
A common evaluation mechanism usually looks as follows: 
1.Distribute login credentials for a cluster or workstation (e.g., via mail).2.Resubmit the student’s program after submission deadline to a queuing system and wait for execution. This step can include unzipping mail attachments, copying files and compilation on(to) the target platform.3.Manually evaluate the program’s functionality by verifying its output and afterwards keep track of grades in a dedicated spreadsheet.
