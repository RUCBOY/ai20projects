The development of computing hardware was one of the key factors driving the spread of the Bayesian statistics [3]. The advent of Monte Carlo methods and powerful computing units made the application of Bayesian inference feasible in a wide range of fields [16], [13]. Recently, the emergence of large datasets raised a new challenge for Bayesian statistics because the previously proposed (approximate) methods were hardly able to deal with datasets involving hundred of thousands or millions of data samples [20]. One of the main lines of research for scaling up Bayesian inference has been based on the combination of variational approaches and stochastic approximation theory [33], [20], [18], [21], while other approaches have also proposed the use of parallel computing architectures as a way to scale up Bayesian inference [8].
Big data processing technologies have quickly evolved over the last years [19]. Most of these technologies provide a simple API which allows, using few lines of code, to easily manage, process, and query large datasets by seamlessly controlling a large number of distributed computing units based on commodity hardware [11], [31]. They are built using sophisticated memory management schemes to cache the data in the main memory of the computing units, which greatly speeds up the iterative processing of the data. Apache Spark2 and Apache Flink3 are probably the most well-known of these big data processing tools.
In this paper we exploit these recent advances in big data processing on distributed computing clusters to define a novel distributed and scalable variational inference scheme. More precisely, our approach is built on a novel interpretation of an existing variational inference approach for conjugate exponential models, the so-called variational message passing (VMP) algorithm [35], which is shown to behave as a projected natural gradient ascent algorithm [20], [24]. Using this interpretation of VMP, we propose two alternative distributed schemes with different convergence properties. We empirically evaluate our approach on different latent variable models (LDA, Factor Analysis, Mixture of Gaussians and Linear Regression) over different real-world datasets (Pubmed abstracts, GPS trajectory, and real-world financial dataset). We compare our results with stochastic variational inference (SVI) [20], and streaming variational Bayes (SVB) [8] and we show that our methods converge quicker and to better solutions than these alternatives. We analyze the scalability of our approach using a model with more than one billion nodes (and approximately 75% latent variables) running on a computer cluster with 128 processing units.
Contributions: We present a novel approach for scaling up variational methods, enabling the methods to exploit modern distributed computing technologies. For this purpose, we cast VMP as a projected natural gradient ascent algorithm, which gives a theoretical and practical foundation for the parallelization of variational methods over general conjugate exponential family models. When compared to SVI and SVB, we find that our approach scales better and provides superior solutions, is defined for a broader class of models, and has the ability to produce important quantities like the posterior over all latent variables and the full evidence lower bound. The proposed methods are released as part of an open-source toolbox for scalable probabilistic machine learning (http://www.amidsttoolbox.com) [29], [27], [9].
