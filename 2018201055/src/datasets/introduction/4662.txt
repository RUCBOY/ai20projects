Visual question answering is a task that was proposed to connect computer vision and natural language processing (NLP), to stimulate research, and push the boundaries of both fields. On the one hand, computer vision studies methods for acquiring, processing, and understanding images. In short, its aim is to teach machines how to see. On the other hand, NLP is the field concerned with enabling interactions between computers and humans in natural language, i.e teaching machines how to read, among other tasks. Both computer vision and NLP belong to the domain of artificial intelligence and they share similar methods rooted in machine learning. However, they have historically developed separately. Both fields have seen significant advances towards their respective goals in the past few decades, and the combined explosive growth of visual and textual data is pushing towards a marriage of efforts from both fields. For example, research in image captioning, i.e automatic image description (Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2015, Karpathy, Joulin, Li, 2014, Mao, Xu, Yang, Wang, Yuille, 2015, Vinyals, Toshev, Bengio, Erhan, 2014, Wu, Shen, Hengel, Liu, Dick, 2016a, Yao, Torabi, Cho, Ballas, Pal, Larochelle, Courville, 2015) has produced powerful methods for jointly learning from image and text inputs to form higher-level representations. A successful approach is to combine convolutional neural networks (CNNs), trained on object recognition, with word embeddings, trained on large text corpora.
In the most common form of Visual Question Answering (VQA), the computer is presented with an image and a textual question about this image (see examples in Fig. 3, Fig. 4, Fig. 5). It must then determine the correct answer, typically a few words or a short phrase. Variants include binary (yes/no) (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, Parikh, 2015, Zhang, Goyal, Summers-Stay, Batra, Parikh, 2016) and multiple-choice settings (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, Parikh, 2015, Zhu, Groth, Bernstein, Fei-Fei, 2016), in which candidate answers are proposed. A closely related task is to “fill in the blank” (Yu et al., 2015), where an affirmation describing the image must be completed with one or several missing words. These affirmations essentially amount to questions phrased in declarative form. A major distinction between VQA and other tasks in computer vision is that the question to be answered is not determined until run time. In traditional problems such as segmentation or object detection, the single question to be answered by an algorithm is predetermined and only the input image changes. In VQA, in contrast, the form that the question will take is unknown, as is the set of operations required to answer it. In this sense, it more closely reflects the challenge of general image understanding. VQA is related to the task of textual question answering, in which the answer is to be found in a specific textual narrative (i.e reading comprehension) or in large knowledge bases (i.e information retrieval). Textual QA has been studied for a long time in the NLP community, and VQA is its extension to additional visual supporting information. The added challenge is significant, as images are much higher dimensional, and typically more noisy than pure text. Moreover, images lack the structure and grammatical rules of language, and there is no direct equivalent to the NLP tools such as syntactic parsers and regular expression matching. Finally, images capture more of the richness of the real world, whereas natural language already represents a higher level of abstraction. For example, compare the phrase ‘a red hat’ with the multitude of its representations that one can picture, and in which many styles could not be described in a short sentence.
Visual question answering is a significantly more complex problem than image captioning, as it frequently requires information not present in the image. The type of this extra required information may range from common sense to encyclopedic knowledge about a specific element from the image. In this respect, VQA constitutes a truly AI-complete task (Antol et al., 2015), as it requires multimodal knowledge beyond a single sub-domain. This comforts the increased interest in VQA, as it provides a proxy to evaluate our progress towards AI systems capable of advanced reasoning combined with deep language and image understanding. Note that image understanding could in principle be evaluated equally well through image captioning. Practically however, VQA has the advantage of an easier evaluation metric. Answers typically contain only a few words. The long ground truth image captions are more difficult to compare with predicted ones. Although advanced evaluation metrics have been studied, this is still an open research problem (Hodosh, Young, Hockenmaier, 2013, Li, Kulkarni, Berg, Berg, Choi, 2011, Vedantam, Zitnick, Parikh, 2015b).
One of the first integrations of vision and language is the “SHRDLU” from system from 1972 (Winograd, 1972) which allowed users to use language to instruct a computer to move various objects around in a “blocks world”. More recent attempts at creating conversational robotic agents (Cantrell, Scheutz, Schermerhorn, Wu, 2010, Kollar, Krishnamurthy, Strimel, 2013, Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012, Roy, Hsiao, Mavridis, 2003) are also grounded in the visual world. However, these works were often limited to specific domains and/or on restricted language forms. In comparison, VQA specifically addresses free-form open-ended questions. The increasing interest in VQA is driven by the existence of mature techniques in both computer vision and NLP and the availability of relevant large-scale datasets. Therefore, a large body of literature on VQA has appeared over the last few years. The aim of this survey is to give a comprehensive overview of the field, covering models, datasets, and to suggest promising future directions. To the best of our knowledge, this article is the first survey in the field of VQA.
In the first part of this survey (Section 2), we present a comprehensive review of VQA methods through four categories based on the nature of their main contribution. Incremental contributions means that most methods belong to multiple of these categories (see Table 1). First, the joint embedding approaches (Section 2.1) are motivated by the advances of deep neural networks in both computer vision and NLP. They use convolutional and recurrent neural networks (CNNs and RNNs) to learn embeddings of images and sentences in a common feature space. This allows one to subsequently feed them together to a classifier that predicts an answer (Gao, Mao, Zhou, Huang, Wang, Xu, 2015, Ma, Lu, Li, 2016, Malinowski, Rohrbach, Fritz, 2015). Second, attention mechanisms (Section 2.2) improve on the above method by focusing on specific parts of the input (image and/or question). Attention in VQA (Andreas, Rohrbach, Darrell, Klein, 2016b, Chen, Wang, Chen, Gao, Xu, Nevatia, Jiang, Wang, Porikli, Li, Xu, Saenko, Yang, He, Gao, Deng, Smola, 2016, Zhu, Groth, Bernstein, Fei-Fei, 2016) was inspired by the success of similar techniques in the context of image captioning (Xu et al., 2015). The main idea is to replace holistic (image-wide) features with spatial feature maps, and to allow interactions between the question and specific regions of these maps. Third, compositional models (Section 2.3) allow to tailor the performed computations to each problem instance. For example, Andreas et al. (2016b) use a parser to decompose a given question, then build a neural network out of modules whose composition reflect the structure of the question. Fourth, knowledge base-enhanced approaches (Section 2.4) address the use of external data by querying structured knowledge bases. This allows retrieving information that is not present in the common visual datasets such as ImageNet (Deng et al., 2009) or COCO (Lin et al., 2014), which are only labeled with classes, bounding boxes, and/or captions. Information available from knowledge bases ranges from common sense to encyclopedic level, and can be accessed with no need for being available at training time (Wang, Wu, Shen, Hengel, Dick, Wu, Wang, Shen, Dick, Hengel, 2016c).Table 1. Overview of existing approaches to VQA, characterized by the use of a joint embedding of image and language features (Section 2.1), the use of an attention mechanism (Section 2.2), an explicitly compositional neural network architecture (Section 2.3), and the use of information from an external structured knowledge base (Section 2.4). We also note whether the output answer is obtained by classification over a predefined set of common words and short phrases, or generated, typically with a recurrent neural network. The last column indicates the type of convolutional network used to obtain image feature.MethodJointAttentionCompositionalKnowledgeAnswerImageembeddingmechanismmodelbaseclass. / gen.featuresNeural-Image-QA (Malinowski et al., 2015)✓GenerationGoogLeNet (Szegedy et al., 2015)VIS+LSTM (Ren et al., 2015)✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)Multimodal QA (Gao et al., 2015)✓GenerationGoogLeNet (Szegedy et al., 2015)DPPnet (Noh et al., 2016)✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)Multimodal-CNN (Ma et al., 2016)✓classificationVGG-Net (Simonyan and Zisserman, 2014)iBOWING (Zhou et al., 2015)✓ClassificationGoogLeNet (Szegedy et al., 2015)VQA team (Antol et al., 2015)✓classificationVGG-Net (Simonyan and Zisserman, 2014)Bayesian (Kafle and Kanan, 2016)✓ClassificationResNet (He et al., 2016)DualNet (Saito et al., 2016)✓ClassificationVGG-Net (Simonyan and Zisserman, 2014) & ResNet (He et al., 2016)MLP-AQI (Jabri et al., 2016)✓ClassificationResNet (He et al., 2016)MCB (Fukui et al., 2016)✓ClassificationResNet (He et al., 2016)MRN (Kim et al., 2016)✓✓ClassificationResNet (He et al., 2016)MCB-Att (Fukui et al., 2016)✓✓ClassificationResNet (He et al., 2016)LSTM-Att (Zhu et al., 2016)✓✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)Com-Mem (Jiang et al., 2015)✓✓GenerationVGG-Net (Simonyan and Zisserman, 2014)QAM (Chen et al., 2015a)✓✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)SAN (Yang et al., 2016)✓✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)SMem (Xu and Saenko, 2016)✓✓ClassificationGoogLeNet (Szegedy et al., 2015)Region-Sel (Shih et al., 2016)✓✓classificationVGG-Net (Simonyan and Zisserman, 2014)FDA (Ilievski et al., 2016)✓✓ClassificationResNet (He et al., 2016)HieCoAtt (Lu et al., 2016)✓✓ClassificationResNet (He et al., 2016)NMN (Andreas et al., 2016b)✓✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)DMN+ (Xiong et al., 2016)✓✓ClassificationVGG-Net (Simonyan and Zisserman, 2014)Joint-Loss (Noh and Han, 2016)✓✓ClassificationResNet (He et al., 2016)Attributes-LSTM (Wu et al., 2016a)✓✓GenerationVGG-Net (Simonyan and Zisserman, 2014)ACK (Wu et al., 2016c)✓✓GenerationVGG-Net (Simonyan and Zisserman, 2014)Ahab (Wang et al., 2015)✓GenerationVGG-Net (Simonyan and Zisserman, 2014)Facts-VQA (Wang et al., 2016)✓GenerationVGG-Net (Simonyan and Zisserman, 2014)Multimodal KB (Zhu et al., 2015)✓GenerationZeilerNet (Zeiler and Fergus, 2014)
In the second part of this survey (Section 3), we examine datasets available for training and evaluating VQA systems. These datasets vary widely along three dimensions: (i) their size, i.e the number of images, questions, and different concepts represented. (ii) the amount of required reasoning, e.g whether the detection of a single object is sufficient or whether inference is required over multiple facts or concepts, and (iii) how much information beyond that present in the actual images is necessary, be it common sense or subject-specific information. Our review points out that existing datasets lean towards visual-level questions, and require little external knowledge, with few exceptions (Wang, Wu, Shen, Hengel, Dick, Wang, Wu, Shen, Hengel, Dick). These characteristics reflect the struggle with simple visual questions still faced by the current state of the art, but these characteristics must not be forgotten when VQA is presented as an AI-complete evaluation proxy. We conclude that more varied and sophisticated datasets will eventually be required.
Another significant contribution of this survey is an in-depth analysis of the question/answer pairs provided in the Visual Genome dataset (Section 4). They constitute the largest VQA dataset available at the time of this writing, and, importantly, it includes rich structured images annotations in the form of scene graphs (Krishna et al., 2017). We evaluate the relevance of these annotations for VQA, by comparing the occurrence of concepts involved in the provided questions, answers, and image annotations. We find out that only about 40% of the answers directly match elements in the scene graphs. We further show that this matching rate can be significantly increased by relating scene graphs to external knowledge bases. We conclude this paper in Section 5 by discussing the potential of better connection to such knowledge bases, together with better use of existing work from the field of NLP.
