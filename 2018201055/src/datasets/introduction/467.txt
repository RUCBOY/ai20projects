There is a huge amount of data generated in diverse domains, such as social networks, sensor networks, biomolecular networks, citation and authorship networks, and e-commerce networks [13]. Many aspects of daily life are being recorded at all levels generating large-scale datasets such as phone trajectories, health data on monitoring devices, banking and financial records, shopping preferences, and so on. These data are irregular with complex structures [9]. Graphs offer the ability to characterize the complex interactions by data modeling. Thus researchers proposed various graphs to represent real-world data, such as scale-free graphs, ring graphs, nearest-neighbor graphs, and random geometric graphs [26]. Entities such as users on Facebook or sensors in a field are modeled as nodes and the connections among users or sensors as edges. For graphs that have a data matrix residing on each node, these data matrices form a tensor with graph structures called graph-tensor, where each frontal slice corresponds to a graph node and the connection between two frontal slices is an edge, as shown in Fig. 1.
Graph-tensor operations are widely used in various applications, such as graph neural networks (graph convolution, graph SVD, and graph QR), computer vision (graph convolution), image processing (graph filter), data completion (graph Fourier transform and inverse graph Fourier transform), and video compression (graph SVD). However, existing works are insufficient to support efficient large-scale graph-tensor computations. First, the running time of CPU-based graph-tensor operations increases rapidly with the number of nodes or the dimension of data on nodes; thus existing CPU-based graph-tensor tools cannot meet the real-time requirements of applications, such as localization and online recommendation. There is a growing impact of graph computing on high-performance GPUs [24]. Second, there are no high-performance graph-tensor computation libraries that support key graph-tensor operations as far as we know. Researchers have to implement and optimize their own graph-tensor operations in a case-by-case manner, which is inefficient and error-prone. For instance, GSPBOX [27] is a popular CPU-based graph processing toolbox. However, the graph operations in GSPBOX have long running time for big graphs, as revealed in our preliminary experiments (Fig. 18 shows g-FT, inverse g-FT, and g-filter on a graph vector of length 20,000 take 3022.36 s, 3006.91 s, and 20,443.11 s, respectively). Moreover, GSPBOX supports only scalar data on nodes and lacks several key graph operations such as graph convolution, graph shift, g-product, g-SVD, and g-QR, which have wide applications in network analysis and image processing. Although the cuTensor-tubal library [36], [37], [38] provides high performance tensor computations that are closely related to our work, it does not support the structural properties of graphs and is unsuitable for graph analyzing applications. Therefore, we are motivated to develop a library of high-performance graph-tensor operations to support diverse applications.
 In this paper, we implement high-performance graph-tensor operations for big data and IoT applications on GPU. We design, implement, and optimize a library called cuGraph-Tensor for fast and accurate graph-tensor operations. cuGraph-Tensor implements eight key graph-tensor operations and provides three data types on graph nodes: scalar, vector and matrix. Graphs with these data types can model many real-world applications such as sensor networks, social networks, IoT wireless camera networks, and so on. A graph data completion application is presented in Section 4.3 to demonstrate the usage of the proposed graph-tensor operations. We further propose optimization techniques to improve the computation and memory access efficiency and reduce CPU–GPU communications, which contribute significant performance improvement. We build the cuGraph-Tensor library on top of existing highly optimized NVIDIA CUDA [7] libraries, including cuBLAS, cuSolver, and existing libraries, including Magma [1], KBLAS [4] for efficient GPU computations, as shown in Fig. 2.Download : Download high-res image (123KB)Download : Download full-size imageFig. 1. A third-order graph-tensor is constructed by stacking the matrices on graph nodes in node order. Each data matrix becomes a frontal slice of the graph-tensor.Download : Download high-res image (230KB)Download : Download full-size imageFig. 2. Position of the cuGraph-Tensor library in the system.
Our contributions are summarized as follows.

•We develop a high-performance GPU library called cuGraph-Tensor of eight graph-tensor operations, including graph shift (g-shift), graph Fourier transform (g-FT), inverse graph Fourier transform (inverse g-FT), graph filter (g-filter), graph convolution (g-convolution), graph-tensor product(g-product), graph-tensor SVD (g-SVD) and graph-tensor QR (g-QR). We encapsulate these operations into an open-source library and provide BLAS-like interfaces for ease of use.•We propose optimization techniques on batched computing, computation reduction, memory accesses, and CPU–GPU communications to improve the performance. As a demonstration, we further develop a graph data completion application using the high-performance graph-tensor operations of the cuGraph-Tensor library.•We perform extensive experiments to evaluate the performance of the graph-tensor operations and the graph data completion application. The g-shift, g-FT, inverse g-FT, g-filter, g-convolution, g-product, g-SVD and g-QR operations achieve up to 133.98×, 96.09×, 90.14×, 12.64×, 130.61×, 141.71×, 51.18× and 142.12× speedups, respectively, comparing with CPU-based GSPBOX [27] and CPU MATLAB implementations running on two Xeon CPUs. These graph-tensor operations with the optimizations in Section 3.2 are on average 35.27×, 18.54×, 18.12×, 4.47×, 38.16×, 27.60×, 7.91×, and 23.83× faster over the GPU baseline implementation. The graph data completion application achieves up to 174.38× speedup over the CPU MATLAB implementation, and up to 3.82× speedup with higher accuracy over the GPU-based tensor completion in the cuTensor-tubal library [36], [37], [38].
The remainder of this paper is organized as follows. In Section 2, we describe the notations and eight graph-tensor operations. Section 3 shows the implementation and optimizations of the graph-tensor operations on GPU. In Section 4, we evaluate the performance of the graph-tensor operations and the graph data completion application. Section 5 discusses the related works. The conclusions are drawn in Section 6.
