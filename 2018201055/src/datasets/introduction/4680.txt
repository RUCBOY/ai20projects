Particle Simulation Methods (PSM) involving short-range interactions, such as Smoothed Particle Hydrodynamics (SPH), Moving Particle Semi-implicit (MPS) method, and Discrete Element method (DEM), have been widely used to solve continuum mechanics and granular dynamics [1], [2], [3]. These mesh-free methods are suitable for problems with complex geometry and boundaries. Their Lagrangian nature allows non-diffusive advection, which is useful for tracking history-dependent properties such as rheological, chemical, and thermal evolution. These potential advantages over mesh-based methods offer effective numerical approaches to geodynamical flow systems and engineering processes, for example, the interaction of tsunami run-up with structures and debris [4], [5], [6], [7], magma intrusion into rock fractures [8], [9], and the industrial processing of powders and grains [10], [11], [12], [13], [14], [15]. For such PSM, a number of particles are essential for improving the accuracy and extending the system size. For example, we are currently working on a DEM simulation using over 2 billion particles to run a realistic sandbox experiment [16]. The huge computational power needed to handle such particle systems is currently available only from large parallel supercomputer systems. Therefore, developing efficient parallelization of the PSM code is important.
Multiple level of parallelization must be considered in today’s high-performance computing (HPC) environment: shared memory, distributed memory, and typically SIMD or vector processor. In shared memory parallelization, different parallel algorithms have been studied for both multi-core (CPU) and many-core (GPU and MIC) processors, such as OpenMP and CUDA [17], [18], [19], [20], [21]. In our earlier study [22], by carefully controlling memory access, high computational performance was demonstrated for each processing unit. In distributed memory computing, a Message Passing Interface (MPI) is commonly used. To parallelize the PSM using MPI, a natural approach is to use the spatial domain decomposition of each logical process. Moving Lagrangian particle methods, however, inherently suffer from workload imbalance problems in the fixed sub-domains because particles move around and change the spatial distribution of the workload in the course of the simulation. Such load imbalances with a fixed domain may significantly degrade the parallel performance of the product run.
Dynamic load-balancing is a key technique for overcoming such imbalance problems. The basic idea is to control the domain partitioning to give each process an equal workload during the simulation run. Historically, dynamic load-balancing schemes for PSM were developed for applications such as cosmological N-body simulation [23], [24], [25], plasma particle-in-cell (PIC) simulation  [26], [27], [28], and molecular dynamics (MD) simulations [29], [30], [31]. In these earlier studies, several efficient geometric and graph algorithms for spatial partitioning have been proposed, such as Orthogonal Recursive Bisection (ORB) [23], space filling curves [32], Voronoi region [33], and K-way graph partitioning [34], [35], [36] incorporated with hierarchical tree structures [37]. In comparison, to our knowledge a relatively small number of reports have addressed the dynamic load-balancing of short-range interaction PSM such as SPH, MPS, or DEM [38], [39], [40], [41], [42].
From the viewpoint of parallel implementation, there are three main differences between our target PSM and the N-body, PIC, and MD simulations. First, the interactions in our PSM are limited to neighboring particles, given by the effective or physical particle radius. In contrast, the original equations for N-body, PIC, and MD simulation assume forces of infinitely long range. In general, to avoid prohibitively high calculation costs, the long-range terms are simplified by a cluster of particles defined on the mesh. This makes efficient computation of the multi-length-scale interactions crucial, whereas our PSM deal only with pairwise interactions. Second, our target PSM show a gradual contrast in particle density across space, as the models assume incompressibility. We can therefore expect that the sub-domains for equal workloads do not differ in size by orders of magnitude. In contrast, other PSM, especially for N-body and PIC simulations, generally show drastic changes in the particle density, potentially making the optimal size of sub-domains vary by several orders of magnitude. Earlier proposed schemes that utilize approaches such as hierarchical mesh and tree-based structures have clear advantages in handling multi-length scale interactions and sub-domain sizes naturally. Such advantages, however, may not apply to our target PSM. Finally, there are relatively large changes of workload for our target PSM during the product run. In particular, in MD, the change of particle density from the original value is typically small [43]. In such cases, efficient parallel performance can be attained with fixed domain decomposition adjusted to the initial workload balance [31], [34]. Computationally expensive but theoretically optimal domain decomposition techniques for minimizing the surface area, such as graph partitioning schemes, can be applied [44]. In our target PSM, the particles actively move and significantly change the distribution, especially near the surface. Consequently, for efficient performance, the change of parallel domain should be more dynamic than that of MD [39].
In this study, we investigated new iterative dynamic load-balancing algorithms for high-resolution simulation of a PSM with short-range interactions over a large domain. The 2D orthogonal grid mesh was used for domain decomposition, in which the boundaries in the column are independently chosen for each row. This type of domain decomposition is simple to implement and flexible when splitting the domain into arbitrarily sized rectangle sub-domains [23]. The grid discretization size is given by the radius of the particles, which is a reasonable unit of domain change for splitting the particle distribution of nearly incompressible PSM.
As a balancer to control the domain decomposition, an iterative algorithm was used to redress the imbalance through a succession of small domain changes [31]. The relaxation of the imbalance was applied in the framework of an iterative nonlinear solver with an approximated Jacobian form for the increased/decreased workload from the former domain changes [45], [46]. The iterative method is suitable for relaxing gradually changing load imbalances in space and time, as in our target PSM. In contrast, several of the earlier proposed algorithms generate the optimal domain decomposition directly at a single time [34], [35], [36]. Such direct methods, however, require the master process to hold the distribution of the workload over the whole domain, stored as whole mesh data or sampled particles [23], [42]. Our relaxation scheme is computationally cheaper in communication and memory cost than these direct methods because it uses only the workload for each MPI process. It is therefore suitable for use with our iterative strategy for adjusting the sub-domain frequently to achieve dynamical load-balancing.
In this paper, we focus on SPH simulation as a representative target PSM. In Section 2, we briefly examine the SPH model. Our implementation in earlier work of SPH simulation for efficient parallel performance on multi-core platforms is presented in Section 3. In Section 4, we propose an iterative load-balancing algorithm based on the observed elapsed time, Jacobian estimation, and a smoother using a multigrid level technique. The setup of numerical tests to confirm the performance of the novel algorithm is presented in Section 5 and these results, as well as parallel scaling on current HPC systems are analyzed in Section 6. Conclusions and suggestions for future work are given in Section 7.
