Researchers in information science, network analysis, and science of science currently have access to an unprecedented volume of data. Researchers are increasingly working with datasets that include millions of observations (e.g. Börner, 2010, Börner, 2015, Boyack et al., 2005, Evans and Foster, 2011, Foster et al., 2015, Rzhetsky et al., 2015, Shi et al., 2015, Sinatra et al., 2015, Skupin et al., 2013, Sugimoto et al., 2013, Uzzi et al., 2013, Wang et al., 2013). In 2015, there were more than 3.8 million records indexed in ProQuest Dissertations and Theses, more than 23 million in PubMed, more than 60 million in Scopus, and the number of cited references indexed in the Web of Science surpassed 1 billion. The Scholarly Database – hosted by researchers at Indiana University – currently contains over 25 million records (LaRowe, Ambre, Burgoon, Ke, & Börner, 2009). The text and network datasets that can be extracted from these databases are often enormous. As de Solla Price (1963) predicted, we are in a period of abundant data, and more is being produced all the time.
In addition to being “bigger” than they used to be, bibliometric datasets are becoming more complex as researchers link them with data from online repositories, social media, blogs, surveys, and administrative data from institutions, granting agencies, and governments (e.g. Cronin and Sugimoto, 2014, Haustein et al., 2014, Kronegger et al., 2012, Sugimoto et al., 2013). Making the most of this abundant data requires access to sufficient infrastructure and software that scales efficiently, reduces opportunities for human error, and is compatible with open and reproducible workflows. Using these tools appropriately requires computing skills that have not traditionally been necessary for conducting sophisticated research on the structure, evolution, and content of science.
There are currently many excellent software options for constructing and analyzing bibliometric datasets, small or large. There is specialized software for historical bibliometrics (e.g. Garfield's (2009) HistCite, Van Eck and Waltmen's (2014) CiteNetExplorer, Thor, Marx, Leydesdorff, and Bornmann's (2016) CRExplorer, and Comins and Leydesdorff's (2016b) RPYS i/o) and for mapping the topic and network structures of science (e.g. Van Eck and Waltmen's (2010) VOSViewer, Chen's (2006) CiteSpace, and WoS2Pajek for Pajek (De Nooy, Mrvar, & Batagelj, 2011)). Katy Börner and her collaborators developed Sci2 and the Network Workbench (NWB) as modular “plug and play” programs, intended to be collaboratively developed by scientometric researchers as the field evolves (Börner, 2011). All of these programs have their own parsers for converting raw data files into something useful for bibliometric and scientometric research. Most tend to focus on very specific research ends (e.g. creating topic maps) and attempt to cover an entire research workflow from parsing raw data to producing graphs intended for publication. They are all primarily graphical user interfaces (GUIs) with drop down menus that require repetitive user input.1
GUI systems dramatically lower the barriers to conducting bibliometric and scientometric research, but many of the most exciting and promising developments in the field require computing workflows that are better suited to scripted data analysis, for example in R, Python, or Stata. Almost all research workflows include many small sequential tasks, some of which have to be repeated many times. A GUI program can require hours of tedious and error prone user input every time the workflow is executed. This is a waste of researcher time and effort. It could be automated and made reproducible with data cleaning and analysis scripts. While we fully support efforts to empower as many researchers as possible to leverage access to data and computing power to advance research in information science, network analysis, and science of science, there is a trade-off. GUI software plays a central role in research, but we also require software that is optimized for scalability, speed, reproducibility, easily linking open data, and open workflows.2
This article introduces metaknowledge, a Python package for computational research in information science, network analysis, and science of science.3 The package name is adopted from Evans and Foster's (2011) brief article in Science. In short, it accepts raw data inputs from the Web of Science, PubMed, Scopus, Proquest Dissertation and Theses, and administrative data from some funding agencies. It outputs tidy datasets for a wide range of quantitative analyses, including but not limited to longitudinal analysis, Standard and Multi Reference Publication Year Spectroscopy (RPYS), computational text analysis (e.g. topic modeling, burst analysis), and network analysis (including multi-mode, multi-level, and longitudinal networks). Although metaknowledge is aimed at researchers with some programming knowledge, who are working with large and complex bibliometric datasets and / or who are committed to open and reproducible research, it fits into any research workflow in bibliometrics and scientometrics. In the sections below, we discuss the design and core functionality of metaknowledge, explain how to get started, and demonstrate some of its most useful functions.
