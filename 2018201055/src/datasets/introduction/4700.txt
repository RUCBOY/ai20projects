Human action recognition, one of the key technologies in computer vision domain, has been widely applied in human surveillance, scene understanding, human–computer interaction, etc. While reliable human action recognition has been achieved in simple scenes (KTH [1] and Weizmann [2]), the recognition task remains challenging in complex scenes. The diversity of realistic videos, such as movies [3] and web videos [4], [5], [6], has shown significant challenges with foreground clutter, background variations, camera motion, view changes and partial occlusions.
Human action modeling is a fundamental problem for action recognition. Modeling an action in video sequence starts with feature representation. Previous research efforts for action representation were mainly focused on the following four aspects:
•Local features: For each given detected interest point, a feature descriptor is computed for a 3D video patch descriptor. As local space–time features allow to build efficient action representation without object detection or motion segmentation, they have been successfully utilized in action recognition and thus, leads to a trend of generalizing descriptors such as STIP [7], Cuboids [8], 3D-SIFT [9], HOG3D [10], HOG/HOF [3], Hierarchical SIFT Trajectory [11], LTP [12], MoSIFT [13], MPEG Flow [14], and CGME [15], iMoSIFT [16].•Dense sampling: Among the local space–time features, dense sampling methods have drawn more attention and provided promising results. The main idea is to densely sample feature points in each frame, and track them in video sequences based on optical flow. Multiple descriptors are computed along the trajectories of feature points to capture motion information, e.g., MBH [17], extended SURF [18], Dense [19], V-FAST [20], Stacked ISA [21], Saliency [22], OVDS [23], DT [24], LPM [25], DCS [26], MBI [27], Motionlets [28], iDT [29], DTD [30], Concept Relevance [31], [32], and iDT-RCB [33].•Global representation: Despite encouraging results have been obtained using the methods above on several datasets, low-level features limit the semantics of actions. Therefore, representing actions by pose-based methods [34], [35], [36], [37], [38], [39] or global templates have been explored, for instance, MHI [40], Shapes [2], Action Bank [41], NTraj [42] and DMMs [43]. However, it is difficult to estimate poses or high quality templates due to the diversity of real world videos, except in special cases (e.g., puppet [42], simple scenes [2], [34], [37], [40], accelerometers [39], and depth camera and inertial sensor [44], [45], [46]).•Deep learning: As hand-crafted descriptors mentioned above may lack discriminative capacity for action representation, deep learning methods aim to automatically learn the semantic representation from raw video by using a deep neural network. Typical methods include 3D ConvNets [47], Deep ConvNets [48], Two-Stream ConvNet [49], TDD [50], Latent Concept Descriptors [51], H-FCN [52], FCLN [53] and Conv Two-Stream [54].
1.1. Motivation and contributionsAs shown in the last two rows of Fig. 1, complex human body poses, partial occlusions and motion blurs often appear in action videos, the human detector [29] and faster RCNN [55] do not always work perfectly. These components may lead to incorrect region detection problem when estimating the homography with feature matching. To automatically detect action regions without expensive training data and any human detector, motivated by saliency detection research [22], [29], [56], [57], a global contrast based segmentation algorithm was introduced to produce region-based contrast maps (RC-map).Download : Download high-res image (1MB)Download : Download full-size imageFig. 1. Example illustrating the characteristics in action videos, e.g., background motions and foreground variations. The 1st row shows white removed trajectories under various camera motions. The 2nd row illustrates camera motion types via red underlying trajectories. The 3rd row demonstrates the failure cases of human detector due to complex human pose variations. The last row describes the failure cases of faster RCNN owe to illumination variations and partial occlusions. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)Although RC-maps can constrain feature points on salient regions, the global contrast based segmentation algorithm, which uses image contrast under the assumption that a salient object exists in an image, was not suitable for action videos. Because it may result in unstable masks with respect to consecutive frames. Therefore, partly inspired by motion boundary researches [17], [27], we applied morphological gradient to optimize RC-maps to generate more robust masks. It is named region-based boundary maps (RCB-map). The RCB-maps could capture discriminative appearance information on salient region boundaries.However, action recognition becomes a challenging problem due to the motions of camera and the variations in pose, appearance, background, etc. It should be noted that action recognition cannot be achieved by merely employing an object detection or segmentation algorithm. As Fig. 1 shows, there are numerous irrelevance trajectories in real world videos due to camera motion. Hence, the action representation is prone to be inaccurate. Meanwhile, merely using improved dense sampling on RCB-maps (iDT-rawRCB) also cannot promote the recognition, as RCB-maps may not be able to capture the relative displacements while resisting background motions in a subset of consecutive frames.To address the above problem, traditional motion estimation approaches model the global camera motions by using motion vector decomposition [58], [59], or warp optical flow with a robustly estimated homography [5], [29]. In this paper we assume that the true flow can be established by a normalized warped optical flow at each point of consecutive frames, and then a normalized magnitude of warped flow was defined to capture salient relative displacements, while the tiny ones lower than a threshold are regarded as meaningless. Since recognition task always benefit from dense features but sparse [60], we should replace feature points with those points sampled by original iDT after excluding minimum warped flow, when the sampled ratio becomes relative small.The proposed method including iDT on RCB-map and warped flow pruning with dense feature supplementary scheme is named iDT-RCB. We extensively evaluate our method on Hollywood2 dataset. Inspired by the deep learning approaches [49], [50], [54], we also evaluate the fusion methods combining Convolutional Neural Network (CNN) architectures with our iDT-RCB. The fusion of them achieves the state-of-the-art performance on Hollywood2 and UCF50 datasets. The contributions of this paper are summarized as follows:•We propose a salient region-based dense sampling method to conquer the region detection and motion evaluation problem. Different from the previous dense sampling methods [22], [27], [29] on pruning features, our method mines the saliency of distant regions from consecutive frames without automatic human detection.•We evaluate the effectiveness of removing tiny motions from warped optical flow. When pruning tiny motions by a suitable magnitude threshold, the remainder of warped flow are regarded as salient motions between frames. In other words, the action recognition can benefit from salient region masks and salient motion displacements.•We exploit information not only for the hand-crafted features but also for the fusion of deep-learned features. We separately present the results of iDT-RCB and our best results obtained with early fusion of TDD and iDT-RCB.A preliminary version of this work appeared in [33]. This paper extends the earlier work [33] as follows. Firstly, we reveal the motion cues between salient displacements and warped optical flow through analysis and experiments. It demonstrates that our method can benefit from salient region boundaries (i.e., RCB-maps) and salient warped flow. Secondly, we propose a dense supplementary scheme to overcome the problem of extremely sparse features, when RCB-maps fail to capture enough features in some cases. Experimental results show that a suitable pruning of feature points represents good compromise between saliency and density of the sampled points. Thirdly, we comprehensively compare our method with state-of-the-art approaches. Extensive experiments on Hollywood2 dataset are also conducted. By combining CNN features with our method, the recognition results are further improved.The remainder of this paper is organized as follows. In Section 2, a brief review of related work on dense sampling for action recognition is given. In Section 4, we describe the reason of region-based contrast boundary mapping. Then, we evaluate the influence of removing tiny motions according to warped optical flow, and provide a dense feature supplementary scheme in Section 5. Section 6 shows experimental results and finally Section 7 concludes this paper.
