Natural language text contains considerably causal knowledge, as shown in Fig. 1. In recent years, causality extraction has become increasingly important for many natural language processing tasks, such as information retrieval [1], [2], event prediction [3], [4], question answering [5], [6], [7], generating future scenarios [8], [9], decision processing [10], medical text mining [11], [12], [13] and behavior prediction [14]. However, due to the ambiguity and diversity of natural language texts, causality extraction remains a hard NLP problem to solve.Download : Download high-res image (78KB)Download : Download full-size imageFig. 1. A sentence expressing causal relations, in this case, “financial stress” is the cause, “divorce” is the effect caused by financial stress.
Traditional methods for causality extraction can be divided into two categories: methods based on patterns [1], [11], [15], [16] (Section 5.1), and methods based on a combination of patterns and machine learning techniques [5], [17], [18], [19] (Section 5.2). The former often has poor cross-domain applicability, fails to balance precision and recall and may require extensive domain knowledge to solve problems in a particular area. The latter usually requires considerable human effort and time on feature engineering, relying heavily on the manual selection of textual features. Generally, it divides causality extraction into two subtasks, candidate causal pairs extraction and relation classification (filtering noncausal pairs). The results of candidate causal pairs extraction may affect the performance of relation classification and generate cascading errors.
[20] first proposed a tagging scheme that makes it possible for models to extract entities and relations simultaneously. In their tagging scheme, they apply a cartesian product of the entity mention tags and the relation type tags, and then assign a unique tag that encodes entity mentions and relation types for each word. Inspired by their novel idea, we focus on a causal triplet that is composed of two event entities and their relation. For instance, the sentence in Fig. 1 contains a causal triplet: “{financial stress, cause-effect, divorce}”. Thus, we can model the causal triplets directly, rather than breaking causality extraction into two subtasks. Based on the motivations, we formulate causality extraction into a sequence tagging problem and propose a causality tagging scheme (Section 2.1) to achieve direct causality extraction. However, the tagging scheme proposed by [20] cannot identify the overlapping relations in a sentence; it only considers situations where an entity belongs to one triplet: if an entity participates in multiple relations, its tag should not be unique. To address this problem, we design a tag2triplet algorithm (Section 2.2) to handle multiple causal triplets and embedded causal triplets in the same sentence. Finally, we combine the causality tagging scheme with a deep learning architecture (Section 2.3) to minimize feature engineering while efficiently modeling causal relations in natural language text.
We notice that some researchers have also proposed deep learning technique-based methods for causality extraction in recent years (Section 5.3). Although their works are commendable, some works [21], [22], [23], [24] are only a classification of causal relations rather than an extraction of complete causal triplets, and others [25], [26] mainly focus on the identification of the linguistic expressions for causality instead of the commonsense causality extraction in this paper.
By applying our causality tagging scheme, we use the model based on BiLSTM-CRF [27] to extract causal triplets directly. However, we find that two obstacles hinder the further improvement of the performance of the deep learning model.
First, it is difficult to train a superior deep learning model without any prior knowledge in the case of data insufficiency in the existing corpus [28], [29], [30]. To alleviate this problem, we incorporate Flair embeddings [31] into our task, which use the internal states of a character language model trained on a large corpus to create word embeddings (Section 2.3.2). Experimental results show that this contextual string embedding that has initiated a new technology trend in NLP can drastically improve the performance of causality extraction.
Second, in terms of their positions in the text, cause and effect are sometimes far from each other, as Fig. 2 shows. The long-range dependency in the causal triplet creates difficulty and ambiguity in the deep learning model, but a set of logical rules based on dependency trees can easily and accurately extract such triplets. To learn this kind of long-range dependency between cause and effect, we introduce the multihead self-attention mechanism [32] into our model (Section 2.3.4). Unlike the LSTM-based model that recursively processes each word, the self-attention mechanism can conduct direct connections between two arbitrary words in a sentence and thus allows unimpeded information flow through the network [33].Download : Download high-res image (240KB)Download : Download full-size imageFig. 2. The second causal triplet: “{[lesions], cause-effect, [distally predominant and a less severe proximal weakness]}” spans almost the entire sentence.
The contributions of this paper can be summarized as follows:
1.We design a novel causality tagging scheme to directly extract causalities in texts and can easily transform the causality extraction into a sequence labeling task and handle multiple causal triplets and embedded causal triplets in the same sentence.2.Based on our causality tagging scheme, we propose SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings), a neural-based causality extractor with transferred contextual string embeddings trained on a large corpus. To the best of our knowledge, we are the first to transfer Flair embeddings into causality extraction.3.We introduce the multihead self-attention mechanism into SCITE, which enables the model to capture long-range dependencies between cause and effect.4.Extensive experimental results (Section 3) and and further analysis (Section 4) show that our method achieves significant and consistent improvement compared to other baselines. We release the code and dataset to the research community for further research 1.
