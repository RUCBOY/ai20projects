Recently, with the renaissance of deep learning, end-to-end neural machine translation (NMT) [1], [2], [3], [4] has gained remarkable performances [5], [6], [7]. Conventional NMT approaches are typically optimized by maximizing the likelihood estimation (MLE) of each word in the ground truth translations during the training procedure. However, such an objective cannot guarantee the sufficiency of the generated translations, due to the lack of mechanism for quantitatively measuring the information transformational completeness from the source side to the target.
Some existing work alleviates this problem by directly incorporating coverage or fertility mechanism to an NMT model [8], [9], [10]. However, the problem is that attention weights based coverage calculation for NMT [8], [9], [10] is insensitive to translation errors and sometimes makes mistakes. Furthermore, it is also unreasonable to consider all kinds of source words equally, since disparate words contribute differently to sentences in semantics and syntax. For example, as illustrated in Fig. 1, translation errors are recorded as positive examples for calculating the coverage, and the alignments between function words also dilute the contribution of key words.Download : Download high-res image (164KB)Download : Download full-size imageFig. 1. Model generated attention weights matrix of two Chinese  →  English translation examples of Transformer-based and LSTM-based NMT systems. The source language (Chinese) is in the vertical direction, and the target language (English) is in the horizontal direction. The lighter color in the attention matrix represents the higher attention weight. The token “〈eos〉” is the end-of-sequence symbol. “☆” and “▵” in the matrix represent the “good” and “bad” word alignments (attention), respectively. Each alignment will be counted as a coverage of the source word (each source word is covered at most once). According to the attention-based coverage, LSTM-based NMT covers more source words than Transformer-based NMT in this example, which is opposite to human judgments.
In this paper, we address the inadequate translation problem by introducing novel sentence alignment constrains to NMT. Specifically, we first propose a sentence alignment oriented discriminator D that learns to estimate an alignment score between source and target sentences. We employ a gated self-attention based encoder for bilingual sentences encoding in D in order to capture the semantic alignment evidence of the input data. An N-pair loss [11] is introduced to the training procedure of D for preventing the correct but not human generated translations from being overly penalized.
Then, we apply an adversarial training framework as well as an alignment-aware decoding strategy to incorporate the sentence alignment constrain into NMT. Under the adversarial training framework, a standard NMT model G is trained to produce an appropriate translation that gains higher score assigned by D. We leverage Gumbel-Softmax (GS) [12], [13] approximation for G to solve the problem of discrete samples, making the response from D to G differentiable. As for alignment-aware decoding, we conduct D to guide the NMT generated translation by combining the alignment score with the decoding log-likelihood. A sentence alignment based value-network [14] is also employed to study the effectiveness of our approaches.
To sum up, the proposed approach has the following advantages:
•We propose a gated self-attention based encoder for bilingual sentence embedding. The proposed encoder learns to focus on important lexical evidence for sentence aligning and enhance the contribution of the key words. This lexical and semantic knowledge can be transfered to G through the proposed training and decoding framework.•We introduce a novel end-to-end NMT adversarial training framework that heightens adequacy in translation. Under the framework, an NMT model is encouraged to generate translations that match the semantic knowledge learned by a discriminator for sentence aligning. This can be viewed as an instance of “knowledge transfer”.•We also propose an alignment-aware decoding method for NMT. It incorporates the sentence alignment score into the NMT decoding step, which allows the NMT decoder to take into account both the adequacy and fluency of translations.•The N-pair loss [11], [15] encourages samples closed to the gold-standard to get higher score. Unlike the binary classification used in previous work [16], [17], [18], translations that are correct but different from the ground-truth ones will not be overly penalized.
