As technology becomes further embedded in everyday life, utilization of online data collection methods has become increasingly popular for the distribution of surveys and questionnaires. Online data collection offers many benefits in both the clinical and research realms. For clinical purposes the use of an online questionnaire offers advantages for patients, such as the ability to provide information to the clinician prior to the appointment, and at a time and location that is convenient for them (Buchanan, Johnson, & Goldberg, 2005). Relevant to both clinical and research operations, online questionnaires allow for the minimization of data entry and scoring errors as data acquisition, scoring, and analysis can be automated, which also allows for expedited feedback to the clinician or participant (Buchanan and Smith, 1999, Johnson, 2005). Similarly, the use of an online questionnaire has been found to reduce the number of missing data points, as online platforms can require respondents to answer specific questions before moving on (Stanton, 1998). Constraints placed on the structure of data can also be more effectively enforced using the online data collection format; such as content validation regarding response format (e.g., constraining reported dates to MM/DD/YYYY format). Costs are minimized with online administration of questionnaires, as is environmental impact (Naus, Philipp, & Samsi, 2009). In the research setting, online data collection enables studies to include a sample of increased geographic diversity and size (Johnson, 2005, Smith and Leigh, 1997) without concurrent increase in cost.
Research has also identified several drawbacks to the use of the online format for data collection. Chief among these are concerns about sample representativeness as a result of differences in access to and experience with computers and the internet. Higher levels of education are associated with greater access to the internet and less computer anxiety (Stanton, 1998); thus, use of an online format is unlikely to yield a sample that is representative of the population as a whole with respect to sociodemographic factors. Familiarity with computers may also influence responding or performance, though research in this area has offered mixed findings (Luecht et al., 1998, Mazzeo et al., 1991, Taylor et al., 1998) suggesting differential impacts of computer familiarity depending on task type (Russell et al., 2015, de Beer and Visser, 1998). Equally important is the question of whether measures administered online are equivalent in terms of reliability and validity to measures administered via paper and pencil (P&P). Few measures have yet developed distinct norm sets for online and P&P administration formats; however, the psychometric properties of the two formats have been compared.
Examination of the comparability of the two formats has been focused largely on self-report and performance-based measures, offering mixed results. In the self-report domain, differences in anonymity of responses, the testing environment, and other extraneous variables that may influence responding have been hypothesized to contribute to differences in responding across formats; however, the influence of these factors seems to be minimal (Buchanan and Smith, 1999, Reips, 2000). Self-report measures of personality tend to demonstrate largely similar psychometric properties such as inter-item reliability and factor structure across administration formats (Buchanan et al., 1999, Johnson, 2000, Surís, Borman, Lind, & Kashner, 2007, Woolhouse and Myers, 1999, Rammstedt et al., 2004); however, even slight variations in factor structure across administration formats may have implications for measure scoring that must be addressed (Buchanan, 2000, Buchanan, 2002, Buchanan et al., 2005). Measures of clinical constructs such as anxiety and depression have also demonstrated similar but not identical psychometric properties across administration formats (e.g., Naus et al., 2009; Davis, 1999, Stones and Perry, 1997).
Thus, while administration format seems to have relatively little influence on responding in terms of the psychometric properties of a measure, the possibility of minor differences in factor structure and item loadings calls for evaluation of these characteristics on an individual, measure by measure basis in order to ensure that subscale, index, or standardized scores are comparable across formats. Comparability of caregiver ratings across administration formats has, as yet, been neglected in the research literature, and, as online survey dissemination becomes increasingly utilized, establishing psychometric comparability becomes even more critical. The goal of the present study is to evaluate the impact of administration format on the psychometric properties of three frequently used caregiver-report ratings of children’s behavioral, academic, and emotional functioning. On the basis of the existing literature comparing self reports across administration formats, we hypothesize that caregiver ratings will be psychometrically equivalent across administration formats, though we expect to find slight differences in item factor loadings.
