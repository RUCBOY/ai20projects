The evaluation of academic journals has been an important issue in academia for decades. The evaluation results of journals in which researchers’ papers are published have a critical impact on such researchers’ academic careers. First, the typical premise on which one obtains a doctorate degree and a faculty position is publishing several papers in high-level journals. Second, even after successfully being recruited, researchers must seek funds to conduct their research. In addition to the quality of a scientific research proposal, successful funding depends to some extent on the applicant’s publications. Third, one of the most important criteria for a researcher’s promotion is whether he (she) has published papers in high-quality journals. To this end, everyone in the scientific community is deeply involved in the evaluation of academic journals. For example, during the first half of 2016, China’s Ministry of Education (MoE) attempted to promote a new classification of academic journals, which caused widespread debate in the academic community and was eventually postponed.1
Undoubtedly, the most widely used journal evaluation method is the impact factor (IF) proposed by Garfield (1953, 2006) and as published in the annual Journal Citation Reports of Thomson Reuters. Hirsch (2005) proposed the h-index, which has recently become one of the most influential methods for assessing researchers and institutions. Some scholars argue that the h-index addresses to some extent the deficiencies of the IF. However, both the IF and the h-index have received significant criticism (e.g., for the IF, Seglen (1997), Block and Walter (2001), and Glänzel and Moed (2002); e.g., for the h-index, Vanclay (2006), Vinkler (2007), and Adler et al. (2008)). Recently, the American Society for Microbiology (ASM) declared that it is waiving the IF of its eight subordinated journals.
Several drawbacks to the IF exist: (1) Overgeneralization (Falagas et al., 2010; Kravitz, 2011; Seglen, 1997). The IF measures the overall average of all papers published by the journal. However, although papers may have been published in the same journals, there may be significant differences in these papers’ actual quality. Thus, a strong bias may exist when measuring the quality of a certain paper using a journal’s IF. (2) Time lag. The IF normally covers the citation numbers of articles published in the two or five preceding years and fails to include all citation numbers over time. (3) Fluctuation. The IF likely changes as the number of articles published per year changes (Garfield, 1953). Some studies found thatthe IF of journals that publish less than 35 articles per year fluctuates by approximately 40% between adjacent years. Although some journals publish more than 150 articles per year, the fluctuation rate is approximately 15% (Moed, 1999; Seglen, 1997). (4) Statistical bias. Because citations are from the Web of Science (WoS) database and are primarily in English, citations from journals not included in the WoS database and written in another language are ignored. (5) Misleading the public. A high IF does not equate to high academic quality or a research frontier. It is possible to ignore truly important research questions or research tasks when perusing a high IF journal. Other drawbacks include field differences (Dong, 2005), negative citations (Thorne, 1977), and so on. Some problems are not unique to the IF but are common to the IF, the h-index, and other indexes or techniques based on citation analysis. For additional discussions on the flaws of the IF and the h-index, interested readers can refer to Leydesdorff (2012) and Malesios and Arabatzis (2012).
Few researchers noticed that a major flaw of the IF is that it is oversimplified. From the perspective of the input-output system, the IF can be viewed as a single-input and single-output system. The single input is “articles” and the single output is “citations”. Obviously, citations are not restricted to articles, as noted by a review of the publishing process and generating citations. When the author deems it ready, the manuscript is submitted to a journal’s editorial office. Subsequently, after editors’ preliminary examinations, the manuscript is assigned to several anonymous reviewers depending on the journal’s specific regulations. When reviewers’ comments are returned to the editorial department, decisions take one of three forms: (1) accept, (2) reject, or (3) revise. If the author(s) decides to revise, then a new review round is started if needed. After the article is published, other researchers who find it inspiring or having reference value will cite it in their papers. Obviously, many factors affect citations and articles. For example, certain influential factors (e.g., the IF itself and journals’ publishing schedules) may affect citations. Therefore, suppose a journal evaluation is a problem with multiple inputs and outputs, making it important to select the appropriate method to address this problem. Therefore, analyzing academic journals’ performance using multiple inputs and outputs is appropriate.
In particular, this research focuses on the multi-year trend in the change in the performance of academic journals. Using as an example international academic journals in the field of Management Science & Operations Research (MS & OR), this paper aims to observe the change in performance of academic journals over multiple years and the factors that contribute to such a change. We also investigate the difference in performance and its changes between high-level and low-level journals, and the corresponding influential factors. To the best of our knowledge, no existing research exists on this issue, especially performance evolution over time. Two papers also used multiple inputs and outputs and data envelopment analysis (DEA) to assess academic journals (Lee & Shin, 2014; Petridis et al., 2013). However, they failed to consider performance evolution in the framework of multiple years.
To summarize, in the field of academic journal assessment, we have not found research that uses the DEA-Malmquist approach to measure the change in journals’ performance. Two related studies measuring journal efficiency using the DEA method fail to conduct a time-series analysis. This paper uses the DEA-Malmquist approach to conduct an academic journal assessment and a performance evolution analysis. DEA is a non-parametric method that measures the efficiency or performance of decision-making units (DMUs) with multiple inputs and outputs, and is widely used in the assessment of scientific research (see, e.g., Anderson & Peterson, 1993; Banker et al., 1984; Chames et al., 1978; Cook & Seiford, 2009; Cooper et al., 2000; Doyle & Green, 1994; Emrouznejad & Yang, 2017; Yang et al., 2014; Yang et al., 2016). DEA is suitable for application to the citation generation system whose production function is not observable. Malmquist (1953) proposed the Malmquist index, which can be used to analyze productivity changes in multiple periods. Färe et al. (1994) established the Malmquist productivity index in the DEA framework to examine total factor productivity (TFP), which is used to analyze the evolution of productivity over time.
The remainder of this article is organized as follows. Section 2 summarizes previous studies that used the DEA-Malmquist method to measure academic journal performance. Section 3 describes the DEA approach used in this paper. Section 4 provides the data sources, indicators, and analysis steps. Section 5 illustrates the empirical results of measuring international academic journals in the field of Management Science & Operations Research. Section 6 concludes this paper.
