Interaction between brain and computer is traditionally mediated by the peripheral nervous system using motor nerves and muscles to convey information/instruction to the computer. A brain-computer interface (BCI) is a device that bypasses the peripheral nervous system and permits the direct transfer of information from the brain to a computer. A BCI is essentially a device that maps an input onto an output. The input is the activity of the brain as measured by some sensing device and the output is the control of a device such as cursor movement, spelling device, wheel chair, or prostheses.
The electroencephalogram (EEG) measures local field potentials that have propagated from the cortex to the scalp. EEG is an attractive signal measurement tool for a BCI as it is both inexpensive, non-invasive, and has a high temporal resolution. The control signal in a BCI is the pattern of brain activity that reflects the intention/desire of the user to effect control over the BCI. The primary objective of a BCI is to accurately extract and interpret this control signal. Numerous EEG phenomena have been exploited as control signals for non-invasive BCIs, with one of the most prominent being sensorimotor rhythms (SMR). SMR are oscillatory EEG activity generated in cortical sensorimotor areas. SMR can be modulated by preparation for movement, execution of movement, or by imagination of movement [1]. It is the effect of motor imagination on SMR that has been exploited by numerous BCI research groups. Specifically, SMR in the μ (8–12 Hz) and β (18–26 Hz) frequency bands have been shown to be effective control signals for BCI operation.
An emerging trend of recent years has been the use of advanced machine learning algorithms to create BCI's that are optimized to subject specific patterns of brain activity. Numerous groups have implemented machine learning algorithms to minimise the need for user-training [1], [2], [3], [4], [5], [6], [7], [8], [9]. Machine learning based BCI systems typically operate using some variant of following steps:
1.Calibration: data is recorded whilst the user repeatedly performs some mental task in which the intention of the user is controlled.2.Training: the BCI uses the calibration data to optimize the weights of its classifier in a way that accurately classifies the predefined intention of the user3.Feedback: the trained classifiers are applied to new brain activity to predict the intention of the user
The volume of data obtained when acquiring EEG from many channels with high sample rates and repeated trials can be large. The dimension of EEG data is the number of channels × the number of features (e.g. samples, spectral power, etc.) in a single trial and is usually very high. The high-dimension of EEG data means that many trials are required for successful classification otherwise the problem is underdetermined. The process of acquiring many calibration trials can be both time-consuming and laborious for the user and may span multiple sessions and/or days. In addition, the process of training the classifier on large volumes of high-dimensional data can be computationally expensive and also very time-consuming, often spanning days. These combined time constraints limit the real-world and real-time utility of BCI. Ideally a BCI would be calibrated on a small amount of data acquired in a very short time, and the classifiers trained rapidly to produce a functional and practical BCI.
Dimension reduction (DR) can be utilized to reduce the volume of data prior to classification, and as a consequence reduce computational demands and processing time. The challenge of DR is to maximise compression whilst minimizing loss of information and maintaining performance. DR can be performed on channels, features, or both. Decomposition-based methods (e.g. principal components analysis) are popular choices for feature and electrode reduction however they use a procedure that mostly isolates a set of features or electrodes that satisfy a certain statistical criterion without considering their actual impact on the classifier's learning rule. Evolutionary methods such as Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) are alternative approaches that can be utilized for dimension reduction and are not subject to this limitation. These approaches evolve their population toward a subset of features and/or channels that achieve the best classification performance. However these evolutionary methods are very computationally intensive which further exacerbates the time constraints of the BCI system [10], [11]. An evolutionary dimension reduction approach, called PSO-DR, that allows simultaneous feature and electrode reduction was proposed in [12]. In this approach, PSO is utilized to identify a subset of feature and electrode indexes that best represent the performed task and offers better generalizability. In order to address the computationally intensiveness of the PSO-DR, subject transfer, which is the use of previously recorded samples originating from other subjects [13], [14], [15], [16], is utilized to pre-identify the subset of feature and electrode indexes by the PSO-DR [17], [18].
The aim of this study is to combine subject transfer with evolutionary dimension reduction (PSO-DR) to reduce the dimensionality of the data and the number of training trials required for classification. Advantages of this methodology are:
•Extra training samples originating from other subjects can be prerecorded and preprocessed off-line.•Minimum dependency on training trials from the target subject (the subject that performs the tasks in on-line).•Low dimensionality of the data as a result of evolutionary dimension reduction that reduces the required classifier training time.
