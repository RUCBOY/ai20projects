The video salient object detection emphasizes objects that draws attention in a video, and this has emerged as an active part of research due to its increasing attention on applications like object tracking, object segmentation, and action recognition.
Different video salient object detection Cheng et al., 2015, Zou et al., 2015, Li and Yu, 2016, Zhu et al., 2014, Li and Yu, 2015, Girshick, 2015 approaches have been developed so far on still image benchmark datasets. These methods underperform if they are applied to dynamic videos due to four different phenomenon: the first phenomenon is that the blurred vision of moving targets in video frames that degrades the performance of salient objects appearance. Secondly, occlusion of salient objects partially or totally by background regions in video frames. Thirdly, the rapid or arbitrary movement of objects determines poor saliency performance. Finally, rapid or arbitrary change of background regions during the salient object movement that causes difficulty in salient object extraction. These four phenomena ensure that the detection of salient objects is difficult in video frames.
It is well known that information from a moving object is essential and intuitive for supporting the salient video object estimation. Utmost all existing approaches calculate the salient object motion using optical flow since it is highly sensitive to variation in illumination and changes in localization. The localization and illumination frequently occur in videos, and this affects the stability of salient objection motion estimation. Specific methods operated on utilizing contour detection in fields of color frames and optical flow, however, it suffers from limited scalability in extracting the contour regions from the cluttered backgrounds. Hence, the exclusion of dynamically changing irrelevant background poses a significant difficulty in modeling a salient object motion estimation. Hence, it needs a complete address in the field of video saliency. On the other hand, the salient object detection suffers mostly from high computational load due to the accommodation of huge visual scenes. Further, it suffers mostly from time efficiency, which acts as a bottleneck for video saliency algorithm in detecting or estimating the salient objects.
To resolve the aforementioned limitation, spatiotemporal optimization is used that exploits spatial and temporal cues with a local constraint for optimizing the video saliency. This leads to object detection using a local constraint that supports the temporal and spatial cues to achieve the task of global optimization. In this regard, the proposed work aims at the usage of Objectness to analyze the motion energy of the foreground object in a video frame for evaluating the foreground region probability.
In existing approaches, the distinctive regions are detected using Convolutional Neural Networks (CNNs) Girshick, 2015, Ren et al., 2015, Li, Wu, Zhang, and Du (2016), Chen et al. (2019), Zhao et al., 2015, Long et al., 2015. However, it poses a serious problem due to the larger availability of labelled training data. This makes the system more time-consuming and complex. Many deep learning models are introduced to address the concerns in video saliency detection; however, CNN (Girshick, 2015, Ren et al., 2015; Li et al., 2016; Chen et al., 2019; Zhao et al., 2015, Long et al., 2015) is the most important of all. The convolutional layer, designed with multiple layers, is responsible for convolving the regions of local images independently. The responses from this layer are combined based on the region coordinates. The feature responses are then summarised using pooling layer, which uses a pooling kernel size and a fixed stride to process the responses. This confined setting does not take into account the adjacent regions during computation, which offers disadvantages. For instance, if the convolution or pooling is performed on the bottom right regions in an image, the features in this region remain with regardless of the appearance on the top left region. Hence, the contextual dependencies are not captured that leads to poor representation of an image.
To read all the regions in an image, the cost of the computation and usage of resources using CNNs may surmount, and it fails in tolerating the structure variance. An ideal network is pretended to have a boosting memory that should get a hold on the scanned regions of an image and its spatial correlations. The RNN serves such a purpose that models the dependencies among the regions in an image in a time sequence manner. The feedback connection in RNN and a hidden layer may retain its previous state inputs, and hence the correlations are found between the regions in an image, even if the sequences states are different. This has motivated the present study to develop a deep learning model to generate labelled training data.
This enables the video data to be accessed easily and generated rapidly closer to realistic video motion sequences.
The proposed method avoids the challenges of CNNs by replacing it with CRNN in many video processing applications with dynamic video saliency detection. The proposed method computes the maps of dynamic video saliency models by considering the temporal and spatial information of video frames. The CRNN video saliency model produces spatiotemporal saliency by exploring the dynamic and static video saliency information. The CRNN is adopted to predict the pixel-wise video saliency. The static saliency is exploited and encoded in CRNN learning stage through transfer and tuning of video frame classification Simonyan and Zisserman (2014). The dynamic saliency is learned using labelled data that includes both natural and human-generated data via supervised learning. Finally, the detection process of static video saliency is integrated with the detection process of dynamic video saliency, and thus, it produces a final estimation of spatiotemporal video saliency. The deep learning video saliency model is considered to be computationally efficient than other video saliency models.
The proposed video saliency model is said to be effective and efficient that reduces computational load and time efficiency. These objective functions are achieved by capturing the temporal saliency via CRNN from the video frame pairs. The CRNN is designed using a novel technique that combines both Convolutional Neural Network (CNN) with Recurrent Neural Network (RNN). This architecture has reduced the pitfalls of analyzing a video frame in faster response time than conventional CNN. The design of CRNN is carried out in such a way that it reduces the computational load in analyzing the video frames with an aggregation of layers in both CNN and RNN. The reduction in computational load is the usage of separate modules for static and dynamic saliency, where module 1 (CNN) and module 2 (RNN), is used respectively. The proposed method is evaluated on FBMS dataset Long et al. (2015), and it shows accurate salient maps than existing methods at 26fps. Hence, it could be regarded that the proposed method is effective in terms of both accuracy and speed.
The main contribution of the work is threefold:
•The authors investigate the design of CNN with RNN to form CRNN for saliency prediction (pixel-wise) for video salient object detection in dynamic scenes.•The authors propose a training scheme using video data generated synthetically from image datasets. It further encodes both dynamic and static video salient information into CRNN.•The proposed method is effective and efficient than existing deep video saliency models over dynamic scenes.
List of the endeavour advances in the proposed field:
•We have utilized the advancement in Salient object detection with deep learning architecture including CNN and RNN.•The CRNN video saliency model produces spatiotemporal saliency by exploring the dynamic and static video saliency information.•The CRNN is adopted to predict the pixel-wise video saliency.•The static saliency is exploited and encoded in CRNN learning stage through transfer and tuning of video frame classification.•The dynamic saliency is learned using labelled data that includes both natural and human-generated data via supervised learning.•We used two different modules to address the problem one is static and the other is dynamic that comprises of CNN and RNN to process each image, respectively.
The outline of the paper is given below: Section 2 provides the related works. Section 3 discusses the proposed framework for video salient static and dynamic information. Section 4 provides the experimental results and discussions. Section 5 concludes the paper with future work.
