eScience involves the execution of complex HTC (High Throughput Computing), HPC (High Performance Computing) applications and long-running workflows. This requires a significant amount of computing power and memory capacity that can be only obtained via distributed computing. Indeed, large-scale Distributed Computing Infrastructures (DCIs), such as the European Grid Infrastructure (EGI)1 have been tremendously successful in supporting the computational requirements of many scientific communities across Europe (Vella, Cefal, Costantini, Gervasi, Tanci, 2011, Camarasu-Pop, Glatard, Benoit-Cattin, 2013). However, one of the main limitations of Grid infrastructures is that applications have to be ported to the execution environments provided by the machines involved, what results in a rigid structure composed by several Virtual Organizations (VOs) that support a set of applications. This inability to provide customized execution environments for applications is addressed by Cloud Computing by means of Virtual Machines (VMs) that encapsulate the Operating System (OS) together with the user application and its dependences in a Virtual Machine Image (VMI) that can be run on a physical machine by means of a hypervisor.
Indeed, the ability to provide ubiquitous, on-demand network access to a set of configurable computing resources, according to the NIST definition (Mell and Grance, 2011) of Cloud Computing, has paved the way for the rise of many public Cloud providers (such as Amazon Web Services (AWS),2 Microsoft Azure3 or Google Cloud Platform4), different Cloud Management Frameworks (such as OpenNebula or OpenStack) and even initiatives to create large-scale community Clouds (e.g. EGI Federated Cloud5). Cloud computing has provided researchers with access to unprecedented customizable computing resources, either on-premises or on public Clouds. However, these computing resources still require a coordinated use for applications to efficiently use them. For that, Local Resource Management Systems (LRMS) such as Torque, SLURM (Jette et al., 2002) or HTCondor (Thain et al., 2005) are job schedulers that are commonly used to dispatch jobs across nodes (Ahn et al., 2016). Indeed, computing clusters are still widely-used computing facilities to support the execution of many types of applications.
A scientific computing cluster is a type of parallel or distributed processing system, which consists of a collection of interconnected stand-alone computers working together as a single integrated computing resource (Buyya, 1999). The access to a scientific cluster is usually made by means of a SSH connection to a “front-end” computer, and the users submit tasks to a middleware that will coordinate the working nodes to run these tasks. All the computers usually share filesystem to ease the distribution of the applications and the data that they need.
Virtual Elastic scientific computer Clusters (VEC) deployed on Cloud infrastructures have introduced many benefits when compared to physical clusters, as we addressed in our previous work (Caballer et al., 2013), avoiding upfront investments and the ability to adapt the execution environment to the applications (and not viceversa). This work was later extended to create EC36 (Calatrava et al., 2016) an open-source tool to create self-managed cost-efficient virtual hybrid elastic clusters across Clouds that is currently offered as a free online service, being used for scientists to provision their own clusters on public, on-premises and federated Clouds.
In the quest for increased performance with respect to virtualisation techniques, Linux containers appeared as a lightweight alternative to VMs. Linux containers enable to run multiple isolated processes in a host without the overhead caused by the hypervisor layer introduced by VMs. While hypervisors provide hardware abstraction, container-based virtualization is characterised by multiple isolated user spaces running at the operating system level (see Fig. 1). This provides process isolation at a fraction of the overhead introduced by the hypervisor. Container-based virtualization proved to be an alternative to traditional hypervisor-based systems, as it reduces the overhead caused by VMs in CPU, memory and storage, as described in Felter et al. (2015) and Scheepers (2014). Linux containers can be run on top of VMs to achieve multi-tenant isolation using the VM as the boundary of security and containers as the boundary of resource allocation to applications. However, the main benefits of containers arise when used on bare metal, in order to obtain increased performance compared to VMs. Among the different existing container platforms, Docker7 stands out as a software containerization platform that can encapsulate an application in a complete filesystem that contains all the dependences required to be executed (code, runtime, system tools and libraries, etc.). This guarantees portability across multiple platforms, regardless of the execution environment.Download : Download high-res image (185KB)Download : Download full-size imageFig. 1. Virtual machines and containers possible architectural configurations.
Our hypothesis is that container-based technology can be effectively integrated with cluster-based computing to create virtual computer clusters of Docker containers with the very same functionality as virtual clusters of VMs, and physical clusters of PCs, but with enhanced capabilities that include: (i) improving the performance of resource-intensive applications that will run isolated on bare metal; (ii) improving the elasticity of the cluster, by reducing the time required to spawn and terminate additional containers and (iii) supporting customised execution environments via low-footprint images.
Therefore, this paper introduces an architecture to deploy container-based virtual scientific computer clusters that feature automated elasticity and the ability to provide customised virtual execution environments across a bare-metal backend on which containers managed by a Container Orchestration Platform (COP) are executed. Several computer clusters customised for the execution of different scientific applications can be provisioned to share the same physical computing backend. This provides increased resource utilisation and performance while maintaining isolation across workloads coming from different clusters.
To this aim, this paper describes EC4Docker,8 an open-source tool to deploy, configure and manage container-based virtual computer clusters that can be run on bare-metal nodes (as well as on VMs). These virtual computer clusters expose the very same user interfaces expected by users (accessed via SSH, supporting a LRMS, etc.) but they are completely backed by Docker containers that are dynamically deployed, depending on the workload, across a distributed Docker Swarm (Luzzardi and Vieux, 2016) backend that can be deployed either on bare metal or on public and on-premises Clouds.
After the introduction, the remainder of the paper is structured as follows. First, Section 2 introduces background information and covers the state of the art related to containers, revising existing tools, performance studies and clustering solutions of containers. Next, Section 3 exposes and analyses the proposed architecture to deploy these container-based virtual computer clusters. Then, Section 4 addresses different scenarios in which the proposed solution is evaluated and analyses the significant benefits of these approach. Finally, Section 5 summarises the paper and points to future work.
