Current deep neural network architectures achieve superior performance on a number of computer vision tasks, such as image classification, object detection, and object class segmentation. Most of these tasks focus on extracting information from a single image. Deep neural networks compute increasingly abstract features, which simultaneously become more and more semantically meaningful, and incorporate larger contexts.
A real-world vision system will have to deal with the time dimension as well. Content is increasingly generated in the form of videos by Internet users, surveillance cameras, cars, or mobile robots. Video information can be helpful, as looking at a whole sequence instead of single frames may enable the interpretation of ambiguous measurements.
Similar to increasingly abstract features on images, we are interested in neural networks which produce high-level features on sequences. In a recursive computation, these high-level features should help to interpret the next frame in a sequence. In addition to a semantically meaningful localized content description, such features should form high-level descriptions of motions with increasing temporal context.
In this paper, we introduce a recurrent convolutional neural network architecture which produces high-level localized sequence features. We evaluate it on the NYU Depth v2 (NYUD) dataset, an RGB-D object class segmentation task, where every pixel of the input image must be labeled with the category of the object it belongs to. In this challenging and established benchmark, most methods focus on prediction based on single frames, while our method profits from image sequences.
In short, our contributions are as follows:

•We introduce a recurrent convolutional neural network model for processing image sequences.•On toy datasets, we show that our recurrent models are able to keep an abstract state over time, track and interpret motion, and retain uncertainty.•We show that our model improves RGB-D object class segmentation accuracy on the challenging NYUD dataset when compared to other CNN models. When combined with a CRF, RNN performance is close to carefully tuned transfer-learning approaches initialized on much larger datasets.•We analyze the obtained networks and show that the improved performance of our network stems from the recurrent processing combined with the exploitation of temporal, i.e. video, information.
