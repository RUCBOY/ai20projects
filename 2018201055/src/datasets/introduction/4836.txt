Syntax and semantics define a programming language. Informally, a language has many features. A language’s syntactic rules provide the most direct means to measure the use of a language’s features. Thousands of programming languages exist; each embodies a different set of possible language features. Language designers usually have limited knowledge on how programmers actually use a language (Knuth, 1971). This leads to many unnatural and rarely used features being introduced, while expected ones not introduced (Strangest language feature, Your language sucks). In addition, many language features, especially language syntax, remain a significant barrier to novice programmers (Denny, Luxton-Reilly, Tempero, Hendrickx, 2011, Stefik, Siebert, 2013).
We tackle the question of how to systematically understand these features and their usage. Rather than ad-hoc characterizations of features, we propose the use of language grammars to precisely and systematically characterize language features. Indeed, most programming language features quite directly map onto syntactic constructs. Therefore, we study how programmers use language features by analyzing their use of the language syntax.
Knuth conducted the first study to understand how programmers use fortran over 40 years ago (Knuth, 1971). Similar studies were subsequently performed on COBOL (Salvadori, Gordon, Capstick, 1975, Chevance, Heidet, 1978), APL (Saal and Weiss, 1977) and Pascal (Cook and Lee, 1982) between the 1970s and 1980s. In recent decades, there has been little quantitative study demonstrating how a modern programming language is used in practice, especially from the perspective of language syntax. Previous studies have investigated the use of subsets of language features (e.g., Java generics (Parnin et al., 2011) and Java reflection (Livshits et al., 2005)). Although Dyer et al. (Dyer et al., 2014) investigated the use of newly-introduced features over three main language releases, they only examined a relatively small subset of language features and did not consider pre-existing features.
Studying how a large number of real-world programs use language syntax may help validate or disprove the many popular ”theories” about what language features are most popular, most useful, easiest to use, etc. that abound in popular literature about programming and on the Internet. In addition, the gap between language features and their actual usage may guide pedagogy, giving teachers insight into how to teach a programming language in a better way. Language designers may leverage data on actual syntactic rule usage to optimize the design of languages, e.g. simplifying unpopular features or identifying boilerplate that could be eliminated. We will provide concrete examples when presenting our detailed study results.
To this end, we perform a large-scale empirical study on a diverse corpus of over 5,000 real-world Java projects to gain insight into how syntactic rules are used in practice. We generate abstract syntax trees (ASTs) for approximately 150 million SLoC, and tabulate and analyze the occurrences of all syntactic rules. In particular, to understand how syntax rules are used over time, we have checked out over 13,000 versions from the studied projects’ revision histories to understand rule usage evolution.
We also perform depth-2 bounded nesting analysis to investigate dependent rule usage. Indeed, when using a grammar to parse a string, some nonterminals in the grammar can be reached only after another nonterminal has been traversed. For X, Y ∈ N, the set of nonterminals, and α, β ∈ (N ∪ T)* where T is the set of terminals, we write X→*αYβ to denote that Ydepends on X. We bound this dependency because, in the limit, all nonterminals vacuously depend on the grammar’s start symbol. In this work, we consider k=2 and report our dependency results for X→2αYβ, as these short range dependencies are closer to the sentences that programmers write and think about and thus are better candidates for identifying idioms.
In summary, this paper makes the following contributions:

•It presents the first effort in 30 years to conduct a large-scale, comprehensive, empirical analysis of the use of language constructs in a modern programming language, namely Java;•This work is the first to study dependent rule usage and quantify its contextual nature. This is also the first to study the evolution of rule usage over time, the adoption of new rules, and how new rules impact the usage of pre-existing ones.•The results show that: (i) 20% of the most-used rules account for 85% of all rule usage, while 65% of the least-used rules are used < 5% of the time and 40% only < 1% of the time; (ii) 16.7% of the rules are unpopular and are adopted in < 25% of the projects (e.g. assert statement, labeled statement, and empty statement); and (iii) for dependent rule usage, 6% of the combinations exhibit strong dependency with > 50% probability.
Taken together, our results permit language designers to empirically consider whether new constructs are likely to be worth the cost of their implementation and deployment. They also identify boilerplate (i.e. repetitive rule usage) that new constructs may profitably replace. For example, we have observed a reduced use of anonymous class declarations, while an increased use of the enhanced-for constructs w.r.t. all syntactic rule usage. We believe that work like ours enables data-driven language design, analogous to how Cocke’s study at IBM in the 1970s on the actual usage of CISC instructions eventually led to the RISC architectures.
