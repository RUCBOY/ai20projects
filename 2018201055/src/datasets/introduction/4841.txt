Subjectivity detection and sentiment analysis consist of the automatic identification of the human mind’s private states, e.g., opinions, emotions, moods, behaviors and beliefs [1]. In particular, the former focuses on classifying sentiment data as either objective (neutral) or subjective (opinionated), while the latter aims to infer a positive or negative polarity. Hence, in most cases, both tasks are considered binary classification problems.
To date, most of the work on sentiment analysis has been carried out on text data. With a videocamera in every pocket and the rise of social media, people are now making use of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa, Facebook) and audio files (e.g., podcasts) to air their opinions on social media platforms. Thus, it has become critical to find new methods for the mining of opinions and sentiments from these diverse modalities. Plenty of research has been carried out in the field of audio-visual emotion recognition. Some work has also been conducted on fusing audio, visual and textual modalities to detect emotion from videos. However, a unique common framework is still missing for both tasks. There are also very few studies combining textual clues with audio and visual features. This leads to the need for more extensive research on the use of these three channels together. This paper aims to solve the two key research questions given below -

•Is a common framework useful for both multimodal emotion and sentiment analysis?•Can audio, visual and textual features jointly enhance the performance of unimodal and bimodal emotion and sentiment analysis classifiers?
Studies conducted in the past lacked extensive research [2], [3], [4] and very few of them clearly described the extraction of features and fusion of the information extracted from different modalities. In this paper, we discuss the feature extraction process from different modalities in detail and explain how to use such features for multimodal affect analysis. The YouTube dataset originally developed by [5] and the IEMOCAP dataset [6] were used to demonstrate the accuracy of the proposed framework. We used several supervised classifiers for the sentiment classification task: the CLM-Z [7] based method was used for feature extraction from visual modality; the openSMILE toolkit was used to extract various features from audio; and finally, textual features were extracted using a deep convolutional neural network (CNN). The fusion of these heterogeneous features was carried out by means of multiple kernel learning (MKL) using support vector machine (SVM) as a classifier with different types of kernel.
The rest of the paper is organized as follows: Section 2 proposes motivations behind this work. Section 3 discusses related works on multimodal emotion detection, sentiment analysis, and multimodal fusion. Section 4 describes the used datasets in detail. 5 Extracting features from visual data, 6 Extracting features from audio data, and 7 explain how visual, audio, and textual data are processed, respectively. Section 8 proposes experimental results. Section 9 proposes a faster version of the framework. Finally, Section 10 concludes the paper.
