Engineering computations often employ cheap response surfaces that mimic the input/output relationship between an expensive computer model’s parameters and its predictions. The essential idea is to use a few expensive model runs at particular parameter values (i.e., a design of experiments [1]) to fit or train a response surface, where the surface may be a polynomial, spline, or radial basis approximation [2], [3]. The same scenario motivates statistical tools for design and analysis of computer experiments [1], [4], [5], which use Gaussian processes to model uncertainty in the surrogate’s predictions.
The cost of constructing an accurate response surface increases exponentially as the dimension of the input space increases; in approximation theory, this is the tractability problem [6], [7], though it is colloquially referred to as the curse of dimensionality [8, Section 5.16]. Several techniques attempt to alleviate this curse—each with advantages and drawbacks for certain classes of problems; see [9] for an extensive survey. One idea is to identify unimportant input variables with global sensitivity metrics [10] and fix them at nominal values, which effectively reduces the dimension for response surface construction; Sobol’ et al. studied the effects of such coordinate-based dimension reduction on the approximation [11]. A generalization of coordinate-based dimension reduction is to identify unimportant directions—not necessarily coordinate aligned. If the scientist can identify a few important linear combinations of inputs, then she may fit a response surface of only those linear combinations, which allows a higher degree of accuracy along important directions in the input space.
A ridge function [12] is a function of a few linear combinations of inputs that takes the form g(UTx), where x∈Rm, U∈Rm×n with n<m, and g:Rn→R. The term ridge function is more commonly used when U is a single vector (n=1). Pinkus calls our definition a generalized ridge function [12, Chapter 1], though Keiper uses the qualifier generalized for a model where U depends on x [13]. A ridge function is constant along directions in its domain that are orthogonal to U’s columns. To see this, let v∈Rm be orthogonal to U’s columns; then (1)g(UT(x+v))=g(UTx+UTv︸=0)=g(UTx).If U is known, then one need only construct g, which is a function of n<m variables. Thus, constructing g may require exponentially fewer model evaluations than constructing a comparably accurate response surface on all m variables.
Let f:Rm→R represent the simulation model’s input/output map to approximate, and let its domain be equipped with a probability function ρ:Rm→R+. The function ρ may model uncertainty in the simulation’s input parameters, which is a common modeling choice in uncertainty quantification [14], [15]. The ridge approximation problem may be stated as: given f and ρ, find g and U that minimize the approximation error. After a brief survey of related concepts, we define a specific ridge approximation problem in Section 2. We then study a particular U derived from f’s gradient known as the active subspace [16]. We show that, under certain conditions, the active subspace is nearly stationary—i.e., that the gradient of the objective function defining the approximation problem is bounded; see Section 3. This result motivates a heuristic for the initial subspace when fitting a ridge approximation given pairs {(xi,f(xi))}. In Section 4, we show a simple bivariate example that exposes the limitations of the heuristic. We then study an 18-dimensional example from an airfoil shape optimization problem where the heuristic succeeds; in particular, we demonstrate a numerical procedure for estimating the active subspace using samples of the gradient, and we show how the estimated active subspace is a superior starting point for a numerical optimization heuristic for fitting the ridge approximation.
1.1. Related conceptsThere are many concepts across subfields that relate to ridge approximation. In what follows, we briefly review three of these subfields with citations that point interested readers to representative works.1.1.1. Projection pursuit regressionIn the context of statistical regression, Friedman and Stuetzle [17] proposed projection pursuit regression with a ridge function model: (2)yi=∑k=1rgk(ukTxi)+εi,where xi’s are samples of the predictors, yi’s are the associated responses, and εi’s model random noise—all standard elements of statistical regression [18]. The gk’s are smooth univariate functions (e.g., splines), and the uk’s are the directions of the ridge approximation. To fit the projection pursuit regression model, one minimizes the mean-squared error over the directions {uk} and the parameters of {gk}. Motivated by the projection pursuit regression model, Diaconis and Shahshahani [19] studied the approximation properties of nonlinear functions (gk in (2)) of linear combinations of the variables (ukTx in (2)). Huber [20] surveyed a wide class of projection pursuit approaches across an array of multivariate problems; by his terminology, ridge approximation could be called projection pursuit approximation. Chapter 11 of Hastie, Tibshirani, and Friedman [21] links projection pursuit regression to neural networks, which uses ridge functions with particular choices for the gk’s (e.g., the sigmoid function). Although algorithm implementations may be similar, the statistical regression context is different from the approximation context, since there is no inherent randomness in the approximation problem.1.1.2. Gaussian processes with low-rank correlation modelsIn Gaussian process regression [22], the conditional mean of the Gaussian process model given data (e.g., {yi} as in (2)) is the model’s prediction. This conditional mean is a linear combination of radial basis functions with centers at a set of points {xi}, where the form of the basis function is related to the Gaussian process’ assumed correlation. Vivarelli and Williams [23] proposed a correlation model of the form (3)C(x,x′)∝exp−12(x−x′)TUUT(x−x′),where U is a tall matrix. In effect, the resulting conditional mean is a function of linear combinations of the predictors, UTx—i.e., a ridge function. A maximum likelihood estimate of U is the minimizer of an optimization similar to the one we define for ridge approximation; see Section 2. Bilionis et al. [24], use a similar approach from a Bayesian perspective in the context of uncertainty quantification, where the subspace defined by U enables powerful dimension reduction.1.1.3. Ridge function recoveryRecent work in constructive approximation seeks to recover the parameters of a ridge function from point queries [25], [26], [27]. In other words, assume f(x)=g(UTx) is a ridge function; using pairs {xi,f(xi)}, one wishes to recover the components of U. Algorithms for determining U (e.g., Algorithm 2 in [25]) are quite different than optimizing a ridge approximation over U. However, the recovery problem is similar in spirit to the ridge approximation problem.
