Time series prediction, as a major branch of dynamic data analysis, has gathered extensive interest in many research fields [1]. In this field, various forecasting methods, such as univariate and multivariate models, local and global models, have been developed [2]. In univariate forecasting methods, the future direction of a time series is detected only by studying its past values, while multivariate methods usually model the dependency structure between the time series. Global methods are applicable to predict the demand of large amount of similar time series, where model parameters are collectively assessed based on all the time series, while in local methods, parameters are individually assessed for each time series.
Most existing time series prediction methods suppose that sufficient labeled data are available. However, this assumption is invalid in some actual applications. To address this limitation, it is desirable to leverage the knowledge digesting from the labeled data of similar nature to the target prediction task. Transfer learning can be applied to this situation [3].
As an important component of deep neural networks, convolutional neural network (CNN), provides an attractive option in the classification community [4]. Profited from the specific layered structure, CNN commonly facilitates the recognition of meaningful characteristics of input data. Its translation invariant property in time-frequency domain allows for efficient extraction and processing of features. And compared to some other neural networks, parallelization can be more easily implemented with the support of convolution operations. Recently, techniques based on CNN have also been developed for sequence modeling issues. In [5], a winning CNN-based WaveNet is proposed and successfully applied to raw audio generation techniques. In this architecture, causal and dilated convolutions are employed to ensure the data ordering and extend the receptive fields. Inspired by WaveNet, in [6], authors tactfully improve the deep convolutional networks and successfully apply it to time series. Motivated by the attractive success of CNN in various fields, in this work, a novel deep transfer learning method in virtue of the architecture of CNN, denoted as DTr-CNN, is proposed. It inherits the advantages of CNN and tries to alleviate the problem of insufficient labeled data. Due to the chronological characteristic of time series, causal convolutions is incorporated in the CNN architecture.
In many practical cases, due to information missing or high cost of manual annotations, the scarcity of labeled training data may often occur. To alleviate this problem, leveraging knowledge from different datasets to help accomplish the target prediction task is worth considering. However, though several related datasets are existent, the relevance degrees between them and the target dataset usually appear ambiguous. Blindly transferring knowledge from datasets less relevant to the target one may be a drag on the prediction performance. Considering this issue, DTr-CNN firstly, from several potential datasets, selects a most suitable one as the source dataset, and then utilizes it to help accomplish the target prediction task.
In the field of transfer learning, some excellent approaches have been proposed recently. For example, in [7], a novel Convolutional Fuzzy Sentiment Classifier is proposed. It creatively applies the combination of deep convolutional networks and fuzzy logic to sentiment analysis, and successfully solves the multi-domain and multi-modal transfer challenge. Since in many actual cases, time series, such as the univariate ones, as part of our research, may be single-modal with only one time-variate sequence. Multi-modal transfer is possibly difficult to implement under these scenarios. Hence, the proposed DTr-CNN algorithm does not consider multi-modality but focuses on extracting effective knowledge, such as the CNN network parameters, from the source domain. Unlike the implementation of transfer learning in time dimension in our previous work [8], which employs long-historical data as source domain data, transfer learning process is implemented across different datasets in this work. Since blindly employing all of the potential source datasets for training may result in unsatisfactory performance, selection of a proper source dataset is of great significance. In our method, we incorporate dynamic time warping (DTW) and Jensen-Shannon (JS) divergence to quantify the similarity between each potential source dataset and the given target dataset. In this way, both inner distributions of aligned series and shapes can be considered, which offers a general estimation of distance between the target and potential source domains. Fig. 1 intuitively shows the process of our proposed method.Download : Download high-res image (1MB)Download : Download full-size imageFig. 1. The overall flow diagram of DTr-CNN.
The main contributions of this work can be summarized as follows:
Firstly, to alleviate the data absence problem, we construct a CNN-based transfer learning framework. Considering the chronological order of time series, causal convolution is applied to CNN layers to ensure that prediction is completed without any future data.
Secondly, we attempt to perform the transfer learning process across different datasets in this work. It is challenging because, in some real-world scenarios, some of the available potential source domains might be less related to the target task. Fondly using all of them may not yield preferable results. To address this problem, we try to select a suited source dataset based on the incorporation of DTW and JS divergence.
Thirdly, instead of transferring knowledge learned from source domain by only fine-tuning, we try to embed the transferring phase to feature learning process. In this way, the recognition of meaningful patterns similar to source samples can be accomplished. Knowledge extracted from the source domain can then be applied to the target task. Besides, since CNN exhibits superior feature extraction ability and is generally used in classification scenario, the proposed method employs stacked casual convolutions as the automatic feature extractor and applies it to forecasting problems.
The rest of this paper is organized as follows. In Section 2, we briefly review some previous work about time series analysis. In Section 3, the details of the proposed method DTr-CNN are elaborated. Experiments and corresponding analyses are implemented in Section 4. Finally, we summarize our work in Section 5.
