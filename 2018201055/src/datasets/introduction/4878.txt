Occlusions impose significant challenges to many computer vision applications. Since it is often difficult to directly solve the occlusion problem using conventional imaging procedures from a single viewpoint, a natural solution would be to extract and process information from multiple viewpoints. In particular, camera array based synthetic aperture photography (SAP) has been shown to be a powerful technique for occluded object imaging [1], [2] or tracking [3]. In this technique, images from a camera array are warped by some projection matrices induced by a virtual focal plane. The camera array is utilized to simulate a virtual camera with a large synthetic aperture so that its depth of field is very limited. Thus, objects on the virtual focal plane would stay sharp, while occlusions, which are off the focal plane, would be blurred.
The intention to use multiple cameras in an array is to acquire and integrate visual information of the occluded object from different viewpoints. Therefore, the number of viewpoints and their perspectives are important factors of the performance of imaging through occlusion. However, in a typical camera array system, the number of cameras and their positions are fixed. Not only it is inconvenient to add new cameras to the array, but also additional calibration work is needed if the positions or the perspectives of the cameras in the array are changed. As well, it is expensive to build and difficult to move a camera array with many cameras.
Recently, more and more computer vision researchers are interested in smartphones and tablet computers for data capturing. These mobile devices are equipped with one to two on-board cameras as well as multiple micro-electromechanical sensors like gyroscope and accelerometer. Moreover, thanks to the built-in operating system, data from the cameras and the inertial sensors are synchronized, which makes smart mobile devices an excellent camera-IMU (Inertial Measurement Unit) system. In this paper, we intend to utilize such a camera-IMU system that moves on a planar surface to simulate a camera array. As shown in Fig. 1 (a), by moving a tablet on a plane, the 4D light field data of the scene can be captured and directly used to generate the Synthetic Aperture Image (SAI) (Fig. 1(b)) of the scene at a certain focal depth. Based on the geometric properties, we show that the problem can be decomposed into two parts and handled using separate data and thus, the computational complexity is reduced. Compared to conventional fixed camera array systems, we could get more flexible viewpoints when capturing the light field data of the scene with a much lower cost. As well, the system is light and mobile.Download : Download high-res image (457KB)Download : Download full-size imageFig. 1. (a) System setup: a handheld tablet is moving on a planar surface to capture the 4D light field of the scene. (b) An example of the synthetic aperture image that focus on a poster (“Now Hiring” in ASCII code).
The core of SAP is to estimate the transformation matrices that project multiple images onto a virtual focal plane. In this paper, we first analyze the geometric properties of our system. Since the transformation matrices can be factorized into an infinite homography and a 2D shift, a novel and effective framework to generate the SAI using both the camera and the IMU data is proposed. The inertial sensor data is used to estimate the infinite homographies, and the camera data is used to incrementally estimate the 2D shift, which is proportional to the relative translations between two frames. We compare our SAI result with a benchmark result on a publicly available Stanford Light Field dataset [4] to show the correctness of our approach. Extensive experimental results on datasets captured by ourselves show the performance of our approach.
The rest of the paper is structured as follows. In Section 2, several related works are summarized. In Section 3, we describe the geometric foundation of the projection procedure in the proposed approach. Section 4 presents the proposed approach. Section 5 describes the details of the experimental results. We give the limitations and conclude the paper in Section 6.
