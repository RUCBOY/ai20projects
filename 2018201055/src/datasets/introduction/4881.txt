Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification (He, Zhang, Ren, Sun, 2016, Simonyan, Zisserman, 2015), object detection (Redmon, Divvala, Girshick, Farhadi, 2016, Ren, He, Girshick, Sun, 2015b), and activity recognition (Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2015, Karpathy, Toderici, Shetty, Leung, Sukthankar, Fei-Fei, 2014, Simonyan, Zisserman, 2014). Given enough data, deep convolutional neural networks (CNNs) rival the abilities of humans to do image classification (He et al., 2016). With annotated datasets rapidly increasing in size thanks to crowd-sourcing, similar outcomes can be anticipated for other focused computer vision problems. However, these problems are narrow in scope and do not require holistic understanding of images. As humans, we can identify the objects in an image, understand the spatial positions of these objects, infer their attributes and relationships to each other, and also reason about the purpose of each object given the surrounding context. We can ask arbitrary questions about images and also communicate the information gleaned from them.
Until recently, developing a computer vision system that can answer arbitrary natural language questions about images has been thought to be an ambitious, but intractable, goal. However, since 2014, there has been enormous progress in developing systems with these abilities. Visual Question Answering (VQA) is a computer vision task where a system is given a text-based question about an image, and it must infer the answer. Questions can be arbitrary and they encompass many sub-problems in computer vision, e.g.,
•Object recognition - What is in the image?•Object detection - Are there any cats in the image?•Attribute classification - What color is the cat?•Scene classification - Is it sunny?•Counting - How many cats are in the image?
Beyond these, there are many more complex questions that can be asked, such as questions about the spatial relationships among objects (What is between the cat and the sofa?) and common sense reasoning questions (Why is the girl crying?). A robust VQA system must be capable of solving a wide range of classical computer vision tasks as well as needing the ability to reason about images.
There are many potential applications for VQA. The most immediate is as an aid to blind and visually impaired individuals, enabling them to get information about images both on the web and in the real world. For example, as a blind user scrolls through their social media feed, a captioning system can describe the image and then the user could use VQA to query the image to get more insight about the scene. More generally, VQA could be used to improve human-computer interaction as a natural way to query visual content. A VQA system can also be used for image retrieval, without using image meta-data or tags. For example, to find all images taken in a rainy setting, we can simply ask ‘Is it raining?’ to all images in the dataset. Beyond applications, VQA is an important basic research problem. Because a good VQA system must be able to solve many computer vision problems, it can be considered a component of a Turing Test for image understanding (Geman, Geman, Hallonquist, Younes, 2015, Malinowski, Fritz, 2014b).
A Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images (Geman, Geman, Hallonquist, Younes, 2015, Malinowski, Fritz, 2014b). Passing this test requires a system to be capable of many different visual tasks. VQA can be considered a kind of Visual Turing Test that also requires the ability to understand questions, but not necessarily more sophisticated natural language processing. If an algorithm performs as well as or better than humans on arbitrary questions about images, then arguably much of computer vision would be solved. But, this is only true if the benchmarks and evaluation tools are sufficient to make such bold claims.
In this review, we discuss existing datasets and methods for VQA. We place particular emphasis on exploring whether current VQA benchmarks are suitable for evaluating whether a system is capable of robust image understanding. In Section 2, we compare VQA with other computer vision tasks, some of which also require the integration of vision and language (e.g., image captioning). Then, in Section 3, we describe currently available datasets for VQA with an emphasis on their strengths and weaknesses. We discuss how biases in some of these datasets severely limit their ability to assess algorithms. In Section 4, we discuss the evaluation metrics used for VQA. Then, we review existing algorithms for VQA and analyze their efficacy in Section 5. Finally, we discuss possible future developments in VQA and open questions.
