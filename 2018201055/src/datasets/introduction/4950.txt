Local detectors and descriptors which can locate and describe meaningful, stable, and representative keypoints in an image have become prevalent in diverse areas in computer vision, such as object and scene recognition [1], [2], 3D object reconstruction [3], visual tracking [4], [5] and multimedia information retrieval [6], [7], [8], [9], [10], [11], [12], [13], [14]. Most of the local keypoints algorithms contain two parts: a detector and a descriptor. The detector locates a set of distinctive points which can be invariant to various transformations (e.g. scaling, translation, viewpoint changes), meanwhile the descriptor encodes the important information from the local patch centered on the keypoint into a feature vector, which allows to reliably match correspondences across different transformations of the same object or the same scene.
Typically, object recognition, 3D reconstruction and visual tracking mainly rely on the correctly matched correspondences between two compared images. These applications start by extracting local descriptors from each image and insert the obtained local descriptors into an index space for efficient correspondence matching. The RANSAC algorithm [15] is further adopted to eliminate outlier matches and to estimate the homography between the compared images. Therefore, a local detector with high repeatability and a local descriptor with discriminatory power is required for these applications.
Moreover, empirical experiments conducted over the past decade have demonstrated that one of the most popular and successful approaches towards similar image and visual concept analysis is to use local keypoints combined with visual words and approximate nearest neighbors (ANN) search [16]. This is mainly the result of local descriptors that are distinctive under various geometric transformations and also the introduction of the visual words model, which significantly improved the search efficiency and the adaptability to a particular image dataset. Current visual words systems are predominantly built using SIFT [17] and SURF [18] whose descriptor type is a real value. In contrast to the real value descriptors, binary string descriptors were proposed in order to generate the feature descriptors more efficiently (i.e. BRIEF [19], ORB [20], BRISK [21], FREAK [22], BinBoost [23] and LATCH [24]). Another goal of this paper is to give insights into the performance and requirements of these descriptors for large scale image search.
However, accurate correspondence matching under large viewpoint changes is still a major challenge, because greater image viewpoint transformations result in a significant decrease of saliency and repeatability of keypoints. Yu et al. [25] proposed to use the framework of fully affine space to overcome this issue. The basic idea behind the framework of fully affine space is that the projective transformation induced by camera motion around a smooth surface can be approximated by an affine transformation. A notable method is ASIFT which generates all image views in the whole affine space and extracts SIFT local features in these synthetic images to increase the matching precision. As the high dimensionality of the SIFT descriptor leads to a high computational complexity in the framework of fully affine space, we combine the recent lower computational complexity local detectors and descriptors with the framework of fully affine space and evaluate their performance under the extreme viewpoint changes.
Several related reviews present the performance evaluation of various local detectors and descriptors. In contrast to these related local detector and descriptor reviews [5], [26], [27], [28], [29], [30], [31], [32], our work mainly focus on evaluating the performance of visual words representation conducted on local descriptors for large scale image search, as well as testing the viewpoint invariance of each local detector and descriptor in the fully affine space.
The main contributions of this paper are summarized as follows:
First, the repeatability performance and the computational cost of each local detector are presented. Additionally, the efficiency and accuracy of both the real valued descriptors and binary string descriptors in terms of recall and precision on two benchmark datasets are evaluated.
Second, the visual words constructed from real value descriptors and binary string descriptors are evaluated for the application of large scale image search.
Third, we calculate the accuracy and time complexity of each local detector and descriptor in the framework of fully affine space such that researchers could make a trade-off between precision and efficiency under extreme viewpoint changes.
The rest of the paper is organized as follows: In Section 2, we present the background as well as an overview of recent local detectors and descriptors. In Section 3, we present the generation of real value type and binary string type visual words. In Section 4, we describe the details of the fully affine space framework. The details of experimental setup are described in Section 5. The evaluation results and discussions are given in Section 6, and conclusions are given in Section 7.
