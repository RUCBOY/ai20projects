At present, most commodity CPUs feature instruction set architectures that can utilize vector instructions on multiple data sets coupled with a vector processing unit (VPU) that can process instructions in a single clock cycle. This is especially true of CPUs tailored for numerical computations, such as Intel's Xeon Phi processor, which features a 512-bit wide VPU. Consequently, programmers must effectively expose data-level parallelism in their applications in order to maximize the performance on these architectures. While many scientific codes can be optimized for such architectures with modest changes (e.g. modifying data layout and alignment or reordering loops), data-level parallelism poses a problem for Monte Carlo (MC) neutron transport because it is characterized by the frequent use of conditional branching, random memory access patterns, and performance that is often limited by memory latency due to high cache miss rates.
Researchers agree that significant changes in algorithms and data structures will be necessary in order to exploit vectorization in MC neutron transport simulations. One approach that has been attempted is to restructure the transport algorithm from one that is history based, wherein the basic unit of work is the history of a single particle from birth to death, to one that is event based, wherein the unit of work is a particular event within a single particle's history. What constitutes as an event may differ from one implementation to another, but generally events can be categorized into actions such as a collision with a target nuclide, the free flight of a particle between two spatial locations, or a particle crossing a material interface. Event-based algorithms were initially proposed and studied [1], [2] in the 1980s when the largest supercomputers at the time relied on vector processors. The algorithms were implemented in several codes, including a general geometry, continuous-energy MC code [3].
The past few years have seen a resurgence of interest in event-based algorithms inspired by the higher levels of data-level parallelism inherent in two products aimed at high-performance computingâ€”the Intel Xeon Phi processor and the NVIDIA Tesla GPU. Three recent studies considered vectorization of one-dimensional, one-group MC neutron transport codes [4], [5], [6]. Early attempts have also been made at employing event-based algorithms for more general, continuous-energy MC codes targeting GPU architectures [7], [8]. In addition to the efforts in the nuclear engineering community, we note that similar algorithms are being explored in high-energy physics [9], [10], [11].
As discussed in [1], restructuring a Monte Carlo code to employ an event-based algorithm is not a trivial change because all the data structures and loop organization need to be modified. Often, architecture-specific optimization is also necessary in order to attain optimal performance. The goal of the present study is not to look at a specific implementation of event-based algorithms but rather to study them from a theoretical standpoint. Specifically, this study seeks to determine practical limits on the efficiency of event-based algorithms based on data flow within a hypothetical implementation. Coupled with data from a full-scale Monte Carlo code, OpenMC [12], we can quantify limits without the need to implement an event-based algorithm in a real code. We view this as an easier first step toward understanding the performance of the event-based algorithm if it were to be implemented in a vectorized general geometry, continuous-energy Monte Carlo code.
