Design as an activity can be interpreted in different ways: as a social practice that requires interaction between designers (Oak, 2011), as a cognitive process in the form of a dialogue between the designer and their sketch (Goldschmidt, 1991), or as an elicitation of tacit knowledge by the designer (Henderson, 1998). In all, action, interaction, and communication are essential components of the designer's process. To understand the designer's thinking and the design process, it is necessary to study how designers interact with each other and their environment.
Such studies, often falling under the broader category of protocol studies involve recording the design session on video and audio, followed by transcribing, segmenting, and coding such data. Dinar et al. (2015) discuss such analyses in their detailed review of empirical studies of design, including traditional protocol analyses that involve coding designer utterances and actions. Typically verbal data forms the basis of such analyses, such as think-aloud protocol analysis where a designer verbalizes her activities, giving insight into her thought process (Ericsson, 2006), or latent semantic analyses of a design team discussion to assess coherence between team members (Dong, 2005).
Analysis of verbal data requires manual transcriptions of participant verbalizations by transcribers—often lacking domain knowledge—provided with a relevant glossary of terms. These transcriptions and segmentations then need to be verified by employing multiple coders. The design researcher may not be the one coding the data, although they may need to explore the data to identify questions that need answering. In addition to audio and video data, protocol studies often also involve analysis of sketches, notes, and other “marks-on-paper” (Ullman et al., 1990, p.269) that are integral to designer behavior. To designers, sketches function as external memory, or as a medium to think. To analysts, sketches are externalizations of the designer's thoughts, providing insights into their mind. Sketches are also logs of their thought sequences.
Qualitative analyses of protocol studies—both in-situ and in-vivo—may require making sense of multimodal data. This data can include, but is not restricted to, audio/video recordings, artifacts created by subjects, or electronic records of user activity, typically through the use of Computer-Aided Qualitative Data Analysis Software (CAQDAS). However, while there are many options in CAQDAS software for analyzing and coding video, transcripts, and document data, there are few that provide support for broader forms of data, including movement, activity and position data from smartphones or sociometric sensors, or even psychophysiological measurement devices such as electroencephalography. Dinar et al. (2015), in their review of 25 years of research in design theory and methodology, suggest the use of automated data collection and analyses to process the increasing volume and heterogeneity of data collected in such studies.
To address these challenges, we look to the field of visual analytics for inspiration. Visual analytics is “the science of analytical reasoning facilitated by interactive visual interfaces” (Thomas and Cook, 2005, p. 4). Specifically, it focuses on interactive visual representations that exploit the human ability to spot patterns and anomalies, while allowing the computer to process large datasets. Information visualization (Infovis), defined as a “graphical representation of data or concepts” (Ware, 2012, p.2), naturally forms an essential part of visual analytics. Keeping these needs and possibilities in mind, we present VizScribe, a web-based framework that supports the representation of multimodal data, including traditional as well as newer forms discussed earlier. The framework allows analysts to generate appropriate visual representations of temporal datasets, and link them to the existing video and transcript displays. These visualizations include line or area charts, timeline plots, color maps, or any other visualization that the user can construct using the underlying JavaScript library. VizScribe supports interactive, coordinated visualizations that can be customized. The coordination between visualizations is performed through brushing and linking (Li and North, 2003), an infovis technique that, when the user selects a part of the data shown on visualization, updates the other coordinated views to reflect the selection. The technique allows analysts to easily and intuitively explore the data and interactively code it. In the context of protocol analysis, this supports contextual exploration of data, allowing the analyst to attend not only to singular data sources, but also intersectional and contextual factors. This in turn helps analysts develop a “thick description” (Geertz, 1973) of the designer's behavior: a description that explains both the mechanics and the context of the behavior.
The contributions of this work are twofold: (1) a web framework for protocol analysis inspired by visual analytics techniques, and (2) results from formative and summative studies of protocol analysts using the framework to analyze a team brainstorming session. The framework itself enables the analyst to:
•Explore and analyze time-stamped verbal protocol data by creating coordinated, interactive timeline visualizations, word-cloud views, and transcript views that are linked to the corresponding video data,•Visualize other protocol data such as server logs and biometric/sociometric sensor data in the form of interactive timeline visualizations, linked to the visualizations of the verbal protocol data above,•Select and code the verbal protocol data, and finally•Customize or add new timeline visualizations that are pertinent to their study.
To test the usefulness and to refine the usability of the VizScribe framework, we performed two studies: (1) a formative study involving both prescribed and open coding tasks, and on further refining the framework, (2) a summative study in which participants analyzed a 60 min design session through video, transcript, sketch, speech, and activity data. Our studies demonstrated that timeline and text views of the transcript, and the timeline view of sketching activity were the most widely-used visualizations for most tasks, while word cloud visualizations were used as a filter to identify salient parts of the transcript. We observed that participants both with and without training in CAQDAS tools were comfortable using VizScribe to sift through the data. VizScribe allowed analysts to orient themselves with a medium that they found most comfortable—sketches, transcript, or the video of the design session shown—with little difference in the task outcome or the user feedback.
In the following sections, we review related work in protocol studies, motivate the need for visualization techniques to represent both temporal and text data, and detail the design and implementation of the VizScribe framework. We then describe the user studies, and participant feedback, finally highlighting challenges and future work.
