Computer systems have become inseparably entangled with people's daily lives, ever growing in complexity and sophistication. Apart from many beneficial effects, research has also explored unpleasant experiences that result from engaging advanced technologies. A prominent contribution to this field, the uncanny valley theory (1970) by Japanese robotics engineer Masahiro Mori illustrates how complex human-like replicas (such as robots and digital animations) can evoke strong feelings of eeriness if they approach a high level of realism while still featuring subtle imperfections (Fig. 1).Download : Download high-res image (101KB)Download : Download full-size imageFig. 1. Uncanny valley model (redrawn from Mori, 1970).
Although its basic assumptions have remained mostly unchanged for more than four decades, the model has not lost any relevance due to the continued success and advancement of digital technology. Even more so, the exploration of uncanny valleys has ceased to be a merely academic venture, as modern robotics keep unfolding their economic potential and big-budget entertainment media stand and fall with the perception of their virtual characters (Barnes, 2011, Tinwell and Sloan, 2014).
Traditionally, research on the uncanny valley effect has focused on an object's specific appearance or motion patterns to explore which features might come across as abnormal and unsettling (Bartneck et al., 2009, Hanson, 2006, Seyama and Nagayama, 2007). As numerous studies have succeeded in exposing such visual imperfections and connected them to negative evaluations, the phenomenon has been framed by theories such as pathogen avoidance (Ho, MacDorman, & Pramono, 2008), mortality salience (MacDorman & Ishiguro, 2006) or the fear of psychopathic individuals (Tinwell, Abdel Nabi, & Charlton, 2013). Pursuant to these evolutionary psychological approaches, the aversion against human-like entities with slight defects might serve as part of a behavioral immune system (Schaller & Park, 2011), shielding individuals against potential dangers to themselves or their progeny.
Concurrently, another research direction has put aside evolutionary factors in favor of an underlying cognitive dissonance effect as explanation for the uncanny valley (Ramey, 2005, Yamada et al., 2013). This theory builds upon the paradigm that people use a combination of perceptual cues and former experiences to categorize a subject (e.g., as “human“ or “robot”) so that they can efficiently anticipate its behavior. Once they encounter an entity that violates their expectations, however, observers are likely to experience cognitive dissonance, which then manifests emotionally as uneasiness, disgust, or fear. Notably, this line of thought corresponds to one of the first definitions of the “uncanny“ term by German psychologist Ernst Anton Jentsch, who coined it as an eerie sensation arising from “doubts about the animation or non-animation of things” (Jentsch, 1906, p. 204). More than a hundred years later, Jentsch's conceptualization has become firmly embedded in the natural sciences, as studies applying eye-tracking and neuroimaging methods continue to support the cognitive dissonance hypothesis (Cheetham et al., 2013, Saygin et al., 2012). At the same time, literature has remarked upon stimulus novelty as an essential factor for mental categorization conflicts (Grinbaum, 2015); given multiple interactions, people should be able to form new templates for elements that have repeatedly defied expectations, resulting in the “infill“ of previously prevalent uncanny valleys. On the other hand, with analogue and digital human simulations advancing constantly, categorization conflicts might just shift to higher levels of realism, as people get increasingly sensitive in detecting visual flaws (Tinwell & Grimshaw, 2009).
1.1. Mind in a machineApart from the many studies on visual influences, a large body of research has demonstrated that the attribution of certain mental capacities (such as goal direction and interactivity) is also an important factor in the perception of an entity's animacy and therefore its categorization (Fukuda and Ueda, 2010, Tremoulet and Feldman, 2006). As most modern computers and robots can provide an animate impression by acting in seemingly goal-directed ways, people have been shown to “apply social rules and expectations“ to them (Nass & Moon, 2000, p. 87), inferring ideas about a machine's “personality” or some form of digital mind. However, research has also indicated that people tend to attribute only one of two mind dimensions to non-human entities: Unlike experience (defined as the ability to feel), they merely ascribe agency (the ability to plan and act) to their technology, reserving the former as a distinctively human trait (Gray et al., 2007, Knobe and Prinz, 2008). Even more so, a pioneering experiment by Kurt Gray and Daniel Wegner has illustrated that blending this differentiation—by presenting a “feeling“ computer system, even without mention of a human-like appearance—could lead to significant unease among participants (Gray & Wegner, 2012). In another study of the same paper, the authors found that a human subject bereft of any emotions was also rated as eerie, hinting at the possible uncanniness of emotional experience from the other side of the man-machine continuum. Following this groundwork, research about job replacements by robots has shown that people feel increased discomfort if they consider losing an emotion-related job to a machine, rather than one that relies on cognitive tasks (Waytz & Norton, 2014). In contrast to this, studies on embodied conversational agents in training contexts have indicated that people might actually prefer a digital character that expresses emotions to a neutral counterpart (Creed et al., 2014, Lim and Aylett, 2007). Recent findings from the field of social robotics even suggest that people may only rely on visual cues to assess a human-like entity, taking its presumed mental abilities into little consideration (Ferrari, Paladino, & Jetten, 2016).Undoubtedly, the diversity of these results invites further investigation of the circumstances under which attributions of mind place a creation into an uncanny valley. It seems particularly necessary to explore different facets of artificial minds that eventually contribute to observers' discomfort. As research has indicated that people feel anxious about machines expressing their own emotional experience (Gray & Wegner, 2012), it stands to reason to focus next on machines that also understand emotional experience in others—considering that feelings are rarely confined to a single consciousness, but serve a social function between individuals (Frijda & Mesquita, 1994). Therefore, a scenario in which digital entities recognize emotional states and react to them in a socially aware manner should shed new light on the uncanny valley of mind—a phenomenon that might, after all, relate to a basic understanding of human uniqueness.
1.2. Threats to human distinctivenessThroughout history, many cultures have regarded a consciousness enriched by emotional states as inherently human domain, closely related to philosophical concepts like a person's spirit or soul (Gray, 2010). Although theology, natural sciences and social studies vary in their understanding of artificiality and spiritual essence, the Cartesian interpretation of humans as “ghosts“ in (bodily) machines has been a prominent philosophical consensus for many, especially Christian, civilizations (Fuller, 2014). Influenced by countless myths about golems, homunculi and other revolting creations, “the Western man puts all his pride in [a] delta which is supposed to be specifically human” (Kaplan, 2004, p. 477)—a mental (and, to some, spiritual) component that clearly distinguishes humans from other beings. Considering the long-standing prevalence of this worldview, it can be argued that many people would sense a fundamental threat to their identity—their differentia specifica—if previously “soulless“ machines began to share their more complex mental abilities. In consequence of this threat to human distinctiveness hypothesis, the aversion against intelligent non-humans constitutes a sociocultural form of threat avoidance (MacDorman & Entezari, 2015), which serves to protect not only the individual, but also humanity in general. As culture studies reveal a more generous conceptualization of the “soul” in East Asian societies (Kaplan, 2004), this theory also accounts for the higher robot acceptance in countries like Japan; their inhabitants, influenced by everyday Buddhism and Shintoism, might simply be more accepting of “spirited“ machines instead of feeling replaced or violated (Borody, 2013, Gee et al., 2005).However, not only the possession of mental states, but also the ability to ascribe them to oneself and others—known as social cognition or having a theory of mind (Premack & Woodruff, 1978)—has been discussed as essential difference between humans and other creations (Adolphs, 1999, Gallagher and Frith, 2003, Pagel, 2012, Vogeley and Bente, 2010). Although studies continue to present evidence for a basic theory of mind in some animal species (Call and Tomasello, 2008, Tomonaga and Uwano, 2010), the declaration of humans as “pride of creation“ due to abilities like perspective-taking and empathic processing remains widespread. More recently, research on the role of a mirror neuron system in human neurophysiology has offered scientific footing to the anthropocentric idea of human uniqueness (Azar, 2005, Iacoboni, 2009), albeit not without controversy (Spaulding, 2013). From a more philosophical standpoint, the idea of superior human minds might even emerge as a species-related form of narcissism, which arguably presents itself in the careless destruction of other creations. But no matter if mental states and the ability to ascribe them are interpreted as a natural product of neurological processes, spiritual privilege or the essence of human exceptionalism, it seems likely that the human identity would suffer severe consequences if virtual entities demonstrated their own, sophisticated theory of mind. Even more than two decades ago, when artificial intelligence was far less refined than it is today, scientists worried about diffusing the long-standing dichotomy of man and machine, and advised caution in the development of new human-like features (Nass, Lombard, Henriksen, & Steuer, 1995).
1.3. The current studyTo explore the presented reasoning, this paper focuses on the perception of emotions and social cognition in a human replica as primary cause for an uncanny valley response. Following the theoretical groundwork, we devised an experiment that manipulated the mind attribution to human-like characters, while keeping constant their visual appearance and verbal expressions. Several groups of participants observed the same friendly and empathic dialogue scene of two virtual characters, but received different instructions as we claimed the 3D models to be either human-controlled “avatars“ or computer-controlled “agents”. Secondly, we manipulated the alleged autonomy of the characters, stating the dialogue to be either a work of the controller's “own imagination“ or an intensely prepared script. In summary, this resulted in a 2 × 2 factorial design with the conditions “human, scripted”, “human, autonomous“, “computer, scripted” and “computer, autonomous“.According to the interpretation of social cognition as distinct human privilege, we expected an alleged artificial intelligence (“computer, autonomous“) that shows awareness of another character's emotions to strongly violate category expectations. Specifically, we theorized that only such an autonomous agent—but not a scripted one—would be attributed the mental processes underlying its empathic behavior, and that this digital social cognition would appear uncannily human. At the same time, we suspected a human avatar that only acts as a “vessel” for scripted content to cause more eeriness than an autonomously acting one, as it approaches the category border between human and non-human from the other side of the uncanny valley.H1aPeople will perceive autonomous virtual agents that display emotions and social cognition as more eerie than scripted virtual agents.H1bPeople will perceive autonomous human avatars that display emotions and social cognition as less eerie than scripted human avatars.Taking inspiration from the work by Gray and Wegner (2012), our assumptions intended to advance their notion of uncanny minds by turning the emotional computer into a social entity—a machine that perceives emotions, interprets them, and adapts its behavior accordingly. As we expected a particularly strong aversion against such a category-defying creation, we further hypothesized that autonomous virtual agents would receive the lowest attractiveness rating due to the subconscious impulse to avoid further contact.H2People will perceive autonomous virtual agents that display emotions and social cognition as more human-like than scripted virtual agents.H3People will perceive autonomous virtual agents that display emotions and social cognition as least attractive among the four groups.
