The discontinuous Galerkin (DG) finite element method [1], [2], [3], [4], [5], [6] is currently the most rapidly developing method in the field of computational fluid dynamics. The growing popularity is mainly due to its stability, robustness, low artificial damping and the ability to achieve high-order spatial accuracy. The DG discretization produces a large number of degrees of freedom in comparison with the finite element method or even more so to the finite volume method for the same computational mesh. In other words the DG method has higher computational demands. Papers [7], [8] marginally deal with this drawback and compare different time integration methods, namely implicit [9], explicit [4] and explicit local time stepping methods [7], [8], [10]. The outcome of studies [7], [8] is that the computational efficiency of the implicit and explicit local time stepping schemes are comparable, whereas classical explicit schemes are considerably less efficient. In this paper, we employ implicit methods. Namely the backward Euler method is sufficient alternative for problems of finding the steady state. In order to find the time dependent solution we choose a second-order implicit method. We solve the resulting system of linear equations by GMRES [11] with the Jacobi preconditioner.
The main target of this study is to overcome high computational demands of the DG method using parallel computing. The aim is not only to be able to perform the parallel computation on a supercomputer, but also on PCs which might be found in an average office or in a computer laboratory in a college. Such computers are typically connected by slow Ethernet or Wi-Fi and have various hardware setups and various operating systems installed. The computation may run in the background while the computers are being used, since the hardware is rarely fully utilized during ordinary office work. For these purposes the Java programming language seems to be an appropriate choice, since software developed in Java is not dependent on the type or version of the operating system. It is still a common misconception that code written in Java, which is a dynamically compiled language, is considerably slower in comparison with statically compiled languages such as C, C++ or Fortran. While there is still a noticeable difference between the execution times in favour of statically compiled languages, the gap significantly shrank after the introduction of the JIT compiler in 1998 and after the release of extensive performance updates to the JIT compiler in the following years. The fact that Java is catching up with C and Fortran was already reported in 2001 by Bull et al. [12] who concluded that ’the performance gap between Java and more traditional scientific programming languages is no longer a wide gulf’. We believe that although the performance of Java does not match the performance of C, C++ or Fortran, the ease of coding and platform independence compensates for this inconvenience. Furthermore, if necessary, the critical sections of the code can be written in other languages and then included in the Java application using the Java Native Interface (JNI).
The communication among computers is usually realized by the Message Passing Interface (MPI). In the present work, we apply the Java Remote Method Invocation (Java RMI) included in the Java programming language. The Java RMI technology enables to call a Java method on a remote virtual machine and, as opposed to MPI, is easily applicable to a heterogeneous computer network. Studies [13], [14] compare Java RMI with MPI.
For parallelization of algorithms for solving boundary value problems on systems with distributed memory, the domain decomposition method is commonly used. Examples of domain decomposition methods are the Schwarz method [15], [16], [17], [18], [19] or the Schur complement Method [20]. In this work, there are two levels at which the implicit DG algorithm is parallelized - among different nodes (computers) in a computer network and within each one. At the level of network, we employ the Schwarz method [15], [16], [17], where the computational domain is divided into several overlapping sub-domains. The computation is then performed on each sub-domain by a different node. At the level of individual nodes, we subject the GMRES solver to parallelization. More specifically, the individual vector operations involved in GMRES (e.g. matrix-vector multiplications) are parallelized within each node using Java threads.
This paper also addresses stabilization of the DG method. The dissipation due to the numerical fluxes, which arises from the jump terms, is not sufficient to stabilize the solution in presence of shocks when high order of approximation is used. One option is to damp the solution near shocks, which in combination with mesh refinement is an effective approach. A very popular form of damping is adding artificial viscosity into the shock regions [9], [21], [22]. Another possibility is to subject the solution in the critical region to a limiting process [23], [24], which unfortunately decreases approximation order. We adopt a novel technique developed by Huerta et al. [25], which stabilizes the approximate solution near a shock with the aid of numerical fluxes, by introducing basis functions, which have discontinuities inside elements, in regions with a shock. We also propose a simplification of this stabilization approach, which decreases the implementational and computational demands and is still fairly effective.
We choose three test problems, namely the subsonic viscous flow in a 2D convergent channel, supersonic inviscid flow in the 2D Mach 3 wind tunnel with a step and subsonic inviscid flow in the 2D GAMM channel. For each test problem we perform several computations on various numbers of nodes and threads and calculate the efficiency and speedup of the parallelization.
The outline of this paper is as follows. In Section 2 we define the mathematical model based on the compressible Navier–Stokes equations in two dimensions and briefly review the DG discretization along with the implicit time integration. The stabilization approach is presented in Section 2.3. Section 3 contains the main topic of the present paper, which is parallelization. The numerical test are performed in Section 4 and their results are discussed and conclusion is drawn in Section 5.
