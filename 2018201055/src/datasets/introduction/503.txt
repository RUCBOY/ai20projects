IDC reports that the digital universe creates and copies 4.4 zettabytes data in 2013, and by 2020, it will reach to 44 zettabytes [1]. The explosive growth of massive requirements for data storage and backup leads to a serious storage crisis in data centers [2]. To mitigate storage costs, data deduplication is proposed. Data deduplication partitions the backup streams into fixed (fixed-sized partition, FSP [3]) or variable-sized (content-defined chunking, CDC [4], [5]) chunks (e.g., 8 KB on average [6], [7]), and identifies each of them via a cryptographic hash (e.g., SHA-1, MD5), commonly called fingerprint. Fingerprint index maps the fingerprints of stored chunks to their physical addresses in disk, and chunks with the same fingerprints are considered redundant. During backup, redundant chunks are eliminated and only unique chunks are stored to the disk. In the meanwhile, the fingerprints of unique chunks are added to the fingerprint index. Storage space can be saved by 10% or more in backups of primary workloads with data deduplication [8], [9], [10].
Millions of images are uploaded and shared every day over the Internet, e.g. Oscars-host Ellen DeGeneres' “celeb selfie” tweet was viewed 26 million times across the web during a 12-hour period [1]. To reduce the storage cost and network traffic, images are usually stored in a compressed form (PNG, JPEG, GIF, etc.). JPEG is one of the most popular still image formats, and it is widely used in our daily life (such as camera, social network, shopping website). As a lossy compression, JPEG compression [11] completely changes the binary stream on each reedit to a JPEG file, which makes it difficult to find redundancy among JPEG files. Therefore, most of existing deduplication techniques use file-level deduplication (FLD) which regards each file as a single chunk rather than chunk-level deduplication (CLD) to deal with JPEG files. That means they just find duplicated JPEG files and remove them.
In contrast to traditional deduplication techniques, we try to find duplicate data in JPEG images based on visual redundancy. Several experiments were done to compare the RGB color values of pixels in the same position of these visually identical images. The results show that more than 99% of these pixels just have few differences in RGB color value (no more than 5), and these differences are not sensible to human eyes. According to this, the visually identical images are regarded as duplicate data. Furthermore, considering that local changes commonly exist on the images (such as adding an watermark), we decide to explore the visual redundancy in chunk level rather than file level. Our deduplication approach is named PXDedup.
PXDedup decompresses a JPEG file to an image first and partitions the decompressed image (instead of the JPEG file) into several chunks. Then, it identifies and eliminates the visually identical image chunks. Before storing unique image chunks, PXDedup recompresses them with appropriate quality parameter for further data size reduction. Our major contributions are as follows:
(1) Image deduplication. We propose a new deduplication technique aiming to eliminate visually identical data of JPEG images. It contains a new chunking algorithm for image data and a new hashing algorithm for image chunks.
(2) Two algorithms. We design a quality assessment algorithm which can quickly identify the quality level of JPEG images compressed by different applications, and a quality selection algorithm to get the appropriate quality parameter for recompression according to the value returned by the quality assessment algorithm.
(3) Throughput optimization. The image decompression process and the chunk recompression process cause additional computing overhead. We optimize it via concurrent message queues.
The remainder of this paper is organized as follows: We briefly describe related work in Section 2, and discuss the features of JPEG images in Section 3. Section 4 presents our deduplication approach PXDedup. After that, Section 5 talks about the experimental results of PXDedup and the comparisons to traditional deduplication techniques. We draw a conclusion in the last section.
