Extreme Learning machine (ELM) [8], [34], [4] is an effective and efficient neural network, which has been demonstrated in various problems such as classification & regression [26], [39], [30], clustering [16], [10], [24] and sparse learning [12], [36]. Sparse Bayesian Learning is firstly proposed in [27] with the use of Bayesian Evidence Maximization [18], [19]. In many industrial or engineering applications with limited memory and cpu (e.g., medical, fault diagnosis, onboard controls, robots devices and other Internet of Things(IoT) devices), training time is not the main concern but the execution time with high accuracy. Hence, sparse learning is important to many practical applications requiring a memory-critical or/and timely-responsive model. In sparse learning, the goal is to eliminate as many as possible the unnecessary or redundant network connections wk from output weight w=[wk]T for a sparse and accurate model.
For multi-class sparse learning, it is popular to employ class decomposition [22], [41], [11] using a set of sparse binary-classifiers, e.g., sparse extreme learning machine (SELM) [3], [2] and sparse Bayesian extreme learning machine (SBELM) [17], [35]. Similar to Support Vector Machine (SVM), SELM adds an inequality constraint into its primal optimization problem. When the inequalty constraint is satisfied, the corresponding wk is pruned for sparsity.
In SBELM, the likelihood of data is assumed to be Bernoulli-distributed using sigmoid activation as output decision function and automatic relevance determination ARD prior [4], [33] is imposed on the parameters of hidden nodes to obtain a sparse model.
However, both SELM and SBELM are proposed directly for binary classification. Although class decomposition such as pairwise coupling [13], [22] enables a set of binary classifiers for multi-class classification, there are still three drawbacks:
i)the size of the trained model becomes exponentially large when the number of classes grows up;ii)without considering multi-class data distribution, the classifier easily suffers from the issues of classification ambiguity and uncovered class regions [31], [32];iii)although the decisive class value lies between 0 to 1, it is an estimate of the multi-class probability from multiple estimated binary-class probabilities. Hence, there may exist larger bias to the quality of decision making (i.e., accuracy). For multi-class classification problems with requirement of probabilistic dististribution over classes, rather than only outputting the most likely class, both ELM and SBELM could not satisfy.
An alternative decomposition method for extending binary classifiers to multiclass, one-vs-all learning strategy is usually adopted, which may require lower computation cost than pairwise coupling, but it may be more sensitive to the problems of imbalanced dataset and usually achieves less accuracy [7]. However, both strategies are not directly built on the likelihood of data, which could not provide probabilistic uncertainty prediction over distribution of each class, and usually result to longer training time and larger model size.
Compared to class decomposition methods, direct multi-class sparse learning is preferred, e.g., optimally pruned extreme learning machine (OP-ELM) [21], and double-regularized optimally pruned extreme learning machine (TROP-ELM)[38]. OP-ELM employs multi-response sparse regression (MRSR) to rank all w, and then evaluated a subset of w based on leave-one-out (LOO) validation method. Finally a subset of w with the least LOO error is selected. TROP-ELM improves OP-ELM by replacing MRSR with L1 penalty, and also applying L2 penalty on the output weights of the model for higher stability and better model generalization.
With this selection strategy, the pruning procedure in both OP-ELM and TROP-ELM becomes sensitive to the validation set. The sparsity and accuracy of OP-ELM and TROP-ELM depend on whether the validation set is well chosen or not. However, it is difficult to determine the validation set because data distribution is usually unknown, resulting to an unstable and suboptimal sparse model. Compared to OP-ELM and TROP-ELM, SBELM [17] is more accurate and stable because Bayesian method does not require validation set for pruning.
Furthermore, in many practical applications such as medical diagnosis [25], [35], click through rate (ctr) in Ads RTB auction and recommender system [28], [23], the probabilistic outputs under Bayesian method built on estimation of likelihood of data [5], [40] can provide significant decision support, e.g. balance on accuracy and recall by tuning probability decision margin of each class, and more evaluation metrics like auc and roc [29] could be employed. In this work, SBELM is extended to direct multi-class classification without class decomposition for higher accuracy and possibly higher sparsity, along with probabilistic information.
To directly handle multi-class data, the key obstacle of SBELM is the assumption of Bernoulli distribution with sigmoid activation which was designed for binary classification only. Therefore, an original multi-class distribution called multinomial distribution with softmax activation [20] is employed to extend the SBELM framework for multi-class classification. This new method is called multinomial Bayesian extreme learning machine (MBELM). In the literature, there are two common learning strategies for sparsity, namely, ARD prior and L1 penalty, which are applied to MBELM for comparison, and they are referred to as ARD-MBELM and L1-MBELM respectively.
Both methods share the following advantages:
i)a hybrid approach that combines Bayesian method, the universal approximation, and a multi-class ELM classifier;ii)as opposed to OP-ELM and TROP-ELM, no validation set is necessary for pruning. Hence, a more stable model with high model generalisation is obtained;iii)the issue of classification ambiguity and uncovered class regions has been resolved, resulting to stable and accurate performance;iv)With class decomposition in SBELM, the number of CÃ—(C-1)/2 binary classifiers are learned to handle multi-class classification, while our method directly learns a single multi-class classifier, resulting to faster training time and execution time;v)From issue i) and iii), the multi-class probability in SBELM induces a larger bias to the final calculated multi-class probability. Our method directly obtains the multi-class probability without the issues from i) and iii).
The paper is organized as follows. Section II gives a review on SBELM, and presents the proposed MBELM, L1-MBELM, and ARD-MBELM. In Section III, the experimental setup and the evaluation of the proposed work are discussed. Finally, conclusion is given in Section IV.
