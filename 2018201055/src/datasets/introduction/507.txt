Ensemble methods are a popular approach to improving the possibilities of individual supervised classification algorithms (base learners, base classifiers) by building more stable and accurate classifiers [1]. Methods that use multiple base classifiers to make one decision are known as Multiple Classifier System (MCS) or Ensemble of Classifiers (EoC) [2] and are one of the major development directions in machine learning. MCS were also used in many practical aspects [3], [4], [5] where EoC proved to have a significant impact on the performance of recognition systems. In general, the procedure for creating an EoC can be divided into three major steps [6]:
•Generation – a phase where individual classifiers are trained [7].•Selection – a phase where only several (or one) individual models from the previous step are selected to EoC [8].•Combining – a process of combining outputs of base classifiers to obtain an integrated model of classification [9].
The combining phase can be achieved using different types of the classifier’s output: a class label [6], a subset of labels ordered by plausibility [10], a vector of all possible labels with corresponding scoring functions [11]. The scoring function may have a different nature depending on the type of individual learners. The generative classification models are probabilistic in nature, and therefore return joint probability distribution over the examples and the class labels directly. The discriminative base learners, on the other hand, focus only on the conditional relation of a class label to the given example. In this case of individual learners, non-probabilistic scores are used as classifier output. Because the various outputs have to be made comparable beforehand to represent the scoring functions in a common space, the calibration method is applied [12], [13]. The approaches of classifier output calibration focus on: continuousness, non-decreasingness, universal flexibility, and computational tractability [14].
The score function proposed in [15] provides information about the relative position of the recognized object in the feature space. This method uses the nearest neighborhood of the recognized object to calculate the score function value.
In this work, we propose a novel approach for calculating a scoring function based on the distance of the recognized object from the decision boundary of a given base classifier and the distance to the class centroid. Accordingly, we proposed that the new method for determining the scoring function takes into consideration not only the classifier’s geometrical properties of the decision boundary created by the base learner but also the geometrical properties of the input space (training subset). Therefore, our approach provides information about the relative position of the recognized object in the feature space, which depends on class centroids and not on the nearest neighborhood of the recognized object.
Given the above, the main objectives of this work can be summarized as follows:
•A proposal of a new scoring function that uses the location of the class label centroid and the distance to the decision boundary determined by the linear classifier.•The proposed score function has the same nature for all linear base classifiers, which means that outputs of these classifiers are equally represented and have the same meaning.•An experimental setup to compare the proposed method with an approach that uses only the distance to the decision boundary of the base classifier to calculate the value of a score function, as well as with other existing combining base classifiers methods using different performance measures.
The outline of the paper is as follows: In the next section (Section 2), related works are presented. The proposed algorithm is presented in Section 3. In Section 4, the experiments that were carried out are presented, while results and discussion are present in Section 5. Finally, we conclude the paper and propose some future works in Section 6.
