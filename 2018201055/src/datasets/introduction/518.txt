Deep neural networks (DNNs) are powerful models that have been widely used to achieve near human-level performance on a variety of natural image analysis tasks such as image classification [1], object detection [2], image retrieval [3] and 3D analysis [4]. Driven by their current success on natural images (eg. images captured from natural scenes such as CIFAR-10 and ImageNet), DNNs have become a popular tool for medical image processing tasks, such as cancer diagnosis [5], diabetic retinopathy detection [6] and organ/landmark localization [7]. Despite their superior performance, recent studies have found that state-of-the-art DNNs are vulnerable to carefully crafted adversarial examples (or attacks), i.e., slightly perturbed input instances can fool DNNs into making incorrect predictions with high confidence [8], [9]. This has raised safety concerns about the deployment of deep learning models in safety-critical applications such as autonomous driving [10], action analysis [11] and medical diagnosis [12].
While existing works on adversarial machine learning research have mostly focused on natural images, a full understanding of adversarial attacks in the medical image domain is still open. Medical images can have domain-specific characteristics that are quite different from natural images, for example, unique biological textures. A recent work has confirmed that medical deep learning systems can also be compromised by adversarial attacks [12]. As shown in Fig. 1, across three medical image datasets Fundoscopy [6], Chest X-Ray [13] and Dermoscopy [14], diagnosis results can be arbitrarily manipulated by adversarial attacks. Such a vulnerability has also been discussed in 3D volumetric medical image segmentation [15]. Considering the vast sums of money which underpin the healthcare economy, this inevitably creates risks whereby potential attackers may seek to profit from manipulation against the healthcare system. For example, an attacker might manipulate their examination reports to commit insurance fraud or a false claim of medical reimbursement [16]. On the other hand, an attacker might seek to cause disruption by imperceptibly manipulating an image to cause a misdiagnosis of disease. This could have severe impact for the decisions made about a patient. To make it worse, since the DNN works in a black-box way [17], this falsified decision could hardly be recognised. As deep learning models and medical imaging techniques become increasingly used in the process of medical diagnostics, decision support and pharmaceutical approvals [18], secure and robust medical deep learning systems become crucial [12], [16]. A first and important step is to develop a comprehensive understanding of adversarial attacks in this domain.Download : Download high-res image (662KB)Download : Download full-size imageFig. 1. Examples of adversarial attacks crafted by the Projected Gradient Descent (PGD) to fool DNNs trained on medical image datasets Fundoscopy [6] (first row, DR=diabetic retinopathy), Chest X-Ray [13] (second row) and Dermoscopy [14] (third row). Left: normal images, Middle: adversarial perturbations, Right: adversarial images. The left bottom tag is the predicted class, and green/red indicates correct/wrong predictions.
In this paper, we provide a comprehensive understanding of medical image adversarial attacks from the perspective of generating as well as detecting these attacks. Two recent works [12], [16] have investigated adversarial attacks on medical images and mainly focused on testing the robustness of deep models designed for medical image analysis. In particular, the work of [16] tested whether existing medical deep learning models can be attacked by adversarial attacks. They showed that classification accuracy drops from above 87% on normal medical images to almost 0% on adversarial examples. Work in [16] utilized adversarial examples as a measure to evaluate the robustness of medical imaging models in classification or segmentation tasks. Their study was restricted to small perturbations and they observed a marginal but variable performance drop across different models. Despite these studies, the following question has remained open “Can adversarial attacks on medical images be crafted as easily as attacks on natural images? If not, why?”. Furthermore, to the best of our knowledge, no previous work has investigated the detection of medical image adversarial examples. A natural question here is to ask “To what degree are adversarial attacks on medical images detectable?”. In this paper, we provide some answers to these questions by investigating both the crafting (generation) and detection of adversarial attacks on medical images.
In summary, our main contributions are:
1.We find that adversarial attacks on medical images can succeed more easily than those on natural images. That is, less perturbation is required to craft a successful attack.2.We show the higher vulnerability of medical image DNNs appears to be due to several reasons: 1) some medical images have complex biological textures, leading to more high gradient regions that are sensitive to small adversarial perturbations; and most importantly, and 2) state-of-the-art DNNs designed for large-scale natural image processing can be overparameterized for medical imaging tasks, resulting in a sharp loss landscape and high vulnerability to adversarial attacks.3.We show that surprisingly, medical image adversarial attacks can also be easily detected. A simple detector trained on deep features alone can achieve over 98% detection AUC against all tested attacks across our three datasets. To the best of our knowledge, this is the first work on the detection of adversarial attacks in the medical image domain.4.We show that the high detectability of medical image adversarial examples appears to be because adversarial attacks result in perturbations to widespread regions outside the lesion area. This results in deep feature values for adversarial examples that are recognizably different from those of normal examples.
Our findings of different degrees of adversarial vulnerabilities of DNNs on medical versus natural images can help develop a more comprehensive understanding on the reliability and robustness of deep learning models in different domains. The set of reasons we identified for such a difference reveal more insights into the behavior of DNNs in the presence of different types of adversarial examples. Our analysis of medical adversarial examples provides new interpretations of the learned representations and additional explanations for the decisions made by deep learning models in the context of medical images. This is a useful starting point towards building explainable and robust deep learning systems for medical diagnosis.
The remainder of this paper is organized as follows. In Section 2, we briefly introduce deep learning based medical image analysis. In Section 3, we provide an introduction to adversarial attack and defense techniques. We conduct systematic experiments in Sections 4 & 5 to investigate and understand the behaviour of medical image adversarial attacks. Section 6 discusses several future work and summarizes our contributions.
