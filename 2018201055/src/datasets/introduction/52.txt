Since the origin of numerical simulations of radiative transfer, it has been claimed that statistical approaches such as Monte Carlo were the only practical “way or path” towards the simultaneous handling of all the spectral and geometric complexity of radiation in 3D realistic systems. This complexity tends to infinity as soon as either
1.the degree of detail that describes the heterogeneous medium of propagation tends to infinity,2.the spectral resolution of the optical properties of the medium tends to zero,3.Fredholm integral equations are involved, e.g., in the context of multiple scattering, which theoretically yields an integration domain of infinite dimension.
The reason why “Monte Carlo is the only numerical tool that passes infinite dimension” is the use of the double randomization technique: the expectation of a linear function of another expectation is still just one expectation, or(1)E[h(E[X])]=E[h(X)]where h is a linear function and X a random variable. This is trivial and can be easily demonstrated using the linearity of the integral operator. However, the most immediate implication of this property is the key to the renowned power of Monte Carlo: when estimating the expectation of a random variable that is a linear function h of the expectation of other random variables (representing, for example, spectral, spatial or directional variables), then only one sample of each “secondary” random variable is needed to provide one sample of the global random variable. This technique is described in detail in Sabelfeld [2] and identified as a key point in the Monte Carlo community of applied mathematics (see, for example, [3], [4]).
However powerful, this property is strictly limited to cases where the h function is linear. Radiative transfer is, of course, “linear transport” and the exponential extinction of a beam (after Beer’s law) is a signature of this linearity. But in the context of Monte Carlo methods, with the necessary writing of the computed quantity as an expectation, the nonlinearity of the exponential function is at the origin of severe difficulties as soon as the medium of propagation is spatially heterogeneous. Indeed, the integral over the heterogeneous extinction field appears within the exponential function, whereas the integration over multiple scattering optical paths appears “outside” the exponential (these paths define the line-of-sight along which Beer’s extinction is applied). The integrals are combined nonlinearly; double randomization can no longer be employed (Eq.  (1) is no longer true); a crucial feature of the Monte Carlo technique is lost (see [5] for more advanced considerations on the matter).
A solution to bypass this nonlinearity without biasing the results or decreasing convergence rates has been commonly used in various parts of the Monte Carlo literature [6], [7], [8], [9], [10], [11]. The main idea is to make use of “null colliders”, fictitious particles that are added to the true extinction field in order to make it homogeneous, which reduces the optical-depth integral to a simple product. When a path encounters one of these null colliders, it simply continues forward as if the collision had not occurred. From an intuitive or physical point of view, null colliders are pure scatterers characterized by a strictly-forward phase function. In the scope of probability theory, null collisions can be seen as the rejected samples that come from sampling a density function that overestimates the frequency of collisions instead of the true distribution of free-path lengths, as per a standard rejection method.
The null-collision method has long been considered a numerical trick to avoid a heavier, deterministic integration of the extinction field. The fact that the line-of-sight integration was shifted from inside to outside the exponential in the underlying mathematical formulation was only recently made explicit, in the seminal paper of Galtier et al. in 2013 [1]. Subsequently, the authors and colleagues have highlighted the important implications of this reinterpretation for a diversity of research and applied fields, including combustion, spectroscopy, solar energy, atmospheric radiative transfer and image rendering [5], [12], [13], [14], [15].
Some examples of the new ideas that could only be explored and implemented thanks to the revisiting of the null-collision method are listed hereafter:
•Villefranque et al. [15] investigated the question of the acceleration of path tracing in spatially heterogeneous media using null collisions in combination with tools from the computer graphics community. At the junction of physics and computer graphics, they structured the data describing highly heterogeneous extinction fields into octrees to allow both fast traversal of the arbitrarily complex spatial domain and limited time spent in sampling null-collision events. In the same way, the field of image rendering has largely benefited from the formal framework that resulted from Galtier et al.’s work in 2013, bringing them to develop and implement new null-collision algorithms with renewed efficiency and confidence [16], [17].•Tregan et al. [18] propounded a solution to avoid a convergence issue due to the use of null-collision algorithms to compute sensitivity estimates of a radiative quantity. This was only possible by shifting from the intuitive to the probabilistic point of view on the null-collision method, thereby modifying the integral formulation associated with the first null-collision algorithm into a new formulation (and hence, algorithm) yielding better statistical properties.•Galtier et al. [13] have recently applied the idea of using null collision algorithms to spectrally integrate radiative quantities without pre-computing the absorption spectra. In this proposition, the null collisions are again a way to bypass the nonlinearity of the exponential: by shifting the sum over the energetic transition line contributions to the local absorption coefficient from inside to outside the exponential function, the double randomization technique can again be used where it could not have previously been applied without the null-collision method. In the conception of this algorithm, the formal or mathematical point of view was infinitely more helpful than the intuitive or physical one.
Obviously, since their first apparition in the literature, justifications for the use of null-collision algorithms (NCA) have been based on various arguments, depending on the community and context of the application. As the method enjoys growing interest and is used by many scientists from fields as diverse as those cited above, we find it interesting to suggest a classification of the different interpretations of the concept of null collisions. Our intention is not to compare or rank these viewpoints, but rather:
1.to try and reduce the classification to a limited number of ideas (that is, three points of view);2.to argue that although the second and third viewpoints are less immediate than the similitude viewpoint (the most standard definition of null collisions), they produce quite different theoretical developments toward the same conclusions, offering new formal perspectives;3.to illustrate the practical benefits that can be expected from these new perspectives (algorithms that would be difficult to establish from only the similitude viewpoint).
In the following sections, we present three readings of the method that we think allow convenient changes in perspective regarding the concept of null collisions: from the physical viewpoint (Section 2, a similitude), to the statistical (Section 3, a rejection method), to the mathematical (Section 4, a Taylor expansion of the exponential). To illustrate the practical signification of these three viewpoints, we briefly describe recent research that was made possible or facilitated by a shift in the mental representation of the null-collision concept.
The ideas behind these examples and the supporting illustrations are not always original, in the sense that some have been published already. The originality of this work rather resides in the reformulation of the null-collision concept, as well as in the connections that are made between the reported studies. Furthermore, as this research covers a wide range of scientific fields, publications were sometimes addressed to specialists of a particular domain rather than to the community of radiative transfer scientists. With this paper, we hope to facilitate access to this literature.
