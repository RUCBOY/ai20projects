Spoken words are the result of a complex orchestration involving the shaping of sound produced by vocal cords as air is expelled by the lungs (Sataloff, 1992). The shaping is done by the tongue, cheeks, teeth, and lips as they move (see Hiiemae, Palmer, 2003, Perkell, Cohen, Svirsky, Matthies, Garabieta, Jackson, 1992). As the facial biomechanical structure moves to produce precise sounds, it moves in a manner that has an underlying uniqueness between vocalizations of the same words. For example, when one says the word “two”, the jaws barely move. On the other hand when one says the word, “seven”, the jaw drops while producing the first “e” then closes to allow the bottom lip to produce the “v”, soft “e”, and “n” sounds.
For people speaking over the phone, these movements can be propagated through the side of the face and cheek bones, into a phone. This causes slight perturbations of the motion sensors in the device. Fig. 1 shows a user speaking on phone in a typical posture. The phone is in contact with the side of the face around the ear and upper jaw. As this region moves back and forth during speech, some of the movement (or, vibration) is transmitted to the phone and captured by its motion sensors. Our core research question is whether these vibrations might be distinct enough to decode information about the speech content or identity of the user speaking on the phone.Download : Download high-res image (268KB)Download : Download full-size imageFig. 1. Location of the phone on the face of a user during a typical phone conversation. As the jaws and face move during the articulation of words, motion sensors in the phone pick up some of these minute movements. Our work explores if these movements might be distinct enough to enable the decoding of spoken words. Observe that the phone’s speaker is directed straight into the ear while the microphone is directed at the mouth. The user’s quest to attain this speaker-to-ear and microphone-to-mouth mapping is the reason why most users hold the phone in some variation of this general posture. However, the predictability of this phone holding posture means that certain jaw movements emanating from speech might produce consistent patterns in sensor data (within a reasonable margin of error), which in turn lends itself to potential attacks such as the one studied in this paper. Photograph by Hassan OUAJBIR, via Unsplash (OUAJBIR, 2019).
To understand the underpinnings of this threat vector, we initiate our investigations by selecting a subset of phonetics from the International Phonetic Alphabet (IPA) and studying their relationship with smartphone motion sensor patterns. Because phonetics are the canonical building blocks of speech, they capture the “atomic-level” facial movements whose aggregation produces the more sophisticated word-level face movement patterns that an attacker using this attack vector would seek to exploit. We find several of the phonetic sounds to imprint distinctive patterns on the motion sensor data. Building on these observations, we take the case study of the digits zero to nine to evaluate how accurately words could be inferred from motion sensor data. In practice digits would be of significant interest to certain attackers since sequences of digits are often meant to remain secret such as social security numbers, PINs, bank records and phone numbers, all of which are high value targets for hackers. We find a number of users to be highly vulnerable to the word-level attack as we get digit recognition accuracies of 50+% in the best attack configuration.
Because the attacker would in practice need to pinpoint the sensor data segment for which digits are spoken in order to decode them, we also build a classifier that separates digits from other English words that users spoke during free conversations. We show the classifier to accurately separate digits from the other English words for users in our population. Finally, we investigate if the motion sensor vibrations could be used to determine user identity and gender. Such a line of attack would for instance be of interest to intelligence agencies seeking to track a high value target who might regularly switch devices. The same line of attack could however also be used for benign purposes — e.g., by banks seeking to augment their already existing mechanisms to more strongly validate the identities of callers. We again find certain segments of the user population to produce motion sensor patterns that enables highly accurate prediction of identities and gender.
The contributions of our paper are as follows:
•A New Attack on Speech Privacy Using Facial Biomechanics: We propose a new attack on smartphone call privacy using movements of the facial musculature during the pronunciation of words. To develop the attack we first explored the relationship between a selection of phonetics from the International Phonetic Alphabet (IPA) and features generated from accelerometer measurements on a smartphone. In addition to phonetics, we also collected data when words containing those phonetics were spoken on the phone. This experiment showed that not only can phonetics be differentiated from one another, but that words are more clearly separated due to their containing a combination of phonetics. This preliminary investigation prompted the development of additional experiments to evaluate the risk posed by this side-channel attack. Our work suggests a new family of attacks based on smart devices placed against or connected to the head (e.g., smartphones or smart earbuds with sensors)•Evaluating Word-Level Inference Attacks: Building on patterns observed during initial experiments using phonetics, we study how motion sensor data can predict digits (0–9) spoken on the phone. We show, based on a deep neural network comprised of Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM) layers, that for the most vulnerable users, spoken digits can be recognized with an accuracy of up to 50+%. Even for the least vulnerable users, we find that the attack outperforms random guessing, making it appealing to the attacker even in its worst case scenario. Further, additional experiments to classify speech as digits or non-digit words achieved an accuracy of 80+%, indicating that the attacker would feasibly delimit the sensor segment mapping to numbers before embarking on decoding them. Although our study is focused on digits for the sake of focused exposition, its implications are beyond just these 10 digits. Our findings imply that an entity aiming to detect a small dictionary of words (e.g., see trigger words used by Homeland Security (D. of Homeland Security, 2020)) might be able extract them via biomechanical movements extracted from motion sensor data.•Inference of User Identity and Gender: Finally, we study the question of whether the face movements captured by the phone’s sensors during phone calls could be used to infer the identity and gender of the user. We find that the user’s (or victim’s) identity could be determined with an accuracy of between 36% and 45% on the first guess, depending on the type of content being spoken (digits or non-digits). Additional experiments find that the user’s gender could be determined with a classification accuracy of between 64% and 88% depending on whether some data from the victim is part of the training set or not. Potential applications of the gender and identity inference attack include the surveillance of high value targets (e.g., terrorists) who might use multiple devices over time or activists in nations where their activity is not sanctioned by the government. Our findings, coupled with the fact that motion sensor data on certain devices (e.g., Android) can be accessed by any app without restriction, reveal that such surveillance might be feasible.•Demonstrating the Effect of Variations in Hardware and User Behavior: Finally, we conducted a series of experiments to evaluate the impact of variations in hardware and user behavior on the performance of the attack. Unlike our other experiments, participants used 10 different models of phone for this investigation. The phones held by users for these experiments had a weight ranging from 5.1 to 7.2 ounces, with a length of 5.84–6.39 inches, and width of 2.76–3.06 inches. It’s possible that varying sizes of phone are held differently by a user, causing the attack to misidentify movements of the face if it is not trained on similar data. Additionally, phones might propagate facial movements to the device’s sensors in a different manner based on weight or size characteristics. Another difference between devices was in the sampling rate of the sensors, with devices ranging from 50 to 243 Hz for the accelerometer and gyroscope. Users also held their devices differently, with some users only having contact between the phone and their ear, while others held the device at drastically different angles (e.g., nearly vertical or horizontal). We found that large variations in either the sampling rate of the data or the way the participant held the phone led to the attacks performing poorly. For example, participants that either held the phone with the only contact point between their phone and head being their ear were highly resistant to attacks classifying the digit spoken or the participant’s gender. These results suggest that the viability of our attacks are somewhat dependent on the ability of attacker to obtain a large dataset with a wide variety of user behavior and hardware.
Road-map: The rest of the paper is organized as follows. We develop the intuition behind the attacks in Section 2 and review related work in Section 3. Then we describe our threat model in Section 4, data collection experiments in Section 5, the neural network architecture in Section 6, and analyze and discuss the results of our attack in Section 7. Section 8 is a discussion of our process and results for evaluating how differences in hardware and user behavior impact the viability of our attacks. We finally provide an overall discussion and conclusions for this work in Section 9.
