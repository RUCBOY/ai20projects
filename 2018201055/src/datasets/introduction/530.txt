1Problems such as global warming, national and international energy supply, water complications, growing fuel costs, and computational business economics entirely bring the necessity for energy and performance, consequently, cost-efficient computation into sharp focus. Depletion in power plants that operate using coals, specifically, in the UK, offering an estimated safety margin for energy [i.e. capacity and demand ratio] of just 0.29% in 2017 (Shehabi et al., 2016), and the termination of several nuclear power plants in Germany and France, bring the actual risk of power outages and load-shedding in the very near future. Due to growth in renewables, a minor upsurge in energy safety margin of the UK can be realized in 2018 (i.e. an uptake from ~29.0% to ~36.0%). If we presume similar rates of consumption to the world of about 3.0% of total energy usage, then ~9.6% rise in datacenter energy efficiency will transform to approximately two times growth in the UK's energy safety margin (Shehabi et al., 2016), (Zakarya, 2018a). Similarly (Shehabi et al., 2016), also indicates that, until 2020, datacenters energy efficiency will remain unchanged, since industrial private workloads will migrate from internal private clouds to the public clouds. However, due to increase in mobile services and number of users, Internet of Things (IoT), and computing at scale, an increasing trend in energy consumption of the current datacenters can still be seen. Such an increase in energy consumption and the expected level of service performance would certainly affect the environmental sustainability (3% Greenhouse gases), user's monetary costs and cloud economics [\euro 183.98billions in 2016 to \euro 217.05billions in 2017 - ~18% increase]. For example, AWS experienced approximately 1% reduction in their sales due to only 100 ms loss in performance (Zakarya, 2018a). Therefore, it is essential to look deeply into the problem and identify possible causes, opportunities and appropriate solutions for energy savings and performance improvements (as agreed in Service Level Agreement - SLA document) (Zakarya, 2018a), (Zakarya and Gillam, 2017).
The above issues advise the necessity to investigate for the sources and reasons of growing energy consumption in IaaS (Infrastructure as a Service) clouds and try to get rid of the reasons and/or manage them using conceivable solutions under workload performance constraints. The growing quantity and practice of ICT (Information & Communication Technology) equipment in IaaS cloud datacenters takes a consequential influence on the workload performance and IaaS energy consumption levels. Similarly, the falling practice of non-renewable energy sources, like coal, power plants, rises the necessity to design solutions to manage IaaS clouds resources in order to diminish the rising levels of energy usage, worldwide (Zakarya, 2018a). In respect of the former statement, datacenter's resources are usually under-utilised and idle; thus, making it possible to use methods like virtualisation and containerisation to save energy. In respect of the later statement, workloads might be moved, across resources powered by various energy production methods such as coal and renewables, when it is essential (as renewables are intermittent) or more beneficial (cost-efficient) to do so.
Virtualisation and containerisation enable same hardware for sharing among different users that: (i) increases resource utilisation; and (ii) creates opportunities for energy savings using resource consolidation. Besides these gains, virtualisation, containerisation and consolidation technologies could create performance-related problems due to migration and co-location (workloads compete for resources) leading to higher users’ monetary costs, VM runtimes, and energy consumption. Moreover, public clouds may also achieve IaaS energy and performance efficiencies through appropriate resource management, allocation, and placement policies (Zakarya, 2018b). In hyper-scale cloud environments such as Intel, Google and AWS, containers have nearly replaced VMs as computational instance of choice. Compared to traditional VMs, containers have lower overheads of deployment and can, therefore, offer the best performance for certain workload types, as demonstrated in (Felter et al., 2015), (Kozhirbayev and Sinnott, 2017), (Kominos et al., 2017), (Mondesire et al., 2019), (Chae et al., 2019). Various applications have dissimilar business goals; few of them might run proficiently within VMs whereas few would perform best within containers or over bare-metal resources. Additionally, through running containers in VMs, supreme levels of resource utilisation are guaranteed through consolidation. Nevertheless, this may produce performance problems, in particular, when containers and VMs are being migrated collectively crosswise heterogeneous resources. Moreover, if workloads are running over various platforms in a datacenter, then there would be various migratable entities such as containers, VMs, hybrid (containers—VMs) and bare-metal workloads. Some of them would be more effective than the others and vice versa. For example, inter-platform migrations may occur in a particular platform; and intra-platform migrations may occur within platforms.
VMs and containers have allowed the quick adoption of the cloud computing environment, and the necessities, in terms of utility computing, moved to incorporate various kinds of sand-boxing technologies including virtualisation, containerisation, bare-metal and virtualised containerisation (Kominos et al., 2017). Generally, HPC (high performance computing) workers will favour provisioning the raw hardware (bare-metal) in order to deploy and run their workloads which decreases the hazards of performance degradation due to virtualisation. This is evidenced through the recent introduction of the bare-metal instances in the AWS cloud; which allows users to have full control over their provisioned resources. Moreover, certain workloads would perform better on containers than VMs and vice versa. For example, bank applications would run more securely in VMs than containers (isolation). In such circumstances, as shown in Fig. 1, variations in applications runtimes would create questions on datacenter energy consumption, workload performance and cloud economics i.e. users monetary costs and energy bills. For example, Fig. 1 demonstrates that the performance of Bzip2 workload over E5-2630 (CPU model) significantly varies across various platforms i.e. 300–500 min on bare-metal, 400–500 min on VMs, 300–500 min on containers, 550–700 min on containers over VMs. This suggests that the containerised applications' performance is comparable to the bare-metal infrastructure. The CPU models correspond to processor families with different clock speed, instruction set architecture (ISA), cache size, type, and performance variations during the fabrication process. Interested readers should refer (O'Loughlin, 2018), for further discussion of various CPU models and workload performance.Download : Download high-res image (696KB)Download : Download full-size imageFig. 1. Variations in applications performance when running over various sand-boxing technologies and CPU models [from left to right and top to bottom: VMs, containers, containers—VMs and bare-metal] – performance of Bzip2 workload over E5-2630 significantly varies across various platforms.
Big data and IaaS providers, e.g. Intel, Google, Microsoft, Rackspace, and AWS, examine and explore leading-edge solutions made over the VMs and/or containers technologies. The desires of these progresses partake a crucial influence on the IaaS management systems that are utilised to design dedicated services to handle with heterogeneities of resources and users' workloads. In consort with the difficulty of resource management system, up till then, such evolutions had been accomplished independently, deprived of demonstrating whether accurate abstractions will let the supervision of any type of sand-boxing technologies in a combined way (centralised) or in a distributed style. Furthermore, how various combinations of resource allocation and migration policies would affect IaaS energy consumption and workload performance. These sand-boxing technologies provide possibilities of affective resource scheduling, placement and consolidation. For example, workload could be scheduled or migrated to resources where its performance, high resource utilisation and energy efficiency are guaranteed. However, consolidation requires migrations that could be expensive in regard to energy consumption and performance loss (Zakarya, 2017). Moreover, similar workloads may perform quite differently on various platforms as described above. Similarly, certain cloud users may need full access to bare-metal resources in order to get total control of their provisioned hardware. This will, probably, soon force IaaS providers to rethink of using various platforms in their datacenters. Therefore, management complexities would further grow, when cloud providers will use a mixture of these technologies – in order to maximise their resource usage and reduce their operational costs. Thus, certain workloads may execute faster over the containers, or bare-metal hardware (non-virtualised) platforms; but, might perform the worst over VMs (technical white paper, 2016). The lowest execution times might mean the highest energy efficiency, and the least users’ costs. Moreover, IaaS energy efficiency might also relate to these sand-boxing technologies, workload types and energy profiles of hardware (Zakarya, 2017).
This brings possibilities for hybrid datacenters which concurrently implement all sand-boxing technologies, e.g. the Intel's CIAO (Cloud Integrated Advances Orchestrator),2 Magnum,3 Kolla4 and, subsequently, higher opportunities for efficient workload placement, consolidation and migration decisions across various technologies. This could be achieved through clustering the IaaS resources such that each cluster corresponds to a particular sand-boxing technology. Furthermore, each cluster may have either its own scheduler or share a centralised scheduler. Using individual schedulers for each sand-boxing technology such as containerisation, virtualisation, containers—VMs, bare-metal might not be suitable regarding energy and performance efficiencies; due to the absence of entire datacenter’ state and resource usage details at each scheduling (platform) level. If these schedulers, can communicate and share entire datacenter state with each other (i.e. centralised scheduler); appropriate energy and performance efficient management decisions could be triggered (Zakarya, 2018a). Moreover, this will provide support for inter-platform and intra-platforms migrations; which are, to the best of our knowledge, unexplored in the existing literature of cloud computing. The former one occurs among the hosts of a particular platform e.g. VMs that could increase resource contention. The latter one would be more appropriate if certain workloads are misplaced during allocation. When both approaches are concurrently assumed, high levels of resource utilisation could be achieved.
This research aims to identify additional probable savings through efficient resource placement, allocation and consolidation with migrations (i.e. resource management) to reduce datacenters energy usage so that the workload performance is not affected undesirably due to resources and workloads heterogeneities. Furthermore, we investigate the impact of inter-platform and intra-platforms migrations on IaaS energy efficiency and workload performance, therefore, costs. The objective is to deal with these challenges through suggesting an architecture (reference) and a single, platform-independent, resource manager. The key contest would be, possibly, to decide the right set of abstractions for the development of a combined-style service which leverages the key approach/methodology i.e. deprived of implementing a specific, dedicated service, e.g. individual schedulers (distributed approach) and platform-specific monitoring, for each sandboxing technology. We, then, propose a centralised, workload-aware, scheduler and a consolidation technique which reduces the datacenter's energy consumption, and increases workload performance. In public clouds, reasonable best efforts would mean no loss in performance, as this will certainly affect the SLA's; and violation to SLA's would require a penalty to service providers. Whereas, in private clouds, increase in performance would be essential for certain workloads types such as HPC and database applications. We perform expansive simulations of the suggested framework using real workloads from large-scale IaaS providers such as Intel (Shai et al., 2013), Microsoft Azure (Cortez et al., 2017) and Google (Reiss et al., 2011) clusters that correspond to HPC (bare-metal), virtualisation and containerisation workloads, respectively.
The major contributions of our research are:
•a reference architecture and a single, platform-independent, resource manager is advised;•an energy, performance and cost (EPC-aware) resource scheduler is presented that could effectively manage hybrid IaaS cloud infrastructures that run different kinds of sand-boxing technologies;•an EPC-aware orchestrator is proposed that migrates various workloads energy, performance and, therefore, cost-efficiently;•in order to concurrently simulate and evaluate hybrid clouds with various sand-boxing technologies, a cloud simulator is developed; and•investigate the impact of datacenter resource configuration (physical order of hosts) on energy consumption and workload performance.
The rest of the paper is organised as follows. In Sec. 2, we discuss the resource allocation, placement and consolidation issue. In Sec. 3, we propose HeporCloud – a heterogeneity-aware hybrid approach that places and migrates workload appropriately. Sec. 4 describes various models to demonstrate energy and performance heterogeneities of various cloud platforms. We evaluate and validate HeporCloud through real workload datasets from Google, Intel and Azure clusters in Sec. 5 and demonstrate its efficiency in terms of energy, performance and, therefore, cost with respect to existing methods. In Sec. 6, we offer an overview of the related work. Finally, Sec. 7 summarises the paper along with several future research directions.
