The use of deep learning in modern applications has increased significantly in the past few years. The rapid development of artificial intelligence applications, such as computer vision, language recognition, secure communication, traffic safety, natural language processing, autonomous vehicles, etc., has further increased the use of deep learning and neural networks as a core supporting technology [1], [2], [3], [4], [5]. The scope of its use is not limited to these, rather many other research works have used it in other application areas as well [6], [7], [8], [9]. Given the tremendous benefits offered by deep learning, it is also prone to several security problems. Work by Szegedy et al. [10] was the first to describe the different vulnerabilities of the deep neural networks. The simplest and easiest method of attacking a deep learning network is through adversarial example. This attack can cheat the deep learning classifier by disturbing very little in the image, while the changes are not perceptible to the human eye. Hence, the classifier outputs a wrong label for the adversarial example with a high confidence level. The same adversarial phenomenon can occur in object detection [2]. For example, if the attacker changes the traffic signs to an adversarial image, the unmanned vehicle may ignore or incorrectly detect the traffic information, which may lead to serious consequences.
Image classification and object detection are two major applications of deep learning technologies. The attack methods in this regard can be classified as white-box or black-box attacks [11], [12], [13], [14]. For the white-box attacks, the attacker can analyze the details of the victim model, hence, the attackers can find different opportunities to implement their attack. The white-box attack methods also form the fundamental principle of the black-box attacks. On the other hand in a black-box method, the attackers cannot obtain the details of the victim model such as the infrastructure, parameters, defense techniques, etc. The only returned information is the output label (in some cases along with the confidence level). Thus, it is significantly difficult to attack the black-box model, which is why most of the applied real deep neural networks are black-box.
Several works on white-box methods are available in the literature, hence we here we only describe the most significant contributions in this regard. Szegedy et al. [10] proposed an L-BFGS attack based on the simple bound constraint. The Fast Gradient Sign Method (FGSM) was initially designed by Goodfellow et al. [15], while Kurakin et al. [16] improved it by proposing an Iterative Fast Gradient Sign Method (I-FGSM) attack. Work of Dong et al. [17] significantly improved the generation of adversarial example by Momentum Iterative Fast Gradient Sign Method (MI-FGSM), while the Jacobian-based Saliency Map Attack (JSMA) was proposed by Papernot et al. for targeted attacks [18]. Carlini and Wagner [19] presented a C & W method (Carlini and Wagner Attack) which has a very high success rate.
The black-box model attacks are less researched due to difficulty in execution as discussed earlier. The work by Papernot et al. in [20] presented a substitution model attack method by utilizing the transferability of the adversarial example. It first generates the adversarial examples using a white-box technique on the substitution model and then applies the attack on the black-box model. Su et al. [21] proposed the One Pixel attack using an adversarial example, and claims to have 98.7% average confidence by only modifying one pixel in the test image. The Boundary Attack (BA) was proposed by Brendel et al. [22] for the black-box attacks, and only depends on the outputs (such as the classification label of top-1) to successfully execute the attack.
It is important to note that as the use of deep learning techniques in different application domains has increased [23], [24], [25], [26], [27], hence the attacks on image classifiers have also become prominent. The research on adversarial examples can improve the security and safety of deep learning systems. Based on this motivation, in this work, we present a boundary attack in the frequency domain based on Discrete Cosine Transform (DCT) towards black-box object detectors. This attack generates robust object adversarial examples, which can then be used for attacking the system. The major contributions and highlights of this work are as follows.
•It is difficult to attack the black-box object detectors since the feedback information is very limited. Most of the black-box attacks are based on Gradient Descent, Boundary Attack, or other alternative models. To the best of our knowledge, this is the first work that proposes targeted black-box attacks on object detection systems based on DCT.•An object detector can recognize the position, size, and category of the objects in the image, while it is difficult to disturb the detector for designated objects since the attacker is unaware of the details of the detectors. Most of the methods implement an indiscriminate attack for all objects in the image. This is the first work, which can attack a single specific object rather than the total image on object detectors.•The query times are the key indicator for the black-box attacks, which leads to attack concealment and practical feasibility. In the proposed approach, we successfully increase the query efficiency of Boundary Attack via DCT by decreasing the query times.
