Capture–Replay tools are commonly used by testers to perform functional testing of web applications. In the web application domain, testers perform end-to-end testing of their applications by creating test scripts using Capture–Replaytools such as Selenium (The Selenium Project. http://seleniumhq.org/docs/03_webdriver.html/), TestComplete (TestComplete. https://smartbear.com/product/testcomplete/), QTP (Unified Functional Testing (UFT). http://www8.hp.com/us/en/softwaresolutions/unified-functional-automated-testing), and Watir (Watir WebDriver. http://watirwebdriver.com). These test automation tools allow the testers to record the various test scenarios as a sequence of user actions (such as mouse clicks, keyboard entries, and navigation commands) performed on the web application and later replay (re-execute) these with the same or different data to test a web application.
Similar to other systems, web applications evolve over time. The evolution of web applications can introduce several different types of changes of web elements, for example, changes in visual representation due to new style sheets being applied, changes in visual labels for various fields, or distribution of fields on one page to multiple pages. The test scripts of Capture–Replay tools are strongly coupled with the elements of web pages and are very sensitive to any change in the web elements of the web application. Even simple changes, such as slight modifications in a web page layout may break the existing test scripts, because the web elements that the scripts are referencing may not be valid in the new version. A detailed taxonomy of various types of web application test breakages is presented by Imtiaz et al. (2019) and Hammoudi et al. (2016b). Fixing these test scripts manually requires a significant effort. According to a study (Memon and Soffa, 2003), such changes can require around 75% of the test suites to be fixed. This overhead is considered as a major obstacle for organizations to move towards automated web application testing (Chen et al., 2012).
There are approaches in the literature that focus on automating the test script repair for web applications (Stocco et al., 2018, Choudhary et al., 2011, Hammoudi et al., 2016a), however, these approaches are limited and do not cater for a large number of breakages that occur in practice as defined in the web test breakage taxonomy (Hammoudi et al., 2016b). The approach presented in Stocco et al. (2018) only focuses on fixing the breakages caused due to the change of field locator. Furthermore, this approach is not applicable in case of style sheet changes or when the visual representation of page elements is changed (i.e., a button change to an icon, etc.,) and hence produces many false positives. The DOM-based web test repair approach WATER (Choudhary et al., 2011), repairs the test scripts corresponding to the locator-based changes and the broken assertions. These are only two of the five different types of breakage categories that may occur in an application (Hammoudi et al., 2016b). For example, the breakages due to change in page layouts of web applications are very common, however, such breakages are not correctly repaired by WATER. Similarly, the approach does not repair the test cases that are broken due to change in XPath. The WATERFALL (Hammoudi et al., 2016a) approach builds on top of WATER and focuses on identifying and repairing breakages that occur during intermediate commits of a web application. However, the WATERFALL approach has the same limitation as the WATER approach and both of these approaches only focus on repairing the broken locators and assertions in test scripts.
In this paper, we propose a model-based automated approach to repair the broken test scripts of Capture–Replay testing tools. The proposed approach covers the various types of changes of web elements that may result in the breakage of test scripts. The work is based on the taxonomy by Hammoudi et al. (2016b) that defines all such changes of web elements that may occur in practice. No other existing work provides such a comprehensive test repair strategy. Our approach is also not specific to any particular Capture–Replay tool. To provide a tool independent approach, we develop a UML profile that allows the capture of various concepts related to Capture–Replay test scripts for web applications. For this purpose, we extend the UML Test Profile (UTP) (UML Test Profile (UTP), 2005) and add important concepts relevant to capture and replay test scripts of web applications. Our approach uses a DOM-based approach for capturing the differences between two versions of a web application. We define a number of repair strategies corresponding to different categories of test breakages. The repaired test models can be automatically transformed into test scripts of specific capture–replay tools, such as Selenium, QTP, and Watir. We also develop an open-source tool1  that automates the proposed test repair approach.
We also evaluate the effectiveness of our proposed approach by conducting a series of four evaluations on one industrial and six open-source case studies. Our first experiment focuses on evaluating the effectiveness of the approach in terms of the number of broken test scripts that are repaired and the comparison of achieved DOM coverage of repaired and original test suite. The second experiment uses mutation analysis to compare the fault-finding capability of the repaired test scripts with the original test suite. The third evaluation focuses on the usefulness of the repair of the broken test scripts. For this, we asked a pool of experts to inspect the test repairs done by the proposed approach. In the fourth evaluation, we performed a comparison in terms of the number of test script repairs of our approach with a state-of-the-art DOM-based web test repair approach (WATER (Choudhary et al., 2011)) on the seven case studies.
Our results indicate that the proposed approach effectively repairs the 91% of broken web test scripts and achieves a similar DOM-coverage, as by the original test suite, on the evolved versions of subject applications. A team of professional testers found the suggested repairs, for different types of test breakages, useful for the regression testing of evolving web applications. Furthermore, the DOM-based fault-finding capability of the repaired test suite is equivalent to the original test suite. Our empirical evaluation on 528 Selenium web driver test scripts of seven web applications shows that the proposed can effectively repair 83% of the overall breakages, whereas the existing technique WATER repairs 58% test breakages which only includes attribute-based locators and broken assertion values.
To summarize, the main contributions of this paper are:

(1)We develop an approach for automated repair of Capture–Replay test scripts that break due to the changes in user interfaces of evolving web applications.(2)To make our approach generic, we develop a DOM difference meta-model for the representation of DOM changes detected between earlier and the modified versions of the same web application.(3)We extend the UML Testing Profile (UTP) (UML Test Profile (UTP), 2005) to capturing the details of capture–replay web test scripts and develop a corresponding UML profile for modeling the action sequences encoded in automated test scripts to provide tool independent repair solutions.(4)We devise a strategy to classify the existing test suite into reusable, retestable, and obsolete test scripts as per (Leung and White, 1989).(5)We design a set of heuristics to repair the test scripts covering all the various types of breakage categories defined in web test breakage taxonomy (Hammoudi et al., 2016b).(6)We develop an open-source tool2  to automate our approach for repairing the broken web test scripts.(7)We perform a series of empirical evaluations on an industrial and six open-source case studies to evaluate the effectiveness of our proposed approach in terms of repairing the broken test scripts, coverage of DOM elements, and fault-finding capability. We also evaluate the usefulness of the repaired test scripts according to the opinion of professional testers. We also compare our approach with the only other available DOM-based test repair approach, WATER (Choudhary et al., 2011).
The rest of the paper is organized as follows: Section 2 explains the background and running example of test breakages of evolving web applications. Section 3 defines our model-based test script repair approach for evolving web applications. Section 4 presents the tool implementation of our approach. Section 5 presents the empirical evaluation of the proposed approach. Section 6 discusses the threat to the validity of our results. Section 7 compares our work with related research and Section 8 presents the conclusion.
