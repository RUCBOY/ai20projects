In recent years, convolutional neural networks (CNNs) have been successfully applied in many computer vision fields, such as image classification [1], blind quality assessment for screen content images [2], super-resolution [3]. ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [4] accelerates the development of CNNs. The primary trend for obtaining better performance is building deeper and larger CNNs, so the networks [5], [6], [7], [8], [9] have dozens or hundreds of layers and thousands of channels. AlexNet [10] has 61 million parameters, and VGG-16 [5] has 138 million parameters. ResNet-100 [6] and DenseNet-40 [8] have 1.7 million and 1.0 million parameters, respectively, whose parameters are less than that of AlexNet and VGG by using a large number of 1×1 convolutional layers. However, these networks have abundant parameters and floating-point operations (FLOPs), and the redundant parameters lead to large memory and computational cost. Such a large number of parameters and computational cost increase processing latency and reduce energy efficiency. Therefore, the parameter redundancy is an urgent problem to be solved and the model compression work is remarkable.
Many compression methods have been proposed, which can be separated into two kinds of approaches. Compress existing architectures and design new efficient architectures from scratch. The first approach is usually based on pruning [11], quantization [12] and low-rank decomposition [13]. The second approach is usually based on two dimensions of the convolutional kernel, spatial dimension and channel dimension. In channel dimension, group convolution [14], [15] and depthwise convolution [16], [17], [18] are the popular methods to reduce parameters. In the spatial dimension, low-rank kernels [5], [19], [20], [21] are popular methods to improve parameter efficiency.
This paper mainly focuses on the spatial dimension of convolutional kernels based on the second approach. The kernel decomposition strategy can reduce parameter redundancy and computation cost. Inspired by the Flattened CNNs [19], InceptionV4 [21] and VeckerNet [20], we proposed an efficient Diagonal-kernel. Using a pair of diagonal and anti-diagonal kernels to replace a standard square kernel. Moreover, only a diagonal kernel or an anti-diagonal kernel also can replace a standard kernel because the diagonal kernel has a large local receptive field. In order to evaluate the proposed Diagonal-kernel, DiagkerNets are constructed by using diagonal-kernels. The results show that Diagonal-kernel can maintain high accuracy and reduce parameter redundancy. The proposed Diagonal-kernel is effective for both traditional convolution and depthwise convolution. However, the effective for depthwise convolution is not very obvious because the number of weights for depthwise convolution is very small.
Our main contributions are as follows:
•The Diagonal-kernel is proposed, which has fewer parameters and larger local receptive fields than standard square kernel convolution.•The standard convolutional layers can be replaced by different combining of diagonal-kernels layers and their performances are different.•Some popular CNNs are chosen to be the base model for evaluating the performance of diagonal-kernels. The results show that the diagonal-kernels layers can replace standard square kernel layers with some loss of accuracy.•Compared with Vector-kernel, the proposed Diagonal-kernel has the same parameters, higher accuracy and larger local receptive fields.
The remainder of this paper is organized as follows. In section 2, some related works about compressing CNNs are presented. The details of the Diagonal-kernel are described in section 3. The experimental results are analyzed in section 4 and we give the conclusion in section 5.
