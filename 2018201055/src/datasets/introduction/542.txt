Network representation learning (NRL), i.e., network embeddings, can map the semantic similarity of vertices into a low-dimensional space where the similar vertices presented in the network are assigned to the nearby areas in the space [8]. The learned representations are significant for the subsequent applications, such as link prediction [15], vertex classification [25], and community detection [35]. The vital part of network embeddings is to discover the neighbors of vertices and learn the representations which can well preserve the proximity in networks. For this objective, DeepWalk [25] explores neighbor relations by using depth-first search (DFS) and Line [29] utilizes breadth-first search (BFS) to discover vertex proximity. To obtain more complete exploration, Node2vec [15] employs both DFS and BFS on networks. After searching out neighbors of vertices, most of NRL methods adopt Skip-Gram [21], a language model that maximizing the probability of word co-occurrence (corresponding to vertex neighbors in graphs) within a sliding window, to learn vertex representations. Due to the computational complexity of the objective in Skip-Gram, these methods mainly use negative sampling (NS) [22] to approximate a variant objective of Skip-Gram. Though other optimization methods such as Hierarchical Softmax [24] and Noise Contrastive Estimation [17] are proposed, the experimental results in [29], [15], [26] show that NS can generally achieve better performance in large-scale datasets. The major idea of NS is to encourage a target vertex to be close to its neighbors and in the meanwhile be far from its negative samples.
Nevertheless, existing NS methods draw negative samples based on the frequencies of vertices and ignore if they are really negative to the given vertex. In other words, for a target vertex, its adjacent vertices may be drawn as the negative samples, which leads to undesirable embeddings. To address this problem, R-NS [1] defines Distance Negative Sampler which chooses negative samples for each vertex u from the set V-{N(u)∪u}, where V is the set of vertices and N(u) is the neighbors of u. To give a more clear explanation, an illustration is presented in Fig. 1. For the targeted vertex 1, R-NS aims to choose negative samples except from rank 1 neighbors1 (e.g., the direct neighbor vertices 2, 3, and 4 marked in red). However, it is not sufficient to merely take only rank 1 neighbors into consideration. For example, in the rank 2 neighbors, vertex 11 is associated with vertices 3 and 4 which are highly relevant to the targeted vertex 1. Therefore, it may lead to inferior embeddings if we choose vertex 11 as the negative sample of vertex 1. Nonetheless, to the best of our knowledge, there are no efforts have been devoted to considering the rank N neighbor information when sampling negative samples.Download : Download high-res image (182KB)Download : Download full-size imageFig. 1. An example of the neighbors for the targeted vertex 1. From center to periphery, vertices in the first circle are the rank 1 neighbors and marked in red. Vertices located in the second and the third circles are the rank 2 and rank 3 neighbors and marked in blue and green, respectively.
In addition, some recent attempts are proposed for improving NS. Chen et al. [7] propose an adaptive sampler in the Skip-Gram model which can increase the precision of the estimation in NS by ranking all words in the vocabulary every few iterations of the stochastic gradient descent. Wang et al. [37] propose a GAN-based framework that incorporates Generative Adversarial Networks to obtain high-quality negative samples with a generator and learns the embeddings of vertices with a discriminator. To reduce the training time on GAN, Zhang et al. [49] propose NSCaching which essentially is a cache mechanism for storing informative negative samples. However, all these methods draw negative samples without considering the neighbors’ information. In brief, for the targeted vertex 1 as mentioned before, they may still choose vertex 11 as its negative sample.
Moreover, it is always difficult to determine an optimal rank number for reserving neighbor areas where the vertices are not allowed to be chosen as negative samples for the targeted vertices. For example, if we only use rank 1 neighbors, some vertices in rank 2 or 3 which are highly related to the targeted vertex may be selected as the negative ones. On the contrary, if we preserve rank 2 and 3 neighbors, irrelevant vertices may be retained and lead to undesirable embeddings.
Motivated by the above discussions, in this paper we propose a new negative sampling method, called Hierarchical Negative Sampling (HNS), to model the rank N neighbor information. Rather than computing the frequencies of vertices or setting a hard threshold for reserving neighbor area, HNS adaptively discovers the latent community structures of vertices and computes the probabilities of their being negative samples. More specifically, for each targeted vertex, HNS firstly assigns it to a community, then formulates a reversed community-vertex distribution to draw negative samples for optimizing its objective function. By modeling the neighbor proximity information into the hierarchical community structures, HNS can obtain more appropriate negative samples and learn better representations of vertices.
Contributions of our work are summarized as follows:
•We propose a new negative sampling method, HNS, which can explore the latent structures of vertices and exploit the rank N neighbor information of targeted vertices when selecting negative samples. To the best of our knowledge, this paper is the first attempt to investigate drawing negative samples with modeling neighbor information.•The reversed community-vertex distributions learned by HNS can be regarded as the probabilities of vertices to be drawn as negative samples, which avoids setting rank numbers of reserved neighbors for each vertex manually.•We conduct experiments on three real-world datasets including citation networks and language networks. Comparing with the state-of-the-art models, experimental results demonstrate that HNS can achieve better performance in vertex classification tasks at different training scales.
The code and datasets are released athttps://github.com/junyachen/HNS. The rest of this paper is organized as follows. In Section 2, we give preliminaries of notations and related models. Section 3 presents our proposed model and the parameter inference method. We discuss experimental results in Section 4, and introduce the related work in Section 5. Section 6 concludes our work.
