Classification is a basic task in data mining and pattern recognition [12]. Due to its easiness to construct but surprising effectiveness, naive Bayes (NB) continues to be one of the top 10 data mining algorithms [33].
Given a test instance x, represented by an attribute value vector ⟨a1, a2, ⋅⋅⋅, am⟩, NB utilizes Eq. 1 to predict its class label.(1)c(x)=argmaxc∈CP(c)∏j=1mP(aj|c),where C is the collection of all possible class labels c, m is the number of attributes, aj is the value for the jth attribute Aj of x, P(c) is the prior probability of the class c, and P(aj|c) is the conditional probability of Aj=aj given the class c, which can be estimated by the following m-estimation:(2)P(c)=∑i=1nδ(ci,c)+1qn+1,(3)P(aj|c)=∑i=1nδ(aij,aj)δ(ci,c)+1nj∑i=1nδ(ci,c)+1,where n is the number of training instances, q is the number of classes, ci is the class label of the ith training instance, aij is the jth attribute value of the ith training instance, nj is the number of values for the jth attribute Aj, and δ(•) is a binary function, which is 1 if its two parameters are identical and 0 otherwise.
NB predicts the class label with the highest posterior probability, and its performance is competitive with other state-of-the-art classifiers such as C4.5 [9]. Nonetheless, NB makes the conditional independence assumption that all attributes are fully independent given the class. Since the assumption required by NB hardly holds true in real-world applications, a mass of enhancements have been proposed to relax this assumption. We summarize them into six categories as shown in Fig. 1.Download : Download high-res image (444KB)Download : Download full-size imageFig. 1. Different paradigms of improved naive Bayes.
Attribute weighting is practical to alleviate NB’s primary weakness [17], [21], [37]. In the attribute set, some attributes are more important than others, so they should have more influences to the final model than less important attributes. Thus, attribute weighting assigns a discriminative weight to each attribute. Then the attribute weights are incorporated into the naive Bayesian classification formula to build an attribute weighted NB.
Instance weighting is another effective method to relax NB’s primary weakness [16], [18]. Some training instances are more reliable than others, and they should have more influences to the final model than less reliable instances. Thus, instance weighting assigns different weights to different instances. Then the weight of each instance is incorporated into the formula of the prior and conditional probabilities to get a more accurate probability estimation.
To the best of our knowledge, however, essentially all the existing weighting approaches are either attribute weighting or instance weighting. Few of them simultaneously pay attention to attribute weighting and instance weighting. We argue that different attributes should have different importance, meanwhile different instances should have different reliability. Thus, in this study, we propose a new improved model called attribute and instance weighted naive Bayes (AIWNB), which combines attribute weighting with instance weighting into one uniform framework. To learn attribute weights, a correlation-based attribute weighting approach is used. To learn instance weights, we single out an eager approach and a lazy approach, and thus two different versions are created, which we denote as AIWNBE and AIWNBL, respectively. In AIWNBE, we employ an eager learning approach called attribute value frequency-based instance weighting to obtain instance weights. In AIWNBL, we employ a lazy learning approach called similarity-based instance weighting to obtain instance weights. Finally, the learned attribute weights are incorporated into the naive Bayesian classification formula, meanwhile the prior and conditional probabilities are estimated using instance weighted training data. We expect AIWNB could inherit the advantages of both attribute weighting and instance weighting simultaneously. To validate the effectiveness of AIWNB, we conduct two groups of empirical studies on a collection of 36 benchmark datasets from the University of California at Irvine (UCI) repository. The experimental results demonstrate that both AIWNBE and AIWNBL perform much better than their state-of-the-art competitors.
In summary, the main contributions and innovations of this paper can be highlighted as follows:
1)We summarize the improved NB paradigms into six categories and conduct a comprehensive survey on them. For each category, we provide detailed analysis and review on some representative approaches.2)We propose a new improved model called attribute and instance weighted naive Bayes (AIWNB) and give the general framework of AIWNB, which pays attention to attribute weighting and instance weighting simultaneously.3)We single out a correlation-based attribute weighting approach to learn attribute weights. Meanwhile, we single out a frequency-based eager learning approach and a similarity-based lazy learning approach to learn instance weights. Thus two different versions are created, which we denote as AIWNBE and AIWNBL, respectively.4)We conduct two groups of extensive experiments to validate the effectiveness of AIWNBE and AIWNBL, respectively. Furthermore, we make a thorough discussion to give some instructive suggestions on where and when do our AIWNB algorithms perform better than their competitors.
The remainder of the paper is organized as follows. Section 2 provides a comprehensive survey of the existing improved approaches for NB. Section 3 proposes our AIWNB approach. Next, Section 4 presents the experimental setup and results. Finally, Section 5 concludes the study and outlines the main directions for future work.
