As developments in computational power have steadily and tremendously increased over the past few decades, so with them the field of computational science. Digital applications have become increasingly complex and often comprise an intricate combination of scientific computational methods and data analysis. The sequence of steps in these applications is often encoded in a workflow and the need to automate these processes has led to an increase in the number of workflow management systems (WMS’s) being developed. A WMS provides the necessary functionality to define and subsequently execute workflows that essentially encode a sequence of data transformations [1]. In recent years, many WMS’s have been developed which have greatly simplified and streamlined the creation and analysis of data. However, with these improvements have come new challenges in managing the massive amounts of data that are produced.
The most apparent challenge is finding an efficient method of storing the data itself. Although simple storage approaches can solve the problem of data persistence, they fail to address the question of data reproducibility, which carries particular importance in the scientific method and plays a direct role in making data reusable, according to the FAIR Data Principles [2]. Indeed, the reproducibility of data can only be guaranteed if the provenance of data is treated with the same importance as the data itself, as it is the data’s provenance that enables its validation and verification [3], [4], [5], [6]. Here, it is critical to realise that not only the data themselves, but also the workflows that create them should be part of the tracked provenance. As a consequence of this observation, efforts are underway to extend the FAIR Data Principles to workflows as well [7]. With the rate of data production made possible by modern technologies, it has become untenable to reconstruct the provenance of data a posteriori, calling for tools that automatically record it as it is created. Some WMS’s have started addressing this challenge, however, their data and workflow provenance guarantees are often insufficient to be able to retrace the origins of a piece of data or to recreate it.
In this paper, we describe in detail the workflow system of AiiDA [8], an open-source, high-throughput, scalable computational infrastructure for automated reproducible workflows and data provenance, implemented in Python. While the design of AiiDA and its workflow system is generic enough to be applicable to any computational, and potentially experimental, scientific domain, its origin and strengths lie in applications that make use of high-performance computing (HPC) systems. Users of these environments are often used to scripting, which is why the workflow engine of AiiDA provides a rich application programming interface (API), unlike the majority of WMS that provide a graphical user interface (GUI), such as Kepler [9], Taverna [10] and Triana [11]. An API provides a more direct and seamless integration of the workflow system with the simulation codes and data analysis tools that it manages and that are typically used on HPC systems.
An additional benefit of an API-based workflow language and engine is that it allows for the definition of dynamic workflows, whose exact path is not pre-determined but evolves during execution based on the results of completed steps. In AiiDA, for example, the code that defines a workflow is directly executed by the engine and there is no intermediate translation layer. The majority of WMS’s, however, interpret workflows that are defined through a static markup language such as XML in the case of Karajan [12], custom XML derivatives such as Askalon’s [13] AGWL [14] or workflow specific standards such as the Common Workflow Language (CWL) [15]. Some of them may provide bindings to programming languages in order to define workflows through an API, such as Pegasus [16], however, this is a mere pre-processing step as the workflows are still converted into a Directed Acyclic Graph (DAG) representation in XML, before being executed by the workflow runner when launched. The big disadvantage of these document based workflow definitions is that they are static, in the sense that the exact flow of the workflow needs to be known before it is executed1. The mechanism also naturally limits the available programming structures to DAGs or directed cyclic graphs (DCG), if loops are supported by the markup language.
The recent Python-based workflow managers Signac [17] and Parsl [18] have chosen a different approach and rely on implicit dataflow to define and control workflows. In this model, new data operations, bound by data dependencies, are executed as their dependencies are fullfilled by other data becoming available in the workspace. The Fireworks system [19], which supports the definition of workflows through documents in JavaScript Object Notation [20], has made important steps toward supporting dynamic workflows. A workflow can insert new steps or spawn additional logical branches while it is running, based on intermediate results produced by previously completed steps. However, while this enables runtime-mutable workflows, specific mutations are limited by the constraints of the custom static JSON markup language through which they are defined. In contrast, workflows in AiiDA are implemented directly in Python and as such have all the dynamic expressiveness of a programming language directly at their disposal, as well as full access to the entire provenance graph with the data that is already stored in the database. This proves to be a very powerful mechanism to deal with, for example, the problem of error handling when running high-throughput simulations.
In the field of materials science specifically, other libraries and frameworks exist that provide advanced workflows with automated handling and recovery of errors encountered in ab initio calculations, such as Aflow [21], Atomate [22], MAST [23] and OQMD [24]. However, all of these are typically only compatible with one specific ab initio code (with some expanding support to others), whereas the ecosystem of density functional theory (DFT) features a great variety of popular codes [25]. By tightly coupling the WMS and the workflow implementations themselves to any single or a few codes, interoperability is naturally hamstrung. In stark contrast, the workflow system of AiiDA is completely agnostic of the external software that performs the computation and provides an integrated abstract interface for any simulation code, with built-in support for all major resource managing systems, such as PBSPro [26], SLURM [27], SGE [28] and Torque [29]. Through its flexible plugin system, AiiDA allows any code to be made compatible via plugins, which are registered on the AiiDA registry [30] and can be installed with a single command through the Python package manager pip [31].
With these considerations, the workflow system of AiiDA has been designed to satisfy the following criteria. The workflow system should (i) facilitate the definition of fully dynamic workflows, (ii) with an interface generic enough to support running arbitrary external codes, (iii) automatically store the full provenance of executed workflows (iv) in a way that makes the data easily queryable while scaling towards exascale applications (v) with an overhead of the provenance storage that does not outweigh the cost of the computational workflows themselves. AiiDA’s workflow system can be roughly split into two components: the user interface (or API) that allows users to implement workflows and interact with the provenance graph, and the engine that is responsible for automatically executing those workflows and storing the results. In this paper, we first describe the user interface followed by a technical description of the architecture and implementation of the engine.
