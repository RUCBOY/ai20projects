Crowdsourcing software engineering offers developers new ways to contribute to software projects, enabling workers outside a traditional software development team to take part in building software (LaToza and van der Hoek, 2016). One form of crowdsourcing is microtask crowdsourcing, in which a workflow is used to decompose tasks into a sequence of microtasks. By reducing the necessary task context for newcomers to learn, microtask programming reduces the cost of onboarding and enables developers to contribute more quickly. Small contributions also open the possibility of increasing parallelism in software development. As many hands make light work, decomposing software development tasks into microtasks might enable some of this work to be completed in less time by parallelizing work across many developers.
A variety of systems have explored the promise of applying microtask crowdsourcing to programming (Chen et al., 2017, Goldman et al., 2011, Goldman, 2012, Lasecki et al., 2015, LaToza et al., 2015a, LaToza et al., 2018, LaToza et al., 2015b, LaToza et al., 2014, LaToza et al., 2013, Nebeling et al., 2012, Schiller and Ernst, 2012, Mao et al., 2017). For example, in Apparition (Lasecki et al., 2015), a client developer narrates a description for a user interface in natural language, and crowd workers translate this description into user interface elements, visual styles, and behavior. In CodeOn (Chen et al., 2017), a client developer narrates a request for help from their IDE, and crowd workers use this request and relevant context to author answers and code.
A long-term vision of microtask programming is to enable software to be built entirely through microtasks (LaToza et al., 2013). This differs from current approaches in scale: rather than facilitating individual tasks to be manually requested and managed by a requester, mechanisms must be found for the crowd to coordinate and work more effectively together. In microtasking, required coordination is described through a workflow, describing the microtasks which developers will complete, the handoffs that occur between microtasks, and the resulting dependencies between microtasks (Kittur et al., 2013). A key challenge in applying microtask crowdsourcing approaches to the domain of software engineering is the decontextualized nature of microtask work. Developers work without awareness of the complete program, reducing the necessary context which must be learned to contribute but increasing the potential for work going off track. Key to the success of microtasking approaches is to ensure effective mechanisms for coordination and aggregation exist, enabling workers to obtain feedback as quickly as possible to ensure their contributions are beneficial and coordinate contributions so that they do not conflict. When this does not occur, problems may ensue. Developers may write code which references fields and functions that do not exist, making poor implementation choices, or making implementation choices which conflict with other choices (LaToza et al., 2014, LaToza et al., 2018). As a result, time and effort are wasted, as further work is required to fix these issues, and the potential for undetected defects increases.
In this paper, we introduce a new approach for feedback based on behavior-driven development. In contrast to existing approaches which rely on either a client or manager (e.g., Lasecki et al., 2015, Goldman, 2012) or later crowd contributions (e.g., LaToza et al., 2014, LaToza et al., 2018), we enable developers to receive initial feedback within the microtask itself. We accomplish this by re-envisioning the scope of the microtask. In our workflow, developers receive feedback from three different sources while they are programming: through syntax errors, through running unit tests, and through the ability to debug their code. Adapting the idea of behavior-driven development to crowdsourcing work (North, 2006, Beck, 2003), each microtask encompasses the work to test and implement a behavior, a specific identifiable use case of a function. Developers work on a behavior end-to-end, identifying it from a high-level description of a function, writing a test to exercise it, implementing it in code, and debugging any issues that emerge. Through the use of stubs, developers can work on an individual function in isolation from the rest of the program being constructed while still receiving information about how their code executes. As developers join a new project, they complete a tutorial and can immediately begin making small contributions to the project.
We apply our behavior-driven approach to microtask programming to implementing a microservice. Large and complex web applications are often built as an interconnected network of smaller single-function microservices. Microservices offer a well-defined interface between a client (e.g., a team consuming a future microservice) and a crowd of developers, defined by the behavior of a set of HTTP endpoints handled by the microservice. Microservices offer a natural decomposition boundary in large web applications, offering a mechanism for a traditional software project to quickly crowdsource a module within a larger project. Rather than require an extended onboarding process for new developers to become familiar with a large software project, our approach enables a team to simply describe the desired functionality and it to be constructed separately by a crowd.
We instantiated our approach in a novel cloud IDE, CrowdMicroservices.1  CrowdMicroservices includes an editor for clients to describe requirements for the system as a set of endpoints, a web-based programming environment in which crowd workers can identify, test, implement, and debug behaviors, and an infrastructure for automatically generating and managing microtasks. Persistence is exposed through an external API, which supports a sandboxed environment for development. After completion, the microservice may be automatically deployed to a hosting site.
To evaluate our approach, we conducted a user study in which 9 crowd workers implemented a simple ToDo microservice. Our results offer initial evidence for the feasibility of the approach. Participants submitted their first microtask 24 min after beginning, successfully submitted 350 microtasks, implemented 13 functions and 36 tests, completed microtasks in a median time under 5 min, and correctly implemented 27 of 34 behaviors.
In this paper, we contribute

1.a novel behavior-driven microtask programming which offers immediate feedback from syntax errors, unit tests, and debugging.2.CrowdMicroservices, the first programming preconfigured environment for implementing microservices through microtasks3.initial evidence that behavior-driven microtasks can be quickly completed by developers and used to successfully implement a microservice.4.evidence that the approach has low onboarding time and a high potential for parallelism.
In the rest of the paper, we review related work, present our approach, illustrate the approach with an example, and report on a user study evaluation. We conclude with a discussion of limitations and threats to validity as well as opportunities and future directions.
Table 1. Applying the dimensions of crowdsourcing to concrete collaboration examples.De-compositionTask inter-dependenceTask contextTask lengthActivitiesOnboardingQuality controlLocus of controlPreconf-igured IDECrowdMicroservicesAutomaticLowA description, function and unit tests<=5 minCrowd develop/debug/test<=15 minUnit tests & reviews by crowdClientYesCrowdCode (LaToza et al., 2018)AutomaticLowA function<=5 minCrowd develop/debug/test<=15 minUnit tests & reviews by crowdClientYesApparition (Lasecki et al., 2015)ManualLowWhole design<=1 minuteReal-time UI design–NoneDesignerYesCrowdDesign (Nebeling et al., 2012)ManualLowA statement to a module<=15 minImplement UI element–Reviews by managerManagerYesCrowdForge (Kittur et al., 2011)ManualLowA partitionMinutes–hoursCrowdsourcing complex tasks–Reviews by managerManagerYesCodeOn (Chen et al., 2017)ManualLowA statement to whole codebase<=11 minHelp seeking in development–NoneRequesterYesCollabode (Goldman, 2012)ManualHighWhole codebaseMinutes–daysSynchronous collaborative coding–NoneCollaboratorsYesCodePilot (Warner and Guo, 2017)ManualHighWhole codebaseMinutes–hoursSynchronous collaborative coding for novices–Unit testsCollaboratorsYesOpen-sourceManualMedium–highWhole codebaseHours–daysCrowd develop/debug/testHours–daysUnit tests & reviews by crowdSenior contributorsNoTopCoderManualLowA function to a moduleMinutes–hoursDesign/developmentMinutes–hoursManagerClientNo
