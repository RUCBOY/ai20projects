Generative Adversarial Networks (GANs) [1] are a class of deep generative models that have been successfully applied to many real-world applications [2], [3], [4]. A vanilla GAN consists of two components, a discriminator D and a generator G. G learns to generate new candidates with the same statistical distribution as the training samples, while D estimates the probability that a sample came from the training dataset rather than G. In the training process, both components play a minimax game and contest with each other until they reach the Nash Equilibrium. Although GAN shows powerful generative capabilities [5], [6], training robust GAN is still a challenge because it can be easily trapped into the problem of mode collapse in which the generator only concentrates on several, or even only one single, modes rather than all modes.
When we use a neural network to map the real data from original data space into the latent space, the mapped data points usually lie on areas with different sizes. We take the MNIST dataset as the example. Some digits are represented over a very small area and others over a much larger area [7]. It causes that each time the neural network attempts to pick data points from the larger area in the latent space, even though the sampling is random. In other words, such a mapping renders that the model focuses on the data points with large mapping area [8], missing many other modes. Moreover, the Jensen Shannon Divergence is maxed out when the generated data distribution (pG) and real data distribution (pr) have a negligible overlapping area, resulting in vanishing gradient. Under such a scenario, the generator apparently fails to capture all the modes of the data, further aggravating mode collapse. The term “mode” in our paper refers to the category and diversity within the category. The category indicates the labels of instances, while the diversity within the category indicates the diverse generated instances with the same label. Hence, mode collapse in our paper indicates: 1) the vanilla generator cannot cover all categories; 2) the generated data within the same category tend to be identical. See Fig. 1 for a descriptive illustration of mode collapse. In Fig. 1(b), there exists identical instances and categories ‘2’, ‘5’ and ‘6’ are not captured.Download : Download high-res image (116KB)Download : Download full-size imageFig. 1. Illustrative example of transformation from low dimension Zto high dimension χ. The generator G(z):Z→χwith prior z ~ pz(z) expects to learn all modes of original data, however, such a transformation is not surjective. Sub-figure (a) shows an example of a non-surjective transformation. In sub-figure (a), the disconnected red lines indicate the different original data modes (e.g., categories ‘0’, ‘1’, etc.) while the blue curve indicates the learned modes by G. Sub-figure (b) shows the transformation results. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)
Although many GAN variants have been proposed to tackle this problem, there are still challenges. Wasserstein GAN (WGAN) [9] adopts wasserstein distance to measure the dissimilarity between real data distribution (pr) and generated data distribution (pG). However, the weight clipping method adopted by WGAN cannot avoid information loss, resulting in capacity underuse issue [10]. Spectral Normalization GAN (SN-GAN) [11] adopts spectral normalization (λ) to make discriminator Lipschitz continuous, so it can reserve the weight array (W) information to the greatest extent by using Wλ. However, since the 1-Lipshitz continuity is achieved by Wijλ1,the generated data would be inevitably mapped into a specific color sub-space, rendering all generated objects taking the same color. This can also be viewed as mode collapse.
The very recent attempt is to employ multiple generators to address mode collapse, motivated by the limitations of single generator for learning different modes [12], [13]. Multiple Generators GAN (MGAN) [13] employs a mixture of generators and adopts “shared parameters” method to simultaneously train all generators. Since there are no restrictions enforcing generators to learn all modes, these generators tend to learn the same modes, resulting in producing identical instances. Considering the significance of these challenges and the potential benefits of overcoming them, it is worth to develop a novel approach to addressing the problem of mode collapse.
In this study, we propose a novel approach to training multi-generator model with orthogonal vectors. The distributions of generated data from multiple generators are jointly induced to a mixture distribution for matching the original data distribution while encouraging each generator to learn different information of original data. To achieve this, we employ orthogonality strategy [14], [15] to construct orthogonal vectors based on the outputs of generators, and enforcing orthogonal vectors contain different information during training. Hence, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Initially, the generated data of all generators are fed into an extra encoder to obtain feature vectors. Then, we calculate the inner product between each pair of feature vectors to obtain the orthogonal value, which loyally reflects the correlation between two vectors. The lower the orthogonal value is, the more information the two vectors hold. The lower orthogonal value also demonstrates how different information has been learnt by the two generators. Therefore, we need to minimize the orthogonal value during training, and this is achieved by back-propagation along with minimizing the generator loss. After that, the orthogonal value is integrated with the generator loss to jointly update the corresponding generator’s parameters, rendering this generator learning such information while other generators hold less. A novel loss function is able to be established among a set of generators, an encoder and a discriminator. Furthermore, we provide theoretical analysis that the Jensen-Shannon (JS) divergence [16] between the mixture distribution and the original data distribution is minimal, and the orthogonal value between any two feature vectors is also minimal.
In summary, the major contributions of this study are described as follows:
•This paper proposes a novel MGO-GAN model which learns a mapping function parameterized by multiple generators from the randomized space to the original data space, overcoming the problem of mode collapse.•This paper utilizes the back-propagation to minimize the orthogonal value in GAN and combine the orthogonal value with the generator loss to jointly update the parameters of generator from both theoretical and empirical perspectives, offering new insights into the success of MGO-GAN.•Through comprehensive experiments on three datasets with different resolutions, we demonstrate the effectiveness of the proposed approach.
The rest part of this paper is organized as follows. In Section 2 existing works are discussed. We review the GAN model and orthogonal vectors in Section 3. MGO-GAN is demonstrated in detail in Section 4. In Section 5 we show our experimental results. Section 6 serves as our conclusion.
