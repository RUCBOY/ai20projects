Examination of dysfunctional brain dynamics often needs to tackle the difficulties of analyzing neural data of multiple modalities as recordings from a single source (such as electroencephalogram, abbr. EEG) do not always suffice in characterizing the brain states of interest. A typical example is sleep scoring from Polysomnography (PSG) for evaluation of sleep disorder, which has long been an onerous task and constantly results in unstable performance. The ultimate goal here is to extract the features embedded in the (raw) neural data and reveal their latent correlation with abnormalities in brain states, thus automatic evaluation (or diagnosis) may be possible. Past decades have witnessed tremendous efforts and successes along this direction especially those rooted from the signal processing and computing communities.
Instead of relying on labels (features visually inspected) manually set by experts that are notoriously onerous and unreliable, conventional methods largely rely on handcrafted features followed by classification based on machine learning. Basically, features are defined in the context of time, frequency, non-linear measures and complexity (e.g., entropy) based on the signal processing theories [1], [2], [3], [4]. Meaningful features are then singled out for the subsequent classification, i.e., correct mapping between the features to the labels signifying abnormalities. These methods excel in interpretability, but they are largely subject to abundant a priori knowledge (expertise in clinical practices) as well as their performance is totally confined by the quality of the features that cannot be guaranteed.
When the problem domain explicitly involves multiple modalities, features are still acquired by selectively applying these methods across all the modalities according to their different characteristics [5], [6], [7]. It should be noted that the features associated with different modalities are heterogeneous to each other in term of data attribute, structure, and/or semantics. It is not trivial to correlate the features originating from different data spaces to construct a (series of) unified multi-modal features. Direct fusion without reliable cues will inevitably incur bias of interpreting the problems under examination. Successful attempts have been made to fuse multi-modal neural data [8], [9], [10], [11], especially EEG and brain imaging data such as MEG and fMRI, and those prove useful in functional brain imaging [8] and event-related bio-electric activities [9], [11]. However, existing methods are largely hand-crafted and application specific. In previous work of this study, factorization methods [12], [13] have been proposed to extract latent features of multi-dimensional EEG, but these cannot directly apply to multi-modal signals.
The booming of deep learning techniques provides an unrivalled opportunity for accurate classification with the capabilities of learning the data-driven features. Recent years have witnessed considerable progresses in various communities. For example, deep learning approaches have been widely applied in deriving hierarchical, abstract representations from temporal-spatial big data [14], [15], [16], [17]. Typically, Convolutional Neural Network (CNN) excels in adaptive selection of features. With a convolution operation capable of extracting distortion-invariant patterns, CNN has gained great successes in EEG-based brain disease diagnosis [18], [19], [20]. However, when handling a problem involving neural data of multiple modalities, a single deep neural network cannot directly apply due to the following issues:
•The heterogeneity between features originated from different modalities should be properly addressed as each type naturally represents very distinct characteristics. An appropriate combination of multi-modal features is the prerequisite of reliable evaluation of brain functions and malfunctions.•The evolution of brain states of interest is not necessarily abrupt but subject to constraints regarding the problem under examination. A deep network is not designed to explicitly characterize the causality between the features in time order, i.e., the temporal correlations.
A typical example is sleep scoring from Polysomnography (PSG) for evaluation of sleep disorder. Typically, PSG contains EEG and electrooculogram (EOG). EEG is often characterized by its time–frequency distribution (e.g., α and θ waves with different frequency bands) while EOG tends to be evaluated according to its temporal feature regarding several types of eye movements. PSG records the neural activities of the subject through several sleep cycles per night. A cycle consists of some of the five sleep stages (namely W,N1,N2,N3, and R defined by the American Academy of Sleep Medicine) each lasting for a duration measured in every 30 s. A sleep stage may transit to another conforming to the intrinsic rule of human being rather than jumping abruptly, i.e., temporal correlations exist in the neuronal recordings.
The strategy of this study is 1) to first construct an appropriate model to learn the deep features of multi-modal neural data; thus the heterogeneity amongst conventional features of different modalities can be fundamentally removed as those have been transformed to homogeneous deep features and can be easily and seamlessly fused; and 2) to reinforce the correlation of epochs in the features in time order when training the model; then the final classification may be automatically fine tuned. This study develops the deep learning approach to multi-modal sleep scoring via neural data classification in two phases (see Fig. 1):1.Multi-modal Feature Extraction (Section 2.1): A dual Convolutional Neural Network (dual-CNN) framework consisting of two branches of stacked CNN layers has been constructed, which simultaneously processes temporal and time–frequency inputs. The deep features derived from each branch can be easily concatenated as the multi-modal features.2.Classification (Section 2.2): A Recurrent Neural Network (RNN) layer associates the features subject to the constraints of state transition for classification with a fully-connected layer conforming to a data-driven strategy. A customized Markov chain then applies to further fine-tune the final outputs considering the temporal correlations of the features thus to amortize the potential bias resulted from the imbalance of brain states in the deep features (e.g., sleep stages).Download : Download high-res image (225KB)Download : Download full-size imageFig. 1. Conceptual View of the Multi-modal Sleep Scoring Framework.
A case study of sleep scoring has been carried out on the Sleep-EDF dataset [21] 1) to evaluate the performance of the dual-CNN framework in learning the features of neural data with multiple modalities (Section 3.4), 2) to examine the effectiveness of incorporating correlation between brain states, i.e., sleep stages in this case (Section 3.5), and finally 3) to compare the proposed approach with the state-of-the-art counterparts on sleep scoring (Section 3.6).
The main contribution of this study are as follows:
1.A deep learning framework has been fostered for sleep scoring based on PSG. The framework outperforms the counterparts in terms of performance sleep scoring and the capability of self fine-tuning based on temporal correlations of sleep states.2.This study develops a dual-CNN network to effectively learn multi-modal features of neural data without the need for excessive a priori knowledge. It holds potentials in complex neuro-science & engineering tasks regarding discriminating brain states.
