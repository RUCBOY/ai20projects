Script plays an important role in the scene text recognition without any prior knowledge about its language, aiming to identify the text language in the image and thus select an appropriate corresponding text recognition model. In addition, script identification has been widely applied in various areas, including multi-lingual machine translation [1], mobile visual search [2], video script identification (to help selecting the appropriate OCR engine) [3], and scene image understanding [4]. Script identification can be considered as a branch of the generic image classification problem where CNN has been widely applied, yet there are some non-negligible differences, such as character strokes in the text image. Some examples from five common scripts are shown in Fig. 1, where we can see texts from different languages have different stroke shapes and combination structures.Download : Download high-res image (262KB)Download : Download full-size imageFig. 1. Examples of scene texts from different scripts.
In the area of document image analysis, script identification has attracted much attention recently. Sharma et al. [5] determined the category of the input image with a script identification module, and then applied corresponding exclusive recognition methods to different language categories to achieve multilingual text recognition. Liu et al. [6] designed an end-to-end text spotter network which integrated text detection and recognition by a shared convolutional strategy with an implicit script identification module. However, these works only showed the effectiveness on the datasets with few languages (two or three), without demonstrating the ability of handling multi-lingual scene text images.
For the multi-script documents, researchers have achieved high performance on script identification of printed, handwritten scripts or hybrid documents [7]. Hochberg et al. [8] proposed an automated script identification system by creating templates from clustering textual symbols on thirteen different scripts. Spitz et al. [9] carried out the identification task by firstly dividing a total of 26 languages into Han-based and Latin-based scripts, then applying optical density distribution analysis and character shape codes technique respectively. Pal and Chaudhuri [10] presented an automatic technique to separate the paragraph into text lines according to their characteristics and shape-based features. They also proposed a system to identify printed Roman, Chinese, Arabic, Devnagari and Bangla text lines based on shape and statistical features [11].The work[12] proposed to integrate text recognition information in the script identification on the ICDAR PRC-MLT 2017, however, the information of text recognition is usually difficult to obtain in advance.
Script identification has been studied intensively and achieved tremendous advances, but are not solved yet in the scene text images due to following difficulties:
•Low image quality. The background (to be more specific, the non-text content) inside the image can vary in multiple aspects, including but not limited to textures, colours, luminance, and reflections. In addition, when taking the photo, the angle of view, camera motion, perspective, and image resolution can degrade the image quality and introduce distortions and noises. Examples are shown in Fig. 2a.Download : Download high-res image (347KB)Download : Download full-size imageFig. 2. Difficulties in script identification tasks. (a) and (b) demonstrate the low-quality images and diverse text styles that bring difficulties to the identification task. (c) illustrates the local discriminative parts by marking them with red rectangles. In (d), the areas with red bounding boxes show the confusing coincidences brought by the miscellaneous background, which make the characters also look like they are from another language.•Diverse text styles. The text style includes font types, character sizes, spacing and orientation, and other properties such as colour and texture. These variations can bring difficulties for training the classifier. In addition, some datasets also contain various art fonts, which are difficult to predict. Examples are shown in Fig. 2b.•Language similarity. Different languages from the same family of languages often share same or similar characters. For example, French, Spanish, and German characters can be considered as English letters with accent symbols; alphabets of Russian and English have large intersections where characters have nearly the same shapes; Chinese and Japanese share large amounts of Kanji. To better describe this phenomenon, examples are illustrated in Fig. 2c, where texts surrounded by red bounding boxes are so called local discriminative parts in this paper. They hold valuable information to distinguish the text between two similar languages, which we elaborately investigate via the attention mechanism.•Background disturbance. The miscellaneous disturbance from the background can do direct impact to the identification accuracy. For example, an unexpected accent-symbol-like line near the letter e may make the character look like é and hence more likely to be recognized as French instead of English. Examples are demonstrated in Fig. 2d.
In this paper, we utilized a stacked convolutional neural network as the backbone of the feature extractor, and further enhance the feature learning capability by stacking multi-scale feature fusion blocks which can preserve coarse and fine-grained features. Considering the local importance, we then fed the feature maps into a residual-like stacking attention mechanism which can dynamically generate attention maps so that the network will concentrate more on the valuable local areas whose attention weights are relatively higher. Eventually, a fully convolutional classifier is adopted to output the classification confidence of each script class and hence decides which script the input text image belongs to. Unlike conventional classifiers based on fully connected layers, our classifier is dominantly more light-weighted and efficient while the accuracy is still comparable to the former.
The major contributions of this paper are the following: (1) We propose a multi-scale convolutional feature extractor with large receptive field to handle arbitrary input size and aspect ratio. (2) We boost the performance of the feature extractor by adopting attention mechanism to thoroughly learn the graphical features of the text. (3) Our proposed neural network is a fully convolutional network with less network parameters and high efficiency. (4) We evaluated the proposed approach on four benchmark datasets, and achieved higher performances than those competitive models. Specially, the correct rates are 89.66%, 96.11%, 98.78% and 97.20% on RRC-MLT2017, SIW-13, CVSI-2015 and MLe2e, respectively. We also release the code on the Github1 to promote the research of script identification.
The rest of the paper is arranged as follows: In Section 2, we discuss the related works including some recent script identification methods, several involved outstanding CNN classifiers, and attention mechanisms. In Section 3, we provide the network structure and algorithms in detail. In Section 4, we show the implementation details, discuss the performance results on multiple datasets and carry out ablation experiments to evaluate the effectiveness of each building block. Finally, in Section 5, the conclusion and future work will be covered.
