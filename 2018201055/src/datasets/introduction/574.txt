Over the past few years, major machine learning research efforts have focused on dimensionality reduction, which is caused by the dramatic growth in the number of data dimensionality and brings obstacles to the applicability of several computer vision tasks such as video analysis [1], crowd analysis [2], [3], audiovisual learning [4], etc. As a result, the emergence of dimensionality reduction methods designed for feature extraction to learn the intrinsic structure of data is extremely urgent. Linear Discriminant Analysis (LDA) [5] and Principal Component Analysis (PCA) [6] are the most famous supervised and unsupervised feature extraction methods respectively. Several improved LDA algorithms, including robust LDA [7], local LDA [8], [9] achieve many successes on pattern recognition. Furthermore, two-dimensional (2D) extension of LDA and PCA also gain much attention on image process [10], [11]. The task of conventional LDA is to find a transformation matrix to project high-dimensional data into low-dimensional subspace as well as gather the samples within same class and disperse the data points between different classes. In this case, LDA and its varieties involve solving trace ratio problem, namely minWTr(WTSwW)/Tr(WTSbW), which however has a trivial solution. Specifically, the trace ratio problem can be transformed to ratio trace problem, i.e. minWTr[(WTSwW)−1(WTSbW)] which can be solved by Generalized EigenValue Decomposition (GEVD). However, the solution obtained by GEVD is a suboptimal solution which may degrade the subsequent classification performance [5], [12], [13]. Besides, LDA always suffers from other drawbacks such as Small Sample Size (SSS) problem, sensitivity to outliers, limitation of maximum reduced dimension and so on. Consequently, many research works [14], [15], [16], [17], [18], [19], [20] including Orthogonal LDA (OLDA), Uncorrelated LDA and Maximum Margin Criterion (MMC) have been developed to solve aforementioned issues in recent years.
Another drawback of LDA is that it is incompetent to deal with multimodel data. To address this issue, several graph-based dimensionality reduction methods have been proposed to explore the local structure of data by using the relationships among data points [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]. Wherein, Locality Preserving Projections (LPP) [21] learns some projections by embedding a k Nearest Neighbors (kNN) graph in original data for preserving the local geometry structure. Hereafter, the idea of kNN graph has been widely applied to many locality-preserved algorithms. For instance, a supervised version of LPP named Local Discriminant Embedding (LDE) [22] takes the submanifold of each class into consideration seriously and learns the embedding to maintain the neighborships of data points within same class while keeping away neighboring points between different classes. Local Fisher Discriminant Analysis (LFDA) [28] incorporates the fisher criterion in Fisher Discriminant Analysis (FDA) [29] into LPP for better extracting the local structure of data. In Neighborhood MinMax Projections (NMMP) [30], Nie et al. develop an efficient optimization algorithm to solve trace ratio problem directly, and the optimal solution can be achieved. Recently, Wang et al. propose Discriminant Locality Preserving Projection method (DLPP) [23] with nongreedy optimization algorithm for improving robustness of model. Additionally, Cai et al. propose a local dimensionality reduction method, called Local Sensitive Discriminant Analysis (LSDA) [24] which generates a projection matrix to maximize the margin between data points from different class at each local area by utilizing two kinds of pre-defined graph, namely within-class graph and between-class graph. In addition to above methods, there also exist other graph based local dimensionality reduction approaches such as Local Linear Discriminant Analysis (LLDA) [25], Multi-Manifold Discriminant Analysis (MMDA) [26] and so on. Last, several dimension reduction algorithms incorporate sparse learning [31] and low-rank learning [32] also achieve impressive performance in feature extraction task.
However, all aforementioned methods suffer from a common drawback that the similarity matrix is constructed at original high-dimensional space, which is easily affected by noisy and irrelevant features and degrades the subsequent classification performance. In the literatures [9], [33], [34], [35], [36], [37], they suggest that the intrinsic geometry of data are hidden in low-dimensional space, which inspires that the neighborships of each sample should be found in subspace rather than in original space.
On the ground of the above perspectives, we develop a novel local structured feature by using Dynamic Maximum Entropy Graph (DMEG) technique. Extensive experiments conducted on synthetic three-ring data set and several real-world data sets demonstrate the effectiveness of DMEG on exploring the local structure of data. In some related works, Graph-optimized LPP (GoLPP) [34] is an unsupervised algorithm which constructs a full-connected graph in subspace to explore the local structure of data. Locality Adaptive Discriminant Analysis (LADA) [36] exploits the points’ local relationships adaptively in learnt subspace by solving a trace ratio problem. Adaptive Neighborhood MinMax Projections (ANMMP) [38] finds the neighbors in subspace via brute force method. Last, Adaptive Discriminative Analysis (ADA) [37] develops a unified framework that integrates the similarity matrix updating and subspace learning. Compared to above approaches, proposed method has following differences.
1)We derive a more discriminative LDA which generates the whitening transformation, and it can minimize the scatter within same samples while keeping the scatter of total samples unchanged simultaneously.2)Proposed model adaptively selects k neighbors for each data point by imposing ℓ0-norm constraint on similarity matrix. Thus, the graph constructed in our method is k-connected which is more sensitive to local structure of data than full-connected graph in above methods.3)Proposed model learns the similarity matrix and the transformation matrix simultaneously such that the kNN graph is dynamically updated. In such way, the neighborships of each sample can be found in the optimal subspace rather than in original space. Additionally, we impose a maximum entropy regularization on similarity matrix so that the trivial solution can be avoided naturally [39].4)An efficient iterative optimization algorithm is presented to solve proposed problem with ℓ0-norm constraint, and a strict proof of convergence is provided as well. Experiments conducted on synthetic and several real-world data sets demonstrate the superiority of proposed DMEG compared to related state-of-the-art methods on classification task.
The rest of this paper is organized as follows: in Section 2, some different formulation of LDA will be introduced, Section 3 provides the proposed model and the proof of convergency of our model. Extensive experiments conducted on synthetic data set and several real-world data sets are developed in Section 4. Finally, Section 5 concludes the whole paper.
