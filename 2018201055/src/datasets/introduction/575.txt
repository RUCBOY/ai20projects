Hazy images have low visibility, poor contrast and distorted colors, all of which cause direct and significant performance degradations for image classification, object detection and recognition methods. Single-image dehazing is intended to restore a clear image from a hazy image, a task that is highly important in computer vision and artificial intelligence tasks. The formulation of a hazy image can be modeled by (1)I(x)=J(x)t(x)+A(1−t(x)),where I(x) and J(x) denote the hazy image and the actual scene radiance, respectively, A represents the global atmospheric light, and t(x) represents the medium transmission map. In practice, only the observed image I(x) is known. Hence, the majority of single-image dehazing methods first attempt to estimate the transmission map and the atmospheric light and then obtain the haze-free image. Zhu et al. (2015) developed a linear model to recover the depth information and then recovered the clean image using the depth information. Berman et al. (2016) presented a non-local haze-line prior to estimate the per-pixel transmission according to which the clear image is acquired. Based on observations and statistics of haze-free outdoor images, He et al. (2009) proposed a dark channel prior to directly estimate the atmospheric light and the transmission map. Due to its simplicity and effectiveness, many scholars have performed further research based on the dark channel prior. Gibson et al. (2012) estimated transmission using a median dark channel prior and then restored the haze-free image. He et al. (2017) estimated the initial transmission map under an additional boundary prior and defined a neighboring-structure dictionary to preserve the consistency of the local structure. Bui and Kim (2018) constructed color ellipsoids prior to calculating the transmission values. Based on the dark channel prior, we proposed an iterative dehazing model (Wang et al., 2019) in our previous work that approximates the haze-free image gradually by simultaneously optimizing the estimated atmospheric light and the transmission map. Our proposed algorithm conforms to the physical model and achieves good color fidelity and contrast enhancement. However, its defogging effect is unsatisfactory on some dark hazy images.
Generative adversarial networks (GANs), first proposed by Goodfellow et al. (2014), have previously been employed for single image defogging (Park et al., 2020, Li et al., 2020) because of their great potential to produce realistic images. Zhu et al. (2018) reformulated the atmospheric scattering model into a novel GAN (DehazeGAN) which can learn the intermediate parameters from data simultaneously and automatically. Zhang et al. (2019) presented a unified single image defogging network based on a GAN to jointly estimate the transmission and perform a defogging process. In addition to approaches for estimating the transmission and atmospheric light, many GAN-based methods exist that do not estimate intermediate parameters. Swami and Das (2018) proposed a fully end-to-end dehazing model based on a conditional GAN. Li et al. (2018) proposed a trainable GAN-based dehazing network in which the generator is composed of an encoder and a decoder. Dong et al. (2020) proposed an end-to-end GAN with a fusion discriminator (FD-GAN) for image dehazing. Zhu et al. (2017) presented a cycle-consistent adversarial network (CycleGAN) for image-to-image translation. Engin et al. (2018) applied CycleGAN to single-image dehazing and proposed a model named Cycle-Dehaze that did not require paired hazy and corresponding ground-truth images. GAN-based methods achieve good dehazing performances; nevertheless, they fail to fully consider the physical characteristics of hazy images; consequently, the dehazed images may have color distortions and suffer information loss.
Considering that physical model-based methods cannot completely remove fog from some images and that GAN-based approaches have poor fidelity, to comprehensively remove fog thoroughly and restore a realistic image, we enhanced CycleGAN for single-image dehazing by embedding the iterative dehazing model into the generative process. The proposed ICycleGAN is based on the physical model and is driven by the data; thus, it can effectively remove fog while preserving the natural properties of images. Furthermore, we construct a detail information-consistency loss that compels ICycleGAN to maintain the image details. When dealing with high-resolution images, bicubic downscaling is adopted to downscale the input images to 256 × 256 pixel resolution to match the needs of our network. To capture more edge structures and detailed textures, we apply rational fractal interpolation (Zhang et al., 2018) to upscale the processed images and avoid image quality degradation.
Our main technical contributions are as follows: (1) Considering that the defogging methods based on the hazy image degradation model can maintain the physical attributes of images and that the learning-based dehazing methods have universal applicability, we embed the iterative dehazing model into the generative process of CycleGAN to completely remove haze while maintaining fidelity. (2) We enhance the loss function of the traditional GAN-based defogging approaches by developing a detail information-consistency loss obtained from the physical model. This loss function stimulates the network to maintain detail information.
Download : Download high-res image (266KB)Download : Download full-size imageFig. 1. The framework of ICycleGAN.
