Multimodal medical image registration has long been an essential problem in the field of medical imaging studies. The goal of this task is to find the spatial transformation between images. Image registration represents a pre-requisite for multimodal medical image analysis due to the high variability of tissue appearance under different imaging modalities, and proper integration of useful data obtained from these individual images is desired for physicians to provide accurate and complementary information about a patient. For instance, a CT scan provides information on the structure of bones, and an MRI scan provides information on the structure of tissues such as muscles and blood vessels. Although the multimodal medical image registration process has been investigated for more than three decades, it is still an active research area.
Existing methods for multimodal image registration rely on the premise that the underlying anatomy shares physical features and structures in both modalities. Therefore, open questions remain for conventional methods on the extraction of discriminant features and the definition of a generic matching metric to measure the similarity of a pair of images to be registered. Typical image features include gradient, edge, geometric shape and contour, image skeleton, landmark, the response of Gabor filter, and the intensity histogram. Recently, local invariant features like scale-invariant feature transform (SIFT) have also been widely applied (Liao, Chung, 2010, Liu, Yuen, Torralba, 2010). As for similarity measure, sum-of-squared-differences, correlation coefficient, correlation ratio, and mutual information are commonly used.
However, handcrafted image features and similarity measures fail to conclude a general rule to compare images in the wide range of image modalities (de Vos, Berendsen, Viergever, Staring, Išgum, 2017, Razlighi, Kehtarnavaz, Yousefi, 2013, Litjens, Kooi, Bejnordi, Setio, Ciompi, Ghafoorian, Van Der Laak, Van Ginneken, Sánchez, 2017). Inspired by their success in computer vision, deep learning techniques are well introduced into image registration, so as to automatically learn to aggregate the information of various complexities in images that are relevant for the task at hand. Several works have generated image feature or similarity metric from scratch by using a deep convolutional neural network (CNN) for its strong semantic understanding (Wu, Kim, Wang, Gao, Liao, Shen, 2013, Simonovsky, Gutiérrez-Becker, Mateus, Navab, Komodakis, 2016, Hu, Modat, Gibson, Li, Ghavami, Bonmati, Wang, Bandula, Moore, Emberton, Ourselin, Noble, Barratt, Vercauteren, 2018, Fan, Cao, Xue, Yap, Shen, 2018, Fan, Cao, Wang, Yap, Shen, 2019, Qin, Shi, Liao, Mansi, Rueckert, Kamen, 2019). For example, Wu et al.(Wu et al., 2013) proposed to use CNN to extract features from image pairs for registration. Simonovsky et al.(Simonovsky et al., 2016) devised a feed-forward CNN to estimate the dissimilarity of two cubic patches. These deep-learning-based image registration methods have shown the potential to outperform the methods using handcrafted features or metric.
Recently, another type of image registration methods is also emerging, which leverages CNN to predict registration parameters such as translation and rotation directly. Some methods within this type are regression-based, where a feed-forward network is trained with moving image and fixed image and outputs the registration parameters (Yang, Kwitt, Styner, Niethammer, 2017, Miao, Wang, Liao, 2016). Some other methods follow a fundamentally different paradigm by reformulating the registration task as a decision-making issue, where registration parameters are explored by an artificial agent interacting with the environment (Liao, Miao, de Tournemire, Grbic, Kamen, Mansi, Comaniciu, 2017, Ma, Wang, Singh, Tamersoy, Chang, Wimmer, Chen, 2017, Sun, Hu, Yao, Hu, et al., 2018). However, high data dimensionality and the complex relation between image appearance and the registration parameters make this kind of predictions non-trivial. More specifically, the high dimensionality of data and parameter space (e.g., nine degree-of-freedom) challenges 3D medical image registration. In contrast, the challenges of 2D medical image registration are huge variability in appearances and shapes, as indicated in Fig. 1. Although most current applications of medical image registration require 3D image registration, it is also necessary to carry out 2D transformations in some circumstances, such as the registration of two radiographs before and after contrast is injected (Hill et al., 2001) or the electron microscope image registration for histopathological study of tissue (Wang et al., 2014). Besides, compared with performing 3D registration on whole volume, registering 2D slices that have plentiful textures and edges would be more efficient and convenient for the subsequent analysis and diagnosis, since loading and processing the complete 3D volumes of different modalities (the image data varies between 40M to more than 1GB) is too heavy for some real-time-based surgical tasks (Emrich, Graf, Kriegel, Schubert, Thoma, Cavallaro, 2010, Graf, Kriegel, Schubert, Pölsterl, Cavallaro, 2011)Download : Download high-res image (254KB)Download : Download full-size imageFig. 1. High appearance variability in 2D multimodal image registration. Top: MR images (fixed image). Bottom: CT images (moving image). The goal of image registration is to align the moving image to the fixed image.
This paper belongs to the third type that predicts the image registration parameter within a decision-making framework. This kind of registration method is hard to train and may take a very long time to stabilize. A good representation of input images is hugely beneficial for network convergence. Considering that the representation of 2D registration is more demanding than the 3D case for its huge variability in appearances and shapes, we focus on the design of image representation first and investigate 2D registration in this work. An overview of the proposed method is shown in Fig. 2: An artificial agent explores the parameter space of spatial transformation and determines the next best transformation parameter at each time step. Unlike other similar image registration methods that either use a pre-trained neural network or learn the greedy exploration process in a supervised manner (Miao, Wang, Liao, 2016, Liao, Miao, de Tournemire, Grbic, Kamen, Mansi, Comaniciu, 2017), our method explores the searching space freely by using a multithread actor-critic scheme (A3C). Our major contributions are:•A novel reinforcement learning (RL) framework is proposed for image registration by utilizing a combined policy and value network. This actor-critic network successfully learns the perception-action loop in RL from scratch instead of using any pre-trained convolutional features which were trained on large image recognition datasets (e.g., ImageNet (Russakovsky et al., 2015)) for the perception module.•A new landmark error based reward function is proposed to cope with the transformation parameter unit discrepancy. Different choices for the reward function, encoding various aspects of information about the registration task, were investigated. Experimental results suggest that using the proposed reward function helps convergence.•A Monte Carlo rollout strategy is performed as look-ahead guidance to overcome the unknown terminal state problem in a testing phase. We observe that using such a strategy can further improve prediction accuracy and stabilize the prediction in testing.Download : Download high-res image (338KB)Download : Download full-size imageFig. 2. Overview of our proposed method. (a) A single global model shares network parameters and is asynchronously updated by independent agents. (b) Details of interaction between each agent and environment.
Unlike our previous work (Sun et al., 2018) as well as many published RL-based 2D image registration methods, which leverage information of neighboring frames through the structure of LSTM, this paper reformulates 2D image registration as a spatio-temporal problem. This is based on two observations: 1) convolutional feature maps extracted from a 2D image may repeat over different spatial locations due to the image spatial redundancy (Buades et al., 2005b); 2) neighboring frames are found to have strong motion smoothness since the transformation is subtle at each time step in RL (e.g., ±1pixel or ±1∘). Using such spatio-temporal redundancy may give more clues to the agent for learning the policy of registration more effectively.
Motivated by the papers in computer vision that deal with video-related problem (Siam, Valipour, Jagersand, Ray, 2017, Liu, Zhu, 2018, Wu, Wang, Gao, Li, 2019), we replaced LSTM with convolutional LSTM (convLSTM) in this paper. Standard LSTM treats the input as vectors by vectorizing the input feature map and thereby omitting the spatial information in an image sequence. As no spatial information is considered, the results of LSTM are suboptimal for image sequence analysis. By replacing the vector multiplication with more efficient convolutional operations, convLSTM allows the network to encode both spatial and temporal information, creating a unified model for processing image sequences. This paper improves our previous work in the following aspects:
-We reformulate the 2D image registration in the RL framework as a spatiotemporal problem. Experiments suggest that using spatiotemporal features facilitates better registration accuracy.-We propose a novel architecture for image registration parameter prediction, which fuses a convLSTM (Shi et al., 2015) encoder with a convolutional visual encoder to better capture spatiotemporal features. Regarding that CNN only extracts short-term spatial information, convLSTM is leveraged to learn long-term spatiotemporal features from the extracted short-term spatial features. In this way, not only spatial redundancy is fully exploited, but also variations of the warped moving image are implicitly discovered over time.-More convolution layers are added to make the network deep enough for extracting higher-level contextual features. Intuitively, high-dimensionality of state space and non-trivial large action range make the training of reinforcement learning challenging. It is time-consuming to get an optimal policy over such high complexity. A deeper semantic understanding of the original image would contain less information compared to the original one but include most information needed by an agent to take actions. In other words, with the power of embedding deeper neural networks with reinforcement learning, a great step forward in such complexity is made.-Layer normalization (Lei Ba et al., 2016) is added to reduce the appearance discrepancy between the fixed image and the moving image across different modalities and thus facilitates the convergence of deeper networks.
The remainder of this paper is organized as follows. Section 2 discusses related work on multimodal image registration and decision making. Section 3 details the proposed method. Experimental results and discussions are respectively presented in Section 4 and Section 5. The conclusion is given in Section 6.
