Along with the development of deep learning techniques, conventional close set image classification has achieved the level of human beings. However, in this big data era, an increasing number of new categories of objects are emerging everyday, so conventional close set methods need to be retrained to include the new categories, which is time-consuming and infeasible for realistic applications. To circumvent this issue, Zero Shot Learning (ZSL) (Akata et al., 2013, Lampert et al., 2014, Palatucci et al., 2009, Zhang, Long, Liu et al., 2019, Zhang, Long, Yang et al., 2019, Zhang and Saligrama, 2016) is proposed to recognize novel categories that are invisible during training. This task is achieved by the assist of an auxiliary intermediate information, e.g. attributes annotated by experts (Ferrari & Zisserman, 2008), to establish the bridge between the source and target objects, just as our human beings classify novel categories via the previous knowledge. In recent years, ZSL has achieved great success and attracted much attention, but traditional ZSL only concentrates on classifying novel objects within the scope of unseen categories, which is unreasonable in realistic scenarios because we cannot decide the ascription of the new emerging instance. Therefore, Chao, Soravit, Gong, and Sha (2016) proposed a more realistic Generalized ZSL (GZSL) setting, which extends the testing scope from only unseen classes to all classes, including both seen and unseen.
As shown in Fig. 1, most of the existing ZSL methods address this task by finding the relationships between visual features and semantic embeddings of seen classes and then transferring them to unseen categories. Therefore, how to make the model extract the most representative latent embeddings of seen categories is one of the most crucial issues for ZSL research. Some early efforts such as Direct Attribute Projection (DAP) (Lampert et al., 2014), Attribute Label Embedding (ALE) ALE (Akata, Perronnin, Harchaoui, & Schmid, 2016) and Semantic Auto-Encoder (SAE) (Kodirov, Xiang, & Gong, 2017), have adopted different constraints, e.g., cosine distance, bilinear compatibility function or Frobenius norm, to constrain the pairwise similarity between visual feature and semantic attribute during the training phase. However, visual features and semantic attributes are from two different modalities, where the modality discrepancy exists, thus the learned shared representation will fail to capture the underlying cross-modal semantic information, and finally lead to unsatisfactory classification results. We argue that employing a simple pairwise constraint is far from sufficient because it only considers the pairwise correlation, while the most essential information transferred to target domain should be the high-level semantic consistency, which is needed to be fully modeled.Download : Download high-res image (267KB)Download : Download full-size imageFig. 1. The core problem of ZSL is how to learn the most representative representations in latent space.
Besides, attributes of some categories are very similar to each other, which make them less discriminative, e.g the attribute of ‘Persian Cat’ is similar to that of ‘Siamese Cat’, and consequently lead to a wrong classification. Zhang et al. in Zhang, Long, Guan and Shao (2019) tried to enlarge the gap between the attributes of seen classes by employing triple verifications. However, this method only concentrates on the prototypes of seen classes, while the unseen classes are totally ignored, which makes some prototypes of unseen classes still similar to each other, and finally lead to a wrong classification, especially on the more realistic GZSL setting. In addition, some other efforts disperse the distance of class prototypes of all classes but only focus on semantic modality and ignore the correlation between visual features and semantic embeddings (Jiang, Wang, Shan, & Chen, 2018), which makes the latent representation less discriminative and less representative.
Recently, unseen sample synthetic based methods have attracted more attention due to their excellent performance (Huang, Wang, Yu, & Wang, 2019). Different from compatible methods (Xian, Schiele, & Akata, 2017), they often utilize Generative Adversarial Network (GAN) (Goodfellow et al., 2014) to train a projection from semantic attributes to visual features, and then exploit the learned network model to synthesize samples of unseen classes, which are subsequently combined with seen samples to train a supervised close-set model (Ni, Zhang, & Xie, 2019). However, these synthetic based methods usually encounter a serious problem, that is, they are learned within a close-set, and when a new class is added, the entire model should be retrained with new synthetic visual samples.
In order to solve the aforementioned problems, in this paper, we propose a novel deep framework, called Modality Independent Adversarial Network (MIANet) for generalized zero shot image classification, which is an end-to-end architecture to learn the more discriminative and representative latent representations. The novelties of our model lie in the following three aspects. Firstly, both visual features and semantic attributes are projected into a latent space, where two orthogonal constraints, including semantic to semantic and semantic to visual, are employed to make the latent representations more discriminative. Secondly, an adversarial training mechanism is constructed in the latent space. Considering the former projector as a generator to yield semantic discriminative latent representation, we employ another modality discriminator to distinguish the modalities of the shared representation, which competes with the generator to alternately boost each other. This adversarial training mechanism can facilitate the learned representation more discriminative for semantics but indistinguishable for modalities (Goodfellow et al., 2014), thus it can effectively enhance the cross-modal semantic consistency and will finally lead to performance improvement. It is noteworthy that this adversarial strategy is different from those GAN based unseen sample synthetic based methods (Long et al., 2017, Xian et al., 2018, Zhang, Long, Liu et al., 2019), which often suffer from the retraining problem of the close-set image classification when new category emerges. At last, in order to alleviate the domain shift problem (Fu et al., 2014, Kodirov et al., 2017) and let the latent vectors preserve more information from their respective original spaces, we proposed a reconstruction subnetwork. Instead of directly reconstructing themselves, we propose a novel cross reconstruction submodule to reconstruct the counterpart representation, which can preserve more cross-modal information. The contributions of this work are summarized as follows,

•We propose a novel and effective zero shot image classification framework, namely Modality Independent Adversarial Network (MIANet), which is an end-to-end architecture to learn more representative and discriminative latent representation for visual features and semantic embeddings;•To create a discriminative latent space, two orthogonal constraints are applied to make each vector orthogonal to other if they belong to different categories, otherwise normalized. Furthermore, a cross reconstruction framework for both visual and semantic modalities is employed to make the latent vectors more representative;•Adversarial training mechanism is exploited to confuse the source modality of the latent vectors, which can make the pairwise vectors indistinguishable for modalities, and results in preserving more high-level semantic consistency within them;•Extensive experiments are conducted on five populardatasets, and the results show that our MIANet can outperform most of the state-of-the-art compatible methods on both GZSL and ZSL settings.
The main content of this paper is organized as follows: In Section 2 we briefly introduce the existing methods for ZSL and GZSL. Section 3 describes the proposed method in detail. Section 4 gives the experimental results of comparison with existing methods on several metrics. Finally in Section 5, we conclude this paper.
