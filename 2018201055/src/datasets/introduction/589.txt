The combination of many nonlinear functions and layers makes deep learning extremely effective in many machine learning tasks [19], especially with complex and unstructured data such as: image, text, speech or social network data. Deep Neural Networks have been proved to enhance performance in many domains of application such as: computer vision [64], [30], [67], biomedical [21], [47], text processing [73], social networking [20] and many others [38].
Beside the complexity of the models, the volume of the available training datasets also affects the efficiency of Deep Neural Networks. In theory, more data are used for training the model, the higher accuracy of predictive results will be obtained [3]. For example, many mobile applications can provide more awesome user-experience on the devices by collecting a large-scale users’ behavioral data. SMS Spam detection [56], speech recognition [16], words auto correction and predictive suggestion [22] can be improved by utilizing more and more user data.
However, the collection and annotation of large-scale datasets requires significantly a lot of time and effort. Moreover, the computation cost for training deep networks on massive datasets is usually extremely expensive for a single party [23]. Consequently, collaborative training deep models among a group of parties becomes indispensable demand. Decomposing and parallelizing the training process among different parties could help reduce the effort and resources in computing on any single party. Furthermore, in collaborative learning a global model can help to avoid the overfitting problem. Overfitting might result in inaccurate outcome when predicting unseen data and make the model poor generalization, which is caused by the homogeneity and lack of data in each individual single local party [75].
Unfortunately, to collaboratively train a global model, it requires all parties to share their local data. This poses a great obstacle because most of local data is often private or sensitive, which inevitably creates reluctance to share, even with a trusted third party. Furthermore, data collection and sharing need to consider regulations, legislation, competition, security, and privacy, all of which can complicate the processes [1]. These issues prevent users from gaining the benefit of the whole enormous amount of data.
In order to deal with privacy concerns in collaborative deep learning, privacy preserving deep learning has thus arose [9], [61]. There are three approaches studied in the literature as briefly discussed below.
In the first approach, local datasets are shared directly in encrypted forms and then some special learning algorithms are applied on these encrypted data [72], [10]. In this case, data are often transformed before sharing by homomorphic encryption schemes [33], [76], [36], secure multiparty computation protocols [48], secret sharing techniques [68], or adding some noise [39]. We call this approach as data sharing approach. This approach, however, poses several issues as follows.
Firstly, the huge amount of data sharing among parties makes communication and computation cost to protect it are overwhelming. This makes the approach currently not feasible for practical applications [74]. Secondly, most data transformation techniques such as homomorphic encryption, secure multiparty computation and secret sharing require converting data into integer numbers. Due to the use of integer numbers, nonlinear activation functions and many other functions in deep learning models need to be modified to fit integer computation. Rozycki et al. [57] estimated nonlinear functions in neural networks by polynomial functions. Chabanne et al. [11] replaced ReLU functions by polynomial functions and used sum-pooling instead of max-pooling layers in Convolution Neural Networks (CNN). Xie et al. [70] modified the sigmoid function with quadric function. These replacements not only reduce the accuracy but also restrict the flexibility of deep learning models. In addition, to ensure the security level of cryptographic techniques it requires deep learning models operating on very big integer numbers, that increases the size of data, the cost of communication and computation and can reduce the accuracy of the models [18]. Further, due to the security requirements, each party needs to solve the cryptographic key agreement. This is very difficult and can increase communication and computation costs. Thirdly, even with the most efficient techniques, in which data are transformed with some masking noise before sharing, the data will lose many utilities and information. This reduces the accuracy of the learning models as well [52].
PATE [51] is another approach in privacy preserving deep learning. In this second approach, instead of sharing local training datasets, participants or “teachers” share their knowledge of the predictive output to a “student” server model. Then the “student” server trains the student public model by using a public unlabeled dataset on the ensemble results from teacher models. The drawback of this approach is the requirement that most of teacher models need to be good or have sufficient data. Moreover, without combination with a secure multi-party computation technique, the curious student server can perform model inversion attacks [17] or membership inference attacks [46], [62] on any teacher’s dataset. Additionally, the combination of differential privacy techniques could reduce accuracy of the global model.
The third approach to solve the privacy preserving collaborative learning is the model sharing approach called Federated Learning [34]. In this approach, each party trains its own individual model locally and then sends the model to some intermediate server party for aggregating the training results. Model sharing approach can handle the issues mentioned above. Firstly, since the number of parameters in the models is much less than the amount of data thus the communication and bandwidth to share the models can be much less than to share all data or predictive outputs. Secondly, no raw data are sent to the server so we can avoid direct leakage.
In [41], [42], McMahan et al. proposed Federated Averaging algorithm. In Federated Averaging, the training data are located on the clients (i.e mobile devices), which are never uploaded to the server. The client computes an update to the current global model after local training on this local dataset and shares the local models to an aggregating server. This server will maintain the global model updated by adding and averaging all the shared models. Federated Averaging can decrease the number of communication rounds needed to train the global model. Local data are maintained locally, so that, no direct leakage occurs. However, the averaging scheme is merely an estimating technique, so that the accuracy of the model can be reduced. Furthermore, by sharing all the models without any protection, the clients’ local data can be leaked indirectly [63], [50] due to model inversion attacks [17] or membership inference attacks [46], [62].
From the ideas of transfer learning, in [27], [49] the authors introduced the hybrid transfer learning models, in which the clients pretrain some layers of the global model and then send the pretrained results to the cloud server that will train the rest of models. The authors of [66] divided the model into two parts: user and server, in which the user will train sensitive data and upload the trained non-sensitive data to the server for continuing the training, while the server centralizes the aggregation model. The model does not require a global view of the user data, only a part of it is willing to provide a small part of the real data to guide the model. However, a reduction in accuracy occurs. Further from the information sending to the server, adversarial can obtain much information of the client dataset. The adversarial can exploit the intermediate gradients to launch link ability attack on training data, since the gradients contain sufficient data features [50].
To solve these privacy problems, Reza Shokri et al. [61] introduced selective learning techniques. The model of this scheme is similar to Federated Averaging aforementioned. Each client trains a local training dataset and shares the local models to an aggregating server. However, instead of sharing all models, the clients can selectively share small subsets of their models’ key parameters during training with the global server. And then, the server will update the global parameter and resend to all the participants. Since only a few components of the parameters are shared, this can ensure privacy of the models. It will be more difficult to attack and compromise individual data. However, due to the lack of information, the accuracy of the models can be reduced. In addition, a part of the parameters can also be used to attack and recover some of the information of the clients [24]. Additionally, selective learning does not support parallel processing, the model update performs step by step and has a high latency.
To avoid raw parameters sharing, Abadi et al. [2] applied differential privacy techniques by adding noises in the gradients to upload, achieving a trade-off between data privacy and training accuracy. The authors of [43], [44], [53], [7] added a noised version of the federated averaging algorithm and used some normalized and clip techniques to ensure the accuracy of the models. Although differential privacy extrudes the effects of basic inversion attacks and inference attacks, private data can still be learned by using GAN (Generative Adversarial Network) [24].
Many cryptographic techniques were also proposed to protect local models during the training phase. Phong et al. [54] proposed to use homomorphic encryption techniques to protect the parameters sharing amongst local parties and a semi-honest aggregating server. This scheme assumed that all collaborative participants are honest but not curious and have the same key to decrypt the downloaded models. This assumption might be impractical due to the collusion of a party with the curious server and fail in scenario where some participants are curious. To handle this scenario, Bonawitz et. al [8] combined many cryptographic techniques such as Shamir’s t-out-of-n Secret Sharing, Diffie-Hellman key agreement composed with a hash function, Authenticated Encryption, Pseudo-random Generator, Secure signature scheme, and Public Key Infrastructure to ensure confidentiality of parties gradients. It operates on high-dimensional vectors and provides the strongest possible security under the constraints of a server mediated, unauthenticated network model. This preserves the privacy of the rest of the honest users even if a malicious server collaborates with a malicious user [74]. However, with the assumption that no collusion between participants and parameter server, and nothing revealed about the participants’ local data from aggregated parameters makes this not secure due to the membership inference attack [55]. Therefore, with cryptographic techniques, the communication and computation costs of this model are enormous.
All these approaches require an assumption of a trusted or semi-trusted third party server in training process which might suffer from single point of failure and bottleneck problem. This could make the whole network stop working when the central server fails or is shutdown for maintenance. This can be solved by replacing the central server with decentralized network [29], [40], [69]. In [29], the authors firstly proposed a decentralized machine learning framework which exploit Blockchain with privacy preserving machine learning. DeepChain [69] is a framework based on Blockchain that incite parties to join in collaborative training deep learning model by sharing their local gradients to the training network. The DeepChain’s incentive mechanism ensures participants to behave honestly, thus maintaining privacy and fairness during collaborative training process. Authors of [40] focused on fairness of the collaboration training process. The proposed DPPDL employed deferentially private artificial samples and encrypted model updates to address fairness, privacy and accuracy in collaborative deep learning. Although these models are effective in many cases of decentralized manner, they does not support parallel processing, the model updating performs step by step and thus has a high latency. In addition, they also do not support a large network setting with heterogeneous structures in which parties could differ due to variability in computation and network resources. The training network also can be unstable, resulting in unreliable devices that could be drop out at any given iteration. Furthermore, in a decentralized manner, the communication cost is very high.
Table 1 shows representative studies of these three approaches in privacy preserving deep learning and their pros/cons. In summary, most existing frameworks for deep learning that preserve privacy have a trade-off between privacy and utility. Moreover, there is no efficient approach that can work well in a decentralized manner. Hence, in this paper, we will propose such an efficient approach to address the following problems:•Single point of failure and bottleneck: Training process can perform without any trusted or semi-trusted third party server.•Systems Heterogeneity: the collaborative deep learning model can tolerate to dropped parties in the network.•Statistical Heterogeneity: The trained model has a good performance even in non-identically distributed (Non-I.I.D) manner either unbalance labeled datasets.•Privacy: Either private raw local datasets or local individual model parameters of each honest party need to be securely protected in training process, even if there are some participants colluding together.•Model utility: the performance measures of the model need to be acceptable and not to be lower too much than original model trained on the whole centralized datasets.•Efficiency: The training process should be efficient to be applicable for privacy-preserving computation problems in large-scale distributed scenarios.Table 1. Summary of representative studies in privacy preserving deep learning.ReferenceApproachKey conceptsProsConsTraditional Deep Learning- Ensure high level of accuracyNo privacy preservation- Low cost of computing resourceChabanne et al. [11]Data sharing- Estimated nonlinear functions by polynomial functions.- Ensure high level of privacy preservation- High computation and communication cost- Reduction in accuracy- Train on homomorphic encrypted data- Only suitable with 2 parties modelXie et al. [70]Data sharing- Estimated nonlinear functions by polynomial functions.- Ensure high level of privacy preservation- High computation and communication cost- Reduction in accuracy- Train on homomorphic encrypted data- Only suitable with 2 parties modelPATE [51]Ensemble method- “Teachers” share their knowledge of the predictive output to a “student”.- Support many parties model- High computation and communication cost- Use differential privacy to ensure privacy- Ensure high level of privacy preservation- Reduction in accuracy- Require most of “teachers” having high quality modelsMcMahan et al. [41], [42]Model sharing- Parameter aggregation- Support many parties modelLow level of privacy preservation- Federated learning- Low cost of computing resourceReza Shokri et al.[61]Model sharing- Gradient sharing- Support many parties modelTradeoff between privacy and accuracy- Partial sharing- Low cost of computing resource- Differential privacyAbadi et al. [2]Model sharing- Gradient sharing- Support many parties modelTradeoff between privacy and accuracy- Differential privacy- Low cost of computing resourcePhong et al. [54]Model sharing- Gradient sharingHigh level of privacy preservation- Fail in scenario that two client colludes- LWE homomorphic encryption- High computation and communication costBonawitz et al. [8]Model sharing- Multi party computation- High computation and communication cost- One-time pad- Reduction in accuracy- Shamir’s secret sharing protocol- High level of privacy preservation- Require no collusion between participants and server, and nothing revealed about the participants’ local data from aggregated parameters- Diffie-Hellman key exchange- Support many parties model- Double-masking structure- UF-CMA secure signature schemeDeepChain [69]Model sharing- Gradient sharing- High computation and communication cost- Incentive mechanism- No central point of failure- Reduction in accuracy- Distributed network- Do not support a large network setting with heterogeneous structures
Main contributions. To circumvent these issues, in this paper, we proposed the so-called Secure Decentralized Training Framework (SDTF) that guarantees not only the privacy of the users’ data, but also the utility of the model in decentralized network without any third-party server. In this framework, we used model sharing approach to prevent direct leakage of local data and proposed a Secure Model Sharing Protocol to ensure secure sharing and aggregation for building a collaborative model. Secure Model Sharing Protocol is a combination of randomization technique and our proposed Efficient Secure Sum Protocol. It is secure against attacks from inside and outside the system of parties under the Honest-but-curious model without trusting each other or any third-party server. The main contributions of this paper are summarized as follows:
•We introduced a decentralized collaborative deep learning framework (SDTF) that supports parallel training process to address privacy and utility of the model with tolerating drop out devices in large heterogeneous network without any trusted third-party server.•We developed a new Efficient Secure Sum Protocol (ESSP) that enables a large group of parties to jointly calculate a sum of private inputs. To our best knowledge, the proposed protocol is the first one that can work not only with integer number but also with floating point number without any data conversion.•To ensure the model sharing process we combined randomization technique and Efficient Secure Sum Protocol to protect local models from any Honest-but-curious party even n-2 of them colluding. By a theoretical evaluation of privacy and communication cost, it is shown that our protocol can ensure higher privacy with lower communication cost compared to any other approaches.•We experimentally evaluated the proposed framework on two popular datasets MNIST and UCI SMS spam with Keras Python. With 20% testing data and 80% training data, the experiments demonstrate the approach can obtain the accuracy baseline of 97% after only 10 training rounds with MNIST that is 5× faster than Downpour SGD and robust to the heterogeneity decentralized network and non-IID data distributions. Our approach can achieve great both privacy at the level of cryptographic approaches and efficiency at level of randomization techniques, as well as it retains higher model’s utility than differential privacy approaches. So that it is very efficient and practical in real life applications.
The rest of this paper is organized as follows. In the next section, we give a brief introduction of deep learning model and some background of Federated learning and ElGamal encryption scheme. We present our Secure Decentralized Training Framework (SDTF), Secure Model Sharing Protocol and Efficient Secure Sum Protocol (ESSP) in Section 3. In Section 4, we provide the analysis of privacy level and an evaluation of the new framework followed by the implementation set-up and experimental results. Section 5 presents discussion, and finally Section 6 wraps up the paper with conclusion and future work.
