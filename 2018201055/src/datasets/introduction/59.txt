Kernel ridge regression (KRR) is one of the kernel methods that maps feature space onto a higher dimensional space, and makes use of non-linear kernel function to calculate the inner products of two features explicitly, hence the non-linear relationship can be well-modeled [1]. It has been successfully applied to various fields such as machine learning and data analysis [2], [3]. Some typical applications can be found in the following: Zhang et al. [4] derived an optimal loss function and constructed a new framework of kernel ridge regression technique for short-term wind speed prediction. Exterkate et al. [5] proposed KRR as an approach for forecasting the number of predictors that are related to the target variable nonlinearly. Zhang et al. [6] derived classification modules based on the KRR to fuse the multiple features through their correlation seamlessly.
The instances above have shown great effectiveness of KRR method due to its efficient modeling capacity for non-linear relationship, nevertheless, the performance of KRR is much sensitive to the kernel type and parameters. Improper kernel types or inaccurate parameters may degrade the overall performance dramatically, in some cases, it can even lead to a worse performance than that of linear models. Unfortunately, it is very difficult to determine a kernel in practice since limited prior knowledge can be extracted just from the samples.
To overcome this issue, Multiple Kernel Learning (MKL) is developed. In MKL, multiple kernels are incorporated into a unique model and the regression result comes from the combination of multiple non-linear kernels [7]. As a consequence, the diversity is improved against single kernel case and a better performance can be achieved accordingly. For example, Castro et al. [8] presented a methodology based on a new MKL algorithm capable of achieving a tunable sparse selection of features sets that improves the classification accuracy rate. Tang et al. [9] proposed a new multi-kernel framework for classification task. It provides an alternative optimization algorithm as the efficient solution for multiple kernel learning. However, the improvement of MKL is so limited, the reason is that the kernels are incorporated into one single Reproducing Kernel Hilbert Space (RKHS) without any sifting; hence some undesirable kernels are involved in the model, leading to a degradation in the performance [10].
An alternative technique to improve the performance is ensemble learning, which has been empirically and theoretically [11] demonstrated to better regression performance than the best single regressor. For instance, Thongkam et al. [12] proposed a combination of the Adaboost and random forests algorithm for constructing a prediction model. The experimental results indicate that the proposed method outperforms both single classifier and any other combined classifiers.
Based on above discussion, a novel kernel-based regression method is developed under the philosophy of ensemble learning, which refers to the process of combining multiple regressors to constitute a single and unified regressor. The success of proposed method owes to the fact that the kernels are separated into multiple RKHSs, more importantly, the regressors that are learned in parallel from multiple spaces, are weighted in the training period in terms of the regression errors. Specifically, the kernels are put into a pool in advance, and only those kernels that the values of regression error are lower than some threshold, can be selected. As such, ineffective regressors are depressed while excellent regressors are strengthened.
The second innovation of this paper considered the sparsity of loss in the learning framework. In traditional regression methods, a quadratic loss term is commonly used. However, it is only reasonable under the assumption that, all data samples have approximately same precision. Unfortunately, this assumption is not always true. Actually, some data samples are corrupted due to variety of imperfect factors in practical cases, this implies the regression errors are not evenly distributed; instead, they are inclined to be sparse in most cases. In view of this fact, we introduce sparsity into our regression model, and use an L1-norm to characterize the regression error in a mathematical form, as such, a Sparse Loss induced Kernel Ensemble Regression (SLiKER) is derived.
To sum up, the main contributions of this paper are listed as follows:
(1)We develop a novel regression method based on kernel trick and ensemble principle. Its merit is that multi-kernel selection and parameter decision can be conducted automatically through a pool of kernels.(2)In our proposed method, we introduce sparsity to evaluate the quality of the model. With this sparsity model, well-behaved regressors are selected and the impacts of badly-behaved regressors is decreased.
Experimental results on UCI regression and computer vision data sets indicate that compared to other regression ensemble methods, such as random forest and XGBoost, our method has the advantages of best performances in keeping lowest regression loss and highest classification accuracy.
The structure of the paper is organized as follows: Related work about regression and multiple kernel learning is described in Section 2. Section 3 presents the proposed Sparse Loss induced Kernel Ensemble Regression (SLiKER) in detail. In Section 4, the experiments are tested on UCI data sets and computer vision data sets sequentially. Finally, we conclude this paper in Section 5.
