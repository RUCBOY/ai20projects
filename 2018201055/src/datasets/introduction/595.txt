Spark [25] is an open-source distributed big data processing framework widely used after Hadoop. Based on memory calculation, Spark improves the real-time performance of data processing in big data environments while ensuring high fault tolerance and scalability. It supports a range of advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for image processing, and Spark Streaming. These components ensure that Spark performs well in a wide range of domains including machine learning [2], [11], log file analysis [20], database management [1], and graph computing [4]. One important class of these applications usually run repeatedly with similar input dataset size, which is called periodic jobs [36]. The benchmarking community has theoretically and practically proven that most long-running applications also can be represented by some periodic jobs [38].
Spark defines more than 180 configuration parameters to support efficient operation of periodic jobs. The setting of these parameters has strong impacts on overall performance, which have been set default values when deploying Spark [19]. Choosing the right configuration can significantly improve Spark performance. However, the Spark configuration for optimum performance is application-specific, applying only one configuration or adjusting one single parameter leads to sub-optimal performance [9], [22]. For users to select an optimal configuration for their individual requirements, we first need to understand the relationship between configuration parameters and performance, hence giving rise to the fundamental problem of predicting Spark performance based on configuration.
The Spark configuration parameters are not only numerous but also have complex interactions between them [29], [36]. Besides, when the input data set is large, it takes a considerable amount of time for each run of the Spark application. These difficulties make the performance prediction of Spark applications in any configuration more challenging. Performance models are often an effective way to help solve issues, which can predict much faster than an approach that requires executing the application. For example, Nhan Nguyen et al. [21] propose a fine-grained model to predict the execution time of each stage in a Spark application, aggregating together per-stage prediction then yields an estimate for the total execution time. However, the model only analyzes the impact of single parameters on the performance and does not consider a specific configuration. Ni Luo et al. [18] build a model based on support vector machine (SVM) to predict the performance of Spark applications, but the prediction accuracy of the model will decrease significantly with the increase of the configuration parameters. Problems in the above research prompted us to explore more effective modeling technologies.
This paper proposes a new approach based on Adaboost to predict the performance of a given application with a given Spark configuration efficiently and accurately. The Adaboost [6] algorithm is a kind of ensemble learning in machine learning. It builds and combines multiple learners to complete learning tasks and has high prediction accuracy when dealing with high-dimensional complex problems. We consider the observations obtained from the real Spark system to train the performance prediction model at the stage-level. The model takes the Spark configurations as input and outputs a performance prediction.
The cost of collecting the training samples required to build the model is important for low-overhead prediction [31]. We address this problem using projective sampling, an advanced sampling method in data mining. It approximates the learning curve [27] by using a minimum set of initial sample points and then calculates the optimal sample size required for modeling based on the learning curve. The advantage of projective sampling is that it can provide users with a global optimal sample size which trades off between the accuracy and cost of the model, thus increasing the utility of the entire prediction process.
The approach we proposed has the following advantages over the existing approaches. First, the proposed approach does not make any assumption on the relationship between the configuration parameters. Second, as an ensemble learning approach, Adaboost can overcome the defect of single learner’s poor generalization ability by combining multiple learners. When dealing with new datasets or larger problem spaces, the model built on the AdaBoost algorithm performs more stable and robust. Third, projective sampling uses part of the available data to obtain a partial function of the learning curve and then uses it to analyze and estimate the optimal sample size. This strategy can significantly reduce the cost of the entire modeling process.
In particular, we make the following contributions in this paper:

1.We leverage the Adaboost based model to accurately predict the performance of Spark applications with the given configuration.2.We employ projective sampling to achieve low overhead prediction.3.We evaluated our approach on six generic Spark benchmarks. The results show that the average error of model constructed by our approach is only 9.02% with low cost compared to the existing approach.
The rest of the paper is organized as follows. Section 2 discusses the background and motivation. Section 3 explains our performance prediction model. Section 4 discusses the reusability of the model. Section 5 describes our experimental setup. Section 6 presents the results and analysis. Section 7 discusses the related work and Section 8 concludes the paper.
