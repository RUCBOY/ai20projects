Gaussian processes (GPs) [1] are flexible and effective probabilistic modeling tools. There are wide applications of GPs in many problems of machine learning and statistics. For example, GPs have been used with success in active learning [2], [3], multi-task learning [4], [5], reinforcement learning [6], [7], semi-supervised learning [8], [9], [10], and so on. Generally, standard GPs only focus on the scenario where data are provided by a single view, that is, GPs are usually used to deal with the single-view data. For the moment, there are very few applications in multi-view learning for GPs.
Generally, we can describe the features of the same object from different views. Multi-view data sets, therefore, are common in real applications, which include multiple associated information from different feature sets. For instance, for web-page classification, a web-page can be described by two views, i.e., the words appear on the web page and the words occur in the hyperlink pointing to that page from other web pages. For image classification, we can express an image by the visual features within it, such as color, texture, and shape, and also by the text information surrounding it. Similarly, for speaker recognition, a speaker can be represented by the audio data and also by the visual data, considering the correlations between phonemes and lip poses.
Multi-view data often provide more complete information for us, compared with single-view data. Without loss of generality, usually we will not obtain the optimal results either by the methods that just using one of the multiple views or simply concatenating all views to construct a new high-dimensional view. Some approaches show that multi-view learning can effectively solve this problem and it is possible to get better performance. When there is only one natural view available, the general approach artificially generating other views for multi-view learning is still likely to improve the performance [11]. Therefore, in recent years, there has developed a strong interest in multi-view learning [12], [13], [14], [15], [16], [17], [18], [19].
According to different strategies for the use of data information from multiple views, we can divide the existing multi-view learning methods into three groups [15]: (i) co-training style methods that train alternately the classifiers of different views [11], [20], [21], (ii) co-regularization style methods that minimize discrepancy of different views by adding a regularization term to the objective function [22], [23], [24], [25], [26], and (iii) margin-consistency style methods that exploit the latent consistency of multiple views to deal with classification tasks [27], [28], [29], [30]. Though GPs are effective Bayesian non-parametric methods in machine learning, they are barely directly applied to the scenario of multi-view learning.
In this work, we present two kinds of multi-view GPs frameworks with posterior consistency, called MVGP1 and MVGP2, respectively. Our inspiration is derived from the co-regularization principle, which is to maximize the agreement across different views by adding the regularization term. Similarly, when this idea is introduced into the context of Bayesian learning, we expect that for the same instance, the latent functions on different views have a consistent posterior distribution. Formally, the proposed frameworks utilize the posterior distribution consistency of latent functions across different views and combine multiple views by regularizing the marginal likelihood. Specifically, our frameworks first use a Gaussian process to model each view of data. Then we propose the consistency criterion to regularize the marginal likelihood and optimize the hyper-parameters of the GPs collaboratively by multiple views. Here, we integrate the principle of maximizing the marginal likelihood on each view and minimizing the difference among the posterior distributions on every pair of views. As for the difference among the posterior distributions, we exploit Kullbackâ€“Leibler (KL) divergence [31] to characterize it, which is a way of quantifying the discrepancy between two probability distributions. Moreover, since the MVGP1 does not consistently perform the best, to further improve classification performance, we present the other kind of multi-view GPs framework MVGP2 based on MVGP1. In the experiments, to verify the effectiveness of the proposed frameworks, we have made a comparison with three single-view baseline methods, the representative multi-view baseline method, and the state-of-the-art multi-view learning method on multiple real-world classification data sets. Meanwhile, we perform statistical tests to evaluate the significant difference between the MVGPs and mentioned above five methods. Moreover, for the comprehensive comparison of the proposed frameworks, we not only consider the final classification accuracy and the corresponding standard deviation, but also should think about the training time. Therefore, we further record the training time corresponding to MVGP1 and MVGP2 on all the data sets.
The contributions of our work are summarized as follows. 1) We present two kinds of frameworks for multi-view GPs with posterior consistency, called MVGP1 and MVGP2, respectively. 2) Our work is to develop the GPs into multi-view learning methods to handle classification in machine learning. By regularizing marginal likelihood with the consistency of posterior distributions, our frameworks combine multiple views well, accompanied with elegant inference and optimization. 3) We show that the proposed frameworks work well on real-world classification data sets. Specifically, the MVGP1 significantly reduces the training time of the multi-view GPs with acceptable performance on the accuracy, and the MVGP2 further improves the classification accuracy and achieves state-of-the-art performance. This paper can be regarded as the extension of the work on [32]. Compared with the previous paper, on one hand, we present a new multi-view GPs framework for multi-view classification in this paper, which directly extends the GPs to the scenario of multiple views by the posterior distribution consistency criterion of latent functions among different views. On the other hand, in the experiments, we use more data sets to evaluate the effectiveness of the proposed frameworks.
The remainder of this paper is organized as follows. Section 2 reviews the related work. Section 3 briefly reviews the standard Gaussian Processes. Section 4 presents the MVGP1, our first multi-view GPs framework. In Section 5, we introduce the other kind of multi-view GPs framework, MVGP2. In Section 6, we give illustrations about the extensions in the scenario of multiple views. Section 7 reports the experimental results. Finally, we conclude this paper and point out possible future work in Section 8.
