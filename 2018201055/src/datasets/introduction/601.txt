With the emergence of television, video has become a major form of communication between companies and consumers and has replaced former media to a large extent (Kretschmer & Peukert, 2020). As videos appeal to both the visual and acoustic senses, they tend to be more engaging than other forms of content (e.g., Choi and Johnson, 2005, Dhaoui and Webster, 2020). This growing importance is vastly amplified with increasing bandwidth and the widespread availability of video recording and editing tools, making video a medium of communication for any corporate user as well as the general population. According to industry estimates, by 2022, 82% of all internet traffic will consist of video content (Cisco, 2019). The popularity of videos among private individuals is also evidenced by consumer interest in video services such as YouTube, which is the second most frequented website worldwide (Alexa, 2020). Additionally, video content has spurred many new businesses, such as TikTok, which leverage the popularity of moving images and have grown rapidly (Leskin, 2020). Corporate budget allocation decisions mirror the popularity of videos. For example, online video advertising budgets are projected to grow by 6% annually over the next four years (Statista, 2019).
Accordingly, video formats have gained interest in business research in the past decade, leading to an almost continuous increase in the number of video-related papers published each year (see Fig. 1). Although manually analyzing videos can be a very cumbersome task and almost impossible to scale to larger datasets, only a few publications in business research on videos have employed automated analysis. Nonetheless, many prominent video-related applications have already emerged, e.g., predicting movie success (Eliashberg and Sawhney, 1994, Himes and Thompson, 2007), creating engaging video games (Wood et al., 2004), or explaining virality of commercials (Akpinar and Berger, 2017, Dessart and Pitardi, 2019, Simmonds et al., 2019, Tellis et al., 2019). Recently, a few researchers have started to work with automated video analyses (e.g., Li et al., 2019, Liu et al., 2018). Automated analysis is important because it allows studying more observations to investigate more nuanced effects, interactions, or rarely occurring (but potentially important) events. It also enables researchers to control for a larger variety of video features and avoid potential confounds or omitted variable bias. In addition, automated analysis facilitates research replications and extensions because it does not rely on subjective coding.
Videos consist of sequences of images, with typically 20 to 30 images (frames) per second (fps). The lower boundary of 20 fps roughly represents the threshold at which humans perceive a sequence of frames as fluid motion (Berkeley Institute of Design, 2012). A central component to any automated video analysis is therefore the analysis of individual images. In the past years, significant progress in the field of image mining has been made (e.g., Burnap et al., 2019, Schikowsky et al., 2020), and some of these developments have been translated to moving images. However, we are not aware of a consolidated open-source video mining toolbox for business research. While some recent publications employ automated analyses (e.g., Li et al., 2019, Liu et al., 2018), these are limited to specific subsets of video-based features (typically from 2–3) and often do not provide source code or utilize proprietary software services. We therefore consolidate feature extraction theory from both image- and video-related research to build an open-source tool to extract and aggregate video features that are relevant for the types of econometric models investigated in business research.Download : Download high-res image (299KB)Download : Download full-size imageFig. 1. Number of Video Papers in Business Research.
Mining videos is a nontrivial task with many challenges, starting with data preparation and extending to final extraction. First, a selection of features for extraction must be made. This is a particular issue when manually coding videos, which is likely to be one of the reasons that research has proceeded with only a few features at a time so far. However, such a focus on selected features can create omitted variable bias and erroneous substantive conclusions since video consumption is a holistic experience. It is therefore desirable to find ways to automatically extract as many features as possible and make use of appropriate econometric techniques or machine learning to create dense representations and test their impact. This goal is complicated by the fact that information on how to obtain video features is widely scattered across diverse literature, which drastically increases implementation costs for business researchers. While several commercial services exist, these are mostly a black box to researchers and typically do not report classification accuracy. These services might also be subject to opaque changes and discontinuation by the commercial vendors, endangering key constituents of business research, namely, transparency and reproducibility.
In terms of technical implementation, the information richness of videos can make data processing cumbersome due to large file sizes and datasets. Hence, efficient approaches to (a) extract relevant features and to (b) aggregate them meaningfully from frame level to video level are needed so that data can be appropriately handled in econometric models.
This research structures and discusses interpretable video features, which can be currently extracted with open-source techniques, and consolidates them into a comprehensive analysis framework, including (a) extensions of image mining techniques to moving images, e.g., for face, emotion, and object detection, (b) implementation of formerly manually coded established visual concepts such as colorfulness, and (c) extensions and enhanced combinations of newly developed state-of-the-art techniques based on a systematic screening of video-related GitHub repositories, such as an embedding-based visual variation measure. We additionally implement functions to aggregate those features in a meaningful way. To facilitate application, we provide the consolidated Python scripts and concept implementations as one easy-to-use tool1  with a Colab notebook acting as a graphical user interface (GUI), thereby requiring very little to no programming knowledge while enabling full transparency of the code. We also provide a step-by-step video tutorial to assist readers in terms of application.2  Based on an exemplary case study on movie advertising, we illustrate how the extracted features can be included in econometric models to gain substantive insights. Specifically, we apply our tool to 975 movie trailers. For these data, adding static frame-based video features such as the presence of human faces increases the explanatory power by nearly 17% compared to a baseline model based on movie budget, release timing, and genre. Additionally, adding dynamic features based on frame sequences (e.g., scene cuts) improves the explanatory power by nearly 30%. These findings suggest that relevant effects can be detected with automated analysis. Similar procedures can be applied to a variety of business research problems to complement the growing body of image mining research (e.g., Hartmann et al., 2020, Li et al., 2019).Table 1. Extracted features in video literature.AutomaticNon-automaticContent features• Objects (e.g., Li et al., 2019)• Emotions (e.g., Choudhury et al., 2019, Lu et al., 2016)• Humans, animals (e.g., Bellman et al., 2012, Dessart, 2018)• Objects (e.g., Kumar & Tan, 2015)• Emotions (e.g., Bellman et al., 2012)• Text content (e.g., Fossen and Schweidel, 2019a, Roberts et al., 2015)• Story (e.g., Akpinar and Berger, 2017, Loewenstein et al., 2011)• Branding, sponsorship (e.g., Tellis et al., 2019)• Message tone, e.g., functional, emotional (e.g., Geuens et al., 2011) • Other specific characteristics, e.g., stereotypes (e.g., Avraham, 2018)Structural features• Visual variation (e.g., Couwenberg et al., 2017)• Scene cuts (e.g., Liu et al., 2018)• Color characteristics (e.g., Couwenberg et al., 2017)• Duration (e.g., Li et al., 2019)• Quality (e.g., Hautz et al., 2014, Liu-Thompkins and Rogerson, 2012)• Interactive elements, e.g., skip button (e.g., Jeon et al., 2019)
