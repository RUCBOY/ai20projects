As the prevailing disease with the highest mortality, the research on brain tumors has received more and more attention. In this paper, we study a deep learning-based automatic way to segment the glioma, which is called brain tumor segmentation (BTS) [1]. In this task, the medical images contain four MRI modalities, which are the T1-weighted (T1) modality, contrast enhanced T1-weighted (T1c) modality, T2-weighted (T2) modality, and Fluid Attenuation Inversion Recovery (FLAIR) modality, respectively. The goal is to segment three different target areas, which are the whole tumor area, the tumor core area, and the enhancing tumor core area, respectively. An example of the multi-modality data and the corresponding tumor area labels are shown in Fig. 1.
With the rapid development of the deep learning technique, deep convolutional neural networks (DCNNs) have been introduced into the medical image analysis community and widely used in BTS. Given the established DCNN models, existing brain tumor segmentation methods usually consider this task as a multi-class pixel-level classification problem just as the semantic segmentation task on common RGB image data. However, by omitting the great disparity between the medical image data and the common RGB image data, such approaches would not obtain the optimal solutions. Specifically, there are two-fold distinct properties between these two kinds of data: 1) Very large-scale RGB image data can be acquired from our daily life by the smart phones or cameras. However, the medical image data are very scarce, especially for the corresponding manual annotation that requires expertise and tends to be very time consuming. 2) As a departure from the common RGB image data, the medical image data (for the investigated brain tumor segmentation task and other tasks) usually consist of multiple MRI modalities that capture different pathological properties.Download : Download high-res image (156KB)Download : Download full-size imageFig. 1. An illustration of the brain tumor segmentation task. The top four volume data are the multi-modality MR image data. The segmentation labels for the Whole Tumor area (WT), Tumor Core area (TC), Enhancing Tumor Core area (WT), and all types of tumor areas are shown in the bottom row. The regions without colored masks are normal areas.
Due to the above-mentioned characteristics, BTS still has challenging issues needed to be addressed. Specifically, due to the insufficient data scale, training a DCNN model might surfer from the over-fitting issue as DCNN models usually contain numerous network parameters. This increases the difficulty of training a desired DCNN model for brain tumor segmentation. Secondly, due to the complex data structure, directly concatenating multi-modality data to form the network input like in the previous works [2], [3] is neither the best choice to fully take advantage of the knowledge underlying each modality data, nor the effective strategy to fuse the knowledge from the multi-modality data.
To address these issues, this paper proposes a novel cross-modality deep feature learning framework to learn to segment brain tumors from the multi-modality MRI data. Considering the fact that the medical image data are relatively scarce in terms of the data scale but contain rich information in terms of the modality property, we propose to explore rich patterns among the multi-modality data to make up for the insufficient data scale. Specifically, the proposed cross-modality feature learning framework consists of two learning processes: the cross-modality feature transition (CMFT) process and the cross-modality feature fusion (CMFF) process.
In the cross-modality feature transition process, we adopt the generative adversarial network learning scheme to learn useful features that can facilitate the knowledge transition across different modality data. This enables the network to mine intrinsic patterns that are helpful to the brain tumor segmentation task from each modality data. The intuition behind this process is that if the DCNN model can transit (or convert) a sample from one modality to another modality, it may capture the modality patterns of the two MRI modalities as well as the content patterns (such as the organ type and location) of this sample, while these patterns are helpful for brain tumor segmentation. In the cross-modality feature fusion process, we build a novel deep neural network architecture to take advantage of the deep features obtained from the cross-modality feature transition process and implement the deep fusion of the features captured from different modality data to predict the brain tumor areas. This is distinct from the existing brain tumor segmentation methods or the naive strategies which either 1) implement the fusion process simply at the input level, i.e., concatenating multi-modality image data as the network input, or 2) implement the fusion process at the output level, i.e., integrating the segmentation results from different modality data.
Fig. 2 illustrates the proposed learning framework briefly, from which we can observe that in the cross-modality feature transition process, we build two generators and two discriminators to transit the knowledge across the two modality data. Here the generators are used to generate one modality data from the other modality data and the discriminators aim to distinguish the generated data and the real data. While in the cross-modality feature fusion process, we adopt the generators to predict the brain tumor regions from each modality data and fuse the deep features learned from them to obtain the final segmentation results. In the fusion branch, we design a novel scheme by using the single-modality prediction results to guide the feature fusion process, which can obtain stronger feature representations during the fusion process to aid segment the desired brain tumor areas.Download : Download high-res image (351KB)Download : Download full-size imageFig. 2. An illustration of the proposed cross-modality deep feature learning framework for brain tumor segmentation. To be brief and to the point, we only show the learning framework by using two-modality data.
To sum up, this work mainly has four-fold contributions as follows:
•By revealing the intrinsic difference between the segmentation tasks on the medical image data and the common RGB image data, we establish a novel cross-modality deep feature learning framework for brain tumor segmentation, which consists of the cross-modality feature transition process and the cross-modality feature fusion process.•We present a novel idea to learn useful feature representations from the knowledge transition across different modality data. To achieve this goal, we build a generative adversarial network-based learning scheme which can implement the cross-modality feature transition process without any human annotation.•For implementing the cross-modality feature fusion process, a new cross-modality feature fusion network is built for brain tumor segmentation, which transfers the features learned from the feature transition process and is empowered with the novel fusion branch to use the single-modality prediction results to guide the feature fusion process.•Comprehensive experiments are conducted on the BraTS benchmarks, which show that the proposed approach can effectively improve the brain tumor segmentation performance when compared with the baseline methods and the state-of-the-art methods.
