Multi-label learning (MLL) is a machine-learning problem that aims to make predictions for samples with more than one positive labels [16], [58]. Given a label set L={l1,l2,…,lM}, traditional single-label learning is used to train a model that maps the samples from the n-dimensional input space to the discrete label set, i.e., h:Rn→L. Whereas MLL is used to train a model that maps the samples from the input space to the powerset of L, i.e., h:Rn→2L. The label powerset (LP) method [3] is a traditional solution for MLL. It considers each element in 2L as a single class and constructs a single-label multi-class model to make the prediction, i.e., h(x)=argmaxY∈2LP(Y|x). Because of the large cardinality of 2L, this method is usually data hungry with a high complexity. Thus, many approximate methods were proposed [13], [33], [37], [38].
Notably, MLL can be conducted for batch data or stream data [28], [29], in either supervised mode or semi-supervised mode [5]. In this paper, we only focus on the supervised mode with batch data. A comprehensive and didactic review on the existing software tools for MLL has been provided in the literature [7]. Essentially, there are three groups of MLL methods: problem transformation [14], ensemble [27], and algorithm adaption [54]. Problem-transformation methods transform the multi-label problem into a set of single-label problems and make predictions by integrating the results of single-label models. Ensemble methods can be considered improved versions of problem-transformation methods by introducing some sampling strategies. These two groups of methods can be realized using any single-label model, while an algorithm-adaption method is designed for a specific learning model [15], [53]. Notably, both problem-transformation methods and ensemble methods are easy to compare with each other, as they can adopt the same base classifier; however, the comparisons of both these methods with algorithm-adaption methods will be difficult or unfair. Therefore, we only discuss the first two groups of methods.
Problem-transformation methods have first-, second-, or high-order strategies. Binary relevance (BR) [56] is a first-order strategy used to independently construct a binary classifier for each label [21], [51]. Although it is easy to implement, it neglects the label correlations that may affect the performance [24]. Calibrated label ranking (CLR) is a second-order strategy used to construct a binary classifier for each pair of labels [13]. It considers label correlations but only at the pairwise level. To incorporate high-order label correlations, classifier chain (CC) was proposed [33]. Similar to BR, CC constructs a binary classifier for each label, but the classifiers are trained in order, where the feature vector for the subsequent classifier is extended by adding the preceding labels. Variants of CC include probabilistic CC [11], Monte Carlo optimization CC [35], and group-sensitive CC [20]. Their performances significantly depend on the label order or designed label structure, and the robustness is usually low [9].
Literature has shown that in many scenarios, ensemble methods can achieve satisfactory performance [37] by training and integrating several multi-label models. The ensemble LP (ELP) method uses bagging to generate diverse LP models, but the complexity is higher than even that of a single LP classifier. Ensemble of pruned sets makes ELP feasible by pruning the infrequently occurring label sets [32]. Ensemble BR (EBR) and ensemble CC (ECC) construct multiple BR and CC models, respectively, on random selections of training instances sampled via replacement, significantly improving the robustness.
To further improve the performance, the random k-labelsets ensemble (RAkEL) method was proposed [38]. It randomly generates some size-k label subsets in either disjoint or overlapping mode, and it then constructs an LP classifier for each label subset. In the testing phase, the final prediction is obtained via a voting process of the LP classifiers. RAkEL is a high-order method that controls the degree of label correlations by the value of k, and it is more robust than the previous methods. Furthermore, generalized RAkEL was proposed by considering each LP classifier as a dictionary and learning weight coefficients by minimizing the global error [25]. RAkEL++ was proposed by assigning each classifier a confidence value [37]. Fast RAkEL was realized by learning each subset in a two-step hierarchical structure [23]. An improved RAkEL method was proposed with conformal prediction [49]. Other variants of RAkEL, such as, the balanced label contribution strategy (BALCO), were proposed by applying the set covering technique to balance the label frequencies in the selected subsets [37]. Moreover, RAkEL has been extended and applied to many real-world problems, such as clinical data prediction [4] and recognition of anatomical therapeutic chemical classes of drugs [57]. Notably, in determining the label subsets, all these RAkEL methods neglect the features of the samples. On the one hand, the classes transformed from a label subset may have low separability; on the other hand, some label combinations will cause high level of disequilibrium [6], resulting in difficulties in applying the existing single-label learning algorithms. To solve these problems, the following two issues must be addressed:
1.how to mine the characteristics of samples and design effective metrics to evaluate the quality of a label subset;2.how to achieve quick selection from the large number of candidate subsets based on the quality measurements.
To address the first issue, we apply a measurement based on the Fisher’s linear discriminant ratio [45] to evaluate the separability of LP classes, and then we use joint entropy to describe the imbalance level of data. Because separability will be easier to analyze in a feature space with kernel technique, we adopt kernel support vector machine (SVM) as the base learner [41], [50]. For the second issue, we should iteratively control the label-selection process via designed measurements. Inspired by the idea of active learning [1], [10], in this study, we propose an active sampling framework for the k-labelsets method, named as active k-labelsets ensemble (ACkEL). Active learning is well known as a learning paradigm that employs selective sampling for collecting high-quality training samples, and it has been widely studied for traditional single-label learning [39], [40], [42], [43], [44]. It controls the learning process by designing a sample-selection criterion and iteratively updates the model by adding informative samples. In recent years, active learning has also been applied to MLL for selecting high-quality samples [22], [47] or sample–label pairs [36], [48]. Different from these works, in this study, we attempt to use an active mechanism for selecting labels rather than samples. The learner will iteratively decide which label or label subset should be chosen in the current iteration based on a quality measurement. The following are the contributions of this study:
•By applying the kernel technique, we use a discriminant ratio to evaluate the separability of classes in the feature space of SVM. Additionally, joint entropy is adopted to measure the imbalance level in a label subset.•We propose an active label-selection paradigm based on active learning. The discriminant ratio and joint entropy are used to iteratively select high-quality label subsets for the k-labelsets method.•Two ACkEL algorithms are proposed in disjoint and overlapping modes, and they are realized in pool-based and stream-based frameworks, respectively.•We conduct extensive experimental comparisons of both the proposed methods with many state-of-the-art methods on a wide range of application domains, and we assess the performance using four evaluation metrics.
The remainder of this paper is organized as follows. Section 2 introduces the background knowledge and present motivation. Section 3 proposes two ACkEL algorithms. In Section 4, extensive experimental comparisons are conducted. Finally, Section 5 concludes this paper.
