Two-dimensional breast ultrasound (US) is an important non-radiation imaging technique used to detect and classify breast tumors. It is well tolerated by patients and can be easily integrated into interventional procedures for patient treatments (Berg et al., 2008). However, the accuracy of breast US is limited, and depends on the experience and technical ability of the operator. The Breast Imaging Reporting and Data System (BI-RADS) provides standardized terminology and reporting system to assess the breast mass and describe its features. It has been proven to be an effective system for differentiating between benign and malignant masses; however, many BI-RADS US descriptors are associated with both malignant and benign lesions, particularly category 4 breast masses (Liberman and Menell, 2002). Category 4 breast masses exhibit a wide range of malignancy risks (3–94 %); therefore, there is poor reproducibility among radiologists, especially for cancer subcategories 4a and 4b (Lazarus et al., 2006).
Various computer-aided diagnosis (CADx) systems have been developed to distinguish between malignant and benign tumors on US images of breast cancers. Previously, these systems have been proven to enhance diagnostic accuracy, and decrease the variability among observers (Huang and Chen, 2005; Singh et al., 2011). The classifying procedure using traditional CADx systems includes feature extraction, selection, and classification (Min-Chun et al., 2013; Tourassi et al., 2001). The most important problem with these systems is effective feature extraction (Newell et al., 2010); which affects the overall performance. The extraction of meaningful image features is a complicated and time-consuming task. Moreover, fine-tuning the overall performance of traditional CAD is also challenging.
In previous studies (Chang et al., 2003; Zhou et al., 2015), regions-of-interest (ROIs) (i.e., the tumor regions) needed to be pre-defined. It was either done manually by physicians (semi-segmentation) or through auto-segmentation that detected the tumor texture, shape, or region of occurrence. Unless the parameters of the auto-segmentation algorithm are well adjusted, and can correctly mark the location of the tumor, the ROI needs to be defined manually. Moreover, auto-segmentation does not always generate the correct ROIs that match the tumor region. The vanishing/exploding gradient problem (Pascanu et al., 2012) also cannot be fully served in machine learning / traditional neuronal network. It limits the performance and finally makes the learning a “plateau phenomenon” (Park et al., 2000; Wei et al., 2008). Recently, in comparison with obtaining features by semi-segmentation approaches and traditional machine learning classifiers (Larson et al., 2016; Liu et al., 2010), deep learning models have generated more accurate classification results in several computer vision and medical imaging applications. Deep learning models have the ability to learn and integrate each level of the features by stacking hidden layers in the deep network architecture (O’Shea and Nash, 2015). In this architecture, with its large number of layers of training networks, a residual learning model can solve a convergence problem by learning the residual function with respect to each layer’s input (He et al., 2016). Overall, this architecture has an excellent performance.
However, considerable data are needed for the full training of a deep learning network. It is also computationally expensive to obtain convergence through all the layers of the network. Moreover, in the field of clinical research, it is difficult to obtain numerous labeled or annotated breast US image datasets from specialized institutes, in particular, thereby, limiting the application of deep learning in clinical settings. A technique, named “transfer learning,” can help repurpose the problem from a target domain to the new domain with minor adjustments. We consider it an efficient way to extend the limitations of deep learning, when applied to a clinical setting. Therefore, combining both, deep learning and transfer learning, we aimed to: a) increase the accuracy of diagnostic performance to accurately classify malignant tumors of BI-RADS category 4 using US images and b) achieve comparable performances to those reported by experienced physicians, or even better.
