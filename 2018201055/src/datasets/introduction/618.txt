Extracting explicit semantic representation from images is one of the main challenges in computer vision. Recently, with the emergence of VRD, Visual Genome and other datasets, scene graph generation task [1] has gradually come into researchers’ attention, which use scene graph as semantic representation and deep neural network as model for generating scene graphs. The scene graph is defined as a set of triples in the form of (s, r, o), where the subject s and the object o are both entities in an image defined by their bounding boxes and classes and r is a relation between these two entities.
From the perspective of semantics, each triple in a scene graph is an instance of a relation. Hence, a scene graph can be viewed as a kind of knowledge graph that represents semantics, especially semantics about relations, of a given image. However, current deep neural network models commonly treat scene graph generation as labeling the entities and relations between pairs of entities from images, which ignores the fact that the scene graph is a formal representation of semantic. The standard metric for evaluating the performance of scene graph generation models is Recall@k, i.e. the recall of the top-k triples predicted by the models. Such metric only reflects the portion of annotated triples that occur in the top-k predictions, which is not enough for understanding whether the models learn the semantics or not. As a result, both scene graph generation models and the corresponding evaluation metrics cannot help us understanding whether the deep neural network models really learn to capture the semantics of relations or only learn to fit the incomplete annotations in the training data during scene graph generation. That is, current scene graph generation models based on deep learning are not explainable from the perspective of semantics.
For the explanation of deep learning model, there are already many researches on interpreting neural networks based on visualization [2], explaining the attributions of input to the predictions based on back propagation [3], and understanding whether inference relation is correctly predicted based on test set construction [4]. However, there are still lack of studies for understanding the capability of deep learning models for capturing semantics correctly, which is important for explaining whether the deep learning models’ behaviour is consistent and obeys the constraints of the world. Thus, in this paper, we try to investigate the problem of explaining the capability of deep learning models for capturing different semantic properties of relations (i.e. semantics capturing capability) based on deep learning models for scene graph generation task.
The basic idea of this paper is to improve the explainability of deep neural network models by propose a series of metrics for measuring models’ capability of capturing semantics of relations. We divide commonly used semantics properties of relations in OWL2 [5] into two types based on their characteristics: one includes symmetric, transitive, inverse, subproperty of, and equivalent to; the other includes asymmetric, functional, inverse functional, and disjoint with. Accordingly, two types of metrics are proposed: the conformance metrics and the violation metrics, which are designed to measure whether certain semantic property is exhibited in the generated scene graph. Thus, these metrics can be used to explain the semantics capturing capability of scene graph generation models based on deep learning. By conducting experiments on three representative state-of-the-art models for scene graph generation on the Visual Genome dataset, we verify that the proposed metrics can effectively measure the capability of different models for capturing different semantic properties and reveal the advantage and weakness of different models.
This paper consists of six sections. In Section 2, the scene graph task is introduced along with its standard evaluation metric, datasets, and representative state-of-the-art neural network models. The common semantic properties of relations defined in OWL2 and the metrics for measuring the capability of neural network models to capture these properties are defined in Section 3. Experiments are conducted in Section 4 to evaluate the effectiveness of the proposed metrics on explaining scene graph generation models. The related work is discussed in Section 5. Finally, the conclusion is drawn in Section 6.
