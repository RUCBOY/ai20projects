Clustering, one of commonly used data mining techniques, aims to divide data points into groups such that instances in each group are similar to each other and are dissimilar to data points in other groups [16]. Due to its unsupervised learning nature without label information, clustering has been widely used in various fields [20], [1], [34], [8], [23], [22], [18], [43], [44], such as machine learning, patter recognition, image segmentation, and so on. Owing to the rapid development of clustering research, many new algorithms have been invented [10], and existing algorithms can be roughly categorized into several main branches, including centroid based, density based, distribution based, grid based, network based, and hierarchy based clustering.
Centroid based clustering, one of the most intuitive clustering approaches, assumes that every cluster has a representative point, i.e., cluster centre, and all points should be assigned to respective cluster centres. To find clusters, centroid based clustering crucially depends on the selection of the parameter k, which defines the number of clusters in the data. k-means [25], the most representative centroid base clustering algorithm, chooses k points from dataset randomly at first and assigns the rest of points into k categories according to the principle of proximity. k-means++ [4] is a supplement to k-means, and it can provide more stable initial centres. Although simple and efficient, k-means and k-means++ cannot detect non-spherical clusters because of their clustering strategy [30]. Unlike k-means, BSAS [39] is a basic sequential algorithmic scheme, which analyzes points in order, and updates cluster representative dynamically. Once new data is added, the cluster centres are recalculated. Spectral clustering [47] is another well-known centroid based algorithm, which has been shown to be more effective in finding clusters than some traditional algorithms. Spectral clustering transforms datasets into the Laplacian matrix, and then selects the first k eigenvectors to be clustered by k-means.
Among all clustering methods, density based clustering shares a unique strength in the sense that it can segment complicated datasets with arbitrary shapes. The most representative algorithm is DBSCAN [9], which considers that relations among all data points in every cluster are density-connected. DBSCAN firstly selects a density threshold and finds the noises whose densities are lower than this threshold, and then distributes these noises to other clusters with high density. However, DBSCAN requires two input parameters. One not only needs to choose the optimum range of every parameter, but also needs to balance the two parameters [30]. Due to the outstanding performance of DBSCAN, many improved methods have been proposed. HDBSCAN [27] performs DBSCAN over varying epsilon and integrates the result, it is good at identifying clusters of varying densities. DBSVEC [42] introduces support vector machines into density clustering, it can solve the problem of high computational complexity of DBSCAN. DBSCAN-MS [45] employs k-d tree partitioning technique to equally divide datasets, traditional DBSCAN is transformed into distributed DBSCAN, therefore it can meet the challenges of growing dataset size. OPTICS [3] and DENCLUE [17] are also common density based clustering algorithms.
Distribution based clustering assumes that the points in a given cluster are most likely to be derived from the same distribution [40]. The representative of distribution based clustering is EMA [15] which holds the view that the distribution of points in dataset is a Gaussian mixture model. In reality, it is, however, very difficult to guarantee that all datasets can be described by Gaussian distributions. The main principle of the grid based clustering is to divide the data space into a finite number of cells to form a grid structure, and nearby important cells are grouped into clusters [5]. Even though the grid based clustering algorithm only costs a short execution time, its results depend on segmentation of grid and data distributions. With the popularity of deep learning in recent years, network based clustering algorithms have been successively proposed, such as DSC [19], SyncNet [29], DSCDAN [46], etc. These algorithms embed clustering into neural network, and they usually learn effective features for clustering. However, network based clustering is often computationally expensive and there are many hyperparameters that need to be set. Hierarchy based clustering usually organizes data points into a hierarchical structure. Sometimes, this kind of algorithm cannot determine an accurate partition. AGNES [21], BIRCH [48] and ROCK [14] are the most classical hierarchy based algorithm. Recently, hierarchy based clustering is often combined with other mechanisms. GHHC [28] combines grid based clustering, HSyncNet [35] combines network based clustering, and HDBSCAN combines density based clustering.
Compared with the algorithms mentioned above, Rodriguez and Laio proposed a superior peak clustering method [30], combing both density based clustering and centroid based clustering, with many inherent advantages, including (1) a higher clustering accuracy than k-means and EMA, (2) easier than DBSCAN, network based clustering and hierarchy based clustering to adjust parameters, and (3) partitioning datasets into arbitrary shaped clusters, independent of the data distributions, better than grid based clustering. Due to its novel ideas and outstanding performance, peak clustering has attracted many attentions recently, and it is considered another milestone clustering algorithm after the k-means and DBSCAN.
Despite of its great success, our research (detailed in Section 2) has shown that peak clustering also suffers from two major disadvantages, including (1) bad clustering performance in identifying cluster centre in low density clusters, if the underlying dataset has significant sample density differences among clusters; and (2) prone to identifying normal instance as noise, resulting in deterioration in clustering accuracy. Because of these shortages, peak clustering does not have robust performance on some datasets.
Motivated by the above observations, in this paper, we advance the peak clustering into a new extreme clustering to address its inherent shortages. Extreme clustering changes the definition of cluster centres and identifies a density extreme point in a neighborhood as the cluster centre. In addition, it also removes the original strategy of detecting noises, and uses another method that was once used by delta-open set clustering [41], which considers that all noisy points are contained in noise clusters. Our experiments and comparisons on over 40 datasets, including A-sets, S-sets, shape sets, the face database etc., show that extreme clustering not only inherits the validity of peak clustering, but also improves performance to address its shortages. A face clustering experiment on the Olivetti face dataset shows that extreme clustering results in 115% performance gain, compared to peak clustering. Comparisons with other 10 clustering algorithms also show that the performance of extreme clustering is almost always the best for all experiments.
In addition to the benchmark data, we also validate the extreme clustering on a case study - haze origin detection. In recent years, the Chinese government has paid more and more attention to air pollution. The first step in tackling air pollution is to find out the locations of the haze origins. Fortunately, extreme clustering method is suitable to assist researchers to look for them. In this paper, we focus on a city near Beijing, the capital of China, where air pollutions is rather severe. We analyze data from 550 monitoring stations, and eventually find some major haze origins.
The remainder of the paper is organized as follows. The next Section introduces peak clustering, and discusses its shortages. Section 3 presents the technical details of extreme clustering. Sections 4 Robustness, 5 Comparison report the robustness of extreme clustering and comparison experiments. Haze data are analyzed in Section 6. The conclusion and future work are reported in Section 7.
