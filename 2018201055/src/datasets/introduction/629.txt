Graphs are used in many branches of science as a way to represent the patterns of connections between the components of complex systems, including social analysis, product recommendation, web search, disease identification, brain function analysis, and many more.
In recent years there is a surge of interest in learning on graph data. Graph embedding [1], [2], [3] aims to learn low-dimensional vector representations for nodes or edges. The learned representations encode structural and semantic information transcribed from the graph and can be used directly as the features for downstream graph analysis tasks. Representative works on graph embedding include random walk and skip-gram model based methods [4], matrix factorization based approaches [5], [6], edge reconstruction based methods [7], and deep learning based algorithms [8], [9], etc.
Meanwhile, graph neural network (GNN) [10], [11], [12], as a type of neural network architectures that can operate on graph structure, has achieved superior performance in graph analysis and shown promise in various applications such as visual question answering [13], point clouds classification and segmentation [14], fraud detection [15], machine translation [16], molecular fingerprints prediction [17], protein interface prediction [18], topic modeling [19], and social recommendation [20].
Among various kinds of GNNs, graph convolutional network (GCN) [21], a simplified version of spectral graph convolutional networks [22], has attracted a large amount of attention. GCN and its subsequent variants can be interpreted as smoothing the node features in the neighborhoods guided by the graph structure, and have experienced great success in graph analysis tasks, such as node classification [21], graph classification [23], link prediction [24], graph similarity estimation [25], node ranking [26], [27], and community detection [28], [29].
The current GCN-like models assume that the node feature information is complete. However, real-world graph data are often incomplete and containing missing node features. Much of the missing features arise from the following sources. First, some features can be missing because of mechanical/electronic failures or human errors during the data collection process. Secondly, it can be prohibitively expensive or even impossible to collect the complete data due to its large size. For example, social media companies such as Twitter and Facebook have restricted the crawlers to collect the whole data. Thirdly, we cannot obtain sensitive personal information. In a social network, many users are unwilling to provide information such as address, nationality, and age to protect personal privacy. Finally, graphs are dynamic in nature, and thus newly joined nodes often have very little information. All these aspects result in graphs containing missing features.Download : Download high-res image (198KB)Download : Download full-size imageFig. 1. The architecture of our model.
To deal with the above problem, the traditional strategy is to estimate and fill in the unknown values before applying GCN. For this purpose, people have proposed imputation techniques such as mean imputation [30], [31], soft imputation based on singular value decomposition [32], and machine learning methods such as k-NN model [33], random forest [34], autoencoder [35], [36], generative adversarial network (GAN) [37], [38], [39]. However, the process of feature filling and graph learning are separated. Our experiments reveal that this strategy results in degraded and unstable performance, especially when a large number of features are missing.
In this paper, we propose an approach that can adapt GCN to graphs containing missing features. In contrast to traditional strategy, our approach integrates the processing of missing features and graph learning within the same neural network architecture and thus can enhance the performance. Our approach is motivated by Gaussian Mixture Model Compensator (GMMC) [40] for processing missing data in neural networks. The main idea is to represent the missing data by Gaussian Mixture Model (GMM) and calculate the expected activation of neurons in the first hidden layer, while keeping the other layers of the network architecture unchanged (Fig. 1). Although this idea is implemented in simple neural networks such as autoencoder and multilayer perceptron, it has not yet been extended to complex neural networks such as RNN, CNN, GNN, and sequence-to-sequence models. The main reason is due to the difficulty in unifying the representation of missing data and calculation of the expected activation of neurons. In particular, simply using GMM to represent the missing data will even complicate the network architecture, which hinders us from calculating the expected activation in closed form. We propose a novel way to unify the representation of missing features and calculation of the expected activation of the first layer neurons in GCN. Specifically, we skillfully represent the missing features by introducing only a small number of parameters in GMM and derive the analytic solution of the expected activation of neurons. As a result, our approach can arm GCN against missing features without increasing the computational complexity and our approach is consistent with GCN when the features are complete.
Our contributions are summarized as follows:

•We propose an elegant and unified way to transform the incomplete features to variables that follow mixtures of Gaussian distributions.•Based on the transformation, we derive the analytic solution to calculate the expected activation of neurons in the first layer of GCN.•We propose the whole network architecture for learning on graphs containing missing features. We prove that our model is consistent with GCN when the features are complete.•We perform extensive experiments and demonstrate that our approach significantly outperforms imputation based methods.
The rest of the paper is organized as follows. The next section summarizes the recent literature on GCN and methods for processing missing data. Section 3 reviews GCN. Section 4 introduces our approach. Section 5 reports experiment results. Finally, Section 6 presents our concluding remarks.
