In recent years, dialog systems have developed rapidly and are now widely used in various human–computer interaction scenarios such as voice assistants and robots. Spoken language understanding (SLU) is a key technology for building such systems and typically involves two tasks: intent detection and slot filling [1], [2]. Intent detection determines what the user wants to do, while slot filling extracts semantic concepts related to the user’s intent. As shown in Fig. 1, given an utterance, an SLU system will annotate a specific intent for the whole utterance and different slot labels for each word in the utterance.
As the semantic concepts of an utterance, intent and slot are supposed to share the semantic features and rely on each other. To take advantage of the semantic correlation between the two tasks, multi-task joint models for SLU have been widely proposed. Zhang [3] and Hakkani-Tur [4] utilized the GRU-based and LSTM-based RNN to capture the shared word semantic features for intent detection and slot filling, respectively. These methods can be characterized by shared parameters, but the explicit relationship between the intent and slots is not established. Since the slots usually depend strongly on the intent, Goo [5] and Li [6] proposed different gate mechanisms to incorporate the intent information for slot filling. However, simple gate mechanisms can be risky [7], and none of the works reported to date model the slot label dependency. Zhu [8] proposed a focus mechanism that improved the attention mechanism with exact alignment to model the slot label dependency, but their method does not consider intent detection. Liu [9] utilized an RNN to model the slot label dependency. Recently, Qin [10] proposed to perform token-level intent detection, followed by feeding the inferred intent feature to the aligned LSTM-based model for slot filling, and achieved superior performance. Although the intent and slot features are explicitly considered during slot filling, this method only models the interactions between the input features implicitly by LSTM.
Previous works have sought to capture more richer word, intent and slot semantic features for better performance. Slot filling is essentially a sequence labeling task, and there is a dependency between the slot labels. In addition, the intent information contributes to slot filling. Therefore, it is useful to incorporate the intent and slot information. However, the explicit interactions between these features have not been modeled effectively and may generate beneficial features more efficiently and make the learning process more interpretable.
In this paper, we propose a novel joint model for joint intent detection and slot filling by introducing the position-aware multi-head masked attention (PMMAtt) mechanism, where the explicit feature interactions are modeled as the inner product of the word encoding vector and intent–slot feature vectors. These interactions are further normalized into weights, indicating how much the model should pay attention to the corresponding feature. Moreover, the multi-head attention mechanism is adopted to capture sentence-level semantic information in order to improve the robustness of intent detection. Since the proposed joint model Attends to Intent and Slots Explicitly, we name it AISE.
To summarize, the key contributions of this paper are as follows:Download : Download high-res image (67KB)Download : Download full-size imageFig. 1. An example with intent and slot annotation (BIO format), which indicates the slot of the movie name from an utterance with an intent SearchScreeningEvent.

1.We introduce the PMMAtt mechanism to model explicit feature interactions between multiple tasks, which can guide models to rationally improve the performance and enable more efficient and interpretable learning.2.We conduct extensive experiments, and the results on two publicly available datasets demonstrate the effectiveness of the proposed model.3.We analyze the effect of different model components on the overall performance. The visualization of the learned weights illustrates the interpretability of our model.
