In many research topics, such as data mining, pattern recognition, and computer vision, we encounter very high dimensional data [24]. These data are difficult to handle in various machine learning processes owing to heavy execution times and large memory requirements, as well as redundant information [6]. All these problems deteriorate the performance of learning algorithms and restrict their applications [37]. To overcome this problem, feature selection methods have been introduced. Feature selection is the process of selecting the important features and removing the unnecessary ones from the original feature set according to certain evaluation methods. As a result of the process, the selected feature subset is expected to help machine learning tasks, increase interpretation and visualization, and reduce the curse of dimensionality, the risk of over-fitting, and computational cost [25].
Feature selection methods can be categorized into three categories according to the class information: supervised, unsupervised, and semi-supervised feature selection [26]. Semi-supervised feature selection method evaluates features from a small number of labeled data and a large number of unlabeled data [38]. In supervised feature selection, discriminative features are selected using existing label information [29]. Unsupervised feature selection methods evaluate feature by the capability of keeping particular properties of the data, such as the variance. In many cases, however, the cost of labeling makes it difficult to obtain the label information [32]. Thus, unsupervised feature selection methods have received more attention [2], [4], [13], [14], [36]. In this paper, we focus on unsupervised feature selection.
Many unsupervised feature selection methods use similarity information among patterns, such as the Laplacian matrix, to preserve local structures [2], [4], [10], [14], [32], [36]. However, similarity among patterns is calculated using all of the original features, and the information is not modified during the feature selection process. Table 1 shows a data set consisting of five features and six patterns. Laplacian matrices can be calculated using various feature subsets from the data set. The matrices show different tendencies, as shown in Fig. 1. Fig. 1(a) shows the Laplacian matrix using all features, which is more blurry than Fig. 1(b) and (c) showing Laplacian matrices that use small feature subsets. When using all features, the local structure information can yield incorrect feature selection results because it may include information for noisy features.Table 1. Simple example.f1f2f3f4f5p110100p210100p310010p401010p501001p601001Download : Download high-res image (392KB)Download : Download full-size imageFig. 1. The Laplacian matrices for various feature subsets.
In this paper, we propose a new unsupervised feature selection method that considers the pairwise dependence of features (feature dependency-based unsupervised feature selection, or DUFS). The DUFS method focuses on the dependence among features rather than local structure information such as a Laplacian matrix. The proposed method calculates the dependence among features based on information theory and applies this dependence information to unsupervised feature selection, and achieves robust feature selection by giving independent features priority to avoid redundant features. Experimental results show that the proposed method outperforms existing state-of-the-art unsupervised feature selection methods in most cases.
The main contributions of this paper are as follows:
•We propose a new unsupervised feature selection based on information theory. To the best of our knowledge, the proposed method is the first to consider dependence among all features.•We integrate unsupervised feature selection and this dependency-based approach into a single optimization problem and introduce an iterative algorithm to solve the problem.
The rest of the paper is organized as follows. Section 2 reviews the evolution of unsupervised feature selection methods. In Section 3, we propose an optimization problem for feature selection and an iterative algorithm for solving the problem. In Section 4, various experimental results are analyzed. Conclusions are drawn in Section 5.
