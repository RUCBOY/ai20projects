Due to its great importance in deep natural language understanding and various down-stream applications, word embedding, sentence embedding and document embedding has been drawing more and more attention in recent years. To provide vector representation for a word or sentence or document is still a challenging task in the Natural language processing domain. Word2vec [1] and Golve [2] are the popular methods for providing representation of a word. Socher et al. [3] propose a method to provide embedding representation for phrases and sentences. Further, Le et al. [4] extends this work to provide vector representation for sentences and documents. The problem of obtaining a semantic embedding for a sentence that ensures that the related sentences are closer and the unrelated sentences are farther lies at the core of understanding languages. This is a challenging task and leads to obtaining and improving the embedding for input text sequence. This would be relevant for a wide variety of machine reading comprehension and related tasks, such as sentiment analysis. To solve this problem, we propose a supervised method that uses a sequential encoder-decoder framework for paraphrase generation. The task of generating paraphrases is closely related to the task of obtaining semantic sentence embeddings. In our approach, we aim to ensure that the generated paraphrase embedding should be close to the corresponding true sentence and far from unrelated sentences. The embeddings so obtained help us to obtain state-of-the-art results for paraphrase generation task. The source code for our work is present here.1
In this work, we proposed a pair-wise loss function for the task of paraphrase question generator, which will bring similarly structured sentences close to each other as compared to dissimilar sentences. In this work, we use local cross-entropy loss to generate each word in the sentence and global pair-wise discriminator loss to capture the complete sentence structure in the given set of paraphrase sentences. Our model consists of a sequential encoder-decoder that is further trained using a pair-wise discriminator. The encoder-decoder architecture has been widely used for machine translation and machine comprehension tasks. In general, the model ensures a ‘local’ loss that is incurred for each recurrent unit cell. It only ensures that a particular word token is present at an appropriate place. This, however, does not imply that the whole sentence is correctly generated. To ensure that the whole sentence is correctly encoded, we make further use of a pair-wise discriminator that encodes the whole sentence and obtains an embedding for it. We further ensure that this is close to the desired ground-truth embeddings while being far from other (sentences in the corpus) embeddings. This model thus provides a ‘global’ loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings. This is illustrated in Fig. 1. We further evaluate the validity of the sentence embeddings by using them for the task of sentiment analysis. We observe that the proposed sentence embeddings result in state-of-the-art performance for both these tasks. In this work, we use standard datasets like Quora Question Pair (QQP) dataset for paraphrase question generation task and Stanford Sentiment Treebank (SST) dataset for sentiment analysis task. In Table 1, we analysis various state of the art methods with kind of model is used like“deterministi” or “probabilistic”, loss function, task and dataset.Download : Download high-res image (188KB)Download : Download full-size imageFig. 1. Pairwise Discriminator based Encoder-Decoder for Paraphrase Generation: This is the basic outline of our model which consists of an LSTM encoder, decoder and discriminator. Here the encoders share the weights. The discriminator generates discriminative embeddings for the Ground Truth-Generated paraphrase pair with the help of ‘global’ loss. Our model is jointly trained with the help of a ‘local’ and ‘global’ loss which we describe in Section 2.Table 1. Overview of various Paraphrase Question Generation (PQG) methods and their various properties. AD: Adversarial, CE: Cross Entropy, PA: Pairwise, SA: Sentiment Analysis, RL: Reinforcement Learning, KL: KL divergence Loss Learning.MethodsBase ModelAdversarialLossTaskDatasetSeq-to-Seq [5]deterministicXCEParaphraseCOCOAttention [6]DeterministicXCEParaphraseCOCOResidual LSTM [7]DeterministicXCEParaphraseCOCOVAE [8]ProbabilisticXCE, KLParaphraseQQP, COCORbM-SL [9]DeterministicXCE, RLParaphraseQQP, TwitterVAE-M [10]ProbabilisticXCE, KLParaphraseQQP, COCOEDL (Ours)DeterministicXCEParaphrase,SAQQP, SSTEDLPG (Ours)Deterministic✓CE, AD, PAParaphrase,SAQQPEDLPGS (Ours)Deterministic✓CE, AD, PAParaphrase,SAQQPEDLP (Ours)DeterministicXCE, PAParaphrase,SAQQP, SSTEDLPS (Ours)DeterministicXCE, PAParaphrase, SAQQP, SST
Our contributions are as follows:
•We propose a model for obtaining sentence embeddings using a pairwise discriminator based on an LSTM encoder-decoder framework for paraphrase generation with encoder and decoder sharing the weights.•The use of a local cross-entropy loss to generate each word in the sentence and a global pair-wise discriminator loss to capture the complete sentence structure.•We show that these embeddings can also be used for the sentiment analysis task.
1.1. Paraphrase question generation (PQG) taskIn the paraphrase Question generation task, given an input sequence of words X=[x1,…,xL], we need to generate another output sequence of words Y=[y1,…,yT] that has the same meaning as X. Here L and T are length of the input sentence X & paraphrase sentence Y and these lengths are not fixed constants. Our training data consists of M pairs of paraphrases {(Xi,Yi)}i=1M where Xi and Yi are the paraphrase of each other. In Fig. 2, we show given an input sentence ’Why India is against CPEC?’, the AI system generate its paraphrase sentence ’Why does India oppose CPEC?’ by maximizing the objective function logpθ(y1…yT|x1…xT). In this example both (input X and its paraphrase Y) have similar meaning but different sentence. Given training data (X, Y), the objective of the generator is to maximize the conditional log likelihood:(1)θ̂=argmaxθ∑t=1Tlogp(yt|Y1:t-1,X;θ)We use Quora Paraphrase Generation dataset2 to train and evaluate our model. Sample examples of this dataset are available in Table 2. This paper is organized as follows: In the next section-1.2, we discuss the literature about language embedding methods presented so far. In section-2, we present our proposed method in detail with algorithmic flow of our method. In the following section-3, we discuss about the dataset, the experiments, results, and different ablation of the method for paraphrase question generation task. In section-4, we discuss about dataset, experiments protocols, and results for sentiment analysis task. Finally, in the last section, we concluded the work with future direction.Download : Download high-res image (158KB)Download : Download full-size imageFig. 2. This figure illustrate Paraphrase Question Generation (PQG) Task. The AI model receive an input as text (Input Question) and generate similar paraphrase question about the input question.Table 2. We show few example of Quora Dataset [2] for question pairs. Each row is a example of this dataset , which is consist of “Question1” (input question) and “Question2” (target question), whether both are paraphrase or not, is decided by the value of “Is”, If Is = 1, indicate both questions are paraphrase question otherwise, they are not. “Qid1” and “Qid2” are the unique Id of of both question pair. Is: tends for is duplicate.IdQid1Qid2Question1Question2Is4999100How do I make friends?How to make friends ?150101102Is Career Launcher good for RBI Grade B preparation?How is career launcher online program for RBI Grade B?151103104Will a Blu Ray play on a regular DVD player? If so, how?How can you play a Blu Ray DVD on a regular DVD player?155111112How difficult is it get into RSI?Do you apply for programs like RSI when you’re a rising senior?057115116What are some good rap songs to dance to?What are some of the best rap songs?059119120What are the best ways to learn French?How do I learn french genders?0
1.2. BackgroundGiven the flexibility and diversity of natural language, it has been a challenging task to represent text efficiently. There have been several hypotheses proposed for representing the same. [11], [12], [13] proposed a distribution hypothesis to represent words, i.e., words which occur in the same context have similar meanings. One popular hypothesis is the bag-of-words (BOW) or Vector Space Model [14], in which a text (such as a sentence or a document) is represented as the bag (multiset) of its words. Lin et al. [15] have proposed an extended distributional hypothesis and [16], [17] proposed a latent relation hypothesis, in which a pair of words that co-occur in similar patterns tend to have similar semantic relation. Word2Vec [18], [1], [19] is also a popular method for representing every unique word in the corpus in a vector space. Here, the embedding of every word is predicted based on its context (surrounding words). NLP researchers have also proposed phrase-level, and sentence-level representations [20], [21], [22], [23], [1]. Various works [24], [25], [26], [27], [28] have analyzed several approaches to represent sentences and phrases by a weighted average of all the words in the sentence, combining the word vectors in an order given by a parse tree of a sentence and by using matrix-vector operations. The primary issue with BOW models and weighted averaging of word vectors is the loss of semantic meaning of the words, the parse tree approaches can only work for sentences because of its dependence on sentence parsing mechanism. [3], [4] proposed a method to obtain a vector representation for paragraphs and use it for some text-understanding problems like sentiment analysis and information retrieval.Many language models have been proposed for obtaining better text embeddings in machine translation [5], [29], [30], [31], question generation [32], dialogue generation [33], [34], [35], document summarization [36], text generation [37], [38], [39], [40], [41], [42] and question answering [43], [44]. For paraphrase generation task, Prakash et al. [7] have generated paraphrases using stacked residual LSTM based network. Hasan et al.[45] proposed a encoder-decoder framework for this task. Gupta et al. [8] explored a VAE approach to generate paraphrase sentences using recurrent neural networks. Li et al.[9] used reinforcement learning for paraphrase generation task. Very recently, Yang et al.[10] has proposed another variational method for generating paraphrase questions.Zhang et al. [46] has proposed a character-level sequence-to-sequence learning method, in which the model takes characters as input instead of words, which helps to reduce input feature dimension and also helps to handel unknown or rare words in the sentence. Siamese neural network was proposed for many metric learning tasks [47], [48]. Further [49], [50] have proposed LSTM based Siamese network for learning sentence similarity. Zhang et al.[51] has proposed a method to achieve feature embedding using Siamese LSTM for given item pair of queries and the candidates. In our method we have an encoder and discriminator, whose weights are shared and we obtain a pairwise discriminator loss to capture the complete sentence structure. Our model uses LSTM network which is very similar to the standard LSTM [52] module.In our previous work [53], we have proposed a pairwise discriminator based method to generate paraphrase questions. In this work, we extend our previous work by analyzing other variants of our model, like adversarial learning (EDLPG), as described in section-3.3. In section-3.4, we compare our method with the latest state of the art methods. Further, in this work, we visualize the performance of different variants of our model over various epochs, as in section-3.5. We also provide more qualitative results in both paraphrase question generation task and sentiment analysis task in section-3.6 and section-4.3. In section-3.1, we provide more detail about the QQP dataset, and we provide a few examples of this dataset in Table 4.
