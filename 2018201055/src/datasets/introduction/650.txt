Speech signal contains abundant information that extends the content of a written message by including factors such as the identity of the speaker, their emotional state, information status, and intonational patterns [22]. Typically, the speech recognition system explores speech features behind the speech signal and identifies their content using algorithms. Since the late 1950s, numerous studies were conducted on speech recognition, and they achieved significant progress [31]. At the same time, to obtain a better human–computer interaction experience, empowering machines with emotional expressive ability and making them capable of recognizing the human emotional state has gained increasing popularity in the fields of human–computer interaction. This led to a new research field of speech emotion recognition (SER). SER is considered as one of the most important research areas in the past decade. Numerous researchers are attracted by the automated analysis of human affective behavior. However, despite significant efforts made by speech recognition, SER requires considerable work to achieve more natural interactions between human and machine.
In recent years, deep learning has been widely applied in various fields, naturally including SER [24]. Deep neural networks (DNNs) and convolutional neural networks (CNNs) are two typical feed-forward neural networks that take the place of the traditional structures and are often used as the main frameworks for SER [2]. Recurrent architectures, such as long short-term memory (LSTM), carry temporal information, which can also be useful in SER [40]. In contrast to the traditional methods, deep learning is considered as an end-to-end learning method, which means that it has the ability to spontaneously learn the features behind the data. In the field of SER, even unprocessed original speech signals are used to classify emotions by deep learning architecture. In [47], the author compared the emotional classification between the original speech signal and log-mel spectrograms using the 1D & 2D CNN LSTM networks, respectively. The experimental results show that the emotional classification ability of the original speech signal is inferior to the extracted speech features, indicating that features have a significant influence on SER.
In fact, the factors that affect a person’s emotion are complex and varied. Individuals experience various psychological changes under different emotional states. These changes cause them to attach emotional fluctuations to their speech, thus providing emotional information, which is key to speech emotion recognition. The speech features are extracted to describe this emotional information. Many speech features serve to distinguish between different emotions. For example, the pitch of a fearful or angry voice is significantly higher than that of a neutral or disgusted voice [35]. High arousal emotions such as anger, happiness, or surprise all yield increased energy, while disgust and sadness result in decreased energy [25]. The F0 contour decreases over time during the emotion of anger, while it increases over time during the emotion of happiness [17]. Further, anger lasts shorter than sadness [8]. For a sad voice, the standard deviation of loudness is higher, while the opposite trend is noted for a happy voice [48]. Therefore, the distribution of specific speech features under different emotions is diverse, and this potential information is a useful supplement to the SER task. Thus, to fully utilize this information, we propose an emotional-category based feature weighting (ECFW) method to exploit this potential by calculating the prominence of different speech features under different emotions. First, we calculate the prominence of each feature under diverse emotions by correlation algorithms. Then, we assign the prominence value to each feature to enhance their differences. Finally, the special learning method based on deep learning is employed to classify the newly generated feature sets. However, one problem persists in this approach, namely, determining which speech features must be used as initial input.
To solve this problem, the matching relationship between features and models is studied. According to numerous studies [12], a variety of speech features exists. Some of them are based on frame-level extraction, such as pitch, energy, spectral features, and TEO-based features. Others are based on statistical functions, such as the mean, maximum, variance, and derivative of various speech features. The former are often referred to as handcrafted low-level descriptors (LLDs), and the latter are high-level statistic features (HSFs). Hence, in this study, we experiment with both LLDs and HSFs on different deep learning models to find a set of well-adapted feature combinations. Further, considering the high dimensions of speech features, we compare several feature selection algorithms and obtain a more appropriate feature representation [29].
In the studies on SER, there is no generally accepted set of features for precise and distinctive classification. Speech features are usually selected merely based on the researchers’ experience, and processed with equal weights during modeling. However, studies of speech features show that different speech features have a variety of ranges and trends under different emotional states, i.e., each feature has its unique identity under different emotions. For speech emotion recognition, it is crucial to make better use of the variety of identities in presenting different emotions. Thus, a new algorithm, ECFW, is introduced in this article. Apart from previous modeling processes, where all features are assigned fixed weights, ECFW assigns different weights to the features based on emotion classifications. The identities of different emotions are retrieved through calculation of the relevance of features and strengthened by assigning corresponding weights to improve the accuracy of emotion recognition. Further, to obtain the optimal combination of features and models as the benchmark system, this study investigates the performance of SER under the combination of different speech features and deep models as well as different feature selection algorithms. The main contributions of this study are summarized as follows.
1)An emotional-category based feature weighting (ECFW) method is proposed. In contrast to the traditional SER methods, which process speech features at equal weights, ECFW exploits speech features by calculating the prominence of different speech features under diverse emotions, and fully utilizing this prominence to enhance the differences between features and emotions.2)The relationship between speech features and deep models is critically explored. This study argues that different combinations of models and features result in large differences in the performance of speech emotion recognition(SER), which are evaluated by several experiments. Optimal features that correspond to each deep model are identified, and they can be used as a reference for the feature and model selection in SER.3)The effects of the feature selection algorithm combined with speech features are investigated based on the deep model. The most discriminative feature subsets is selected to form a more approriate feature representation as well as to reduce the feature dimensions.
The remainder of this paper is structured as follows. Section 2 presents related studies. Section 3 introduces the framework of the proposed method. The study on feature engineering for SER is presented in Section 4. Section 5 discusses the details of the proposed method. Our experiments and results are explained in Sections 6, and 7 provides the conclusion.
