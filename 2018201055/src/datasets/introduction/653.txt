Understanding human emotion is important for responding properly in a particular situation for both human-human communication and future machine-human communication. Emotion can be recognized from many modalities: facial expressions, speech, and motion of body parts. In the absence of visual features, speech is the only way to recognize emotion, as in the case of a telephone call or a call-center application (Petrushin, 1999). By identifying caller emotion automatically from a system, appropriate feedback can be applied quickly and precisely.
Speech is a modality in which both acoustic and verbal information can be extracted to recognize human emotion. Unfortunately, most speech emotion recognition (SER) systems use only acoustic features for predicting categorical emotion. In contrast, this research proposes to use both acoustic and text features to improve dimensional SER performance. Text can be extracted from speech, and it may contribute to emotion recognition. For example, an interlocutor can perceive emotion not only from prosodic information but also from semantics. Grice (2002) stated in his implicature theory that what is implied derives from what is said. For example, if someone says that he is angry but looks happy, then the implication is that he is indeed angry. Hence, it is necessary to use linguistic information to determine expressed emotion from speech. A fusion of acoustic and linguistic information from speech is viable since (spoken) text can be obtained from speech-to-text technology. This bimodal features fusion strategy may improve the performance of SER over acoustic-only SER.
Besides the categorical approach, emotion can also be analyzed via a dimensional approach. In dimensional emotion, affective states are lines in a continuous space. Some researchers have used a two-dimensional (2D) space comprising valance (positive or negative) and arousal (excited or apathetic). Other researchers have proposed a 3D emotional space by adding either dominance (degree of power over emotion) or liking/disliking. Although it is rare, a 4D emotional space has also been studied by adding expectancy or naturalness. While some researchers, e.g., Russell (1980), argue that a 2D emotion model is enough to characterize all categorical emotions, in this research, we choose a 3D emotion model with valence, arousal, and dominance as the emotion dimensions/attributes.
Darwin argued that the biological category of a species, like emotion categories, does not have an essence due to the high variability of individuals (Charles et al., 1872). Mehrabian and Russell (1974) developed a pleasure, arousal, and dominance (PAD) model to assess environmental perception, experience, and psychological responses, as an alternative to categorical emotion. The latter, also called as dimensional emotion, may represent human emotion better than categorical emotion. This dimensional emotion view is also known as the circumplex model of affect, and the pleasure dimension is often replaced by valence for the same meaning (the VAD model). Although most research used the 2D model (valence and arousal), recent research shows four dimensions needed to represent the meaning of emotion words (Fontaine et al., 2017). However, current datasets lack from the availability of the fourth dimension label (i.e., expectancy). We evaluate the VAD emotion model since the datasets also present the labels in 3D space.
Deep neural networks (DNN) have recently gained more interest in modeling human cognitive processing for several tasks. Fayek et al. (2017) evaluated some DNN architectures for categorical SER. They found fully-connected (FC) networks and recurrent neural network (RNN) worked well for SER task using acoustic features only. In neuropsychological science, the neural mechanism that integrates acoustic (verbal) and linguistic (non-verbal) information remains unclear (Berckmoes and Vingerhoets, 2004). The paper also stated that ”the various parameters of prosody [acoustics] are processed separately in specific brain areas” while no information is given for linguistic processing. In this understanding, separation of acoustic and linguistic/text processing is better modeled by a late fusion than an early fusion. This research makes use of support vector machine (SVM) for a late-fusion prediction from DNN based acoustic and linguistic emotion recognitions. The small remaining test data after used by DNNs is a reason to use SVM over DNN.
This study aims to evaluate the combination of acoustic and text features to improve the performance of dimensional automatic SER by using two-stage processing. Current research on pattern recognition has also shown that the use of multimodal features from audio, visual, and motion-capture data increases performance as compared to using a single modality (Hu, Flaxman, 2018, Yoon, Byun, Jung, 2018, Tripathi, Beigi, 2018). Meanwhile, research on big data has revealed that the use of more data will improve performance for results from the same algorithm (Halevy et al., 2009). By using both acoustic and text features, SER should obtain improved performance over acoustic-only and text-only recognition. This assumption is also motivated by the fact that human emotion perception uses multimodal sensing, peculiarly verbal and non-verbal information. Many technologies, such as human-robot interaction, can potentially benefit from such improvement in emotion recognition.
The main contributions of this study then are: (1) a proposal of two-stage processing for dimensional emotion recognition from acoustic and text features using LSTM and SVM, and a comparison of the results with unimodal results and another fusion method on the same metric and dataset scenario; (2) an evaluation of different acoustic and text features to find the best pair of acoustic-text pair based on evaluated features, including a frame-based acoustic feature and utterance-based statistical functions with and without silent pause features; (3) evaluation of speaker-dependent vs. speaker-independent scenarios in dimensional speech emotion recognition from text features; and (4) evaluation of using text features on a dataset that originally contains target sentences but removed to avoid the effect of these target sentences.
The rest of this paper is organized as follows. “Related work” reviews closely related work to this research including the difference of this study from previous research, “Datasets and features” outlines the datasets and feature sets used in this research, “Two-stage bimodal emotion recognition” explains the method to achieve the results, “Results and discussion” shows the results and its discussion, and finally “Conclusions” concludes this study and proposes future work.
