Cyber-physical systems (CPS) are increasingly ubiquitous, and include many of the critical systems used in domains such as aviation, aerospace, automotive and healthcare. CPS are subject to extensive testing. A key testing activity is Hardware-in-the-Loop (HiL) testing, which is aimed at testing a CPS after the integration of the system’s actual software and hardware. HiL testing – not to be confused with HiL simulation, where some or all the hardware components may be simulated (Jeruchim et al., 2000) – typically takes place at the far end of the system quality assurance spectrum and as part of acceptance testing (Ammann and Offutt, 2016).
An important characteristic of HiL testing is that, due to the involvement of actual hardware, HiL test cases need to account for physical behavior of hardware. Moreover, HiL test cases have the potential to damage the system under test (SUT) or its environment. This necessitates that engineers should verify HiL test cases, before these test cases are exercised on the actual system, to ensure that the test cases are well-behaved. That is, the test cases must implement valid test scenarios and not pose undue risks to the SUT or its environment. An example of a potentially damaging behavior is attempting to supply a voltage to a hardware component beyond the limits that the component has been designed to support. Although such an abnormal case may be useful for robustness testing of the CPS control software, this is not the objective during HiL testing. It is, therefore, important to ensure that HiL test cases are well-behaved before executing them on the actual hardware.
A second important characteristic of HiL testing is that the behaviors of HiL test cases are highly impacted by environmental factors, e.g., temperature, weather conditions, or the characteristics of hardware interacting with the SUT. Exact environment conditions and hardware characteristics are only known at the actual execution time of HiL test cases. Prior to the actual HiL testing, engineers have only partial and approximative knowledge about the SUT environment and the hardware interacting with the SUT. Hence, when checking well-behavedness of HiL test cases before the actual testing, they may not be able to conclusively determine whether, or not, a test case is well-behaved, i.e., whether it may incur any hardware damage. For example, the well-behavedness of a test case supplying voltage to an external device depends on the voltage range tolerated by the device. Hence, the test case may be safe for some devices and unsafe for others. Without knowing the exact device specification, however, we cannot ascertain the well-behavedness of the HiL test case. In this situation, engineers need to identify the conditions on the environment and hardware parameters under which HiL test cases are well-behaved. If the conditions are met at the time of testing, the well-behavedness of test cases is ensured and they can safely proceed with testing.
The third important characteristic of HiL testing is that the duration of testing is often limited. While time budget constraints apply to virtually all stages of system development and testing, there is an additional major factor at play for CPS HiL testing. Since many CPS are deployed in harsh environments, the time spent on HiL testing can cut directly into the service life of a CPS. For example, once launched into orbit, a satellite has an average lifespan of 15 years. A mere two-month-long HiL testing process – not uncommon for satellites – would reduce the active service life of the satellite by more than 1%. To develop HiL test plans that can run under tight time budget constraints, engineers need to draw up accurate a-priori estimates about the execution time of HiL test cases. Note that similar to the test behaviors, the execution time of test cases is also impacted by the uncertainty in the SUT environment and hardware. For example, a test case may take significantly longer to run when the hardware components of the SUT need to be re-calibrated during test execution, e.g., to adapt to the system’s ambient temperature.
In this article, we develop an executable language for specifying HiL test cases and HiL platforms. Our language aims at enabling the three tasks described above: (1) checking well-behavedness of HiL test cases, (2) identifying conditions on the uncertain environment and hardware parameters under which HiL test cases are well-behaved, and (3) estimating the execution times of HiL test cases. These three tasks are performed before the actual HiL testing stage and using models of HiL test cases and the underlying HiL platform.
The benefits of model-based analysis for CPS are widely acknowledged (Lee, 2008, Jensen et al., 2011, Nguyen et al., 2017, Thacker et al., 2010, Clarke and Zuliani, 2011, Zheng and Julien, 2015). In particular and in the area of model-based testing, approaches exist for automated generation of CPS test cases (Arrieta et al., 2017b, Arrieta et al., 2017a, Zhang et al., 2017). The test cases produced by these approaches are nevertheless partial and abstract, thus requiring considerable manual effort before they can be used as HiL test cases (Wang et al., 2015). Industry standards such as TTCN-3 (ETSI, 2017) and UTP (OMG, 2017c) support detailed specification of tests in general. These standards, however, do not specifically address CPS HiL testing and are, on their own, inadequate for our analytical needs. From a conceptual standpoint, our work is distinguished from the existing work in that it is not motivated by the analysis of a SUT, but rather the analysis of the test cases exercised against a SUT. This type of analysis, which is a necessity for CPS HiL testing and potentially beyond, has not been sufficiently explored to date.
Contributions. The contributions of this article are three-fold:
(1) A modeling language for specifying CPS HiL test cases. We develop the Hardware-In-the-loop TEst Case Specification (HITECS) language. HITECS is a textual language defined using the UML profile mechanism (OMG, 2011). A key characteristic of HITECS is that it has an execution semantics, and it includes specific constructs to capture uncertain and physical behaviors of CPS HiL testcases. HITECS customizes the UML Testing Profile (UTP) (OMG, 2017c) and the UML Uncertainty Profile (UUP) (Zhang et al., 2019b) to the HiL testing context. To do so, HITECS further uses the textual syntax of the Action Language for Foundational UML (Alf) (OMG, 2017a), adopting Alf’s execution semantics. To represent uncertainty in the SUT environment and hardware, HITECS allows engineers to declare uncertain variables and to associate them with probabilistic distributions. For physical behaviors of HiL testing, which are typically captured by equations, HITECS provides mathematical constructs. HITECS is a generic HiL test case specification language which is motivated by our work experience in collaboration with several CPS industries (Abdessalem et al., 2018, Ul Haq et al., 2020, Menghi et al., 2020, Nejati et al., 2019, Liu et al., 2019, Matinnejad et al., 2019) and the existing literature on HiL testing (Asadollah et al., 2015, Ali and Yue, 2015, Abdessalem et al., 2018).
(2) Analysis framework. Leveraging HITECS, we develop a framework to: (i) ensure, via formal verification, that HiL test cases properly manipulate and interact with the SUT as well as any additional instruments that provide inputs to the SUT or monitor its outputs, (ii) identify, via simulation and machine learning (ML), conditions on uncertain parameters of HiL test cases under which the test cases are well-behaved, and (iii) estimate, via simulation, the execution times of HiL test cases and thus improve HiL test planning. For verification, we provide guidelines that help engineers systematically specify assertions regarding the well-behavedness of HiL test cases. We then apply an existing model checker, JavaPathFinder (Visser et al., 2003), to HITECS test specifications in order to determine whether they satisfy their assertions. Due to the uncertainty in the SUT environment and hardware, however, assertions cannot always be verified conclusively. For the inconclusive assertions, we provide an uncertainty resolution approach. The approach samples specific values from the parameters’ value ranges and executes HITECS test cases for these values to determine if the assertions are satisfied or violated. An ML classification algorithm is then used to identify conditions from the sampled data points under which HiL test case assertions are likely to hold. We then use model checking to provably ensure that the assertions hold within the identified ranges. To simulate HiL test cases, HITECS provides customizable, side-effect-free annotations and a simulation engine, allowing engineers to approximate test case execution times based on, for example, expert knowledge and historical data.
(3) Industrial case study. We evaluate HITECS using an industrial case study from the satellite domain. Our evaluation results show that: (i) HITECS is applicable in practice and capable of capturing industry HiL test cases; (ii) HITECS enables engineers to define more complete and effective verification assertions than those specified based on domain expertise alone; (iii) HITECS model checking can verify several satellite HiL test cases in practical time; (iv) HITECS uncertainty resolution allows engineers to identify conditions on uncertain SUT parameters under which HiL test cases are well-behaved; and (v) HITECS simulation provides accurate estimates for the execution times of satellite HiL test cases.
This article is an extension of a previous conference paper (Shin et al., 2018a) published at the ACM/IEEE 21st International Conference on Model Driven Engineering Languages and Systems (MODELS 2018). This article offers important extensions over the previous conference paper by: (1) extending the HITECS specification language to account for the uncertain and physical behavior of CPS testing, (2) developing an uncertainty resolution approach which identifies conditions on uncertain parameters of HiL test cases ensuring that the test cases satisfy their assertions, (3) improving the evaluation of our approach accounting for the newly added uncertainty resolution method, and (4) describing a more thorough discussion and comparison of related work.
Structure. Section 2 motivates the article. Section 3 outlines our approach. Section 4 describes HITECS. Section 5 presents the HITECS analysis framework. Section 6 evaluates HITECS. Section 7 compares with related work. Section 8 concludes the article.
