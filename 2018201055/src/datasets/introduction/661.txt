Although machine learning techniques are now increasingly prevalent in security-related applications [1], e.g., biometric identification [2], their vulnerability has yet to be thoroughly investigated. Recently, many research studies [3], [4], [5], [6] reveal security leakages of machine learning systems in an adversarial environment, in which the samples are carefully crafted to mislead the decision of a machine learning model. Since an adversary attack violates the assumption of the same (or similar) distributions of the training and test sets made by most machine learning methods [7], the efficacy of the model drops significantly in an adversarial environment. For example, a tiny modification on a sample may mislead the decision of a Deep Neural Network [8], and one malicious training sample can significantly reduce the accuracy of a Support Vector Machine [9]. These results raise the public concern on the security issues of machine learning.
The poisoning attack is a popular type of adversarial attack in which the training samples are manipulated by injecting feature noise [10] and modifying the labels [11] to mislead a learning process. Since samples may be collected from unreliable sources, e.g., Amazonâ€™s Mechanical Turk [12] and recommender systems [13], the quality of the training set can be easily reduced by an adversary. Countermeasures against poisoning attacks are categorized into robust learning [14], [15], [16] and data sanitization [17], [18]. Not only the generalization ability but also the robustness of a model are maximized in robust learning [14]. However, a robust learning method is designed for a specific learning algorithm and may not be generalized to others. On the other hand, in data sanitization methods the suspicious samples are identified according to pre-defined criteria, e.g., classification accuracy [18], and data complexity [17]. The time complexity is a concern since classifier training is required in the sample evaluation. In addition, removing the suspicious samples may cause information loss in learning [2], [19]. As a result, a generic method which takes full advantages of given samples is needed.
Our study aims to propose a robust learning model which is generic to any learning algorithm and utilizes the knowledge of the training samples. We investigate countermeasures to defend against poisoning attacks in the framework of transfer learning which is suitable for any classier. Transfer Learning [20] is a mechanism which aims to improve the performance on a target task by using knowledge acquired from a related task called a source task, and has been commonly used in applications without sufficient labeled training samples [21], [22]. A sample in a contaminated training set may be either untainted or contaminated, which is unknown to users. We assume there is a small untainted training set collected from the same problem, which may be obtained by sanitizing a small portion of the contaminated training set manually. The untainted training set is used as target data in transfer learning since all samples in this set are clean. However, since the size of the target dataset is too small to build a satisfactory model, the contaminated set defined as the source dataset is also required. As a result, the adversarial learning problem is formulated as transfer learning, in which the similar knowledge to the untainted set is extracted from of the contaminated set to improve the learning on the target task, i.e. the influence of the contaminated samples to training is reduced.
TrAdaBoost [21] is considered due to its popularity and exemplary performance in many applications [23], [24]. Our preliminary study indicates that TrAdaBoost highly relies on the initial weight values representing the similarity between the source samples and the target problem, and assigning an improper value to the contaminated samples causes the training to be affected significantly. Therefore, the improper weight value of a sample is initialized according to its classification accuracy of an intuitive classifier in our revised model to obtain a better initialization. Our proposed model is more flexible than most existing robust learning methods since our model can adopt any classifier. Moreover, in contrast with data sanitization, contaminated samples are evaluated iteratively to avoid information loss caused by wrongly identifying a clean sample as contaminated. In out method, samples with low reliability still contribute to training but with less importance. Our proposed model is then evaluated and compared experimentally with TrAdaBoost, AdaBoost [25], Reject On Negative Impact (RONI) [18], and SR-LSSVM [26], which are representatives of the traditional classifier, data sanitization, and robust learning methods respectively. The label flipping attack is considered in our study as it is the most common poisoning attack. The Support Vector Machine (SVM), which is a binary classification algorithm commonly used in many applications, is used as a base classifier for TrAdaBoost. Experimental results confirm that our proposed model can significantly reduce the attack success rate.
The rest of the paper is organized as follows. Section 2 provides a brief introduction on the related concepts of this study, including label flipping attacks and its countermeasures. The transfer learning-based poisoning attack defense method and its weight initialization improvement are devised in Section 3. The experimental results and discussion are given in Section 4. Finally, Section 5 concludes this study, and Section 6 discusses future study.
