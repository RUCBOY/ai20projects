The field of ab initio molecular dynamics (AIMD) has been growing enormously over more than two decades providing exciting new insights into chemistry and materials science at atomic resolution. In AIMD atoms are propagated in time according to Newton’s equations of motion to study physical properties and chemical reactions of molecules and condensed phase systems at finite temperature [1], [2]. The necessary energies and forces are derived from first-principles quantum chemistry, whose enormous computational effort has made AIMD a key application in high-performance computing (HPC).
One of the most successful implementations of AIMD is the CPMD code [3], which has been available on almost all major HPC platforms since the late 1990’s. The code is MPI [4] parallel and offers an OpenMP [5] parallelization on top for many time critical routines.
A key factor to its success always was the porting and optimization for new compute architectures, like vector machines or the multi-core architecture of the IBM Blue Gene series. More recent porting efforts focus on the support of accelerators, namely GPUs [6], which are one aspect of a general trend in high performance computing. In the last decade, the compute power, but also the complexity of the nodes, has grown much stronger than the speed of the interconnects. On current supercomputers each node typically hosts dozens of cores with strong vector units and the cores may be complemented by GPU accelerators. The intra-node communication between these units is by an order of magnitude faster than the inter-node network. Thus, to make use of these changes in current computer hardware, the challenge in code modernization is to increase the computational load on the individual nodes and reduce, or even avoid, communication between the nodes. Here, we will demonstrate the implementation of such a strategy not only for a single algorithm or subroutine, but for a complex application code which has evolved over more than two decades.
The underlying method of CPMD is Kohn–Sham density-functional theory (DFT), which describes the electrons through singly or doubly occupied electronic states. In CPMD these states are expanded in a plane-wave (PW) basis and propagated via the Car–Parrinello extended Lagrangian technique. For details of the method and the implementation, see [1], [2].
Some properties of the electronic states are calculated efficiently in momentum space, other operators are diagonal in real space. CPMD uses 3-d fast Fourier transforms (FFT) to switch back and forth between these two representations for a most efficient computation [1], [7], [8]. Thus, the key objects of the method are the grids in real and momentum space that are used to represent the electron density in the simulation cell, the core potentials and the electronic states.
CPMD uses pseudopotentials (PP) to describe the chemically inert atomic core and to limit the calculation to the valence electrons [9]. Furthermore, with pseudopotentials a much smaller plane-wave basis is required than one would need for an all-electron calculation. For the frequently used norm-conserving PP (NCPP) the PW expansion results in real-space grids of a few hundred points per dimension for the FFT. The resolution of the grid can be further reduced by at least 40% by using Vanderbilt ultra-soft PP (USPP) [10], at the moderate expense of some additional computations, which can be conveniently expressed by matrix–matrix multiplications. Thus, USPP do not only offer a reduced computational cost due to the smaller grid size, but they also shift workload from the FFT, which shows a rather limited scaling on modern HPC architectures, to highly efficient calls of the DGEMM routine from the BLAS library. So compared to NCPP, USPP promise a more efficient use of today’s more powerful compute nodes and a better scalability of the implementation.
The USPP method is formulated in N electronic states {|ϕi〉}, which fulfill the constraint (1)〈ϕi|Sˆ|ϕj〉=δijby means of the non-local overlap operator (2)Sˆ=1+∑I,mnqmnI|βmI〉〈βnI|,which depends on the β-projectors {|βmI〉} of the atoms {I} and the integrals (3)qmnI=∫drQmnI(r)of the augmentation functions QmnI(r) provided with the USPP [10], [11]. These augmentation functions are also required to compute the total electron density (4)n(r)=∑i|ϕi(r)|2+∑I,mnQmnI(r)〈ϕi|βmI〉〈βnI|ϕi〉,which contains contribution from the projections of the |ϕi〉 onto the β-functions. In case of NCPP, the augmentation functions are all zero and the overlap operator Sˆ reduces to the unity matrix.
For USPP, the non-local overlap operator (2) leads to the generalized eigenvalue problem (5)Hˆ|ϕi〉=ϵiSˆ|ϕi〉for the time-independent Schrödinger equation with the Hamiltonian (6)Hˆ=−12∇2+Veff+∑I,mnDmnI|βmI〉〈βnI|,where the last term containing (7)DmnI=DI,mn(0)+∫drVeff(r)QmnI(r)describes the non-local part of the potential with given parameters DI,mn(0).
Because the generalized constraint (1) is difficult to handle in the extended Lagrangian dynamics of CPMD [11], a set of orthogonal orbitals {|ψi〉} is used [12], which yield the original (8)|ϕi〉=∑j|ψj〉Tji,by the inverse root T=O−1∕2 of the overlap (9)Oij=〈ψi|Sˆ|ψj〉.Potential energies and forces are still calculated with respect to the {|ϕi〉}, which requires frequent transformations between these two representations. In CPMD the orthonormality of the {|ψi〉} is not enforced by the full set of N2 coupled constraints 〈ψi|ψj〉=δij, but just by the N diagonal constraints 〈ψi|ψi〉=1, which are decoupled and, thus, their Lagrange multipliers are easy to determine. On the downside, orthogonality has to be enforced from time to time by solving an N2 eigenvalue problem [12].
Using USPP, each atomic PP requires about twice as many β-projectors as NCPP to map the interaction of the atomic core with the electronic states [11]. Because of the larger number of β-projectors and the additional terms appearing in the equations above, the USPP scheme requires some extra computation. Ideally, this is more than compensated by the lower grid resolution and much smaller plane-wave basis set (roughly a factor of eight for simulations containing 2p and 3d elements). Also from a parallelization standpoint, bearing in mind the above discussion on the fast growth of the node’s computational power, the USPP approach seems favorable compared to NCPP, since it reduces the amount of communication (a large burden in the FFT) at the expense of some extra computation, which can be mostly done by highly efficient matrix–matrix multiplications. However, in the current implementation of CPMD the USPP code branch is only slightly faster than NCPP and displays rather poor scaling. Nevertheless, USPP calculations in CPMD have been indispensable for systems with a large number of electrons because the much smaller basis set largely reduces memory requirements [11].
The heart of data parallelization in CPMD is a 1-d domain decomposition of the FFT grid, i.e. the yz-planes of the electron density are distributed to the MPI tasks (see [1], [2] for details). Thus, the few hundred grid points per dimension is an effective limit for the number of MPI tasks that can be used efficiently in a calculation. A decade ago, this was not critical on most HPC platforms, but with the emergence of powerful multi- and many-core compute nodes pure MPI parallelization limits the achievable performance of the code. CPMD already can be operated in a hybrid MPI+X mode, where additional thread parallelization is introduced via performance libraries (BLAS, LAPACK, FFTW) [13], [14], [15] and explicit OpenMP [16] directives. However, not all code paths are equally well thread-parallelized and the efficiency of CPMD in hybrid mode often falls behind that of pure MPI. This problem is particularly pressing for simulations that use USPP due to the much smaller FFT grid. Concomitantly, USPP use a much smaller spacial grid and reach the scaling limit in a pure MPI mode earlier. On top, the hybrid parallelization scheme is not as well maintained for the USPP code path as for norm-conserving PPs.
Recently, a second level of MPI parallelization was introduced in CPMD to enhance the scalability [17]. In the so-called cp_groups parallelization, the MPI tasks with their associated memory shares are replicated g times. Each of these g
cp_groups computes the same workload per default. For speed-up, the members with the same index in different cp_groups do work-sharing and data synchronization among each other. Originally, this was implemented to distribute state pairs for Hartree–Fock exchange and to parallelize the FFT routines over the electronic states. For all USPP-specific routines these cp_groups parallelization has been missing completely.
In this paper we discuss our latest revision of the CPMD code to alleviate the parallelization and performance bottlenecks of the USPP code path. It builds upon our intermediate results presented at the Supercomputing Conference 2018 [18]. On the node level our effort aims to fully exploit the powerful compute units of current machines and make use of the fast intra-node communication with hybrid MPI+OpenMP parallelization strategies. Furthermore, we address the time critical parallel tasks, namely the distributed matrix–matrix multiplication to calculate the overlap of the electronic states with the β-projectors and the 3-d FFT of the electronic states. For both routines, we introduce overlapping computation and communication to reduce the communication overhead. The required data partitioning strategy is determined by auto-tuning algorithms. Where possible, inter-node communication is avoided by node-local parallelization, using e.g. MPI shared-memory windows. To achieve maximum scaling we complemented compute routines by a cp_groups parallelization scheme where applicable.
The following Methods section gives an overview of the systems used for benchmarking before we describe details of the code changes in the section Optimization. The benefits of our efforts are discussed in the section Results and Discussion and a short Summary and Outlook concludes the paper.
