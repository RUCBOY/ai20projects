Reinforcement learning (RL) in a multiagent system is a difficult problem, especially in a partially observable setting. A key difficulty is that the agents’ strategic interests are crucially reliant on the payoff structure of the underlying game, and typically no single algorithm performs best across all types of games. In the past, this has necessitated the concept of “targeted optimality” [1], where a learning algorithm targets certain subsets of scenarios to learn (near)-optimally. However, much of the prior work on multiagent learning has focused on settings that allow agents to observe the global state perfectly, and in some cases, to even observe each others’ actions. As real-world applications, such as in robotics, become adept at handling large amounts of observational data that are often noisy and incomplete, it is becoming increasingly imperative to relax these assumptions. Thus in this article, we are interested in partially observable settings where agents do not observe each others’ actions perfectly and noisily observe states.
Action-value based reinforcement learning algorithms such as Sarsa [2] and Q-learning [3] represent some of the best performing RL methods in single-agent settings modeled by the Markov decision process (MDP). Sutton and Barto [2] introduced a refinement to Monte Carlo Q-learning allowing the agent to start with a random state-action pair, and called it Monte Carlo Exploring Starts (MCES). If the state is partially observable, the agent may simply use its current observation as the state of a MDP but the learning may not converge. More preferably, the agent conditions its learning on the recent history of observations in a partially observable MDP (POMDP). A T-step policy is a mapping from observation histories of length up to T to an action. Perkins’ Monte Carlo exploring starts for POMDP (MCES-P) [4] obtains action-value for a policy from sampled trajectories, and uses it to determine whether a transformed policy in the local neighborhood of the current policy is better. Here, MCES-P transforms the policy at a randomly chosen pair of observation sequence and action to obtain a locally transformed policy. This less explored avenue has the strong benefit of directly searching a discrete space of policies and does not require the convergence of action-values for each state before the policy is obtained. Perkins’ elegant and transparent approach serves as the basis for our learning algorithms in multiagent settings presented in this article.
While general multiagent settings are most realistically modeled as partially observable stochastic games (POSGs) [5], it is convenient to consider two popular subcategories under POSGs: self-interested and strictly cooperative settings, the former encompassing adversarial as well as non-adversarial, non-cooperative settings. Cooperative scenarios are usually modeled as decentralized POMDPs [6] and multiagent POMDPs [7] (MPOMDPs can be seen as a type of decentralized POMDP where the agents communicate their observations to each other perfectly), which offer agents opportunities for learning collaboratively, leading to specialized learning algorithms. On the other hand, self-interested settings may lack such opportunities because agents cannot make assumptions about the disposition of other agents. In such cases, agents must act in an individually rational manner, and the learning problem could be approached from an egocentric, self-interested perspective. An interactive POMDP (I-POMDP) [8] is an appropriate model for such scenarios. Consequently, we seek learning algorithms for these two subcategories and their appropriate models.
We introduce three RL templates for settings challenged by partial observability and multiple agents, all of which generalize Perkins’ MCES-P algorithm.
•MCES for interactive POMDP (MCES-IP) that aims to learn a policy for a self-interested agent acting and planning in an I-POMDP setting shared with other agents.•MCES for MPOMDPs (MCES-MP) that generalizes MCES-P to canonical MPOMDPs with a focus on heterogeneous teams. Such teams are comprised of agents with differing rewards cooperating toward a common goal. It learns a single policy mapping joint observations of all agents to joint actions.•MCES for factored-reward MPOMDPs (MCES-FMP) with each agent individually mapping joint observations to their own action.•We instantiate these templates with probably approximately locally optimal (PALO) bounds to provide statistical guarantees of ∊-local optimality that relate with sample complexity. While these guarantees do not relate to global optimality of the learning, nevertheless they provide a useful theoretical footing for RL for POSGs.•We present and exploit a parameterized policy search space pruning technique, trading statistical guarantees from PALO bounds for a reduction in the computational burden.
We empirically demonstrate using six problem domains that MCES-IP and MCES-FMP using the derived sample complexities arrive at improved local optima relative to relevant baselines and competing methods under similar conditions. Our experiments also demonstrate that MCES-FMP is a significant improvement over MCES-MP for cooperative domains converging to drastically improved policies under similar sample complexities. These new templates and their instantiations not only offer RL with reduced model requirements in the context of distinct POSG settings but they also represent the first methods that relate sample bounds to (local) optimality for RL in popular competitive and cooperative multiagent contexts.
The rest of this article is organized as follows. We discuss the related work in Section 2. Section 3 briefly reviews POSGs and the interactive POMDP framework, both of which serves as background for the new methods. Our main contributions start in Section 4 which presents the MCES-P and MCES-IP templates for self-interested POSGs and their PALO instances. Section 5 then introduces the MCES-MP and MCES-FMP templates along with their PALO instances for POSGs that model heterogeneous teams. To promote efficiency, we show how the policy search space can be pruned in Section 6; this technique can be utilized in conjunction with all the presented RL methods. Our evaluation domains, baselines, experiments, and their results are discussed in Section 7. We conclude this article with some remarks and future directions in Section 8. The appendix gives the proofs of all the theorems mentioned in the article.
A portion of this article was previously published in the conference proceedings of AAMAS 2016 [9]. This article expands on the conference paper in several ways. In addition to an expanded exposition with illustrations, the article includes (i) a key new contribution toward model-free reinforcement learning with sample complexity bounds in cooperative settings in Section 5, which presents two new algorithms that generalize MCES-P, and (ii) associated experiments and analyses on three new problem domains evaluating the performances of the new algorithms in comparison to three new baselines.
