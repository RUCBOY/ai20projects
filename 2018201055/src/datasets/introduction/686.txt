The main purpose of feature selection methods is to eliminate irrelevant features in order to produce relevant subsets that are able to achieve a better generalization in classification tasks with a significantly lower number of features [1]. In fact, the generalization error tends to decrease as the dimension of the problem decreases, which can be explained by the fact that the Vapnik–Chervonenkis (VC) dimension of the classifier is reduced [2]. There are other important questions concerning feature selection problems, for instance: runtime, which is crucial to solving the problem in a viable time; the discovery of a small number of relevant features, which is pertinent in some applications, such as bioinformatics; and the possibility of obtaining a better visualization and interpretation of the results [3].
Given a specific classifier, it is plausible to select the best subset of features satisfying a given criterion by exhaustive enumeration of all subsets of features. However, an exhaustive enumeration is prohibitive for a large number of features due to the combinatorial explosion in the number of subsets. Thus, performing feature selection in high-dimensional input spaces usually involves greedy or heuristic methods [4], [5]. Among several possible methods, feature-ranking approaches may select a fixed number of top-ranked features for designing a classifier. Also, a threshold can be set on the ranking criterion. Only features whose criterion exceeds that threshold are retained. It is possible to use the ranking to define nested subsets of features, and select an optimal subset of features with a criterion by varying a single parameter: the number of features. Numerous feature selection methods based on large margin classifiers, such as Support Vector Machines (SVMs), have been proposed for classification tasks in literature [4], [5], [6], [7], [8]. In general, these methods can be distinguished into wrapper or embedded methods.
Embedded methods use large margin classifiers with a regularization method that shrinks the number of features. These methods generate sparse solutions and have been employed as an alternative to methods that use greedy strategies or that explore the search space of feature subsets. Weston et al. [9] introduced an L0 formulation for SVM which minimizes, in an approximate way, the number of non-zero components of the normal vector. This method, named AROM, re-scales the components of the training set at each iteration, after an SVM training.
The Recursive Feature Elimination (RFE) [4] is an example of a wrapper method. It consists of using SVM recursively to eliminate a fixed number of uninformative features, which are associated with the components of the normal vector with the least absolute values. However, even when this recursive elimination process is performed for one feature at a time, the method is not able to detect the optimal subset of features, since it is based on a greedy strategy.
The work reported by Aksu et al. [5] describes the Margin-based Feature Elimination (MFE) for SVMs that is the first method that employs a margin-based approach for feature selection. The authors show that RFE is not consistent with margin maximization, especially for the Gaussian kernel. Although MFE achieves larger margins values and better overall generalization performance compared to RFE, it does not guarantee the achievement of optimal margin values, since it has a greedy heuristic. The method performs a backward strategy for features elimination, using a recursive routine based on the evaluation of a set of functional distances from the points in relation to the separating hyperplane. Besides, the method solves a two-dimensional optimization problem, considering as parameters the bias value and a scalar that adjust the length of the normal vector in order to improve the current margin solution. However, this optimization problem makes nonsense because, if the direction of the hyperplane does not change, the optimal bias value can be directly evaluated by a simple balancing procedure [10].
Both RFE and MFE use the dual formulation of SVM [11], which minimizes the Euclidean norm of the normal vector, producing a separating hyperplane with an L2 margin. However, the L1 norm is more appropriate for feature selection. When the L1 norm of the normal vector is minimized, a separating hyperplane with a maximal L∞ margin is defined. In this case, the distance computed between the training points to the separating hyperplane is related to the maximization of the largest component of the normal vector. Consequently, it can be observed that the separating hyperplane tends to be positioned perpendicular to the axis of the largest component, making it dependent on that coordinate [12]. Therefore, the solution tends to be more sparse, which helps the feature selection process. Indeed, several works tend to use a sparse classifier through the L1-norm minimization. These problems are generally solved in batch mode and require the use of linear programming [13], [14], [15].
Another example of a wrapper method, known as ALMAp-FS, was introduced by Gentile [8]. This method is based on ALMAp, an online large Lp-margin classifier, for p in the interval [2,∞) [16]. ALMAp-FS uses a recursive feature elimination strategy, similar to the RFE method, but instead of using SVM to define its maximal margin classifier, it uses the large Lp-margin classifier found by ALMAp. However, p is set to be the logarithm of the cardinality of the current set of features, since p=∞ is not reached by ALMAp. Therefore, ALMAp does not accurately minimize the L1 norm. In addition, the greedy feature elimination strategy used by ALMAp-FS is also suboptimal.
Taking into account that the RFE, MFE and ALMAp-FS methods, although guided by the margin maximization principle, present limitations in terms of achieving optimal margin solutions, mainly when considering the L1-norm minimization, we propose, in this paper, a novel wrapper method, the Admissible Ordered Search (AOS). Differently from previous approaches, this method uses an ordered backward search process based on margin values, which are obtained from a large margin classifier with any Lp norm, for p∈[1,∞]. AOS contrasts with greedy strategies, which irrevocably eliminate one or a group of features at a time and obtain nested subsets in a suboptimal way. The method explores the space of candidates by executing an ordered search. The exploratory search is implemented with a control strategy that manages the insertion and removal of states, which represent the feature subsets, from a priority queue. Thereby, we implement a best-first search, since the best state, with the largest margin value, receives the highest priority. If necessary, the method can accomplish a bounded search in order to avoid a combinatorial explosion, while preserving the chances of finding optimal subsets. For this, we propose a set of procedures related to branching factors and pruning mechanisms.
AOS is committed to finding small subsets that have good generalization power based on a margin-based feature selection approach. In this sense, the method performs a heuristic search in order to achieve small subsets with larger margin values. This objective is consistent with the margin maximization process that is central to the structural risk minimization (SRM) principle developed by Vapnik and Chervonekys [2].
Although AOS can be coupled with a standard SVM solver for an L2 norm formulation or with a linear programming solver for an L1-norm formulation, we coupled the method with the Incremental p-Margin Algorithm (IMAp) [17]. This classifier computes an approximation to the maximal Lp margin based on an incremental strategy, allowing greater flexibility and avoiding the use of linear and higher order programming methods. This approach allows the use of different norm values in a single primal formulation. Also, in contrast with other classifiers that compute exact solutions, we can impose a fixed runtime to obtain an approximate solution for the maximal margin problem. Considering the use of the projected margin, the method can retain the solution produced by the father state and employ it as the initial solution for the margin maximization problem in the offspring states. For non-linearly separable datasets, we used the dual version of IMA [18].
The AOS method was tested on several different classification problems and the results were compared to four methods, representing the three classes of feature selection methods. One of these methods was Golub’s criterion [19], which is a filter method. The other methods were two embedded statistical methods known as Nearest Shrunken Centroids (NSC) [20] and Least Absolute Shrinkage and Selection Operator (LASSO) [21], and RFE [4], which is a wrapper method.
The main contributions of this work are summarized as follows. First, it proposes a new feature selection method based on an ordered search process to explore the space of feature subsets. Second, it is the first work to consider a monotone evaluation measure based on margin values, calculated by a large margin classifier with any arbitrary Lp norm, especially the L∞ norm, which minimizes the L1 norm and produces sparse solutions, useful in feature selection. Third, it is introduced the projected margin concept, computed as the maximal margin vector projected into a lower-dimensional subspace and used as an upper bound for the margin value, and also theorems proving the projected margin value and the monotonicity of the search. Fourth, computational experiments, including extensive tests to choose branching and pruning parameters, show that the method has superior performance, both in reaching subsets of features with inferior cardinality and in finding subsets with larger margin values.
A preliminary version of the AOS method has been presented in a previous work [22]. When compared to the previous work, this paper has a more detailed and formal description of the proposed method with further experiments and new extensions in order to solve non-linearly separable problems in the kernel space. For instance, we used here the L1 norm in order to obtain more sparse solutions; it is shown extensive tests to choose the parameters related to the branching and pruning strategies; it is presented theorems proving the monotonicity property of the search and that the projected margin value is an upper bound of the real margin value; and also it is done a more complete sequence of experiments to compare the results, with 10 different splits to avoid any bias.
The remainder of this paper is organized as follows. Section 2 describes some recent related work and discusses the difference between these methods and our approach. Section 3 addresses the feature selection problem and presents its three most usual approaches: filter, embedded and wrapper. Section 4 briefly describes some preliminary concepts related to the problem of binary classification and the theoretical basis for the classifier used with AOS. Section 5 presents the AOS method, describing its control strategy, as well as the branching and pruning strategies. In Section 6 the results of computational experiments comparing AOS with different feature selection methods are presented. Section 7 contains the final considerations and conclusions about the work.
