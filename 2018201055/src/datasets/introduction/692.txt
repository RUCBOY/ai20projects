The ever-increasing demands for computation resources by growing and continually emerging applications have posed huge challenges to data center design. Servers not only have to process larger amount of data [1], [2], [3], but also need to be able to support various applications in, for example, distributed computing [4], [5], [6], database services [7], [8], search engine [1], [9], multimedia processing and artificial intelligence [10], [11], [12], [13]. Improvements in performance and power efficiency of general-purpose processors have been the major driving force for keeping data center with the growth of demands. However, these improvements may not last long in the future because power consumption per unit area of multi-core chips continue to grow as transistors become smaller [14]. Reconfigurable hardware [15], [16] provides a solution to this problem. The requirements of servers for homogeneity, flexibility and upgradability defies other alternatives like specializing hardware [17] for specific applications. Programmable hardware such as field programmable gate arrays (FPGAs) is highly desirable.
Several architectures have been proposed for accelerating different applications using FPGAs and demonstrated effectiveness in improving efficiency. Putnam et al. [1] show that a reconfigurable fabric called Catapult helps improve ranking throughput of Microsoft Bing’s web search services. Sukhwani et al. [18] develop an acceleration engine for database operations in analytics queries and shows significant CPU savings in real data. Lavasani et al. [7] use FPGAs for Memcached (a distributed key–value system) acceleration effectively decreasing CPU power consumption. Shan et al. [5] propose a MapReduce framework on FPGA and show that large speedup factors can be achieved on applications of RankBoost, SVM and PageRank compared to pure CPU-based implementation. Ovtcharov et al. [10] implement a single-node convolutional neural network (CNN) accelerator on a mid-range FPGA and demonstrate comparative performance at lower power level as general-purpose graphics processing units (GPGPUs). Zhang et al. [11] further explore how to optimally utilize logic resources and memory bandwidth of FPGA for accelerating deep learning. Jiang et al. [12] employ multiple FPGAs for deep neural network training achieving super-linear speed-up against single-FPGA design.
1.1. Accelerator networksWhile FPGAs show promising usages in accelerating single applications, a few challenges need to be addressed when deploying them as generic accelerators in data centers for multiple applications. One challenge is that limited resources on a single FPGA causes difficulty in accommodating a variety of acceleration functions. A runtime reconfiguration approach relieves the burdens but may be slow due to long reconfiguration time. Using multiple FPGAs per server is impractical due to limited space in a server and higher power consumption. One solution is to build interconnection structures between multiple FPGAs on the basis of the single FPGA per server architecture. This approach allows multiple acceleration functions to be supported within the space and power constraints. Baxter et al. [19] build a general-purpose 64-FPGA computer using 2-D torus interconnection. However, the routing logic is not implemented in FPGAs but is handled by the network of their residing CPU servers. Abdelfattah and Betz [20] implement substantial routing functionality and augment FPGAs with network-on-chip (NoC). The Catapult reconfigurable fabric in [1] is connected through a 6 × 8 2-D torus where each group of eight FPGAs in a column executes the partitioned ranking task in sequence. Caulfield et al. [9] couple intra-FPGA communication to the network plane of a data center and show that this architecture achieves comparable performance in latency to previous work without the secondary network for web search ranking and data encryption.
1.2. Content-based routingIn general, an accelerator network consists of multiple accelerator units such as FPGAs, each of which is configured to a specific function. An acceleration task is decomposed into multiple steps. The data involved in the acceleration task is injected into the network as a data packet, processed at nodes that support respective steps in sequence and finally forwarded back to the source node to complete the acceleration. This content-based routing scheme where the routing destinations are determined based on acceleration steps is a key feature of generic accelerator networks. In content-based routing, a packet is not routed to a specific node as in address-based routing. Any node whose selection predicate (a set of constraints over attributes, for example, type of acceleration function in the case of accelerator networks) matches the packet can be a candidate receiver [21]. Carzangia et al. [22] propose to combine a traditional broadcast protocol and a content-based routing protocol for content-based networking. Chand and Felber [23] propose a protocol for content-based routing in overlay networks which guarantees perfect routing and optimizes network bandwidth while keeping the size of the routing tables small.We address the content-based routing problem on 2-D mesh accelerator networks using a simple idea: compute the shortest paths for each acceleration task and maintain a set of such paths in the routing table for adaptive routing. While Breadth-First Search (BFS) algorithm can be applied to find the shortest paths for each acceleration task, a naive implementation can hardly scale as the size of the search space typically grows polynomially in the network size and exponentially in the number of acceleration steps. For a fixed number of acceleration steps, BFS will be computationally expensive for slightly large network sizes. How to reduce the search space of BFS such that the computation is affordable with a reasonable amount of resources becomes an important problem.Branch-and-bound is a widely-used technique for reducing the search space of optimization algorithms that relies on enumeration of candidate solutions. Typically a tree structure is constructed to store the (partial) solutions to the problem under consideration. Each branching corresponds to a partition of the solution space that produces new sub-problems. Branch-and-bound checks each branching against an estimated bound on the optimal solution and discards the branching if it cannot yield better solutions than the current best one. This technique has been applied to find exact solutions of the traveling salesman problem, the quadratic assignment problem, the graph partitioning problem and many other problems that are NP-hard [24], [25], [26].To tackle the inefficiency of naive BFS, we propose a branch-and-bound method that exploits a special property of shortest cycle on 2-D mesh and prunes sub-optimal solutions from the search tree. We show that by applying this method the time and space complexities of BFS improve significantly from polynomial to constant in the network size. We also provide a theoretic guarantee that the pruned BFS still performs optimally. We name the overall routing algorithm that applies the pruned BFS for path searching the Shortest Cycle Routing Algorithm. We study two properties of shortest cycle routing – locality and path diversity – and how they can be used to develop adaptive routing algorithms and avoid deadlocks. Our contributions are:•Propose a routing algorithm that addresses the content-based routing problem on accelerator networks.•Improve the time and space complexities of BFS significantly from polynomial to constant in the number of nodes in the network.•Analyze locality and path diversity properties of shortest cycle routing and show their applications in developing adaptive routing strategy, restricting global flooding to local neighborhood and reducing the number of path cycles.The rest of the paper is organized as follows. In Section 2, the basic algorithm is described and its time and space complexities are analyzed. In Section 3, we examine the locality property of routing and its application in local instead of global flooding of routing information. In Section 4 we give an approximate quantitative analysis of formation of deadlocks and how path diversity helps in reducing the number of path cycles. In Section 5, we present simulation results on three metrics of performance.
1.3. NotationsLet G=(V,E) be the graph encoding the network topology, where V and E denote the node and edge sets respectively. Let T be the number of different acceleration functions. An acceleration task or request, denoted by s=(s1,s2,…,sk),si∈[T],∀i∈[k] is assumed to be an ordered subset of [T]={1,2,…,T}, which is known before acceleration. We use the notation |⋅| for the length of a sequence and [⋅] for the set of positive integers up to its input. A routing path for a task of length k is represented by a sequence of processing nodes (v1,v2,…,vk),vi∈V,∀i∈[k], omitting intermediate forwarding nodes. The task injection node or source node v0 is usually appended to the beginning and the end of a path such that the extended full routing path forms a closed cycle p=(v0,v1,v2,…,vk,v0). In the following we will always consider the extended version for each path and use “route” and “path” interchangeably. We use the notation Vs for the set of nodes in V of acceleration function s, i.e. Vs≔{v∈V:t(v)=s}, where t(⋅) takes a node as input and returns its acceleration function. Lower index usually denotes component of a sequence while upper index denotes coordinate. For example, pi denotes the ith component of p and vi denotes the ith coordinate of v.We further assume a distance function d(⋅,⋅) which returns the distance between any pair of nodes in V. In particular, for a 2-D mesh network, we are interested in the L1 distance or the Manhattan distance (1)d(u,v)=∑i|ui−vi|,u,v∈Vwhich counts the number of hops between pair of nodes. Given a distance function, the path length function len(⋅) is defined as (2)len(p)=∑i=1|p|−1d(pi,pi+1)
