Recently, dynamic scene deblurring has been extensively investigated, and deep-learning techniques, particularly fully convolutional neural networks (FCNNs), have greatly improved the performance of dynamic scene deblurring methods [1], [2], [3], [4]. However, as it is an ill-posed inverse problem, dynamic scene deblurring remains a challenging task in computer vision even though several effective methods have been developed. Unlike in the case of uniform blind image deblurring [5], [6], [7], [8], the blur kernels of dynamic scenes are highly complicated and more realistic, as blur varies with spatial location.
Conventional methods for dynamic scene deblurring usually require segmentation of a blurry image into layers with different blur, in which the deblurring process is separately performed. Compared with methods based on convolutional neural networks (CNNs), these algorithms depend heavily on accurate segmentation, and although they have achieved relatively high performance using their segmentation map, they often involve iterative, time-intensive, and cumbersome optimization schemes. Moreover, there is no general technique for obtaining the optimal segmentation map. Recently, deep-learning methods have been used for dynamic scene deblurring. These approaches involve either the replacement of some steps (such as kernel estimation) of conventional methods [12], [13], [14], or learning the end-to-end mapping functions from blurry to latent sharp images [15], [1], [2]. The current state-of-the-art approach is in the latter category and was proposed in [15]. It uses a multi-scale CNN to recover the latent images and was recently improved in [1], [3], [4], which developed methods that allow highly efficient generalized dynamic scene deblurring with low memory usage by omitting the iterative optimization stage. In this study, we further improve the performance of existing dynamic scene deblurring algorithms by using FCNNs that consider dynamic scene characteristics.
To improve performance, previous studies have primarily focused on three important factors in the network architecture: depth [16], [17], [18], [19], width [17], [20], and cardinality [18], [21]. For example, Nah et al. [15] stack 19 residual blocks with skip connections at each scale level for deep multi-scale dynamic scene deblurring. Finally, the entire deblurring network contains 120 convolution layers (sufficiently deep), leading to a receptive field that is sufficiently large to handle all patches of a blurry image.
In addition to the aforementioned factors, in this study, we consider two different aspects of the network architecture: attention and geometric variations. Specifically, two CNN-based modules are proposed to enhance deblurring performance: an adaptive-attention module (AAM) and a deformable convolutional module (DCM). These modules can be directly integrated into existing methods. The AAM is designed using channel and spatial attention modules. Moreover, the novelty of the AAM is that it adaptively determines the arrangement of channel and special attention modules (i.e., sequentially or in parallel) to obtain better deblurring performance. The DCM is designed to handle geometric variations. That is, the network learns blur characteristics and their location through the channel and spatial attention mechanisms, respectively, using the proposed AAM, and then learns how to handle them specifically using the proposed DCM. It should be noted that the proposed modules do not have a large computational and memory overhead. Finally, we conduct preliminary experiments to evaluate the effectiveness of the modules by directly incorporating them into existing deblurring methods [1], [15], and it is demonstrated that the performance of dynamic scene deblurring is improved significantly. Furthermore, we empirically verify in Section 4 that the performance achieved by incorporating the modules into an encoder–decoder ResBlock network (ERCNN) without additional components designed carefully compares favorably with that of state-of-the-art methods, indicating the generality of the proposed modules for dynamic scene deblurring. An ERCNN is a typical CNN architecture used for dynamic scene deblurring [1], [4]. Fig. 1 shows the diagram of the proposed AAM and DCM integrated with the ERCNN. An example of deblurring by the ERCNN incorporating the proposed modules is shown in Fig. 2, where it can be seen that the resulting model can restore detailed information (such as text and edges) in a dynamic scene more effectively, as it can handle a large amount of dynamic blur.Download : Download high-res image (157KB)Download : Download full-size imageFig. 1. Proposed modules integrated with ERCNN. The AAM is placed in the middle of ResBlock, and the DCM is placed at the beginning and the end of ERCNN. The modules are lightweight, and thus the computational and parameter passing overhead is negligible.Download : Download high-res image (403KB)Download : Download full-size imageFig. 2. Example of deblurring by ERCNN incorporating the proposed modules on the GoPro dataset [15]. (a) Blurry image, (b)–(e) deblurring by [15], [3], [2], [1], respectively, (f) deblurring by proposed model (shown in Fig. 1).
The contributions of this study are summarized as follows:
i)The proposed AAM and DCM can be directly integrated into existing methods and enhance deblurring performance. The AAM is designed to extract blur characteristics and their location for dynamic scene deblurring, corresponding to the channel and spatial attention mechanisms, respectively. The novelty of the AAM lies is that it adaptively determines the arrangement of channel and special attention modules (i.e., sequentially or in parallel) for better deblurring performance. The DCM is designed to handle geometric variations.ii)The effectiveness of the proposed modules is experimentally confirmed by directly integrating them into existing deblurring methods.iii)We empirically verify that incorporating the proposed modules into an ERCNN without additional components designed carefully can outperform the state-of-the-art methods, indicating the general applicability of these modules for dynamic scene deblurring.
The rest of this paper is organized as follows. In Section 2, related work on dynamic scene deblurring is reviewed. In Section 3, the proposed AAM and DCM are presented and discussed in detail. In Section 4, preliminary experiments are presented to demonstrate the effectiveness of the proposed modules by incorporating them into existing deblurring models. Finally, Section 5 concludes the paper.
