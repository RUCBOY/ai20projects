Much of the fastest-growing data these days is highly repetitive: versioned document and software repositories like Wikipedia and GitHub store tens of versions of each document; whole-genome sequencing projects generate thousands of genomes of individuals of the same species; periodic astronomical surveys regularly scan the same portion of the sky. Such repetitiveness makes those large datasets highly compressible with dictionary methods like grammar-based or Lempel-Ziv compression, whereas typical statistical compression fails to capture the repetitiveness [29].
Lempel-Ziv compression [34] of a string S[1..n] parses S into a sequence of z “phrases”, where each phrase S[i..j] is a new symbol (and j=i) or it appears leftwards in S. Lempel-Ziv compression takes O(n) time [44] and reduces S to O(z) space by encoding the phrases. While Lempel-Ziv is the practical method that best exploits repetitiveness, it has the problem that no way is known to access arbitrary substrings of S without decompressing it from the beginning.
All the previous work in the literature [8], [10], [3], [4], [24] resorts to grammar-based compression when it comes to provide direct access to compressed highly repetitive strings. Grammar-based compression [33] of S consists in generating a context-free grammar that generates S and only S. When S is repetitive, the size g of the grammar can be much smaller than n. While finding the smallest grammar that generates S is NP-complete [46], [13], there are several linear-time approximations that generate grammars of size g=O(zlog⁡(n/z)) [46], [13], [25]. Grammars, however, do not compress as well as Lempel-Ziv in general: it always holds that g≥z [46], [13], and there are string families like S=an where z=O(1) and g=Θ(log⁡n). Yet, grammars permit access to arbitrary positions of S in time O(log⁡n) and space O(g); this is not known to be possible within space O(z).
In this article we introduce the block tree, a data structure that uses O(zlog⁡(n/z)) space and supports direct access to any symbol of S in time O(log⁡(n/z)). It allows other space-time tradeoffs, for example it can use O(z1−ϵnϵ) space for any constant ϵ>0 and extract any substring of length m in RAM-optimal time O(1+m/logσ⁡n), where σ is the alphabet size of S. This is the first structure offering constant-time access to S within O(z1−ϵnϵ) space.
Several more sophisticated objects can be represented in compressed form by resorting to strings [36]. These representations generally need not only direct access to S, but (at least) the following two fundamental operations:S.ranka(i)=return the number of occurrences of the character a in S[1..i].S.selecta(j)=return the position of the jth occurrence of a in S.
Such operations can also be supported on grammar-compressed strings in time O(log⁡n), by multiplying the space by O(σ) [10], [41]. The same holds with block trees, where we can support rank and select in space O(zσlog⁡(n/z)) and time O(log⁡(n/z)), among other tradeoffs.
Block trees can be built in linear time and space. We also explore other scalable techniques that are relevant when S is very large and we cannot afford such extra space.
Finally, we show experimentally that block trees are a practical alternative to grammar-based representations. As expected from the theoretical results, block-tree-based representations are significantly (typically an order of magnitude) faster than grammar-based ones at the expense of sometimes being larger. On highly repetitive strings, however, block trees still offer an extremely compact representation with very fast direct access and rank/select support, competitive in time with statistically compressed representations, which are many times larger.
This article is a revised and expanded version of its conference publication [5], where in particular we have optimized the space of the data structure, removed incorrect claims about lowest-common-ancestor functionality, written the results with more detail, and carried out a much more extensive implementation and experimental comparison of different practical variants. The block tree idea derives from the earlier concept of block graph [19], [20], which is more complex and has less functionality and worse time complexities.
