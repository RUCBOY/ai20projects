Artificial neurons used in conventional ANNs are the first-order (linear) models of biological neurons. In the mammalian nervous system, the biological learning is mainly performed at the cellular level. As shown in Fig. 1, each neuron is capable of processing the electrical signal based on the three individual operations [1], [2]: 1) reception of the other neurons outputs through the synaptic connections in Dendrites, 2) the integration (or pooling) of the processed output signals in the soma at the nucleus of the cell, and, 3) the activation of the final signal at the first part of the Axon or the so-called Axon hillock: if the pooled potentials exceed a certain limit, it “activates” a series of pulses (action potentials). As shown in Fig. 1(b), each terminal button is connected to other neurons across a small gap called a synapse. During the 1940s the first “artificial neuron” model was proposed by McCulloch-Pitts [3], which has thereafter been used in various feed-forward ANNs such as Multi-Layer Perceptrons (MLPs). As expressed in Eq. (1), in this popular model the artificial neuron performs a linear transformation through a weighted summation by the scalar weights. So, the basic operations performed in a biological neuron, that operate the individual synaptic connections with specific neurochemical operations and the integration in the cell’s soma are modeled as the linear transformation (linear weighted sum) followed by a possibly nonlinear thresholding function, f(.), which is called activation function.(1)xkl=bkl+∑i=1Nl-1wikl-1yil-1andykl=fxklDownload : Download high-res image (202KB)Download : Download full-size imageFig. 1. A biological neuron (left) with the direction of the signal flow and a synapse (right) [7].
The concept of “Perceptron”, was proposed by Frank Rosenblatt in his seminal work [4]. When used in all neurons of a MLP, this linear model is a basic model of the biological neurons leading to well-known variations in learning and generalization performances for various problems [4], [5], [6], [7], [8]. In the literature, there have been some attempts to change MLPs by modifying the neuron model and/or the conventional Back Propagation (BP) algorithm [9], [10], [11], or the MLP configuration [12], [13], [14] or even the way to update the network parameters (weights and biases) [15]. The most promising variant is called Generalized Operational Perceptrons [7], [8], which is a heterogeneous network with non-linear operators and has thus exhibited significantly superior performance than MLPs; however, this is still the most common network model that has inspired the modern-age ANNs that are being used today.
Starting from the 1959, Hubel and Wiesel have established the foundations of the visual neuroscience through the study of the visual cortical system of cats. Their collaboration has lasted more than 25 years during which they have described the major responsive properties of the visual cortical neurons, the concept of receptive field, the functional properties of the visual cortex and the role of the visual experience in shaping the cortical architecture, in a series of articles published in The Journal of Physiology [16], [17], [18], [19], [20]. They are the pioneers who found the hierarchical processing mechanism of information in the visual cortical pathway, which eventually led to the Nobel Prize in Physiology or Medicine in 1981. With these advances in neurocognitive science, Fukushima and Miyake [21] in 1982 proposed the predecessor of Convolutional Neural Networks (CNNs), at the time called as “Neocognitron” which is a self-organized, hierarchical network and has the capability to recognize stimulus patterns based on the differences in their appearances (e.g., shapes). This was the first network, which has the unique ability of a biological mammalian visual system, that is, the assessment of similar objects to be assigned to the same object category independent from their position and certain morphological variations. However, in an attempt to maximize the learning performance, the crucial need of a supervised method to train (or adapt) the network for the learning task in hand became imminent. The ground-breaking invention of the Back-Propagation (BP) by Rumelhart and Hinton in 1986 [22] became a major cornerstone of the Machine Learning (ML) era. BP incrementally optimizes the network parameters, i.e., weights and biases, in an iterative manner using the gradient descent optimization technique.
These two accomplishments have started a new wave of approaches that eventually created the first naïve CNN models but it was the seminal work of Yann LeCun in 1990 who formulated the BP to train the first CNN [23], the so-called “LeNet”. This CNN ancestor became mature in 1998 and its superior classification power was demonstrated in [24] over the benchmark MNIST handwritten number database [25]. This success has begun the era of CNNs and brought a new hope to otherwise “idle” world of ML during the 1980s and early 90s. CNNs have been used in many applications during the 90s and the first decade of the 21st century but soon they fell out of fashion especially with the emergence of new generation ML paradigms such as Support Vector Machines (SVMs) and Bayesian Networks (BNs). There are two main reasons for this. First, small or medium size databases were insufficient to train a deep CNN with a superior generalization capability. Then of course, training a deep CNN is computationally very demanding and feasible only with the modern graphical processors present today. This is why during these two decades the application of CNNs has been limited only to low-resolution (e.g. the thumbnail size) and gray-scale images in small-size datasets. On the contrary, both SVMs and BNs in comparison have fewer parameters that can be well optimized especially over such small to medium size datasets and independent from the image resolution. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in the image classification competition in 2012 became the turning point for the application of deep CNNs in the area of large-scale image classification. For this competition, Krizhevsky et al. proposed the deep CNN model for the first time, the so-called AlexNet [26] which is the ancestor of the Deep Learning paradigm. AlexNet was an 8-layer CNN (5 convolutional-pooling layers and 3 fully-connected layers) that achieved 16.4% error rate in the ImageNet benchmark database [27] and this was about 10% lower than the second top method that uses a traditional ML approach, i.e., the Support Vector Machine (SVM) classifier network over the traditional visual features of Scale Invariant Feature Transform (SIFT) [28] and Local Binary Patterns (LBP) [29]. The ImageNet database contains more than one million images for training and divided into 1000 visual categories. The same study [26] also proposed some novel architectural features such as Rectified Linear Units (ReLU) instead of traditional activation functions such as Sigmoids (sigm) or Tangent Hyperbolics (tanh). The AlexNet team also proposed the dropout technique in [30] to improve the generalization capability of the network. However, the most important factor which made CNNs the mainstream method afterwards was the ability to train them over a massive size dataset by using parallelized computational paradigms over the emerging graphical processing units (GPUs).
With the successful introduction of AlexNet, the era of deep 2D CNNs has begun and immediately replaced the traditional classification and recognition methods within a short time. Deep CNNs eventually have become the primary tool used in any Deep Learning (DL) application including the contemporary ILSVRC image classification competitions. The following year, a new network, the so-called ZFnet [31] was proposed by Zeiler and Fergus that became the winning CNN model of the ILSVRC 2013. ZFnet further reduced the error rate down to 11.7% on the ImageNet database. The authors have shown how to visualize each convolution layer of the CNN which in turn has deepened our understanding why CNNs achieve such superior discrimination power among different visual object categories. The following year in ILSVRC 2014, a new breakthrough was achieved by the Google team with a deeper CNN, called as “GoogLeNet” with a codename “Inception”, which almost halved the best error rate down to 6.7% in the ImageNet database. GoogLeNet has been designed by increasing the depth (with a 22 convolutional layers) and also the width of the network while keeping the computational budget constant. Besides using a deeper network with sparse connections, the key idea is that GoogLeNet obtained the top object recognition performance in ILSVRC 2014 with an ensemble of 6 CNNs. Since then, the popularity of the deep CNNs has peaked and eventually they became the de facto standard for various ML and computer vision applications over the years. Furthermore, they have been frequently used in processing sequential data including Natural Language Processing and Speech Recognition [32], [33] and even 1D signals e.g., vibration [34], [35].
Besides the top performance levels they can achieve, another crucial advantage they offer is that they can combine both feature extraction and classification tasks into a single body unlike traditional Artificial Neural Networks (ANNs). While conventional Machine Learning (ML) methods usually perform certain pre-processing steps and then use fixed and hand-crafted features which are not only sub-optimal but may usually require a high computational complexity, CNN-based methods can extract the “learned” features directly from the raw data of the problem at hand to maximize the classification accuracy. This is indeed the key characteristic for improving the classification performance significantly which made CNNs attractive to complicated engineering applications. However, the reign of traditional ML approaches was still unchallenged for 1D signals since deep CNNs were modeled and created specifically for 2D signals and their application was not straightforward for 1D signal signals especially when the data is scarce. The direct utilization of a deep CNN for a 1-D signal processing application naturally needs a proper 1D to 2D conversion. Recently, researchers have tried to use deep CNNs for fault diagnosis of bearings [34], [35], [36], [37], [38], [39], [40], [41], [42]. For this purpose, different conversion techniques have been utilized to represent the 1D vibration signals in 2D. A commonly used technique is to directly reshape the vibration signal into an n × m matrix called “the vibration image” [39]. Another technique was used in [37] where two vibration signals were measured using two accelerometers. Then, Discrete Fourier Transform (DFT) was applied, and the two transformed signals were represented in a matrix which can be fed into a conventional deep CNN. For electrocardiogram (ECG) beat classification and arrhythmia detection, the common approach is to first compute power- or log-spectrogram to convert each ECG beat to a 2D image [43], [44]. However, there are certain drawbacks and limitations of using such deep CNNs. Primarily, it is known that they pose a high computational complexity which requires special hardware especially for training. Therefore, 2D CNNs are not suitable for real-time applications on mobile and low-power/low-memory devices. In addition, proper training of deep CNNs requires a massive size dataset for training to achieve a reasonable generalization capability. This may not be a viable option for many practical 1D signal applications where labeled data can be scarce.
To incorporate such drawbacks, in 2015 Kiranyaz et al. [45] proposed the first compact and adaptive 1D CNNs to operate directly on patient-specific ECG signals. In a relatively short time, 1D CNNs have become popular with a state-of-the-art performance in various signal processing applications such as early arrhythmia detection in electrocardiogram (ECG) beats [45], [46], [47], structural health monitoring and structural damage detection [48], [49], [50], [51], [52], high power engine fault monitoring [53] and real-time monitoring of high-power circuitry [54]. Furthermore, two recent studies have utilized 1D CNNs for damage detection in bearings [55], [56], [57], [58]. However, in the latter study conducted by Zhang et al. [58], both single and ensemble of deep 1D CNN(s) were created to detect, localize, and quantify bearing faults. A deep configuration of 1D CNN used in this study consisted of 6 large convolutional layers followed by two fully connected (dense) layers. Other deep 1D CNN approaches have been recently proposed by [59], [60], [61], [62] for anomaly detection in ECG signals. These deep configurations share the common drawbacks of their 2D counterparts. For example, in [58], several “tricks” were utilized to improve the generalization performance of the deep 1D CNN such as data augmentation, batch normalization, dropout, majority voting, etc. Another approach to tackle this problem is to utilize the majority of the dataset for training which may not be feasible in some practical applications. In the study [58], more than 96% of the total data is used to train the deep network. With that, the assumption that such a large set of training data would be available may hinder the utilization of this method in practice. Therefore, in this article the focus is drawn particularly on compact 1D CNNs with few hidden layers/neurons, and their applications to some major engineering problems with the assumption that the labeled data is scarce, or application or device-specific solutions are required to maximize the detection and identification accuracy. The benchmark datasets and the principal 1D CNN software used in those applications are now publicly shared in [63].
The motivation for this survey is to offer a comprehensive overview of 1D CNNs, both theoretically, and from an application and a methodology driven perspective. This survey includes over 90 papers, most of them recent, on a wide variety of applications of conventional 2D CNNs and of course, the recent 1D variants. When overlapping work had been reported in multiple publications, only the publication(s) deemed most important were included. We expect the search terms used to cover most of the work incorporating compact 1D CNNs and its deep variants. The state-of-the-art 1D CNN applications on real-time electrocardiogram monitoring and anomaly detection were presented in detail in Appendix A. Finally, we leveraged our own experience with the application of compact 1D CNNs to various application domains to provide readers a detailed insight covering the state-of-the-art, some of the current open challenges and overview of research directions which we think that they will become important in the near future.
The rest of the paper is organized as follows. Section 2 provides a general background on adaptive and compact 1D CNNs with the formulation for Back-Propagation (BP) training. Section 3 presents a brief review on popular engineering applications of the 1D CNNs. Section 4 presents a detailed computational complexity analysis of the 1D CNNs and the computational times of the competing methods on a sample application domain. Finally, Section 5 concludes the paper and suggests topics for future directions on 1D CNNs.
