In machine learning, classification involves a training set P⊂Rd of n labeled points in Euclidean space. The label l(p) of each point p∈P indicates the class to which the point belongs to, partitioning of P into a finite set of classes. Given an unlabeled query point q∈Rd the goal of a classifier is to predict q's label using P.
The nearest-neighbor rule is among the best-known classification techniques [2]. It classifies a query point q with the label of its closest point in P, according to some metric. Throughout, we will assume the Euclidean ℓ2 metric. Despite its simplicity, the nearest-neighbor rule exhibits good classification accuracy both experimentally and theoretically [3], [4], [5]. However, it's often criticized due to its high space and time complexities. This raises the question of whether it is possible to replace P with a significantly smaller subset without affecting the classification accuracy under the nearest-neighbor rule. This problem is called nearest-neighbor condensation.
In this paper, we propose the first approach to successfully upper-bound the cardinality of the subsets selected by practical condensation algorithms. Particularly, we propose two algorithms for this problem, along with theoretical guarantees on their worst-case performance. Additionally, we analyze the selection of two well-known condensation algorithms, comparing their worst-case performance to our algorithms.
1.1. PreliminariesFor any point p∈P, define an enemy of p to be any point in P of different class than p. The nearest enemy of p, denoted ne(p), is the closest such point,1 and its distance from p, called the nearest-enemy distance, is denoted as dne(p)=d(p,ne(p)). Similarly, denote the nearest-neighbor distance as dnn(p)=d(p,nn(p)). Define the nearest-enemy ball of p to be the ball centered at p with radius dne(p). Let κ denote the number of distinct nearest-enemy points of P (Fig. 1a-b).Download : Download high-res image (269KB)Download : Download full-size imageFig. 1. Illustrative examples of the definitions of nearest-enemy and border points. (a) Shows the nearest-enemy of a point p ∈ P, denoted ne(p). (b) Highlights the set of points that are the nearest-enemy of some other point in P, with cardinality equal to κ. (c) Describes the boundary between classes (red line). The points of P with a Voronoi cell adjacent to this boundary are called border points, and there are k such points. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)A point p∈P is called a border point if it is incident to an edge of the Delaunay triangulation of P whose opposite endpoint is an enemy of p. Otherwise, p is called an internal point. By definition, the border points of P completely characterize the portion of the Voronoi diagram that separates Voronoi cells of different classes. Let k denote the number of border points of P (Fig. 1c).
1.2. Related workA subset R⊆P is said to be consistent if for all p∈P its nearest neighbor in R is of the same class as p. Intuitively, R is consistent if and only if every point of P is correctly classified using the nearest-neighbor rule over R. Formally, nearest-neighbor condensation involves finding an (ideally small) consistent subset of P [6].Other criteria for condensation have been studied in the literature. One such criterion is known as selectivity [7]. A subset R⊆P is said to be selective if and only if for all p∈P, its nearest neighbor in R is closer to p than its nearest enemy in P. Clearly selectivity implies consistency, as the nearest-enemy distance in R of any point of P is at least its nearest-enemy distance in P.The strongest criteria, known as Voronoi condensation, consists of selecting all border points of P [8]. For the case when P⊂R2, an output-sensitive algorithm was proposed [9] for finding all border points of P in O(nlog⁡k) worst-case time. Unfortunately, it is not known how to generalize this algorithm to higher dimensions, and a straightforward approach suffers from the super-linear worst-case size of the Delaunay triangulation.Note that the Voronoi condensation criteria guarantees the correct classification of any query point in Rd. In contrast, neither consistent nor selective subsets can provide such guarantee, and only ensure the correct classification of points in P.In general, it has been shown that the problems of computing consistent and selective subsets of minimum cardinality are both NP-complete [10], [11], [12]. Thus, most research has focused on practical heuristics. For comprehensive surveys, see [13], [14], [15]. CNN (Condensed Nearest-Neighbor) [6] was the first algorithm proposed for computing consistent subsets. Even though it has been widely cited in the literature, CNN suffers from several drawbacks: its running time is cubic in the worst-case, and the resulting subset is order-dependent, meaning that the result is determined by the order in which points are considered by the algorithm. Alternatives include FCNN (Fast CNN) [16] and MSS (Modified Selective Subset) [17], which produce consistent and selective subsets respectively. Both algorithms run in O(n2) worst-case time, and are order-independent. These algorithms are considered the state-of-the-art for the nearest-neighbor condensation problem, subject to achieving these properties. While such heuristics have been extensively studied experimentally [15], [18], theoretical results are scarce. Unfortunately, to the best of our knowledge, no bounds are known for the size of the subsets generated by any of these heuristics.More recently, an approximation algorithm called NET [19] was proposed, along with almost matching hardness lower bounds for the problem. The idea is to compute a γ-net of P, with γ equal to the minimum nearest-enemy distance in P, implying that the resulting subset is consistent. Unfortunately, while NET has provable worst-case performance, this approach allows little room for condensation, and in practice, the resulting subset can be too large. Thus, the authors propose to run a post processing heuristic to further reduce the training set; this approach is called NET+PRUNE. For the training set in Fig. 2a, while other algorithms select less than 300 points, NET+PRUNE selects 875 points, and NET alone selects over 9000 points.Download : Download high-res image (1MB)Download : Download full-size imageFig. 2. Examples of the subsets selected by FCNN, MSS, RSS, and VSS, on two different training sets. Training set (a) is a set of uniformly distributed points in R2 of two classes: red points lying inside a disk, and blue points lying outside. Training set (f) is a well-known benchmark from the UCI Machine Learning repository, called Banana, consisting of points in R2 of two classes, red and blue.
1.3. ContributionsOne of the most significant shortcomings in research on practical nearest-neighbor condensation algorithms is the lack of theoretical results on the sizes of the selected subsets. Typically, the performance of most of these algorithms has been established experimentally.In this paper, we propose one of the first approaches to successfully upper-bound the cardinality of the subsets selected by practical condensation algorithms. Previous attempts to do this, either work only for the plane [9], [20], or are impractical algorithms (e.g., NET [19]). We establish our upper-bounds in terms of both κ and k, the number of nearest-enemy and border points, respectively. This is the first time κ is used with this purpose, and the first time k is successfully used to upper-bound a practical condensation algorithm in Rd.First, we introduce two new nearest-neighbor condensation algorithms called RSS and VSS, and establish asymptotically tight upper-bounds on the sizes of their selected subsets. Moreover, we prove that these algorithms have similar time complexity and performance as the well-known algorithms for this problem (i.e., FCNN and MSS). Furthermore, we analyze the selection size of FCNN and MSS, showing negative results. The following is a summary of our contributions.AlgorithmSelection sizeRSSO(κcd−1)VSS≤kMSS [17]Ω(1/ε) w.r.t. κ and kFCNN [16]Ω(k)
