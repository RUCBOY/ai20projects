Modern data collection techniques impel researchers to work with nonlinear, high-dimensional, and big data, such as social and sensor networks [1], collective motion [2], and finance [3]. Analyzing such datasets is challenging or sometimes implausible without the presence of Nonlinear Dimensionality Reduction (NDR) techniques. NDR is defined as the process of transforming nonlinear high-dimensional data into a low-dimensional manifold representation [4]. Here, we propose an NDR method that utilizes concepts of low-rank Matrix Completion (MC) to guarantee better performance when the dataset consists of missing data entrees and is contaminated with noise and outliers.
Low-rank MC, a technique for estimating unobserved or partially observed (i.e., bounded) entries of a matrix that satisfies a low-rank assumption has been prominent in many real-world applications such as image inpainting [5] and recommender systems [6]. Since the data sampled from these applications are naturally high-dimensional and large volume, working with all the available data or acquiring all the information for each matrix entry is often not feasible. Thus, MC provides efficient tools to deal with partially observed or unobserved entries of a matrix and recover a low-rank estimate of the fully observed matrix. The proposed low-rank MC algorithm for NDR consists of an additional constraint that assures the recovered matrix represents distances on a manifold. This novel method is theoretically justified based upon recent work in truncated nuclear norm as a relaxation to the original rank [7] and provides superior empirical performance in challenging test problems. There are several MC schemes that use alternative optimization procedures to aforesaid standard nuclear norm relaxation. As examples, MC scheme in Ref. [8] directly minimize the original formulations of trace norm and rank norm, the MC scheme in Ref. [9] uses joint Schatten p-norm and lp-norm to approximate the rank minimization problem, and the work proposed in Ref. [10] utilizes joint capped trace norm and capped l1-norm to tightly approximate the rank minimization.
To set the stage for the description of our approach, we observe that classic techniques, such as Principal Component Analysis (PCA) [11] and Multi-Dimensional Scaling (MDS) [11], assume that the data lay close to a low-dimensional linear manifold (or subspace). Advanced techniques, such as Isometric Mapping (ISOMAP) [12], assume that each point in a dataset lay on or near a nonlinear manifold where the distances on the manifold provide faithful geodesic representations of the distances between the points in the ambient high-dimensional Euclidean space in which the points are embedded. Such methods rely on tight theoretical connections between the inner product matrix of the input data, often called the Gramian matrix, and the matrix of squared Euclidean Distances (EDs) between the points in the data-set [11]. One recent method that is somewhat different than the above, but superficially similar in flavor to our proposed technique is Maximum Variance Unfolding (MVU) [13], which attempts to maximize the variance of the data in the presence of distance constraints between some pairs of points. However, as we will explain in detail in the sequel, our proposed method arises from a quite different theoretical perspective than MVU and extends the capabilities of current methods in several important directions. Local Spline Embedding (LSE) [14] minimizes the reconstruction error of an objective function and embeds the data using smoothing splines that map local coordinates of the underlying manifold to global coordinates. Specifically, LSE assumes the existence of a smooth low-dimensional underlying manifold that is quite similar to our proposed method that assumes existence of a low-dimensional manifold. However, LSE assumes that the data is noise free and unaffected by anomalies whereas our method is made to work on noisy data with anomalies.
With the above ideas in mind, the high-level idea of our proposed algorithm is actually quite simple. Given a collection of points in high-dimensional space, a classic MDS algorithm assumes that the EDs between all the points are a faithful representation of “meaningful” distances between the points (an idea that will be made precise in the sequel). ISOMAP relaxes that assumption by only using EDs between neighboring points and using a shortest path graph distance as a proxy for a “meaningful” distance between far away points. Our method relaxes this assumption even further. Like ISOMAP, we assume that the EDs between neighboring points are “meaningful”, but differing from ISOMAP, we do not assume we have any approximation for far away distances. We only assume that there exist rough upper and lower bounds for the distances between far away points (perhaps provided by a maximum curvature type argument). In fact, there can be pairs of points (perhaps even many of them) for which nothing is known about their distances on the low-dimensional manifold. Such upper and lower bounds, combined with a constraint that the recovered matrix is Positive Semi-Definite (PSD), allow us to apply a MC scheme for the recovery of the unobserved distances and thereby the parameterization of the underlying low-dimensional manifold. Therefore, we name our method Bounded Manifold Completion (BMC).
Recovery of an arbitrary matrix using a low-rank principle is an ill-posed problem since such an optimization problem is non-convex and discontinuous [7]. Recent research in MC [15], [16], suggests that the nuclear norm convex relaxation is an efficient alternative to rank. However, the major limitation of an optimization involving a nuclear norm is that it minimizes the sum of all the Singular Values (SVs), and therefore might not correctly approximate the distances on the manifold. Thus, similar to [7], here we employ a truncated nuclear norm (equivalently, the sum of the smallest SVs) to achieve an accurate approximation for the partially observed distance matrix. We formulate BMC as an optimization scheme with multiple constraints where we use an Alternating Direction Method of Multipliers (ADMM) [17], to reformulate the original problem as a general separable collection of convex problems [7] with an additional constraint to enforce that the returned matrix is a distance matrix. Existing schemes utilizing nuclear norm heuristics such as SV thresholding [18], nuclear norm regularized least squares [19], and Robust Principal Component Analysis (RPCA) [15] have shown noteworthy performance. However, as we show in Fig. 1, using a truncated nuclear norm for nonlinear dimensionality reduction allows the detection of a low-dimensional embedding with an error that is 0, to within machine-precision.Download : Download high-res image (344KB)Download : Download full-size imageFig. 1. Recovery of a non-uniformly sampled nonlinear manifold using BMC. (a) A dataset of 500 points sampled non-uniformly from a semi-cylinder having a hollow region and four isolated clusters. Here, the color spectrum shows the variation of manifold lengths along the y(3) axis. (b) 2-D embedding of the dataset performed using BMC where the color variation is associated with that of (a). Note, the correct 2-D embedding is accurately recovered even though the nonlinear manifold is sampled non-uniformly. (c) Semilogarithmic plot of the first 10 SVs (σ(j)’s) of the recovered squared distance matrix. For this data set, the theoretically optimal embedding, by construction, has 4 non-zero singular values. Note that the fifth singular value is smaller than the fourth singular value by more than 10 orders of magnitude and is merely an artifact of machine precision.
One of the key elements of our proposed MC scheme is the MC algorithm from [16] which is capable of recovering a matrix also having partially observed data. Specifically, the input distance matrix here is given in terms of two matrices, an upper bound matrix and a lower bound matrix. In particular, the two corresponding values in the bound matrices are the same when the value at that location is fully observed and the two values in the bound matrices are not equal when the value of that location is partially observed. In particular, unobserved data can also be handled in this way by setting the lower and upper bounds to be arbitrarily small and large, respectively. This partially observed data technique is implemented in BMC which allows our method to leverage distances that are fully observed (the Manifold Distances (MDs) between two points are known), partially observed (the MDs between two points can only be bounded), and unobserved (the MDs between the points are not known).
Referring back to Fig. 1, we provide a simple example to illustrate the performance of BMC. We sample a dataset of 500 points Y={y1,…,y500} with yi=(yi(1),yi(2),yi(3))∈R3 from a semi-cylinder such that the sample has some hollow parts of unknown size and four isolated clusters in the corners of the data [Fig. 1(a)]. We construct a squared ED matrix De∈R≥0500×500 (real numbers greater than or equal to 0) such that Dije=∥yi−yj∥22. We set the lower bound matrix (Dl) and the upper bound matrix (Du) of the recovered squared distance matrix to be Dijl=0.1Dije and Diju=10Dije. Note, no actual MDs are required. A semi-cylinder represents a 2-D manifold; thus, the rank of the squared distance matrix of this dataset, when properly embedded, is four (we present this as a Theorem in Section 2.1). Then, BMC truncates the first four SVs, generates the recovered distance matrix L, and performs a 2-D embedding of the dataset in Fig. 1(b). We observe in the embedding that BMC is capable of projecting the data onto a 2-D plane such that the topology of the data is preserved even though the data is non-uniformly sampled. We compute the natural logarithm of the first 10 SVs of BMC’s recovered squared distance matrix in Fig. 1(d). We observe that the first four SVs of L are non-zero while the rest of the SVs are (to within machine precision) zero and, therefore, BMC is capable of recovering the correct low-rank representation of the data.
This paper is structured as follows: In Section 2, we will detail our proposed method in three subsections; first, we present the derivation of our BMC method along with some classic MC methods that inspired BMC; then, the numerical implementation of BMC is presented; finally, we present the ADMM solver of BMC. We validate the performance of BMC in Section 3 using one synthetic dataset, a cylinder having a hollow region, and two real-life datasets, face images and handwritten digits. Here, we also present five classic DR methods that we will contrast/compare with BMC. Finally, we present discussions and conclusions in Section 4.
1.1. ContributionsOur proposed BMC approach makes the following contributions to the literature:•BMC is a novel nonlinear dimension reduction algorithm which leverages upper and lower bounds on MDs rather than parameters such as the presumed number of neighbors of a data point.•Even more, BMC is capable of operating when no information is available for many of the MDs.•BMC provides theoretical guarantees as to the performance of the algorithm, leveraging ideas from [7].•BMC provides unique capabilities for detecting low-dimensional manifolds even when the manifold is non-uniformly sampled.•BMC is implemented using a fast ADMM solver allowing large problems to be treated with many data points.•The idea of using bounded low-rank matrix completion has applications outside of low-dimensional manifold detection where the input data is noisy or otherwise only roughly known.
