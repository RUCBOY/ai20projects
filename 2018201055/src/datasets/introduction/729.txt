Human action recognition in videos has always been an active area in computer vision for its wide range of applications, such as human–computer interaction, motion analysis, virtual reality and medical image analysis [1], [2], [3], [4], [5], [6], [7]. The breakthroughs in this area are greatly attributed to the advent of Deep Neural Networks, especially 3D Convolutional Neural Networks [8], [9], [10]. The rich spatial–temporal features extracted from RGB image or optical flow have been proved to be essential for predicting human action. However, most existing RGB-based methods using 3D CNNs are incapable of dealing with the video that contains some irrelevant actions. Figs. 1(a) and 1(b) are the RGB images and optical flows of one video at different points in time. Obviously, it has difficulty in identifying the class of this video because the man dressed in red is kicking the ball while the man in yellow is catching.
With the fast development of human pose estimation [11], [12], [13], [14], [15], pose-based action recognition has been one of the mainstream approaches due to its simple and credible processing [16], [17]. Pose-based methods focus on the subtle cues of human pose rather than the whole features which contain some interference. The skeleton joints extracted from human pose are also vital clues for human action recognition. Recent works have focused on exploring a way of combining the RGB image or optical flow with human pose (skeleton). One mainstream approach is that the model learns these three types of features separately in training, and fuses the features of each type by weighting their scores in testing. In this case, these methods lack the interaction of different types of features.
The challenge for the method based on the RGB image, optical flow and human pose is the way to effectively model the pose features and fuse them with the RGB and optical flow cues. In fact, the RGB, optical flow and pose cues are completely different representations of human actions in the discriminant analysis. Therefore, it is essential for us to establish a correlation between them during fusion. This paper aims to propose a pose module to improve the performance of Inflated 3D ConvNet (I3D) and explore the fusion methods.Download : Download high-res image (299KB)Download : Download full-size imageFig. 1. Comparison of three common inputs for action recognition. (a) and (b) are the RGB images and optical flows of a video at different times. Obviously, it is difficult to identify the category of this video using RGB images and optical flow, because the person in red is kicking the ball and the person in yellow is catching. To this end, we propose a Pose-guided Inflated 3D ConvNet (PI3D) framework to fix this problem. As shown in (c), we embed a pose module to find out the main subject and extract its skeleton joints for the improvement of Inflated 3D ConvNet (I3D).
To address the above-mentioned problem, we propose a novel Pose-Guided Inflated 3D ConvNet framework (PI3D). First, based on I3D, we build the relation between RGB image or optical flow and skeleton data by embedding a spatial–temporal pose module guided by human pose. The pose module consists of pose estimation and pose-based action recognition. Specifically, the pose estimation framework is introduced to find out the main subject by calculating the confidence score of each subject, as shown in Fig. 1(c). Second, the skeleton sequences extracted from each video are fed into the pose-based model, and I3D captures the comprehensive clues of RGB image and optical flow. Each group of skeleton points corresponds to the same video as RGB images or optical flows. Finally, the last convolutional layer of I3D and pose-based models are combined together, followed by a fully connected layer. The pose features are trained with RGB images and optical flows respectively and their results are weighted during testing.
The main contribution of our work can be summarized as follows. First, we propose a novel Pose-Guided Inflated 3D ConvNet framework, which extracts spatial–temporal features from RGB frame, optical flow and human pose. Thus, it can flexibly model different types of clues and distinguish complex videos. Second, we embed a pose module which is composed of pose estimation and pose-based action recognition to provide an essential clue for I3D. Different from the traditional fusion methods, we concatenate the last convolutional layer of I3D with the pose-based network and train the RGB and optical flow features with human pose, respectively. Third, experiments are performed on four popular action recognition benchmarks (HMDB-51, SYSU 3D, JHMDB and Sub-JHMDB). Experimental results indicate that the proposed PI3D framework outperforms the existing methods in human action recognition. This work also demonstrates that posture cues significantly improve the performance of I3D.
