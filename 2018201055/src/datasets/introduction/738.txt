Personalized recommendation is an inevitable problem that different recommender systems need to address (Wan and Niu, 2018, Wan and Niu, 2016). Recommender systems utilize rating prediction approaches i.e. predicting the rating that a user will give to a particular item, to generate a ranked list of items according to the preferences of each user in order to make personalized recommendations. Matrix Factorization (MF) (Koren et al., 2009) has achieved great success in rating prediction of items for users, thus resulting in items’ recommendations (Cheng et al., 2018a). By factorizing the user–item interactions matrix, MF represents users’ preferences and items’ properties as latent factor vectors in a joint latent space. However, a rating only reflects an overall satisfaction of a user towards an item resulting in unexplained recommendations. The limitations of this method are cold start and sparsity problems, since user–item matrix is often very sparse in real world. In order to address the data lacking issue and consequently cold start and sparsity problems, the information from the review text is exploited. Moreover, exploiting review text helps to improve the performance of recommender systems. Some prior techniques (Bao et al., 2014, Tan et al., 2016) achieve better recommendation performance than conventional topic modeling techniques such as Latent Dirichlet Allocation(LDA) (Blei et al., 2003) and the ones that exploit ratings or reviews alone by coupling with the user–item interactions (i.e., rating scores). However, these models ignore the word order and the local contextual information of the reviews.
Recently, some efforts have been made for the joint use of reviews and ratings that adopt deep learning models (Wu et al., 2019, Lu et al., 2018, Cheng et al., 2018b, Wang et al., 2015). These models learn the latent representations better than topic modeling approaches but are still facing some issues: they work in a static and independent way (Zheng et al., 2017, Wang et al., 2015, Lu et al., 2018), and also the fusion strategies face some limitations. Some models use traditional fusion methods as adopted in Wu et al. (2019) based on Factorization Machines (Rendle, 2010), whereas the ones employing sequential concatenation of latent features do not account for any interaction among the features (He et al., 2017). Some recent models like CARL (Wu et al., 2019), TARMF (Lu et al., 2018) and ALFM (Cheng et al., 2018b) use reviews and ratings but they did not learn the mutual interaction between different latent features. For example, TARMF learns the latent features based on latent topics and later use the learned features with ratings by extending Probabilistic Matrix Factorization (PMF) (Salakhutdinov and Mnih, 2007). CARL uses separate learning of reviews and interaction based features combining lately using fusion. Some other works such as aSDAE (Dong et al., 2017) and DHA (Ma et al., 2018) use concurrent/mutual learning of different information sources together. aSDAE uses stacked denoising autoencoders to couple ratings (implicit) with user and item side information and adopt MF for final ratings. Similarly, DHA uses two heterogeneous autoencoders(SDAE and RNNED) for learning latent features from sequential (user purchase and browsing history) and non-sequential (textual descriptions, numerical and categorical data) data sources. To the best of our knowledge, two recently published models A3NCF (Cheng et al., 2018a) and DAML (Liu et al., 2019) jointly exploit reviews and ratings for learning correlations in latent features. A3NCF adopts LDA to extract latent topics thus ignoring the word order and local contextual information. According to our best knowing, DAML is the only unified neural network for feature learning that employs word embedding. Also both of these models adopt one-hot encoded user and item entities as their feature vectors. Then by employing embedding layer, interaction features for user and item are inferred. Whereas in the proposed model, we propose to adopt MF model for exacting latent factors for users and items (user–item interactions).
In this paper, we propose a novel Deep Hybrid Model for Recommendation (DHMR) by mutual learning from the ratings, reviews and metadata information of users, items and reviews. In the proposed model, the latent features for users and items are jointly learned from the reviews and metadata by using two parallel neural networks whereas latent factors are inferred by adopting MF model. The different latent representations obtained are then integrated and higher-order non-linear interactions of features are learned by interaction, attention and the MLP layers for final ratings.
The experimental evaluations validate that DHMR outperforms all the selected baselines in terms of prediction accuracy on the evaluated real-world datasets. In summary, the main contributions made in this paper are listed as follows:

•We propose a novel deep hybrid model for recommendation, named DHMR that models user preferences and item properties using ratings, product reviews and metadata in an integration manner. The fusion, attention and fully connected dense layers on top of the two parallel networks connect them in a way that the most influential features from different latent representations can be drawn for the final rating predictions.•An attention network is used to learn the higher order non-linear feature interactions of the latent features learned from ratings, reviews and metadata that apprehend the interaction of users and items.•The experimental results validate that DHMR achieves superior performance in terms of prediction accuracy on publicly available datasets (in both scenarios; with and without metadata), and outperforms various state-of-the-art baseline methods (Liu et al., 2019, Wu et al., 2019) (see Section 4).
The rest of the paper is organized as follows. A brief literature study pertaining to this work is presented in Section 2. DHMR is described in detail in Section 3. To validate the effectiveness of DHMR and its comparison with the baseline methods, the experiments are presented in Section 4. Finally, conclusions are presented in Section 5.
