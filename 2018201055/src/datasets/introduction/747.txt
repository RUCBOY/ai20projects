Though deep neural networks have dramatically boosted the performance of diverse machine learning problems [16], [17] and applications [19], [28] based on their rich feature representations power, its training process heavily depends on a mass of labeled samples.Since the manually annotation of massive amounts such data for various application tasks is often prohibitively expensive inthe real-worldscenarios. Thus, how to effectively mitigate the huge cost of data annotation in machine learning still remains an open issue. Domain adaptation approaches [1], [26] are a well-studied strategy to tackle this issue via transferring knowledge that employs a labeled training dataset to improve the test datasetwith no label information. However, this promising domain adaptation paradigm poses a major challenge in transferring the category classifiers to target tasks [26], [24].
Most existing domain adaptation approaches commonly assume that training and test samples share consistent class labels but follow different distributions in statistical learning utilizing these domains samples without using target labels, i.e., unsupervised domain adaptation [38], [9], and apply classifier trained from source samples to the target samples. Recent researches have revealed that deep neural networks the ability to disentangle explanatory factors of variations underlying datasets, so learning more domain-invariant representations can dramatically boost the power of domain adaptation [41], [12], [45], [46]. Along this line, these based feature methods follow the idea that domain shift in different domains is diminutive and typically embedding moment matching [35], [31], [22], [43], adversarial networks [33], [34], [13], [2], and Batch Normalization statistics [7], into deep models tolearn domain transferable features [35], [20], [33], [14], [21].
Although these deep domain adaptation methods bridge the source and target domains byreducing thefeature distribution discrepancy, assuming that different two domains share identical label space. However, it is often terrible to expect a source domain that has same category label as the target domain, which is usually unlabeled, inthe real-world applications. Therefore, this paper focuses on a more practical scenario that the source domain label space contains the target domain label space, which is known as partial domain adaptation (PDA) [48], [4], [3], [6]. Moreover, it is usually assumed that the source domain is diverse and large enough, and its label space includes that of target domain. Therefore, it is not an effectiveapproach to apply the whole source and target domains matching as previous methods to the partial domain adaptation scenario. The most commonly partial domain adaptation methods perform feature learning and label prediction, mapping the input image domains to deep features, and then down-weighing the sample of outlier source classes to reduce the divergence across two different domains, finally, generating the predicted labels, as shown in Fig. 1. For instance, Cao et al. proposed partial adversarial domain adaptation (PADA) [4] and selective adversarial networks (SAN) [3], and another related work is IWAN method [48]. Three methods address partial domain adaptation via weighing each sample in a domain discriminator which is trained in the adversarial learning networks to distinguish the two domains. In addition, the latest works is ETN method [6], it improves the weight quality over the above three methods through further unveiling the discriminator structure to the transferability quantifier. While these partial domain adaptation methods only concentrate on high-level features, they ignore excavating low-level features to guide the deep domain adaptation model togenerate fine-grained feature representations.Download : Download high-res image (169KB)Download : Download full-size imageFig. 1. The typical framework of partial domain adaptation.
In this work, we propose a novel Multiple Self-Attention Networks (MSAN) model for partial domain adaptation, which improves the previous works [3], [4], [48], [6] by learning fine-grained feature information. The motivation of MSAN is illustrated in Fig. 2. MSAN is, a general approach to learn fine-grained feature, implemented in an end-to-end trainable fashion.Download : Download high-res image (142KB)Download : Download full-size imageFig. 2. Illustrating the motivation of the proposed method. Most related works mainly focus on learning domain transferable features through down-weighing the data of outlier source classes to mitigate the divergence across source and target domains. Our goal is to learn domain transferable features by attention guided for deep partial domain adaptation methods, which will further boost the recognition performance. (Best viewed in color.) (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)
Overall, our work is summarized in the following:
•We propose a novel attention guided neural network for partial domain adaptation, learning fine-grained features in a gradual feature enhancement manner for boosting model generalization power.•By reporting classification accuracy and classification accuracy curve of varying the number of target classes after the attention, we show how our attention mechanism encourages the framework to learn more detailed domain-transferable representations.•Through extensive experiments on Office-Home and Office-31 datasets, which demonstrates that promising results of the proposed approach exceed the existing state-of-the-art results.
