Clustering is a classic task that is widely examined in various domains. Nonnegative matrix factorization (NMF) and spectral clustering (SC) are two widely used techniques for clustering. NMF aims to learn two nonnegative matrices with a product that can approximate the original data matrix [1]. According to [2], the two matrices represent cluster centroid attributes and cluster indicator information. The clustering result can be obtained directly from the cluster indicator matrix without conducting extra post-processing. SC is a classic tool that is applied extensively to nonconvex patterns and linearly inseparable clusters. Spectral-based methods optimize the process of learning an adjacency matrix and obtain final results by performing Eigen decomposition on the Laplacian matrix [3]. However, most methods are designed for single-view features in the data clustering task.
Increasing amounts of unlabeled data are received and collected daily from diverse sources in multiple views. Unsupervised multiview learning is employed to tackle unlabeled data, which are clustered from the perspective of multiview learning [4], [5]. Multiview clustering aims to segment points into clusters based on representations of an object from different views. We classify this method into three main categories. The first category incorporates multiview integration into the clustering process by optimizing its objective function, such as the method in Kumar et al. [6]. The second category projects multiview data to a shared subspace in low dimension and then conducts post-processing to obtain the final result, such as the approach in Chaudhuri et al. [7]. The last category is called late integration, and this type of algorithms performs clustering on each individual view and fuses the results into one on consensus [8].
In the past few years, many multiview algorithms based on SC and NMF have been designed to improve multiview clustering quality. For SC algorithms, Wang [9] proposed constrained SC with the first view considered as a similarity matrix and the other view as a constraint. However, this method is restricted to data with only two views. Cai [10] introduced a way to unify modals. This method can incorporate diverse data attributes by learning a common Laplacian matrix. Although Kumar raised two spectral-based algorithms [6], [11] to ensure the consistence of eigenvectors on all views, he pursued an implicit clustering consistency among different views [12]. For NMF-based algorithms, Tolic [13] designed a nonlinear orthogonal NMF approach for subspace clustering. Lu [14] attempted to establish a connection between linear discriminant analysis (LDA) and NMF in a supervised or semi-supervised manner, but this approach cannot be applied to clustering. Ma [15] established an unsupervised framework, but the technique is still problematic.
Numerous other multiview clustering algorithms have been proposed and demonstrated excellent performance on multiple benchmark datasets [16]. Wang [17] introduced MultiCC to discover multiple independent ways of organizing a dataset into clusters. Nie [18] suggested a new multiview clustering method that is completely self-weighted. Zhang [19] explored underlying complementary information from multiple views. Nie [20] introduced an auto-weighted way for fast matrix factorization, while Huang [21] tried it from the perspective of deep matrix decomposition. By conducting graph structure fusion on different views, Zhan [22] illustrated a global graph with exactly nc connected components that reflect cluster indicators, thereby making post-processing unnecessary. Zhu [23] suggested a one-step multiview clustering method to solve the previous two-step problem. Hu [24] designed a dynamic way to assign auto weights to different views. Moreover, Huang [25] proposed an auto-weighted multiview clustering method via kernelized graph learning. For an incomplete multiview clustering task, Liu [26] proposed a late fusion approach to simultaneously clustering and imputing the incomplete base clustering matrices via kernel learning. Furthermore, Huang [27] tried a multiview method from intact space to address the view insufficiency issue associated with multiview clustering.
To summarize, the advantages of existing multiview algorithms are as follows. First, such methods consider and balance relationships among views. Second, these methods address the connection among several traditional techniques, such as K-means and SC. Third, constraints on objective functions are well designed to speed up convergence. However, disadvantages remain. Intrinsic data information in the low-dimensional subspace is lacking. Moreover, information obtained from the optimization process cannot be used in most SC and manifold regularization methods. Finally, unavoidable outliers result in residue errors owing to the l2-norm.
The manifold regularization term was combined with clustering models to improve performance [28], [29], [30]. Cai [31] captured local manifold geometry and proposed a graph model. Meanwhile, Zhang [32] adopted adaptive manifold regularization for matrix factorization to learn a satisfactory affinity matrix. A joint framework for integrating sparsity NMF, representation, and adaptive weight was proposed by Zhang et al. [33]. Wang [34] learned a relatively low-dimensional discriminative mapping through a Grassmann manifold. Allab [35] proposed a multi-manifold matrix decomposition method for data co-clustering. Zhang [36] applied manifold regularization terms to the low-dimensional subspace and cluster indicators and learned considerable local geometrical information from raw data. These models show that the manifold regularization term improves the clustering performance and can be expanded to other frameworks.
However, the existing methods have several drawbacks. First, the multiview framework based on NMF factorizes only matrices in high-dimensional space and ignores intrinsic data information in the low-dimensional subspace. Thus, ordinary NMF requires a large number of constraints to capture complex structures. Second, local relationships among the data points obtained from the optimization process, such as pseudo-information, cannot be used in most SC and manifold regularization methods. The geometric structure of data distribution lacks an effective capturing technique. Third, squaring matrix factorization enlarges residue errors caused by the l2-norm framework. Thus, an improved norm should be chosen to avoid the effect of outliers.
In this study, an unsupervised multiview NMF-based framework called discriminative multiview subspace matrix factorization (DMSMF) is proposed for clustering. The structural block diagram of the proposed DMSMF method is shown in Fig. 1. Briefly, the original dataset can be viewed as several matrices according to the different views in Fig. 1. We consider each view as a concatenated matrix and several view-specific matrices. For the concatenated matrix, we extend the proposed unified framework to a multiview version that combines NMF and LDA. For the view-specific matrices, we design two regularization terms by manifold learning. These constraints instruct themselves to learn from their intrinsic structure and avoid outliers. Finally, by incorporating them together, we obtain our proposed DMSMF method. The primary work and contributions of this study are summarized below.1.A unified framework for clustering in the discriminant subspace that combines multiview LDA with multiview NMF is proposed to find and utilize the intrinsic low-dimensional structure in the projection subspace.2.A pseudo-supervised multiview manifold regularization term is proposed to discover the subspace that distinguishes different classes. This regularization term utilizes pseudo-information to instruct itself and refine clustering results.3.An augmented Lagrangian multiplier (ALM)-based optimization algorithm based on the DMSMF model is proposed to effectively seek an optimal solution.Download : Download high-res image (613KB)Download : Download full-size imageFig. 1. The structural block diagram of the proposed DMSMF method.
The remainder of the paper is described as follows. The derivation of our model is shown in detail in Section 2, and the optimization process and corresponding algorithm are presented in Section 3. Several theoretical connections between our framework and other classic methods are discussed in Section 4, and adequate experiments on multiple benchmark datasets and further research are presented in Section 5. Finally, Section 6 elaborates on the conclusions.
