Integral equations have recently found applications in machine learning techniques. For instance, reinforcement learning may be modeled in terms of a Fredholm integral equation [22]. They also have applications in modeling various problems in science and engineering. For example, they are used in prediction of multiple crack propagations in elastic media [26], ionospheric problems [36], the evolution of distributed damage in plasticity [9], heat transfer problems [23], scattering theory on planar curves [11] and option pricing [41], among others. These equations also occur as a reformulation of other mathematical problems such as ordinary and partial differential equations [35]. This reformulation may be carried out to overcome the instability issues of approximations in numerical differentiation. So the study of integral equations and developing new efficient numerical techniques for solving them are useful for applications and still require great attention. To cite a few recent works, Assari et al. [6] proposed numerical solution of integral equations arise in Laplace’s equation, Dehghan et al. [14] estimated error of finite element and finite difference technique for solving integro-partial differential equations, Parand et al. [38] solved some two-dimensional integral equations in non-rectangular domains, Assari et al. [5] presented meshless local discrete Galerkin scheme and Parand et al. [37] presented a radial basis function method for numerical solution of Hammerstein type integral equations.
Fredholm integral equations (FIEs) are an important category of integral equations, together with Volterra, Volterra–Fredholm, and integro-differential equations which appear naturally in signal processing problems and reinforcement learning. Since the analytical methods to find the exact solution is only available in special cases, various numerical techniques have been developed for the approximation of the exact solution of these equations in different forms, covering FIEs of Hammerstein type, a system of equations, high-dimensional and the nonlinear cases. Here, we refer the reader to some recent works on numerical methods for Fredholm integral equations. Assari et al. [4] presented local radial basis functions, Bahmanpour et al. [8] used Müntz wavelets, Kazemi et al. [21] introduced an iterative method based on quadrature formula, Fatahi et al. [16] developed spectral meshless radial point interpolation, Mirzaee et al. [32] improved Hat functions, Marzban et al. [29] presented composite collocation, Arsenault et al. [3] used projected regression, Parand et al. [39] compared Newton–Raphson method Newton–Krylov generalized minimal residual, Bellour et al. [10] presented cubic spline method, Babolian et al. [7] applied Haar wavelets in a weighted residual framework, Nemati et al. [33] used shifted Legendre polynomials as the basis of spectral method, [40] developed Galerkin scheme, Yousefi and Razzaghi [48] used Legendre wavelets, Abbasbandy et al. [1] proposed a homotopy perturbation method for the numerical solution of FIEs.
On the other hand, machine learning algorithms have become increasingly popular for a wide range of problems in recent years, for instance in pattern recognition, recommender systems, health care, theorem proving, cybersecurity and financial services. They have also been used as a tool for the approximation of the solution of ordinary differential equations (ODEs), partial differential equations (PDEs) and integral equations (IEs). To name a few, Mall et al. [28] presented Chebyshev neural networks for solving Lane–Emden type equations, Sirignano et al. [43] used Deep Learning to solve some PDEs numerically, Jafarian et al. [19] used neural networks to solve IEs and Golbabai et al. [17] used radial basis function networks to solve second kind integral equations.
Support vector machines (SVMs) are a well-known learning algorithm introduced by Cortes and Vapnik [13] for binary classification. Based on statistical learning theory, SVM adopts the structural risk minimization principle to dominate the drawbacks of artificial neural networks [44]. Though neural networks showed their ability to solve many machine learning problems, they have many hyperparameters that need to be found while in SVM the hyper-parameters are reduced to just a few parameters by considering a trial solution in terms of known basis functions. An important feature of SVMs is that the classification problem is reduced to a quadratic programming problem with inequality constraints which has a unique solution under certain conditions. Least-Squares Support Vector Machines (LS-SVM) introduced by Suykens et al. [45] can achieve a global optimum value by solving a system of linear equations which is a result of equality constraints instead of inequality constraints in SVMs. Solving a system of linear equations can be much faster than quadratic programming and as a result, LS-SVM is much more computationally efficient than SVM. Although LS-SVM is closely related to the Gaussian process and kernel ridge regression, it emphasizes on optimization theory properties of the model. LS-SVM classification has been used in some practical problems, see [25], [46] and the references therein. Its regression model has been successfully applied for instance, in forecasting electricity consumption [20], predicting thermal conductivity [2] and traffic flow forecasting [12]. The method has been also extended for solving ODEs [27], [30], PDEs [31], IEs [18]. In these works, the authors have used radial basis functions as the kernel. Implementing an orthogonal kernel and ideas behind the weighted residual method in spectral analysis, we propose a least-squares support vector network as a method for the numerical solution of integral equations including the nonlinear and multidimensional cases.
In this work, we consider the following integral equation in the unknown function u(x): (1)μu(x)=f(x)+λ∫Sk(x,s)σ(u(s))ds,x,s∈S⊂Rn,where x=x1,x2,…,xn, s=s1,s2,…,sn, S is a compact set, λ is a constant namely the eigenvalue of integral operator and k(x,s)∈L2(S) is a square-integrable kernel of the integral equation. Depending on the function σ(⋅), Eq. (1) maybe a linear or nonlinear integral equation. If μ=0, it is called an integral equation of the first kind, otherwise second kind.
For recent works concerning numerical methods for Eq. (1), see [4], [15], [24], [47]. For simplicity, we write Eq. (1) in operator form as (2)Lu=f,with L given by (3)L(u)=μu−λK(u),where the integral operator K is defined by K(u)=∫Sk(x,s)σ(u(s))ds.
The main goal of this paper is to construct an efficient learning-based method using least squares support vector machines for the numerical simulation of the integral equation (1) based on two different training approaches, the Galerkin and collocation least squares support vector regression (GLS-SVR and CLS-SVR). We use the shifted Legendre polynomials as the kernel of the network. The method gives rise to an optimization problem that is reduced to solving a system of algebraic equations wherein the case of GLS-SVR the system involves sparse matrices resulting from the orthogonality of kernel. So efficient numerical solvers can be used to get the unknown coefficients by solving the algebraic system.
The rest of the paper is organized as follows. In Section 2, we present some preliminaries and notations for polynomial spaces, numerical integration, and least squares support vector regression. Section 3. is devoted to the formulation of the LS-SVR method for FIEs with some linear algebra discussions. Some numerical experiments are provided in the final section. The paper ends with some concluding remarks.
