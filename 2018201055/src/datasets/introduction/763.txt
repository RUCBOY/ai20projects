Combinatorial optimization problems have attracted interest from the theory and algorithms design communities over the years, due to the practical desire to deal with complex real-world problems, such as routing, scheduling, assignment, are usually NP-hard. Researches vary widely from vehicle routing problem (Braekers et al., 2016, Vinyals et al., 2015, Bello et al., 2016, Khalil et al., 2017, Kool et al., 2018, Nazari et al., 2018, Bektas, 2006, Necula et al., 2015, Rostami et al., 2015, Shuai et al., 2019, Kaempfer and Wolf, 2018), maximal independent set (Li et al., 2018) to minimum vertex cover (Khalil et al., 2017). This paper focuses on a general and widely applied model: traveling salesman problem (TSP). Given n cities on a map, the TSP aims to find a shortest length-n cycle where each city appears exactly once. The TSP is an important model since many real applications, e.g. in planning, logistics, and manufacturing, can be formulated using the TSP. Unfortunately, finding the optimal TSP solution is computationally difficult – it is an NP-hard problem.
Various deep learning methods (Vinyals et al., 2015, Bello et al., 2016, Khalil et al., 2017, Nazari et al., 2018, Kool et al., 2018, Prates et al., 2019) for approximating the solution of Euclidean TSP have recently been proposed. In the Euclidean TSP, each vertex is associates with a point in Rd. The graph is fully connected and symmetric, and the Euclidean distance between the points is used as the edge weight. However, the Euclidean TSP is not sufficient to model many applications. In some applications, such as transportation or delivery scenario, the graph may be sparse, or the distance function may not satisfy the triangle inequality. The problems on such graphs could be modeled as the TSP on arbitrary symmetric graphs.
To solve the arbitrary symmetric TSP, learning-based methods have to embed the graph firstly and then decode a tour by some algorithms. Now the network architectures are able to capture graph information from the initial node features. The initial node features must be able to fully represent the graph. Intuitively, two methods can transform the graph to a set of node features. The first method is to find an intermediate formulation. There are many neural network architectures formulating the graph by this way, including (Vinyals et al., 2015, Kool et al., 2018), they transform the Euclidean graph into a set of node features (coordinates) in the plane, then use recurrent neural network (Hochreiter and Schmidhuber, 1997) and graph attention network (Veličković et al., 2017) to embed the graph given the node features (coordinates). Another method is to make the distances between one node to any other nodes as the feature of the node, thus the node features size is equal to the total nodes in the graphs. This method is rarely used, since such expression makes the neural network work on graphs with a certain size and have poor scalability. However, for sparsely connected graphs, all information live on the edges. It is difficult to design a set of intermediate node features that strongly express the sparse graphs and benefit neural networks to capture important graph features. In such scenarios, prior methods suffer from a trouble of capturing important information from sparse graphs.
At the same time, it is easy to get trapped in a dead end while constructing a TSP solution sequentially on the sparse graphs. For example, in Fig. 1, the optimal solution is {ABCDEA}, but once the first step is taken from A to C, then a completed tour cannot be produced. Moreover, the most important mission of TSP is to find an optimal or near-optimal solution rather than just find a feasible solution (Hamiltonian cycle).
Reinforcement learning (RL) realizes great achievements in many domains. The success depends heavily on being able to sample a large number of positive samples that provide enough information for the neural network to learn. RL also works well for Euclidean TSP (Bello et al., 2016, Khalil et al., 2017, Kool et al., 2018), because each of the n! permutations produce a feasible solution, and it is easy to sample a good solution. However, for sparsely connected graphs, sampling a feasible solution is NP-hard in general; the policy can easily get trapped into a dead end (shown in Fig. 1) while constructing a tour sequentially. The exploration trouble makes RL difficult to learn efficiently for TSP on sparse graphs.Download : Download high-res image (78KB)Download : Download full-size imageFig. 1. The left figure is the original sparsely connected graph with five nodes and six edges. Assume A is the depot, then construct a tour sequentially. The yellow lines in the middle figure shows a completed tour. However, if the partial solution is {A→C→B}, then D and E cannot be visited; similarly, for {A→C→D→E} node B will not be visited.
This paper constructs a bidirectional graph neural network (BGNN) for the TSP on arbitrary symmetric graphs . The BGNN is trained to sequentially predict the next city to visit using imitation learning. In the proposed model, we constructed a bidirectional message passing layer (BMPL), where messages pass from/to an edge to/from its two nodes, so that the dynamic graph could be efficiently encoded from edges and partial solutions. Training with imitation learning avoids the difficult exploration problem of RL, but requires near-optimal solutions for training. The requirement can be solved with the help of search, as done in AlphaGo Zero (Silver et al., 2017).
The experiments illustrate that the proposed BGNN captures graph information from edges and partial solutions effectively and is capable of helping produce near-optimal solutions. As the graph becomes sparser, the simple method that produces a single vertex to visit next fails to produce valid cycles sometimes. In such cases, using a simple beam search substantially increases the probability of producing valid cycles that are near optimal. Alternatively, the predictions of the BGNN can also be used as heuristics for informed search, to reduce the number of nodes searched while producing an approximately optimal solution.
The contributions of this paper are summarized as following:

•A bidirectional graph neural network (BGNN) is proposed for the TSP on arbitrary symmetric graphs, which is a more practical and important model, but ignored by prior works.•A novel graph embedding architecture (bidirectional message passing layer) is introduced that is able to embed dynamic graphs based on information living on edges and partial solutions.•Training BGNN by imitation learning enables BGNN to capture important decision-making features from the graphs and learn a strong heuristic to produce near-optimal solution.•Experiments comparing with meta-heuristic algorithms and learning-based approaches demonstrate the efficiency of the BGNN.
