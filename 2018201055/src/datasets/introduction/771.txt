During the novel coronavirus or COVID-19 global pandemic, the delivery of online services to every household has been a major concern for every service provider. Most of the top service providers (Amazon, Google, eBay, etc.) located across the globe rely on the Cloud computing paradigm for service provisioning [1]. However, in the current situation, the Cloud computing sector has been one of the most affected technologies as they are responsible for handling this situation of the enhanced workload with the huge dependence of the end-user domain. Cloud computing provides flexible and on-demand delivery of services and computation infrastructure (servers, storage, networking and software) to the end-users. Their services are hosted over geo-located cloud data centers (DCs) where ICT resources (servers, storage devices such as disks, communication networks), redundant or backup power supplies, environmental controls (e.g., air conditioning, fire suppression) and security devices are deployed to provide round the clock service through the world. Cloud ecosystem utilizes virtualization technologies to schedule different types of workloads (e.g., scientific workflows, multi-tier web applications, IoT workloads) on the minimal number of servers to ensure the better utilized of resources. But, different workloads may have different resource utilization footprints and may further differ in their temporal variations. Although, cloud computing tries to provide round the clock resource footprints to the end-users to handle their workload from past decades, however, it has to witness the problem of sustainability and scalability in such COVID-like scenarios. Moreover, the Internet of Things (IoT) revolution has already added to the cloud workload from the past decade [2], [3].
The global escalation for cloud resources results in two major challenges, (1) a drastic increase in energy consumption, and (2) degradation in the response time/latency for the desired online services for latency-sensitive applications and IoT systems. The first challenge concerns the growing demand for electricity and related carbon emissions that are caused by the expansion of massive DCs. Looking into the facts, in the year 2000, DCs consumed 70 billion kWh of energy, that further increased to 330 billion kWh by 2007 [4] and it was projected to touch 1000 billion kWh till 2020 [4]. But, the current pandemic scenario can take these projections to a further higher level. This drastic increase in energy consumption ultimately leads to the overall expenditure of the DCs and end up in harmful carbon emissions. Therefore, it is the biggest responsibility of global service providers to design and utilize energy-efficient approaches and solutions [5], [6], [7]. The second challenge relates to the advancements in IoT workloads and mission-critical applications that require lower latency and higher data rate. These stringent requirements if not fulfilled may end up in mission failure or unsatisfactory performances in IoT-based applications and systems (smart homes, smart grid, etc.).
Edge Computing [8] has come up as a promising paradigm that can complement the cloud to provide resources or process IoT workloads closer to the location of the data source. This provides an add-on layer to process the data on local servers rather than forwarding it to the remote cloud, thereby improving the quality of service (QoS). Like the majority of cloud providers often create a geo-distributed multi-cloud environment across different countries [9], similarly, there can be a local multi-edge environment connected via the software-defined network (SDN) for service provisioning in a limited landscape [10], [11]. This provides an alternate solution for the cloud providers to schedule their delay-intensive workloads locally and computationally heavy workload at the remote data centers. However, there may be one another challenges in the local software-defined edge computing ecosystem that relates to the mobility of the end-users (like vehicles or drones in a smart city). This challenge may need more percentage of service or data migrations happening across different edge nodes (or servers) located across a geographic layout (like smart cities). These increased migrations can lead to service breakage, increased energy consumption, and degraded response time. Moreover, it may take additional resources, energy and delay to reconfigure and re-establish the lost like to restart the services. To resolve these challenges, Container-based virtualization, a lightweight approach can minimize the additional energy consumption and delay due to dynamic migrations happening across software-defined edge computing ecosystem [12], [13], [14]. The major research questions that arise from the above discussion are listed below.
•How to execute an IoT or mission-critical workloads across multiple edge servers via SDN?•How to minimize the service breakage due to mobility and control the service link re-establishment consequences using container-based virtualization?•How to optimize the consumption and balance the load among multiple edge servers while avoiding SLA violations?
1.1. ContributionsTo answer the above-mentioned research questions, in this paper, we present a container-based load balancing approach for energy-efficiency in the software-defined edge computing environment. The key building blocks of the proposed approach are as follow:•A multi-layered system model for software-defined edge computing is proposed for handling diverse IoT workloads in an energy-efficient manner. To achieve this, a multi-objective driven task scheduling scheme based on energy, delay, and service level of agreement (SLA) is designed.•A multi-leader multi-follower Stakelberg game is formulated for energy-aware resource allocation is designed for scheduling IoT workload at the edge layer.•An energy-efficient ensemble for container allocation, consolidation, and migration is proposed for horizontal load balancing scheme using container-based virtualization is designed.
