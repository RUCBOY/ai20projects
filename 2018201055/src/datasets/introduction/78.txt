Different from semantic segmentation, instance segmentation and other tasks requiring dense labels, the purpose of salient object detection (SOD) is to segment the most visually distinctive objects in a given natural image [1], [67]. As an important problem in computer vision, SOD has attracted more and more researchers’ attention. It is usually used as a useful pre-processing step for various computer vision tasks, such as visual tracking [2], [3], [4], image captioning [5], person re-identification [6], image segmentation [7], [8], [9] and manipulation [10], [11].
The earliest SOD model can be traced back to 2007. Liu et at. [12] described the image saliency detection as a binary labeling task to separate the saliency object from the background. Subsequently, SOD models based on traditional methods were proposed in large numbers [12], [13], [14], [15], [16], [17], [18], [29], [43], [53]. With the renaissance of deep learning, in 2015, a SOD model using deep learning technology was first proposed. In the following years, there was no doubt that deep learning-based saliency detection solutions became dominant [19], [20], [21], [22], [23], [24], [25], [26], [55], [57], [59]. Compared with traditional methods, deep SOD algorithm has achieved surprising results and has consistently occupied the top of various benchmark rankings. However, the predicted saliency map still has defects in terms of fine structure and clear boundaries (see Fig. 1).Download : Download high-res image (354KB)Download : Download full-size imageFig. 1. Examples of predictive saliency maps with rough boundaries. (a) Input image, (b) Ground-truth (GT), (c) Results using deep CNNs.
To obtain an accurate saliency map, two major challenges must be addressed. First, saliency is mainly defined by the global contrast of the overall image, rather than local or pixel-level features. Therefore, the algorithm must understand the global meaning of the whole image and implement it by aggregating multiple layers of features. Second, most SOD methods use cross-entropy as the loss function. However, cross-entropy loss only provides a general guidance for generating saliency maps. Models trained with cross-entropy loss usually do not have enough confidence in the discrimination of boundary pixels, resulting in boundary-blurring. Therefore, a simpler strategy is needed to emphasize the generation of saliency object boundary details.
To address above challenges, we proposed a new Multi-style Attention Fusion network, named MAFNet, which achieves remarkable saliency map with high-quality boundaries. Specifically, due to the rich spatial structure information of the low-level features, we designed a dual-cues spatial attention module (DSA) to refine the saliency object boundary and focus attention on the foreground area. Intuitively, the middle-level features have more semantic information than the low-level features and richer spatial information than the high-level features. Therefore, we designed a dual attention intermediate representation module (DAIR), which can obtain the importance of each spatial position and feature channel through learning in the middle-level, and can be used as a supplement to the low and high level. For high-level features with high abstract semantic information, we used two different channel attention modules (HCA) to capture specific semantics. We focus on reflecting the ability of middle-level representation learning and the fusion of learnable features at different levels, not just the simple addition or superposition of lower-level and higher-level features. In addition, soft IoU loss proposed in [34] is adopted as an assistant to cross-entropy to guide the network to generate accurate saliency objects.
In summary, the contributions of this paper are as follows:
•We propose a novel network named MAFNet for image saliency detection. For low-level features, DSA is used to filter out some background information. For middle-level features, DAIR is adopted to suppress redundant information and enhance the complement of low and high level features. For high-level features, different HCA are used to enhance feature learning in semantic regions, compressing noise and interference.•Since the features of different levels in the network are not the same at the representation levels, we can not simply add or concatenate low, middle and high level features. In this paper, we propose a learnable feature fusion module (MLFF) to integrate these features effectively.•We evaluate the proposed MAFNet on several widely used datasets compared with 18 state-of-the-art methods. Experimental results show that the MAFNet consistently has competitive over others on all the benchmarks. The comprehensive evaluation of the method proved its effectiveness and superiority.
