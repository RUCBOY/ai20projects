With the great development of modern computer and computational algorithms, computational science and engineering have achieved enormous success in almost all fields, such as physics, chemistry, biology, mechanical, civil, and materials science and engineering. However, many problems in computational science across the disciplines are still challenging. We propose that there are three major classes, or types, of problems puzzling the community of computational science and engineering. These three types are:

1.Type 1 or purely data-driven problems: The class of analyses with unknown or still developing governing physics but abundant data. For these problems, the lack of knowledge of physics can be compensated by the presence of considerable data from carefully designed experiments regarding the system response.2.Type 2 or mechanistically insufficient problems with limited data: The term mechanistic refers to the theories which explain a phenomenon in purely physical or deterministic terms [1]. Type 2 problems are characterized by physical equations that require complementary data to provide a complete solution.3.Type 3 or computationally expensive problems: The problems for which the governing equations are known but too computationally burdensome to solve.
We will attempt to show that artificial intelligence (AI), particularly a subset of AI, deep learning, is a promising way to solve these challenging problems. An AI system is identified by its capability to perform tasks which currently humans perform in a better way [2]. This is famously judged by the Turing test, proposed to measure the intelligence of a machine by its capability to imitate human behavior [3]. An AI system can be classified into three classes, (a) “weak” or narrow AI, (b) general AI, and (c) super AI [4]. A narrow or “weak” AI is designed to perform a specific task and outperform any human in doing that. General AI refers to an AI system that may exhibit intelligent behavior in different areas and might outperform humans [5]. Super AI is a conceptual version of the technology that is the supreme point where machine achieves superhuman intelligence and can perform abstract thinking [6]. Almost all of the AI systems we see around us fall in the category of narrow AI. Super and general AI are still futuristic ideas. Machine learning (ML) is a form of narrow AI [7] and defined as the process by which computers, when given data, create their own knowledge (hence the term learning) by identifying patterns in data [8], [9]. Deep neural network is a subset of machine learning tools by which computers “understand” challenging and complex concepts by building the deep hierarchy of simpler concepts [9]. A generic deep neural network consists of input layer, hidden layers, and output layer where the input (layer) is connected (nonlinear information processing) through an activation function (hidden layer) to the output (layer) [8].
There is a growing tendency across the scientific communities to engage narrow AI (machine learning or deep learning) to solve problems in disciplines such as mechanics [10], [11], [12], biology and bio-medicine [13], [14], [15], materials science and engineering [16], [17], [18], manufacturing process monitoring [19], [20], [21], topology optimization [22], [23], [24], design under uncertainty [25], and miscellaneous engineering disciplines [26], [27], [28]. The scope of machine learning tools to aid or solve computational science problems goes beyond merely regressing non-linear data. Deep neural network and transfer learning are now being applied to discover hidden governing physical laws from data [29], [30], [31], speed up the computation in multiscale and multiphysics problems [32], [33], [34], [35], [36], characterize and reconstruct complex microstructures [37], design of heterogeneous materials and metamaterials [38], [39], discover new materials [40], [41], and to model path- and history-dependent problems [42], [43], [44], [45]. Fig. 1 shows AI tools currently in use to solve state-of-art computational science problems. The AI tools include data generation and collection techniques, feature extraction techniques (wavelet and Fourier transform [46], principal component analysis [46]), dimension reduction techniques (clustering, self-organizing map [21], [46]), regression (neural network, random forest) [46], reduced order models (can be something similar to regression techniques or more advanced technique like self-consistent clustering analysis (SCA) [47], [48] or Proper Orthogonal Decomposition (POD) [46]) and classification (convolutional neural networks or CNN [46]).
There are several practical challenges in directly applying current AI frameworks to solve aforementioned types of problems: (1) it is often difficult to decide on the criteria to identify the type of the problem and on the set of tools to use; (2) for a computational materials scientist or practicing engineer, it might become a challenging task to go back and forth among the different machine learning tools; (3) a design engineer needs to have a closed form relationship among different parameters controlling the desired property of the system. Moreover, the bridge connecting seemingly disparate fields of data-science and computational methods has to be a general one so that a common framework can be used to solve problems of different nature and originating from different physics. One other problem for applying AI frameworks in science and engineering is the paucity of data. Often experiments are too expensive to be useful to generate a large amount of data. Computational and theoretical predictions are limited by inherent assumptions. Considering these current difficulties and constraints, we propose a unified deep learning framework named Hierarchical Deep Learning Neural Network (HiDeNN). An advantage of using the HiDeNN structure is that such a neural network has a universal approximation capability enabling it to correctly interpolate among the data points generated by extremely non-linear relationships. HiDeNN can identify the governing physics from an experimental dataset without any prior knowledge and therefore can fill the missing link between data and mechanistic knowledge. As will be explained, HiDeNN also has the capability to incorporate mechanistic knowledge in training through proper definition of the loss function. This will reduce the necessity of a large amount of data to get an accurate prediction. A practical example is provided in a companion paper by Tajdari et al. [49], submitted to the same issue, to demonstrate in detail how a small amount of medical data available for adolescent idiopathic scoliosis can be used with mechanistic knowledge and deep learning to predict spine curvature. All the machine learning tools and computational methods mentioned earlier can be built into HiDeNN, eliminating the need for the user to decide on specific tools.Download : Download high-res image (480KB)Download : Download full-size imageFig. 1. A comparative picture of state-of-the-art AI tools in computational science and engineering and the proposed Hierarchical Deep Neural Network (HiDeNN) framework. HiDeNN offers the advantage of being a unified framework to solve the problem in computational science and engineering without resorting to different set of tools for different types of problem.
This article is organized as follows: Section 2 introduces and describes the components of HiDeNN, Section 3 presents the application of HiDeNN framework by solving three illustrative problems, Section 4 discusses three examples from each type of challenging problems, how those are solved using state-of-the-art methods, and recast the solution of the problems using HiDeNN, Section 5 proposes possible extensions of HiDeNN for general problems.
Download : Download high-res image (604KB)Download : Download full-size imageFig. 2. Detail construction of the proposed HiDeNN framework. The input layer takes in space, time, and parameter variables of a system. The input layer is connected to the pre-processing function, Hierarchical DNNs, and finally the solution layer. Governing equations can be obtained from solution layers through the operation layer and the loss function.
