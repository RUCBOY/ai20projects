With the evolution of big data analysis, tensors have been applied to various fields such as image processing [1], computer vision [2], and recommendation system [3]. Therefore, tensor-based analytic techniques have gained popularity in HPC community for estimating relationships among huge quantities of multi-dimensional data. Within these techniques, tensor decomposition aims to factorize high-order tensors into the sum of multiple rank-one tensors in order to understand the relationships embedded in the data better. Canonical Polyadic Decomposition (CPD) is one of the most widely-applied schemes of tensor decomposition.
Although there are a large number of research works on how to apply CPD on dense tensors [4], [5], they cannot be directly applied to extremely large and sparse tensors. On the one hand, when using the conventional coordinate format (COO) [6], the enormous dataset and corresponding factor matrices take up too large space to be stored in the cache, which increases cache miss and degrades performance. On the other hand, due to different sparse patterns across different datasets, the conventional CPD algorithm for dense tensors faces the challenge of discontinuous memory accesses that causes potential write conflicts, thus can severely deteriorate the algorithm parallelism. Therefore, advanced researches focus on efficient parallel CPD implementations for sparse and large tensors. For instance, Kang et al. utilized the MapReduce framework [7] to implement Gigatensor [8] on multicore CPUs, which also adopted Hadamard products for reducing memory footprint. In addition, Li et al. proposed ParTI! [9] on both multicore CPUs and manycore GPU to support a variety of tensor operations including CPD. However, the cutting-edge CPD libraries are only implemented on CPUs and GPUs.
Meanwhile, the Sunway many-core architecture has become an attractive architecture improve the performance of CPD further. Ranking the first in the TOP500 list in 2015 [10], Sunway TaihuLight supercomputer system reaches peak performance of 125PFLOPS [11]. The Sunway TaihuLight supercomputer has already been applied in a number of research fields, including numerical algorithms [12], climatic simulation [13] and machine learing [14]. This China homegrown supercomputer system contains 40,960 pieces of Sunway SW26010 processors. Each of the Sunway processors consists of 4 core groups (CGs). There are 64 Computation Processing Elements (CPEs) and one Management Processing Element (MPE) within a CG. Each CPE equips with a 64 KB manually-controlled Local Device Memory (LDM). Moreover, DMA and register communication are enabled on CPEs to improve memory access and communication efficiency.
To implement CPD algorithm efficiently on Sunway, it is of great importance to re-design CPD algorithm to adapt to the unique architectural features of Sunway. In this paper, we propose swCPD, an efficient CPD implementation on Sunway to improve the performance of tensor decomposition. The swCPD contains five optimization algorithms for determining the factor matrices during CPD, including Alternating Least Squares (ALS), Gradient Descent (GD), and Randomized Block Sampling (RBS), as well as the latest fast Levenberg–Marquardt (fLM++) and Generalized Canonical Polyadic Decomposition with Stochastic Gradient Descent (GCP-SGD). These optimization algorithms dominate the performance of Matricized Tensor Times Khatri-Rao Product (MTTKRP), whose performance suffers from discontiguous memory access pattern and synchronization [15]. For the exact MTTKRP in CPD-ALS, CPD-GD and CPD-fLM++, to optimize the memory access, we leverage the blocking mechanism and the CSF tensor format [16] to reduce irregular memory accesses. Specifically, we divide the sparse tensor into eight blocks and CPEs into eight groups, each group computes MTTKRP on one block. We further partition each block into several bands, and each band contains non-empty rows of the tensor. Besides, the bands are divided into tiles, and each tile consists of several fibers. To optimize the synchronization, we divide the eight CPEs within a group into seven workers and one controller. The worker is used for computing MTTKRP, and the controller is used for assigning tensor bands in the blocks to workers and storing the updated parts of factor matrices back to main memory. The synchronization between worker and controller is enforced through register communication with carefully-designed message scheme. Whereas, for the sampled MTTKRP in CPD-RBS and CPD-GCP-SGD, we propose a randomization strategy on top of the exact MTTKRP through shuffling among CPEs with register communication to derive the fully randomized sample.
We implement swCPD on the Sunway processor and compare each the performance of each algorithm in swCPD with corresponding algorithms adopted in cutting-edge CPD implementations. The experimental results show that our approach can utilize Sunway’s architecture features efficiently and thus achieve significant speedup. Specifically, this paper makes the following contributions:
•We leverage the blocking mechanism that partitions the tensors and CPEs into eight blocks and eight groups, respectively. The blocks are further divided into bands, which contain tiles when computing mode-n MTTKRP.•We present a multi-role computation scheme for CPD that divides CPEs within a group as two roles, which are named as worker and controller, respectively. This scheme enables efficient synchronization among CPEs within a group through register communication.•We implement swCPD that includes CPD-ALS, CPD-GD, CPD-RBS, CPD-fLM++ and CPD-GCP-SGD on Sunway processor, and evaluate its performance by comparing with cutting-edge CPD implementations using both synthesized and real-world datasets.
This paper is organized as follows. We introduce the background of CPD and Sunway architecture in Section 2. Section 3 describes our methodologies of swCPD, an efficient implementation of CPD including CPD-ALS, CPD-GD, CPD-RBS, CPD-fLM++ and CPD-GCP-SGD on Sunway processor. Section 4 presents the implementation details of swCPD. Section 5 describes the performance auto-tuning approach adopted in swCPD to determine the optimal parameters. In Section 6, we present the experimental results through comparison with cutting-edge CPD implementations. Section 7 presents the related works and we conclude our work in Section 8.
