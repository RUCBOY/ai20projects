Echo State Networks (ESNs) (Jaeger, 2001, Jaeger and Haas, 2004) are efficient training-free Recurrent Neural Networks (RNNs). They usually consist of three parts: an input layer, a large recurrent hidden layer (reservoir), and a linear output layer. Unlike traditional RNNs, the weights of the input layer and the hidden layer of ESNs are initialized randomly and fixed during the training stage. Only the weights of the linear output layer need to be solved by the simple linear regression. Therefore, ESNs provide a simple and efficient learning mechanism while effectively modeling the temporal data.
Jaeger and Haas (2004) first invent ESNs to predict time series in wireless communication. After that, ESNs are widely applied to time series modeling tasks such as time series prediction (Chatzis and Demiris, 2011, Li et al., 2012, Shi and Han, 2007, Xu and Han, 2016) and time series classification (Ma et al., 2016, Ma et al., 2019, Wang, Wang et al., 2016). The simple linear readout, as original ESNs used, is unable to learn complex high-dimensional representations (e.g., generated by action sequences) provided by the reservoir. These approaches try to improve the training method of ESNs by employing other techniques instead of simple linear regression. For example, Shi and Han (2007) performed linear support vector regression in the high-dimension “reservoir” state space. The Echo State Gaussian Process (ESGP) (Chatzis & Demiris, 2011) offered a measure of confidence on the generated predictions in the form of predicting a distribution. In Li et al. (2012), a robust ESN (RESN) model that inherits the basic idea of ESN learning in a Bayesian framework was proposed. For the time series classification, Wang, Wang et al. (2016) propose the Conceptor-ADE (CADE) that projects the training samples into different state clouds by a conceptor and induces classifiers from these state clouds for different classes. Then they use adaptive differential evolution (ADE) for parameter optimization and obtain the final classifiers. The Functional Echo State Network (FESN) (Ma et al., 2016) replaced the numeric variable readout weights of ESNs with time-varying functions. In addition to these methods, some regularization methods are employed to improve the generalization ability of ESNs, such as dimensionality reduction (Boccato et al., 2012, Han and Xu, 2018, Lokse et al., 2017) and pruning (Dutoit et al., 2009).
However, the conventional ESN-based models are unable to capture the history information far from the current time step, since the echo state at the current time step of ESNs mostly impacted by the previous one. Thus, ESN may have difficulty in capturing the long-term dependencies that exist in temporal data. But history input that has a significant influence on the current echo state may appear at the time step far from the current state. For example, humans are often reminded of memories which are associated with the current happening, and the reminding can play a vital role in ongoing cognition by providing information essential to the current decision (Ke et al., 2018).
To improve the time series modeling capability of ESNs, Jaeger, Lukoševičius, Popovici, and Siewert (2007) proposed a modified version of ESN called Leaky-ESN. Leaky-ESN integrates a leaky integration unit to the state update equation of the standard ESNs, which can accommodate the model to the temporal characteristics of the temporal data. Based on the Leaky-ESN, there are some methods (Lun et al., 2016, Lun et al., 2015, Pahnehkolaei et al., 2017) of enhancing the time series modeling ability of ESNs. Among them, the double activation functions echo state network (ADF-ESN) (Lun et al., 2015) introduces double activation functions to replace the original single activation function in the reservoir state update equation, which can flexibly modify the reservoir state according to different input signals. In Lun et al. (2016), the authors proposed a variable memory length echo state network (VML-ESN). The state updating pattern of VML-ESN is adjusted according to the autocorrelation characteristics of the input signal adaptively, which allows ESNs to acquire longer-term memory. However, these methods only fuse the previous information from the fixed-length temporal window when the echo state is generated at each time step. If the window size is too small, it cannot capture the important history information far from the current time step. When the window size is too big, it may bring some irrelevant information. Moreover, the weights to fuse the information of each previous time step are fixed, reducing the modeling flexibility.
In recent years, deep learning models have been applied to a wide variety of tasks and have achieved great success. Among them, the attention mechanism has become an important concept in the field of deep learning and been successfully applied to computer vision (Xu et al., 2015) and natural language processing (Bahdanau et al., 2015, Luong et al., 2015). The essential idea is to pay attention to the most relevant information corresponding to the input data when extracting its features. For example, Ke et al. (2018) proposed an attention model named sparse attentive backtracking (SAB), which can focus on relevant past states according to the current state at every time step. In addition, Zhao et al. (2020) proposed the long memory filter that can be viewed as a soft attention mechanism, and proved that long-term memory can be acquired by using long memory filter.
Inspired by this work, we propose an end-to-end model named Echo Memory-Augmented Network (EMAN) for time series classification. An EMAN consists of an echo memory- augmented encoder and a multi-scale convolutional learner. In the echo memory-augmented encoder, the time series is first projected into a high-dimensional nonlinear space through the reservoir to produce echo states step by step. We then collect the echo states of all the time steps into an echo memory matrix by time order. After that, we design an echo memory-augmented mechanism that applies the sparse learnable attention to the echo memory matrix to capture important history information of the time series. In this way, the state of each time step in the echo memory matrix is re-encoded as the Echo Memory-Augmented Representations (EMARs), enhancing the temporal memory of ESNs. Since the sparse attention mechanism only focuses on the most important previous echo states, it not only improves the computational efficiency but also reduces the impact of irrelevant information compared to global attention. Further, based on the EMARs, we use a multi-scale convolutional layer to learn multi-scale features from the EMARs, and the discriminative local features are selected by a max-over-time pooling layer. Finally, a fully connected layer and a softmax layer are used to fuse the learned features for time series classification. Experiments conducted on extensive time series datasets show that EMAN is state-of-the-art compared to existing time series classification methods. In addition, the visualization analysis demonstrates that the proposed memory-augmented mechanism can enhance the time series modeling capability of ESNs.
The main contributions of this paper can be summarized as follows:

•We design an echo memory-augmented mechanism, which can capture important history information and incorporate it into the feature representation of the current time step, thereby enriching the temporal characteristics of the current echo state and the time series modeling capability of ESNs.•Based on the echo memory-augmented mechanism, we propose an end-to-end architecture that combines the efficiency of ESNs in dealing with time series and powerful feature learning capability of the convolutional neural network.•Experiments conducted on extensive time series datasets demonstrate the effectiveness of EMAN. In addition, visualization analysis verifies that the memory-augmented mechanism can enhance the time series modeling capability of ESNs.
The remainder of this paper is organized as follows. In Section 2, we give a brief overview of related work. Then we explain our model in Section 3. The experimental results and analysis on extensive time series datasets to demonstrate the effectiveness of the proposed model are given in Section 4. Finally, we conclude this paper in Section 5.
