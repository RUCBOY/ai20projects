Although the research of occlusions handling has made significant progress in the past few years, it is still a challenging issue in the field of computer vision and pattern recognition. SAI is one of the most efficient methods of handling the problem of occlusions. It uses a camera array to mimic a camera with a large virtual convex lens by projecting images of a scene from different views onto a virtual focal plane. In this paper, we denote the image Fi (i = 1,2,...,N) captured by N cameras in the camera array, and the warped image Wi by projecting the image Fi onto a reference view with focus at depth l, a synthetic aperture image S is obtained by averaging all the warped images from different camera views according to Eq. ((1)),(1)Srl(m)=∑i=1NWi,rl(m),where Srl(m) denotes the mth pixel’s value of the synthetic aperture image with reference view r, r∈[1,…,N], at depth l, and(2)Wi,rl=P·Hi,r·Fi,where Wi,rl(m) denotes the value of pixel m in the warped image Wi at depth l. H denotes the homography matrix which is generated from camera array calibration, and Hi,r represents the homography matrix of Fi warp to reference view Fr. P represents the warp matrix including parallax, according to the method in [1], the warp matrix P at depth l can be obtained using Eq. (3),(3)P=[EΔpϕ1],where the parallax matrix Δp=Δx·c, the displacement matrix Δx denotes the relative positions between cameras, and c denotes the ratio of the relative depth between object plane and reference plane. E is a 2 × 2 identity matrix, and ϕ a two-dimensional zero vector. Both Δx and c are computed based on the calibration results of the camera array. In our experiments, the cameras calibration results are obtained by capturing a calibration pattern at different distances. Then different groups of parallax are calculated.
While SAI provides a unique capability of ‘seeing through’ occlusions [1], [2], it is limited to creating a sharp focus region but a blurry out-of-focus region. Recently, Yang et al. [3] and Pei et al. [4] present all-in-focus SAI methods by which the results have sharp focus region and sharp reconstructed out-of-focus region. If an object can be captured by all the camera views, the scene behind the occlusions can be well synthesized. However, if the occlusions are close to the object, or if the occlusions are wider than the baseline of the camera array, then both of these methods cannot handle occlusions well because parts of the occluded object are not captured by any camera view (see Fig. 1). As a result, the missing information caused by heavy occlusions cannot be reconstructed.Download : Download high-res image (958KB)Download : Download full-size imageFig. 1. Example with large occlusion. (a) Reference camera view in the camera array. (b)-(c) Two camera views in the camera array. (d) Failed result of SAI(the center of the person).
To address the above problem, a novel method based on semantic inpainting using a GAN is proposed in this paper. The latest results of GAN indicate that they are promising for image semantic inpainting. For example, Pathak et al. [5] propose an unsupervised visual feature learning algorithm, achieving encouraging results in image completion driven by context-based pixel prediction. Liu et al. [6] introduce the use of a partial convolutional layer with an automatic mask updating mechanism and achieve state-of-the-art inpainting results. Li et al. [7] describe a Deep Generative Network for face completion, which can successfully synthesize visually credible missing key regions of the face. Indeed, our work is motivated by generating a realistic all-in-focus image that not only has a sharp focus region and sharp reconstructed out-of-focus region but also a clear occluded object.
In this paper, we first utilize a labeling method to generate a synthetic aperture image and alpha mattes of the occluded objects. Then, the background is reconstructed via energy minimization to replace the blurry out-of-focus region with a sharp image. Finally, the synthesized image by compositing the partially occluded objects with the background is processed using a GAN, which performs locally and globally image semantic inpainting to complete the missing regions and to significantly improve the visual quality of the result. Hence, our method can generate more realistic all-in-focus synthetic aperture image which have heavily occluded objects than other state-of-the-art SAI methods.
To summarize, our contribution is a novel approach for the first time to address the missing information problem caused by heavy occlusion in SAI. Moreover, our method generates a realistic all-in-focus synthetic aperture image where the information of the occluded region is completely restored. In addition, our method has a better performance than existing methods because it reduces the occluded pixel problem to a labeling problem, which preserves the original pixel information. Finally, extensive experiments on public datasets and our own datasets demonstrate the superior performance of the proposed method over state-of-the-art SAI methods.
