Recent researches about cancer prevalence and incidence of mortality show that lung tumours represent so far the leading cause of cancer death. According to the data of GLOBOCAN that have been collected in 185 countries of the world, there were an estimated 18.1 million cancer cases and 9.6 million cancer deaths in 2018 and the 18.4% of these have been caused by lung tumours [3]. With an early diagnosis, a faster treatment planning and a cleaner air in our cities, most of these deaths could be avoided. Nowadays, the most effective analysis that allows spotting pulmonary nodules is Computed Tomography (CT). A 3D CT scan may contain up to 1000 slices and, therefore, identifying the nodules manually is often time-consuming and tedious. However, a precise nodule segmentation is compulsory for the assessment of the characteristics that determine the malignancy of the tumour, such as shape, volume and change rate. In addition, a fast segmentation will deliver a strong speed up to the procedures between diagnosis and radiotherapy, opening a scenario where the patients can go from diagnosis to irradiation on the same day.
Starting from these premises, we can infer that seeking a fast, fully automatized and reliable nodule segmentation algorithm is of utmost importance. Because of its ability to learn complex patterns, Neural Networks (NNs) such as Convolutional Neural Networks (CNNs) appear to be highly promising instruments to achieve these goals. In the past years, many researchers handled this problem by using a CNN with several methods and gained different levels of accuracy, but there is still room for improvement. The modern approaches are based on networks such as AlexNet [12], VGG [20], GoogleNet [21] that have demonstrated remarkable success not only in many different computer vision fields but also in medical image analysis tasks. The models most frequently used are inspired by UNet [18] and VNet [16].
1.1. Aim of this workThe final purpose of this research is to provide the medical personnel with a reliable tool for automatic segmentation that can be applied to all the possible cases and manage the task without the supervision of the final user. Our eventual goal is to build a full routine that can be used by specialists and clinicians for lung cancer detection and characterization. In this project, segmentation is a vitally important tool for the rapid extraction of all the physical properties of the nodule that can be used to estimate the malignancy of the tumour. An accurate segmentation can prevent a healthy patient from undergoing an useless biopsy and can detect nodule growth, even in the cases where this is difficult to be detected by eye. Non-invasive therapies such as radiotherapy and hadrontherapy need a millimetric accurate three-dimensional segmentation (an example is shown in Fig. 1) in order to be implemented. With the current methods, this process can take up to several hours [26]. Instead, with the help of Convolutional Neural Networks, this could be done just in seconds.Download : Download high-res image (189KB)Download : Download full-size imageFig. 1. Proton therapy estimated dose deposition for lung tumour treatment. The red contour area is the segmentation of the tumour currently made by radiologists (image from the Samsung Medical Center). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)This paper is organized as follows. In Chapter 1.2, we start with showing the actual state of the art concerning the most relevant methods of image segmentation in the case of lung tumour, categorized into automatic and semiautomatic nodule segmentation. In Chapter 2, we present the new method of nodule segmentation developed in this work, showing the model architecture in Chapter 2.1. In Chapters 2.2 and 2.3, we describe the loss function and the fit to the segmentation mask used, respectively. In Chapters 2.4 and 2.6, we show the dataset and the experimental methods used. Then, we present and discuss the results in Chapters 3 and 4, respectively. In particular, we show the overall performance of the method in the bi-dimensional case in Chapter 4.1, the evaluation of the database consistency in Chapter 4.3 and the comparison of the method with the state of the art in Chapter 4.4. Conclusions and outlook are shown in Chapters 5 and 6, respectively.
1.2. Relevant worksResearch in the field of lung nodule segmentation started long before the introduction of artificial intelligence and deep learning. During previous generation, the existing methods where several, and they can be divided into two main classes: region-based segmentation and edge detection. The first ones are achieved studying the homogeneity and similarity between pixels, instead edge segmentation is obtained enhancing the edges with the use of differential operations or kernel convolutions. These methods are still used in current medical softwares and in research, in order to reinforce the results of deep learning algorithms. A deeper description of these methods is presented in Zheng and Lei [28].With the introduction of deep learning, and in particular the structure encoder-decoder, the research flow in segmentation divided into three main branches: the methods based on Generative Adversarial Networks (GANs), resumed accurately in Kazeminia et al. [10]; the ones using reinforcement learning, of which Tian et al. [25] gives a modern overview; and the convolutional neural networks using residual connections, whose milestone is UNet [18]. In this article, we will briefly present some of the best current methods, mostly belonging to this last branch.Nowadays, UNet is the benchmark for basic image segmentation. In most of the cases, it has shown both good properties of reliability and coherence. UNet is a CNN composed by an encoder and a decoder with four levels of depth. The strength of the UNet model lies on the connection between the respective layers of encoder and decoder. In other words, the outcomes of every upsampling layer on the decryption side and every convolutional layer on the encryption side are concatenated and processed together. This allows to reduce the loss of resolution that one can encounter in a deep learning architecture due to the repeated convolution and the pooling layers, responsible for the generalization of the output.1.2.1. Automatic nodule segmentationIn recent years, the amount of literature on automatic nodule segmentation in CT had an appreciable increase. Wu et al. [30] presented one of the simplest structures that we can find for automatic nodule segmentation. They proposed a segmentation algorithm and a malignancy predictor utilizing an UNet structure with just half of the convolutional layers normally used. It has multiple window widths and window centers enriching the nodule information. They show an improvement over the standards of UNet of more than 2% in Dice Score Coefficient (DSC).Aresta et al. [1] developed a model named iW-Net consisting in a concatenation of two UNet networks. It can be utilized with and without user interaction. In the first case, the user draws the nodules diameter and the respective end-point extraction in order to generate a weight map, which is then used for altering the prediction of the network. The algorithm has been designed by taking into account the expected spherical shape of the nodules. The weight map is then incorporated as a feature of the model and as a component of the loss function.A lightly modified version of UNet is the one presented by Keetha et al. [11]. Their method consists in a UNet structure where the residuals are filtered by more hidden layers situated on the connections between encoder and decoder. These hidden layers are further connected with one another. They provide only results regarding Dice score and sensitivity with a good average. They apply the Mish activation function, demonstrating its value with an ablation study.Zhou et al. [29] use another modified version of UNet, similar to the one proposed in the previously discussed article. Here the authors propose many arrangements of the hidden layers, using different shapes of the residual connections and levels of depth. They use a loss function that is a mixture between cross-entropy and soft dice coefficient and do an extensive analysis on six different datasets, including LIDC-IDRI. Due to our interest towards this work, we replicated it and compared it with ours in Section 4.4.Tang et al. [24] proposed an end-to-end 3D Deep-CNN called NoduleNet for solving pulmonary nodule detection, false positive reduction and segmentation jointly. They employed an UNet-like model that firstly calculates the bounding box of the nodule and then runs a segmentation refinement only on the Volume of Interest (VoI) surrounding the bounding box, progressively up-sampling the cropped volumes and concatenating them with low-level semantic features. This method attempts to solve the loss of resolution inside the VoI due to the repeated convolutions and the pooling layers of the image.Hancock and Magnan [6] proposed a method based on the vanilla level set image segmentation method but, instead of designing the velocity function manually, they use machine learning regression methods in order to learn these parameters. The Central focused Convolutional Neural Network proposed by Wang et al. [27] is a network formed by two different parallel branches receiving the input image in 2D and 2.5D,1 respectively. After the elaboration, they concatenate the resulting images for the last layer, where the features are mixed and the nodule mask is provided as a probability map. Their idea of using a central pooling layer is appreciable, because it pools the image without reducing the resolution around the VoI, thus mitigating one of the most relevant problems of deep learning with CNNs.In a similar work, Cao et al. [4] obtained almost the same results as Wang et al.. The main differences in Cao et al. are the use of residuals, which assume more importance, the use of two slightly different pooling layers and the ResNet convolutional blocks that make this network a deep learning network for all the intents. Another added-value consists in a strong post-processing that makes the algorithm able to gain another 0.2% in dice score accuracy.Huang et al. [8] provide a fully automatic routine for lung nodule detection and segmentation. Their architecture for segmentation is a composition of a Faster Regional-CNN (for encoding) and a Fully Connected CNN (for decoding) with a VGG16 [20] backbone, that generally proved to be a reliable method for reconstruction. Especially, if used in fine tuning, as done in this work. The loss function is a composition between cross-entropy and L1 distance. The input image size is 64×64. Similarly, Qian et al.[17] use the VGG16 structure for encoding but introducing a new kind of decoder, called pyramid deconvolutional neural network. This takes the outputs of each level of depth of the encoder and composes them, with the use of deconvolution, in order to obtain the nodule mask. In this way, they reduce the resolution loss coming from a series of stacked convolutions. The stride used for deconvolution has an impact on the balance between sensitivity and precision, allowing the user to tune it as required. They work exclusively on lung nodule segmentation.Finally, we found remarkable the work done by Jiang et al. [9], who developed Multiple Resolution Residual Network as an extension of the ResNet [7] likewise based on a model similar to UNet.1.2.2. Semi-automatic nodule segmentationPart of the literature is dedicated also to hybrid systems combining image segmentation and the supervision of the user in order to optimize the results. Messay et al. [15] presented a hybrid system, based on a regression Neural Network requiring the user to input eight precise points in order to create the mask.We also want to highlight the research of Liu et al. [13], who developed a network for Juxta-Pleural Lung Nodules but only tested it on 50 manually chosen images, making any comparison unfeasible.Roy et al. [19] developed a shape-driven lung nodule segmentation. During the pre-processing, they applied a mask2 erasing all the non-soft tissues around the lungs. This makes the Juxta-Pleura nodules way easier to be segmented, but difficult to be applied in real clinical conditions due to the unavailability of these masks during the analysis of the patient’s data. Furthermore, they chose a subset of the test set, making the comparison with their results impaired.
