Scene parsing, as one of the most significant and challenging topics in AI, computer vision, and video analysis, is aimed at assigning one of the pre-defined labels to each pixel in a given scene. It plays a crucial role in many advanced video-based applications, such as intelligent vehicles and traffic surveillance. To handle these real-world tasks, scene parsing models must be both reliable and fast enough (real-time).
The remarkable progress of deep Convolutional Neural Networks (CNNs) in recent years has increased the reliability of scene parsing model. Notable examples include DeepLab (V1–V3) [4], Refinenet [5], PSPNet [6] and DANet [7], to name a few. Unfortunately, to achieve high accuracy, these models employ very deep and sophisticated network architectures with tremendous computation costs, resulting in low inference speed (see Fig. 1). Consequently, applying them to real-world applications is difficult. Therefore, it is imperative to reduce the computational complexity of scene parsing models, or design powerful deep models with small network size.Download : Download high-res image (156KB)Download : Download full-size imageFig. 1. The comparison of inference accuracy(mIoU), computational complexity(FLOPs) and speed(ms) on Cityscapes test set. Following [1], [2], [3], all models are evaluated on a single NVIDIA Titan X card. The bigger the circle, the faster the speed. Our EKENet-A and EKENet-B achieve a better trade-off between the computational cost(complexity, speed) and performance.
Recently, many studies have tried to accelerate the inference speed. ENet [8] pioneers the research in real-time scene parsing. Specifically, by employing bottleneck structure and reducing channel number, it guarantees a tight network architecture for high inference speed. ESPNet [9] introduces a more efficient building block that involves fewer network parameters. However, as can be seen in Fig. 1, their increased inference speed is achieved at the cost of largely decreased accuracy. Although some works [1], [2], [10] try to boost accuracy by employing additional strategies (e.g., multi-scale inputs or two-column network structure), they are, at best, close to or just beyond the real-time inference speed (30FPS).
To achieve a good trade-off between efficiency and performance, we present a novel network structure called Efficient Knowledge Enhanced Network (EKENet) for scene parsing. Firstly, to ensure high efficiency, Efficient Dual Abstraction (EDA) block, a novel building block, is designed to build our EKENet. Basically, inspired by convolution factorization principle [11] and to reduce the overall network size and complexity, EDA employs two parallel convolution layers, where one focuses on modeling spatial relationship with depth-wise convolutions while the other one deals with cross-channel correlations modeling with pointwise convolution. Secondly, we propose a novel light-weight Encoding-Enhancing (EE) module in the encoder-decoder network architecture to ensure that the learned features to contain both high-level semantic knowledge and adequate spatial details. Specifically, EE module encodes the semantic information from top layers, and uses it to guide the learning of shallow features. Experiments show that our EE module largely improves the overall accuracy.
In summary, our contributions are three-fold:
•We propose an efficient building block, called Efficient Dual Abstraction (EDA) block, which is capable of extracting rich features with low computation complexity. Different from ResNet block [12], our EDA block employs a parallel structure to handle spatial and cross-channel correlations more efficiently.•We introduce a novel light-weight Encoding-Enhancing (EE) module to enhance the representational power of high-resolution feature map, which greatly elevates the overall accuracy.•Our fully-equipped model, Efficient Knowledge Enhanced Network (EKENet), achieves the new state-of-the-art performance in terms of speed and accuracy tradeoff. More specifically, It can run at high resolution Cityscapes images in speed of 43 FPS while obtaining very competitive results (mIoU = 74.2%).
