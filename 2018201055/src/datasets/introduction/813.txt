In natural language processing (NLP), one of the vital lower-level tasks (word-level tasks) is named entity recognition (NER), which aims to identify categories of information such as places, people, or objects from a given portion of text. The strength of a model lies in its capability to correctly identify tokens in a corpus, which is two-fold: (a) it can leverage the performance of subsequent NLP tasks, such as machine translation and question answering, and (b) it represents a potential replacement for dependency on experts, because many existing datasets are dependent on human annotator-based feature extraction. Therefore, a rule-based structure for NER tasks need to be built. Recent advances in NLP obtained with the employment of the word embeddings [1], [2], [3]3  technique have made it possible to extract relevant knowledge about language and morphology from huge unannotated corpora. Recent NER works explore this idea by feeding embeddings to complex models developed using deep learning [4], [5], [6], [7], [8], [9], [10], [11], [12], [13] methods, either by using bidirectional [14] long short-term memory (Bi-LSTM) [15] (Bi-LSTM) or even applying convolutional neural networks (CNNs) to extract language features at a character-level stage [16].
Furthermore, existing work on the NER task relies on building models that are capable of being an expert in classifying every entity type possible, which trade off accuracy for generalization such as recognizing named entities in tweets. The models trained using such corpora typically do not generalize well to several recent NER applications. In this paper, we propose the benefit of applying a different type of model based on the capsule network architecture [17], where neurons are grouped in ‘areas of expertise.’ Throughout the different layers of the model, lower-level capsules propagate their predictions to subsequent layers, and by a capsule ‘agreeing’ with the respective previous layer, the concept of ‘routing’ is created. We present how this approach stands on par with the state-of-the-art NER results and how it can be generalized to other NER models that can be used across multiple languages and do not require further training on annotated corpora. A recent work [18] extends the capsule network’s routing mechanism and has been shown to be quite effective over CNN models.
The structure of this paper is as follows. Section 2 describes related work in NER and how other deep learning techniques have been used to tackle the task of entity recognition. Section 3 presents the architecture of the proposed model with a detailed methodology, followed by Section 4, which presents the dataset used in this experiment and a comparison with state-of-the-art models. In Section 7, we present the open opportunities and challenges in applying this architecture to many other NLP tasks, and then we conclude with a relevant discussion about the proposed method.
