Software defect prediction (SDP) [1] is an active research topic in software repository mining domain. SDP methods can be used to optimize the software quality assurance resource allocation by predicting potential defective program modules in advance. Most of the previous studies focus on within-project defect prediction (WPDP), which constructs the SDP models and then predicts defective modules within the same project. However, in practice, the target projects for SDP are often new start-up projects or only have a few labeled data in most cases. In previous studies [2], most of the researchers mainly utilize the training data collected from other projects (i.e., the source projects) and resort to transfer learning [3] to alleviate the data distribution difference between different projects. This problem is called as cross-project defect prediction (CPDP).
Most of the previous CPDP studies assume that both the source project and target project use the same metrics to measure the extracted program modules (i.e., homogeneous data). However, practitioners may use different metrics to measure the extracted program modules. The reasons can be summarized as follows: First, some metrics are designed based on a specific programming language. Therefore these metrics cannot be used to measure modules written by other programming languages. Second, some metrics can be supported only by commercial tools (such as Understand tool1), and the cost of buying these commercial tools cannot be afforded for small-scale start-up enterprises [4]. Compared to CPDP with homogeneous metrics, performing CPDP with heterogeneous metrics is more challenging, and this problem is called as heterogeneous defect prediction (HDP) [5], [6].
Motivation. Until now, researchers have proposed many novel supervised HDP methods. For example, Nam et al. [4], [5] proposed a HDP method based on metric selection and metric mapping. Li et al. [7], [8], [9] proposed a set of novel HDP methods based on kernel correlation alignment and ensemble learning to solve the linearly inseparable problem and the class imbalanced problem. Recently, we notice some studies [10], [11], [12], [13] have shown the competitiveness of unsupervised defect prediction (UDP) methods for CPDP.
Given the target project, UDP methods can directly identify the defective modules, while HDP methods can identify the defective modules via the models constructed on the source project with heterogeneous metrics. Therefore, we can make a fair comparison between HDP and UDP methods. However, to the best of our knowledge, this issue has not been thoroughly investigated in previous HDP studies [4], [5], [7], [8], [9]. In addition, most of the existing studies often evaluate the performance of HDP methods in terms of non-effort-aware performance indicators, such as F1, AUC. This will result in the bias in previous empirical conclusions, and the performance comparison results in terms of other performance indicators (such as indicators considering testing efforts) are still unknown. Finally, there exists a lack of comprehensive comparison between different HDP methods, especially some recently proposed methods [7], [8], [9].
In our empirical studies, we choose state-of-the-art HDP methods and UDP methods proposed from 2014 to 2019 to make a thorough comparison. In particular, the five chosen HDP methods include a metric selection and matching based method [4], [5], three metric transformation-based methods [7], [8], [9], and one distribution characteristics based method [14]. The four chosen UDP methods include the connection based method [11], the methods CLA and CLAMI [12], and the ranking-based method [10]. Then we choose five groups of datasets (i.e., AEEEM, ReLink, PROMISE, NASA, and SOFTLAB). These datasets are collected from 34 different projects from the open-source community and commercial enterprises, and they have been widely used in previous HDP studies [4], [5], [7], [8], [9].
To systematically investigate this issue, we aim to answer the following three research questions.
RQ1: Do these HDP methods perform significantly better than existing UDP methods in terms of non-effort-aware performance indicators (NPIs)?
For RQ1, we mainly consider two NPIs (i.e., F1 and AUC) and perform statistic significance test based on the Wilcoxon signed-rank test and Cliff’s δ [15]. Final results show HDP methods cannot perform significantly better than all the UDP methods in terms of these two NPIs. Moreover, according to two satisfactory criteria recommended by previous CPDP studies [16], [17], the satisfactory ratio of existing HDP methods is still pessimistic.
RQ2: Do these HDP methods perform significantly better than existing UDP methods in terms of effort-aware performance indicators (EPIs)?
Different from NPIs, EPIs consider the efforts used for inspecting the program modules. For RQ2, we mainly consider four EPIs. In particular, the first two EPIs are Popt and ACC, which were previously used for just-in-time (JIT) defect prediction [18], [19], [20]. Results on these two indicators show the simple unsupervised methods proposed by Zhou et al. [10] can still achieve better performance than state-of-the-art HDP methods. The latter two EPIs are PMI@20% and IFA, which were recently proposed by Huang et al. [21], [22] to revisit previous studies on JIT defect prediction. Results on these two indicators show that using the unsupervised methods [10] needs to inspect more program modules when available resources are limited. Moreover, using these simple methods means developers need to inspect many non-defective modules before encountering the first real defective module, which may hurt developers’ confidence and patience.
RQ3: Do HDP methods and UDP methods identify the same defective modules?
For RQ3, we can compute the complementary degree of different HDP and UDP methods in identifying defective modules. Bowes et al. [23] performed a sensitivity analysis by using diagrams to analyze whether there is a difference between four classical classifiers in terms of the specific defects each detects and does not detect. However, in our study, we consider five HDP methods and four UDP methods (i.e., there are far more than four methods to compare with each other). Therefore, it is difficult to achieve satisfactory visualization results by using the visualization methods used by Bowes et al. [23]. In our study, we use McNemar’s test [24] to perform a diversity analysis on identifying defective modules for different methods. Final results show that the diversity of prediction for defective modules across HDP vs. UDP methods is more than that within HDP methods or UDP methods. This shows these two different kinds of methods are complementary to each other, and ensemble learning [25] is a possible way for further HDP studies. Moreover, we also find a certain number of defective modules, which cannot be correctly identified by either method or even both kinds of methods. These findings implicate there still exist bottlenecks in current HDP studies.
Contributions. To our best knowledge, the main contributions of our article can be summarized as follows.
•We design and conduct large-scale empirical studies to systematically compare HDP methods with UDP methods on 34 projects in five groups under the same experimental setup.•We make a comprehensive comparison between HDP methods and UDP methods from three different perspectives: NPIs, EPIs, and diversity analysis on identifying defective modules.•Empirical results show there is still a long way for HDP methods to go, and we present some observations about the road ahead for HDP.
Article organization. Section 2 analyzes the related work for cross-project defect prediction and motivation of our study. Section 3 introduces the details and experimental setting of our chosen HDP methods and UDP methods. Section 4 shows the experimental setup, including research questions, experimental subjects, and performance indicators. Section 5 performs result analysis for three research questions. Section 6 presents some observations about the road ahead for HDP. Section 7 discusses the main threats of our empirical studies. Section 8 concludes this article.
