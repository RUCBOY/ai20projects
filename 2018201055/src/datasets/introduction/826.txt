In supervised learning, ordinal regression (OR) problems are very common and important. Because, lots of real world applications can be reduced as OR problems, such as medical research [1], credit rating [2], and information retrieval [3]. In OR problems, the training samples are marked by a set of ranks, which exhibits an ordering among the different categories. We use information retrieval as an example. Information retrieval needs to predict the relevance level of the references with respect to the given textual query, using a rank scale like: definitely, possibly, or not relevant. Thus the ordering information can be used to construct more accurate models and misclassification costs are not the same for different errors. OR is similar to standard regression [4], [5], [6] in maintaining the ordering information. However, the multiple thresholds between the ranks need to be learned which makes regression different to OR.
There have been a lot of OR algorithms proposed in the last two last decades, from support vector machine (SVM) [7], [8], [9] formulations [10], [11] to Gaussian processes [12] or discriminant learning [13]. In this paper, we focus on the classical support vector ordinal regression (SVOR) method. Because of the good generalization performance, SVOR is an important method to tackle OR problems. More importantly, SVOR is much more straight-forward and interpretable.
There are several versions of SVOR. For example, the first SVOR algorithm [14] was proposed based on the idea of cumulative odds logit models. Nevertheless, the problem size is a quadratic function of the sample size due to loss function considers pairs of ranks to keep ordering information. To address this issue, several SVOR approaches were proposed to find r−1 parallel discrimination hyperplanes for the r ordered categories. Specifically, SVFMOR and SVSMOR[15] were proposed for OR problems based on the strategies of fixed margin and sum-of-margins. However, for these two SVOR algorithms, the thresholds inequalities (i.e., b1≤b2≤⋯≤br−1) are not considered in their formulation. Thus, the thresholds may be unordered at the optimal solution, that makes these models violate the ordering distribution assumption.
To fix the problem of unordered thresholds in the fixed margin strategy of SVOR as mentioned above, two new SVOR formulations were proposed (i.e., SVORIM and SVOREX) [11]. For SVOREX, they imposed the explicit ordinal inequalities constraints into the original SVOR formulation with the fixed margin strategy. Note that the dual problem of SVOREX have inequalities constraints due to the explicit ordinal inequalities constraints, while multiple equality constraints come from the multiple thresholds. For SVORIM, they allowed the instances in all the categories to contribute errors for each threshold. They proved that the ordinal inequalities of the thresholds can be satisfied automatically at the optimal solution of SVORIM. Without explicit ordinal inequalities constraints, SVORIM avoids the inequalities constraints in its dual problem. As reported in [16], SVOREX and SVORIM are the two of the best methods among 16 state-of-the-art threshold models based on the results on 41 benchmark datasets. The results also verify the significance of the ordered thresholds for OR.
However, in comparison with standard SVMs, the dual formulations of SVOR with ordered thresholds are more complicated because multiple inequalities or equality constraints are involved in the formulation as we describe above. Due to the complicated formulations, large-scale training for SVOR is still vacant as far as we know. An effective incremental method has been proposed for SVOR, which is very suitable in the incremental environment [17]. However, this method needs to compute an inverse matrix of the size of (2r−2+m)2 in every iteration which results in a huge computational cost in the large-scale problems, where m is the size of margin support vectors strictly on the margins.
Support vector methods [18], [19], [20], [21] usually suffer from the high computational cost and memory requirements, because the kernel matrix with the size of O(l2) need to be computed and stored where l is the training sample size [22], [23], [24]. There are several methods proposed to accelerate kernel SVM training on large-scale datasets. Kernel approximation approaches are trying to approximate the kernel matrix by a l × m approximation matrix, then solving a linear SVM [25], [26]. However, as analyzed in [27], [28], the m need to be O(n) to obtain a good generalization ability. To address this issue, asynchronously parallel [29] coordinate descent algorithms (AsyCD) have been proposed to solve these problems, such as asynchronously stochastic coordinate descent algorithms (AsySCD) algorithm [30], [31] and asynchronous parallel greedy coordinate descent (AsyGCD) algorithm [32]. Compared with AsySCD, AsyGCD algorithm can achieve a much faster convergence speed due to the greedy selection of updated coordinates. However, AsyGCD is still not scalable enough for SVOR.
To address the large-scale training of SVOR with ordered thresholds, in this paper, we first highlight a concise SVOR formulation based on the idea of SVORIM. Its dual formulation is a box constrained quadratic programming problem on which we can apply AsyGCD easily. We theoretically show that this formulation maintains the ordinal thresholds at the optimal solution. Then we propose two novel asynchronous parallel coordinate descent algorithms, called AsyACGD and AsyORGCD respectively. AsyACGD is an accelerated extension of AsyGCD using the active set strategy. Active set (or shrinking technique) is a technique to improve the efficiency of dual coordinate descent methods [33]. Specifically, it tries to identify the bounded elements and remove them from the active set, thus, the size of the optimization problem can be smaller. The most famous application of the active set technique is LIBSVM [34]. AsyORGCD is specifically designed for SVOR such that keeping the ordered thresholds during the training process. Thus, a early stopping rule can be used to obtain good performance with much more less time. Specifically, we let each thread update on their local gradients block until they reach optimal locally, AsyORGCD updates the gradients of optimal threads from the global ones every certain iterations. Experimental results on several large-scale ordinal regression datasets demonstrate that our algorithm is much faster than the existing state-of-the-art SVOR solvers and AsyGCD solver while retaining the similar generalization performance.
Note that our AsyACGD algorithm have been published in the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence [35], this paper is a significant extension to [35]. In [35], we only compare the AsyAGCD and AsyGCD for classification and regression. While, in this paper, we explore how asynchronous parallel coordinate descent algorithms (including AsyAGCD) work on SVOR. Furthermore, based on the experimental observations, we propose a novel AsyORGCD to hold ordered thresholds during the training process. For saving space, we did not repeatedly show the results of classification and regression in this paper.
Contributions. The main contributions of this paper are summarized as follows.
1.To make the SVOR suitable for AsyCD algorithm, we highlight a concise SVOR formulation which can be seen as a special case of the general OR framework in [36]. We contribute to first highlight this useful formulation for AsyCD algorithms with some simple but meaningful theoretical analysis. As far as we know, it is the first time that solving SVOR with asynchronous parallel coordinate descent algorithms.2.To further speeded up AsyGCD, in this paper, we propose an asynchronous accelerated greedy coordinate descent algorithm (AsyAGCD) to further accelerate AsyGCD by the technique of active set. It is the first time that exploring how AsyAGCD algorithm works on SVOR. Note that, in [32], [35], AsyGCD and AsyAGCD is only used for classification and regression.3.To keep the ordered thresholds when it is training, we propose an novel asynchronous OR greedy coordinate descent algorithm (AsyORGCD), so that it can obtain good performance with lower time. Our experiments show that our AsyAGCD and AsyORGCD are much more efficient SVOR than existing the existing state-of-the-art SVOR solvers and AsyGCD solver.
Organization.
