Recently, the data-intensive scientific domains have become a new application field, due to the increasing volume of scientific data of up to several petabytes and the complexity of the scientific workload, the I/O has become more and more challenging. Consequently, existing HPC Parallel File Systems (PFSs) based on disk cannot satisfy the I/O requirements. In response to the demand for high I/O performance in an HPC environment, many supercomputers have been introduced with SSD-based file systems. For example, Cray has developed a DataWarp which is a fast storage tier based on NVMe SSDs, and deployed it at NERSC (National Energy Research Scientific Computing Center) since 2015 [5]. As a high-performance storage layer, burst buffers [24] have effectively handled the burst I/Os in many commercial HPC systems.
Unfortunately, all burst buffers based on SSDs have the same problem arising from a characteristic of the NAND flash memory: Garbage Collection (GC) overheads caused by a difference in the operation unit between write/read (page level) and erase (block level). The GC overhead is an additional copy operation for preserving valid pages in the GC operation that secures the empty block, which adversely affects the performance and the lifetime of a flash device when frequent GC operation occur [26], [38]. Specially, SSDs in burst buffer must process a large amount of concurrent and complex I/Os from a lot of scientific applications, because burst buffers are used for absorbing the bursty I/O traffic as a shared resource in HPC systems [22]. As a result, these SSD’s used in such HPC environments, would be exposed to frequent GC operations, which could lead to the losses in performance and endurance [35].
In relation to research on burst buffers, the research has mainly focused on studies addressing the I/O bottleneck problems in HPC systems [5], [24], [25], on burst buffers in local nodes for scalable write performance [4], [37], on I/O scheduling in burst buffer [33], [35], or on burst buffer resource management [23]. To the best of our knowledge, no existing studies focused on improving performance and endurance of the burst buffer itself except for our own previous work [12]. To mitigate the GC overheads for SSDs in burst buffer, our previous work proposed the user-level I/O isolation by using a multi-streamed SSD [17] that allocates same flash block for I/Os in the same stream ID. In that work, we uncovered the expected performance reduction in an SSD based burst buffer and demonstrated the that effectiveness of user-level I/O isolation in the burst buffer. However, that work was not integrated into a actual burst buffer on a HPC system.
In this work, we implement the burst buffer named BIOS, to transparently support I/O separation, and to validate its performance in active HPC environments. To take full advantage of this I/O separation scheme, we design a stream-aware scheduling policy for the workload manager. Based on this implemented system, we explore the benefits and limitations of the BIOS and the framework through extensive experimentation. To implement the burst buffer with I/O separation scheme, we leverage the multi-streamed SSDs to group the I/O streams based on the user stream mapping. This burst buffer is to assign the flash memory blocks exclusively to each user being performed transparently to users. In HPC burst buffer environments, we found that the benefits of I/O separation scheme can be reduced due to user ID-based stream allocation and a limited number of available streams (e.g., 4 to 16) in a multi-streamed SSD [17], [39], [40]. To overcome these problems, we propose managing the burst buffer resource as burst buffer pools and the stream-aware scheduling policy in workload manager and finally, implement the framework by integrating these with the BIOS. This framework optimizes the benefits of I/O separation scheme by alleviating the interference caused by data striping and the problem of skewed stream allocation.
To validate the effectiveness of the BIOS framework in the HPC environment, we use not only synthetic workloads but also real HPC applications as well as burst buffer I/O traces obtained from the Cori supercomputer at NERSC. In our tests, the BIOS shows the up to 44% increase in I/O throughput and up to 20% decrease in write amplification factor (WAF) compared to existing burst buffers.
Our main contributions are as follows: (1) We implement the burst buffer framework with an I/O separation scheme for multiple devices and multiple nodes; (2) We design a resource efficient workload manager with a stream-aware scheduling policy to take advantage of the I/O separation framework; (3) Users do not have to change their I/O functions in order to benefit from our burst buffer framework. (4) Through extensive evaluations with HPC applications and burst buffer I/O traces from the Cori supercomputer, we observe up to 44% increase in I/O throughput and 20% decrease in write amplification.
In the rest of this paper, Section 2 presents the background and challenges of the work presented in this paper and Sections 3 I/O separation scheme in burst buffer, 4 BIOS framework describe the I/O separation scheme and framework. Section 5 shows the experiment results to demonstrate the effectiveness of proposed scheme and framework compared with legacy burst buffer. Section 6 summarizes findings and insights from the evaluation. Sections 7 Discussion, 8 Related work show the discussion and related work respectively, and finally Section 9 summarizes our conclusions and future studies.
