Saliency prediction is a fundamental task in computer vision. Given images and videos, saliency prediction aims to find their most visually attractive and informative regions to replicate the operation of the human visual system. To date, saliency prediction mechanisms have been widely used in many computer vision applications, such as object detection [1], [2], [3], image segmentation [4], [5], quality assessment [6], [7], and visual tracking [8], [9]. Although various saliency prediction models handle RGB images [10], [11], [12], [13], [14], [15], [16], [17], binocular vision helps humans to perceive not only the color but also depth information. Therefore, depth information has been introduced for saliency prediction to supplement the information in RGB images and address challenges such as color distribution similarities between salient regions and the background. Since the introduction of affordable and portable depth sensors such as Microsoft Kinect and Intel RealSense [18], which can provide many useful cues on object structures, saliency prediction using RGB-D images has become a research hotspot.
Although many prior studies have addressed the role of depth information in the prediction of human visual fixation, three main problems remain to be solved in RGB-D saliency prediction: 1) effective and sufficiently integrated cross-modal complementarity between corresponding RGB images and depth maps, 2) prevention of unnecessary information loss from cross-modal features in the encoder of a neural network while leveraging cues at low levels and intermediate layers, and 3) suppression of complex backgrounds along with accurate prediction of salient regions. To address these problems, we propose an attention-based contextual-interaction asymmetric network for RGB-D saliency prediction. This artificial neural network comprises three main parts: a common feature extractor (CFE), joint feature-interaction module (JFIM), and channel-wise attention module (CAM). The CFE consists of two asymmetric backbones to jointly capture information from the RGB image and its depth map. The JFIM extracts sensitive cross-modal information and exploits contextual cues. The CAM enhances visual feature representations and suppresses feature-space inconsistencies.
Compared with previous studies, the key contributions of this work are summarized as follows:
(1)An asymmetric CFE fuses multilevel and multiscale features of an RGB image and its depth map, and captures cross-modal feature representations.(2)A JFIM leverages cross-modal features at different scales and explores contextual cues for improving saliency prediction.(3)A CAM emphasizes prominent regions in the RGB-D image for the network to focus on regions that resemble human visual fixation.
The remainder of this paper is organized as follows. Section 2 provides a brief survey of related work. We detail the proposed network in Section 3. Experimental results on saliency prediction are reported in Section 4. Finally, we draw conclusions in Section 5.
