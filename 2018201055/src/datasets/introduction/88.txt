The data that is extracted from GLOBOCAN 2018 shows that the incidence of prostate cancer is second to that of lung cancer [1], and this condition is a serious threat to man’s physical and mental health. Diagnosis and comprehensive cure are the most effective means to prevent prostate cancer. Currently, ultrasound imaging technology has become an important method for the detection of prostate cancer, because it is cheap, cost-effective, and free of radiation and trauma. However, transrectal ultrasonography (TRUS) usually requires a doctor with clinical experience in diagnosis, and the doctor’s diagnosis accuracy is affected by many factors. Therefore, an accurate and efficient computer-aided diagnosis (CAD) is essential to improve the efficiency of prostate-assisted therapy.
Segmentation is a crucial link in the CAD system of prostate ultrasound images. Considering that prostate ultrasound images have serious speckle noise and low signal-to-noise ratio, and most of the segmentation algorithms of prostate ultrasound images do not have pixel-level segmentation, the current prostate ultrasound image segmentation method is insufficient, and the positioning accuracy is not high. This limitation causes trouble and pressure to the doctor’s judgment and work. Therefore, to obtain the final image containing only the information of the prostate region and reduce the interference of other factors, we adopted the improved S-Mask R-CNN network model to use the region of interest align (ROIAlign) algorithm [2] to segment the ultrasound image of the prostate.
Classification is another important link in the CAD system of the prostate ultrasound image. On the basis of segmentation, the prostate ultrasound image was classified, thus predicting whether the patient has prostate cancer through image classification. Traditional image classification methods mainly involve image preprocessing [3], image feature description and extraction [4], and classifier design. The performance difference of traditional image classification depends on the shape, texture, color, and underlying visual features of the feature extractor, classifier, and image. Various traditional classifiers can be used, including the K-nearest neighbor [5] and support vector machine [6]. However, the classification results are not satisfactory because of slight differences between the images of ultrasonic prostate cancer and serious noise interference. Accordingly, the convolutional neural network was introduced in deep learning [7], [8], [9], thus greatly improving the classification accuracy of prostate ultrasound images without requiring the manual feature description and extraction of target images. During transcrectal ultrasonography [10], the puncture place was marked and analyzed in the image. Subsequently, analysis of the images was performed with the Automated Urologic Diagnostic Expert(AUDEX) system, consisting of a personal computer connected to the ultrasound machine. Azizi et al. [11] used Temporal Enhanced Ultrasound(TeUS) to address the problem of prostate cancer in a clinical study of the biopsy cores [12]. The method involves capturing high-level latent features of TeUS with a deep learning approach followed by distribution learning to cluster. On the above research, the improved Inception-v3 [13] neural network lesion identification method was adopted in this paper, and the improved network depth enabled the convolutional neural network model to automatically extract the characteristic lesion images of prostate ultrasound images for classification. Then, the data set was constructed for training, and parameters were continuously optimized through batch gradient descent. Overfitting was eliminated using the dropout method.
Based on the above research, the present study introduced a deep learning model that integrates S-Mask R-CNN andInception-v3 in the ultrasound image-aided diagnosis of prostate cancer. The results showed that the data set on building ultrasound images of prostate disease recognition rate increased.
A data set of prostate ultrasound images with segmentation labeling was constructed. Exactly 1200 images were randomly selected from the data of prostate ultrasound images, and a new data set was constructed for labeling. Among them, malignant and benignant images in simulated clinical medicine account for an appropriate proportion.
ROIAlign was used in the improved S-Mask R-CNN algorithm to retain the floating-point coordinates on the ultrasonic prostate images, thus boosting the image segmentation accuracy.
A deep learning model that integrates S-Mask R-CNN and Inception-v3 in the ultrasound image-aided diagnosis of prostate cancer was built, and multiple performance evaluation was carried out to identify prostate ultrasound image data sets. These processes are important for the study of prostate cancer identification.
