The performance of machine learning methods has improved dramatically in the last few years due to deep neural networks (DNNs) [1]. These models learn complex tasks by looking at many training examples, a computationally demanding task and that usually requires huge amounts of data.
In many real world problems (e.g., in medical image analysis), obtaining data is often a challenge, particularly because annotations are expensive. Consequently, many of the available data sets lack the necessary variability and representativeness to train DNNs that generalize well. Additionally, these data sets are often long-tailed, meaning that the distribution of samples across different classes is not uniform (class imbalance) and some classes are under-represented. This typically prevents DNNs from learning effectively and often leads to biased systems. The goal of this work is to provide a learning strategy that is robust under the above conditions, and ensure the models make the most of the available data.
There has been an increasing effort to develop more efficient learning strategies for DNNs, such as using quantization approaches [2] or curriculum learning [3]. This paper focuses on the latter, which is inspired by the idea that training samples are not equally relevant during training [4]. Recent works have relied on importance sampling [5], [6], which aims to increasing convergence speed by selecting samples according to their gradients. However, importance sampling requires the normalization of the gradients across the entire training set, which is expensive to compute. Therefore, approximations based on information from previous epochs are usually made, preventing the combination of these strategies with online data augmentation approaches, which are popular and crucial to prevent overfitting [7], [8].
Alternative strategies rely on weighting schemes to obtain improved models. For instance, some works rely on weighted combinations of different DNNs (ensemble strategies) to achieve better performances [9], [10], while others resort to weighting to perform spatial or depth-based feature selection [11], [12]. These works incorporate attention modules in their DNNs to actively select the most discriminative features. Other approaches adopt sample weighting strategies, where non-uniform weights are assigned to the training samples. For instance, the balanced cross-entropy loss [13] has been extensively used to deal with long-tailed data sets, particularly in medical applications [14]. The idea is to assign each sample a weight that captures the distribution of classes in the training set: less represented classes should receive higher weights in the loss function to ensure they are not ignored. A direct consequence of this strategy is that all samples from the same class are assumed to be equally relevant, which may not hold, due to the variability of samples within classes and their distribution. Sample weighting has also been adopted in boosting [15], where incorrectly classified samples receive higher weights when training the following weak classifier. However, sequentially training multiple DNNs is inefficient and often computationally infeasible. The focal loss function [16] has been proposed to address the above issues, optimizing the parameters of a single DNN using a weighted loss function that reduces the importance of well-classified samples. This method has already been applied to medical images [17], but requires a careful tuning of hyper parameters, and has been shown to not always improve the accuracy of the network [18]. Alternatively, the approach proposed in Ren etÂ al. [19] optimizes the weights assigned to samples so that they minimize the loss in a validation set. The limitation of this approach is the need to build a balanced validation set that is sufficiently representative to allow a proper estimation of the gradient correction.
More complex strategies that use teacher-student models [20], [21] have also been proposed to automatically learn the weight of each sample (teacher), based on the performance of the DNN (student). Despite their promising results, these methods are computationally expensive, since they require the training of an additional DNN to act as the teacher model and the careful fine-tuning of the additional hyper-parameters.
In this work, we propose a new sample weighting strategy that overcomes many of the limitations of the aforementioned methods. This new approach, based on Learning Optimal sample Weights (LOW), aims to provide a greater decrease in the loss function at each gradient descent step. Unlike with the balanced cross-entropy loss, the weights in LOW are sample specific, simultaneously addressing class imbalance and intra-class variability issues. Moreover, the computation of its weights adds marginal burden to the training process and does not increase the number of model parameters being learned as teacher-student approaches. Furthermore, LOW can be easily incorporated into the training of any state-of-the-art DNN architecture, and can be combined with different loss functions, such as cross-entropy and focal loss, to improve their performances.
We apply LOW to both popular computer vision benchmarks (MNIST, CIFAR 10, and CIFAR 100) and real world problems (ISIC 2017 and 2018 data sets for the diagnosis of skin cancer). Our results demonstrate that LOW is particularly suited for problems with imbalanced data sets, by forcing the network to learn to classify under-represented examples. With LOW, we are able to outperform conventional weighting strategies and improve the accuracy of the model on all data sets. LOW also provides insights on which samples contribute the most during the training process, making it easier to interpret and analyze the model results. Therefore, LOW is a valuable contribution to the topic of explainable deep learning for efficient and robust pattern recognition.
