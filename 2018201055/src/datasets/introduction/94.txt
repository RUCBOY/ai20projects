Estimating depth information from images is an important computer vision task, which poses a key problem for many research topics, such as self-driving, scene reconstruction, localization, tracking, and recognition. Traditional depth estimation methods rely on the disparity between multiple views of a scene to recover the depth information. Some additional meaningful features of the scene, such as texture information, occlusions, object localization, and object sizes, are also used to obtain the depth relationship between the objects of the scene.
With the rapid development of deep learning, significant work has demonstrated that deep learning proves efficacious in depth estimation. Existing depth estimation methods based on depth learning can be roughly divided into two types: supervised depth estimation and self-supervised depth estimation.
Saxena [1] first proposed a supervised learning-based method with handcrafted multi-scale features. However, the handcrafted features of this method lack the global features to generate the depth. Instead of using the handcrafted features, many methods have recently used the CNN model to learn the features. Many approaches [2], [3], [4], [5] perceive depth estimation as a supervised learning problem and extract features with deep convolutional neural networks. These methods directly predict the depth of each pixel or super-pixel in monocular or stereo images using a CNN model with different structures. In these approaches, loss functions are designed based on photometric errors between network output and the ground truth of datasets. Liu et al. [6], [7] combine the CNN model with continuous CRF to learn features. Eigen et al. [2], [8] predict pixel-level depth with a coarse–fine network structure which has been widely used in the field of depth estimation. Ladický et al. [9] combine semantics with the CNN model to improve pixel-level depth estimations, while Li et al. [10] use CRFs and more robust loss functions to improve accuracy. Cao et al. [11] change the loss from regression to classification. Yin et al. [12] make use of 3D geometric information in supervised monocular depth estimation instead use some local information to improve the performance. In general, these supervised methods rely on the high-quality ground-truth depth of input images at the time of training. Although these methods have achieved impressive success, the sparsity of ground truth limits the accuracy of prediction depth. Moreover, these methods have been restricted to scenes where large image collections and their corresponding pixel-level depths are available.
Compared to supervised approaches, self-supervised approaches could predict depth by minimizing the image-reconstruction error without the ground-truth of input images during training. Most of the self-supervised-based depth estimation approaches train the estimators from the stereo image pairs. These self-supervised learning approaches require rectified stereo image pairs during training. Then the deep model can be used to perform monocular depth estimation by predicting the pixel disparities between the pair. Girshick et al. [13] first presented an iterative framework for learning left–right image matching through consistency reprojection. Garg et al. [14] then combine reprojection algorithms and confidence measures in a left–right matching network. Godard et al. [15] propose a novel method using an image alignment loss in a convolutional encoder-decoder architecture. Additionally enforce left–right consistency is used to predict the disparities in the stereo pair. The approach has been improved with [16], who simplifies the use of unsupervised cues and does not explicitly enforce left–right consistency. Luo et al. [17] propose that superior results can be attained compared to those of other supervised methods by using the left–right image-depth consistency terms.
Stereo-based unsupervised methods (e.g., [15], [16], [18]) have shown comparable accuracy to supervised methods. However, the assumption of using calibrated binocular image pairs excludes these methods from utilizing monocular video, which is easier to obtain and richer in variability.
Compared with stereo-based unsupervised depth-estimation methods, there are various advantages of estimating depth information from an unlabeled monocular image. However, recovering 3D information from a single monocular image poses a serious challenge due to the absence of scale and viewpoint information. Thus, compared with stereo image self-supervision, predicting depth with monocular video has more restrictions, as monocular methods require consecutive frames to provide extra training features. In addition to predicting depth, the monocular methods also need to estimate the camera’s pose between frames. Zhou et al. [19] attempt to solve this problem with a separate pose-predictor element as well as a depth predictor by minimizing the photometric consistency across monocular video datasets during training. However, predicting the camera’s pose is challenging in the presence of object motion. To deal with spatial regions of a scene, [20] uses an additional motion mask in their framework to ignore specific objects that are outside of the rigid scene assumption, and their model achieved superior performance. Inspired by [20], Sfm-Net [21] uses a more efficient motion-mask model (e.g., multiple motion masks) to improve performance. Casser et al. [22] also divid scenes into a moving part and stationary part, explaining object motion with depth and optical flow and improving the flow prediction. Other subsequently developed methods have focused on improving the structure of depth-predictor element to achieve hierarchical feature extraction or finer feature extraction, or improving the loss function to reduce the gradient of the estimated depth. DDVO [23] proved that the major difference between stereo and monocular strategies is the unknown camera pose between frames. It replaced the CNN pose predictor with a direct visual odometry (DVO) pose predictor, achieving impressive results compared with those methods that have been trained using rectified stereo image pairs [18], [16]. Recent approaches have begun to close the performance gap between monocular and stereo-based self-supervision.
In this paper, we have further narrowed the gap between stereo methods and monocular methods. Inspired by recent advances in differentiable direct visual odometry (DDVO), we propose a novel self-supervised monocular depth-estimation method. Our approach focuses on the self-supervised depth-estimation network, wherein unlabeled monocular images are input without any assumptions about scene geometry or pretrained semantic information. The main contributions of this paper are as follows. First, it proposes a novel architecture that combines the DDVO part with a novel multi-scale appearance-matching loss to predict depth in a self-supervised way, which improves the accuracy of depth estimation of monocular video. Second, it introduces a simple auto-mask into the DVO pose predictor and depth predictor to ignore the underlying textural information area and objects that are relatively stationary. Third, it proposes a novel self-supervised loss function to fuse the auto-masking element and the depth-prediction feature. Compared with the CNN pose-prediction method, better pose-prediction performance is thereby achieved. We found similarity between the image-reprojection error in monocular methods [18] and the photometric error in DVO [24], and we used the matching loss proposed in [18] to address the problem of acceded pixels in the DVO pose predictor. After training on an unlabeled KITTI raw dataset [25], our approach yields state-of-the-art monocular depth-estimation results on the KITTI dataset [25], and achieves comparable results to Mono2 [18] and 3DNet [26] etc, which are trained with calibrated binocular pairs.
The rest of this paper is organized as follows. A brief overview of the proposed architecture is introduced in Section 2.1. The auto-mask and improved DDVO are introduced in Section 2.2 and Section 2.3, respectively. Our experimental results for the implemented system are presented in Section 3, and discussion of these results follows in Section 4.
