# 人工智能机器学习和深度学习

###  谢明昊2018202108 吴翔宇2018202103

## 大纲

#### 第一章 AI简介

#### 第二章 机器学习简介

#### 第三章 机器学习中分类器

#### 第四章 深度学习简介

#### 第五章 深度学习：RNN和LSTM

#### 第六章 自然语言处理和强化学习

## 内容

### 第一章 AI简介

本章将对AI进行简要介绍，主要是对该多样化主题的广泛概述。与本书的其他章节不同，本入门章节是从技术内容上讲是“轻”的章节。但是，它很容易阅读，也值得浏览其内容。在本章末尾简要介绍了机器学习和深度学习，在后续章节中将对这两种方法进行更详细的讨论。

请记住，许多专注于AI的书都倾向于从计算机科学的角度以及传统算法和数据结构的角度来讨论AI。相比之下，这本书将AI视为机器学习和深度学习的“伞”，因此以粗略的方式将其讨论为其他各章的前身。

本章的第一部分首先讨论术语“人工智能”，确定智能存在性的各种潜在方法以及“强人工智能”和“弱人工智能”之间的区别。您还将了解图灵测试，这是一种著名的智力测验。

本章的第二部分讨论了一些AI用例以及神经计算，进化计算，NLP和生物信息学的早期方法。

本章的第三部分向您介绍AI的主要子领域，其中包括自然语言处理（使用NLU和NLG），机器学习，深度学习，强化学习和深度强化学习。

尽管本章未讨论特定于代码的示例，但本章的附件包含用于解决Red Donkey问题的基于Java的代码示例，以及解决魔方的基于Python的代码示例（需要Python 2.x）。

#### 什么是人工智能

“人工”一词的字面意思是合成词，通常具有次等替代词的负面含义。但是，人造物体（例如花）可以非常接近它们的对应物，有时，当它们没有任何维护要求（阳光，水等）时，它们可能是有利的。

相比之下，对智能的定义比对“人工”一词的定义更难以理解。R. Sternberg在关于人类意识的文章中提供了以下有用的定义：“==智能是个人从经验中学习，善于推理，记住重要信息以及应付日常生活需求的认知能力。== ”

您可能还记得标准测试中的问题，这些问题要求给定序列中的下一个数字，例如1、3、6、10、15、21。首先要注意的是，连续数字之间的差距增加了一个：从1开始到3，则增加为2，而从3到6，则增加为3，依此类推。根据这种模式，可能的答案为28。此类问题旨在衡量我们在识别模式中显着特征方面的熟练程度。

顺便说一句，“次序列”数字问题可能有多个答案。例如，序列2、4、8可能会建议16作为该序列中的下一个数字，如果生成公式为$2 ^ n$ ，这是正确的。但是，如果生成公式为$2^n+(n-1)*(n-2)*(n-3)$，则序列中的下一个数字为22（而不是16）。有很多公式可以将2、4和8匹配为数字的初始序列，但是下一个数字可以与16或22不同。

让我们回到R. Sternberg对情报的定义，并考虑以下问题：

•  您如何确定某人（某物）是否聪明？

•  动物聪明吗？

•  如果动物是聪明的，您如何衡量他们的智力？

==我们倾向于通过与人们互动来评估他们的智力：我们提出问题并观察他们的答案。尽管此方法是间接的，但我们经常依靠此方法来评估他人的情报。==

对于动物智力，我们还观察它们的行为进行评估。聪明的汉斯（Clever Hans）是一匹著名的马，大约在1900年生活在德国柏林，据称他精通算术，例如加数和计算平方根。

实际上，汉斯能够识别人的情绪，并且结合敏锐的听觉，当汉斯接近正确答案时，他可以感觉到听众的反应。有趣的是，汉斯在观众不在场的情况下表现不佳。您可能不愿意将Clever Hans的行为归因于智力。但是，在得出结论之前，请复习Sternberg的定义。

再举一个例子，一些生物仅在群体中表现出智力。尽管蚂蚁是简单的昆虫，并且它们孤立的行为很难保证将其包含在有关AI的文本中，但蚂蚁群体对复杂的问题却表现出非凡的解决方案。实际上，蚂蚁可以找出从巢到食物来源的最佳路线，如何搬运重物以及如何形成桥梁。因此，集体昆虫的智慧来自各个昆虫之间的有效交流。

脑质量和脑对身体质量的比率是智力的指标，在这两个指标中，海豚都比人类好。海豚的呼吸处于自愿控制之下，这可能导致大脑过多，以及海豚大脑的另一半轮流睡觉。海豚在动物自我意识测验（例如镜子测验）中得分很高，在这种测验中，海豚意识到镜子中的图像实际上是他们自己的图像。他们还可以执行复杂的技巧，因为海洋世界的游客可以作证。这说明了海豚记忆和执行复杂的身体运动序列的能力。

工具的使用是对智力的另一项试金石，常被用来将直立人与人类早期祖先分开。海豚也与人类有同样的特征：海豚在觅食时用深海海绵保护自己的嘴。因此，智力并不是人类独有的属性。许多生命形式都具有一定的智力。

现在考虑以下问题：==无生命的物体（例如计算机）是否可以拥有智能？==人工智能的既定目标是创建计算机软件和/或硬件系统，以表现出与人类可比的思维，换句话说，显示通常与人类智能相关的特征。

思考能力如何，机器可以思考吗？请记住，思维与智力之间是有区别的。思维是推理，分析，评估和制定思想观念的工具。因此，并不是每个有思维能力的人都是聪明的。智力也许类似于高效的思考。

许多人带着偏见来解决这个问题，他们说计算机是由硅和电源制成的，因此无法思考。在另一个极端，计算机的性能要比人类快得多，因此必须比人类更智能。真相很可能介于这两个极端之间。正如我们所讨论的，不同的动物物种具有不同程度的智力。但是，相比于开发用于动物的标准化IQ测试，我们更感兴趣的是确定机器智能的存在。也许拉斐尔说得最好：人工智能是使机器完成人类需要做的事情的科学。

##### 强AI与弱AI

当前有两个主要的关于AI的阵营。弱人工智能方法与麻省理工学院有关，它将任何表现出智能行为的系统视为人工智能的一个例子。该阵营着重于==程序是否正确执行，而不管工件是否以与人类相同的方式执行其任务。==电气工程，机器人技术及相关领域的AI项目的结果主要与令人满意的性能有关。

人工智能的另一种方法称为生物合理性，它与卡内基-梅隆大学相关。根据这种方法，==当人工制品表现出智能行为时，其性能应基于人类使用的相同方法。==例如，考虑一个具有听觉能力的系统：强AI的支持者可能会通过模拟人类的听觉系统来实现成功，而弱AI的支持者只会关注系统的性能。该模拟将包括耳蜗，耳道，耳膜和耳朵其他部分的等效物，每个等效物在系统中执行其所需的任务。

因此，弱AI的支持者仅根据性能来衡量其构建系统的成功率。他们坚持认为，存在的的理由人工智能研究的是解决疑难问题，无论他们实际上是如何解决的。

另一方面，强AI的支持者关心它们构建的系统的结构。他们坚持认为，仅凭具有启发式程序，算法和AI程序知识，计算机就可以具有意识和智力。如您所知，好莱坞制作了属于强AI阵营的各种电影（例如I，Robot和*Blade Runner*）。

#### 图灵测试

上一节提出了三个问题，而前两个问题已经解决：如何确定智力，动物是否聪明？第二个问题的答案不一定是“是”或“否”。有些人比其他人聪明，有些动物比其他人聪明。机器智能问题同样存在问题。

艾伦·图灵（Alan Turing）试图从作战角度回答情报问题。他想将功能（做什么）和实现（如何构建）分开。他设计了一种称为*Turing Test的*东西，将在下一部分中进行讨论。

##### 图灵测试的定义

艾伦·图灵（Alan Turing）提出了两个模仿游戏，其中一个人或一个实体的行为就好像他是另一个。在第一个游戏中，一个人（称为询问器）在一个房间里，窗帘穿过房间的中心。窗帘的另一边是一个人，审讯者必须确定是男人还是女人。询问者（性别无关）通过提出一系列问题来完成此任务。

这个游戏假设男人可能会在他的回应中撒谎，但是女人总是诚实的。为了使询问者无法从语音中确定性别，交流是通过计算机而不是通过口头表达的。如果是窗帘另一边的男人，并且成功欺骗了询问者，那么他将赢得模仿游戏。按照图灵测试的原始格式，男人和女人都坐在窗帘后面，审问者必须正确识别两者。

图灵可能基于此时期的一款流行游戏进行了这项测试，这甚至可能是他进行机器智能测试的动力。

如果您还不知道，埃里希·弗洛姆（Erich Fromm）是20世纪著名的社会学家和心理分析家，他们相信男人和女人平等，但不一定相同。例如，性别在颜色，花朵或购物时间方面的知识可能有所不同。区分男人和女人与智力问题有什么关系？图灵理解可能会有不同类型的思维，理解这些差异并容忍它们很重要。

##### 询问器测试

第二个游戏更适合研究AI。再次，审讯员在一个有窗帘的房间里。这次，一台计算机或一个人在幕后，机器扮演了男性的角色，有时还可以躺着。

另一方面，这个人一直是诚实的。询问器询问问题，然后评估响应以确定她是在与人还是在与机器通信。如果计算机成功欺骗了询问器，它将通过图灵测试，因此被认为是智能的。

#### 启发式

启发式方法可能非常有用，并且AI应用程序通常依赖于启发式方法的应用程序。==一个启发式本质上是解决问题的“经验法则”。换句话说，启发式是通常可以解决问题的一组准则。==将启发式算法与算法进行对比，后者是解决问题的规定规则集，其输出是完全可预测的。

启发式技术是一种用于寻找近似解的技术，该方法可以在其他方法过于耗时或过于复杂（或同时使用两种方法）时使用。使用启发式方法，可能会但不会保证有令人满意的结果，并且启发式方法在AI的早期特别流行。

日常生活中会出现各种启发。例如，许多人更喜欢使用启发式而不是询问驾驶方向。例如，当在晚上离开高速公路时，有时很难找到返回主干道的路。一个可能被证明是有用的启发是，当他们到达一个岔路口时，沿着有更多路灯的方向前进。对于捡回掉在地上的隐形眼镜，或者在拥挤的购物中心找停车位，你可能有一个最喜欢的策略。两者都是启发式的例子。

AI问题往往很大且计算复杂，而且通常无法通过简单的算法解决。人工智能问题及其领域趋向于体现大量的人类专业知识，尤其是如果采用强大的人工智能方法来解决。使用AI可以更好地解决某些类型的问题，而其他类型的问题更适合于涉及简单决策或精确计算以产生解决方案的传统计算机科学方法。让我们考虑一些示例：

•  医学诊断

•  使用带条形码扫描的收银机购物

•  自动取款机

•  两人游戏，如国际象棋和跳棋

医学诊断是科学领域，多年来受益于基于AI的贡献，特别是通过开发专家系统。专家系统通常构建在领域中，那里有大量的人类专业知识，并且存在许多通常采用以下形式的规则：if-condition-then-action。举一个简单的例子：如果您头痛，则服用两个阿司匹林并在早上打电话给我。

特别是，专家系统变得非常流行（并且非常有用），因为它们可以存储比人类头脑中能容纳的规则更多的规则。专家系统是产生全面而有效结果的最成功的AI技术之一。实际上，专家系统可以帮助人们做出更准确的决策（甚至“挑战”错误的选择）。

##### 遗传算法

达尔文的进化论是一种很有前途的范式，它涉及自然选择，其在自然界中以数千或数百万年的速度发生。相比之下，计算机内部的进化要比自然选择快得多。

遗传算法是一种启发式算法，可“模仿”自然选择的过程，其中涉及选择最适合的个体进行繁殖，以为其后代的后代生父。

让我们将AI的使用与动植物世界的进化过程进行比较和对比，在该过程中，物种通过自然选择，繁殖，突变和重组的遗传算子来适应环境。

遗传算法（GA）是通用领域中的一种特定方法，称为进化计算，这是AI的一个分支，其中提出的问题解决方案与动物适应现实世界的环境相适应。

#### 知识表示

当我们考虑与AI有关的问题时，代表性的问题变得很重要。获取和存储知识以对其进行处理并产生智能结果的AI系统还需要具有识别和表示该知识的能力。表示形式的选择是问题解决和理解的本质所固有的。

正如乔治·波利亚（George Polya）（著名的数学家）所说，一个好的表示选择几乎与针对特定问题设计的算法或解决方案一样重要。良好而自然的表述有助于快速而易于理解的解决方案。

作为代表选择的一个示例，请考虑著名的传教士和食人族问题，其目标是用船将三名传教士和三名食人族从西岸转移到一条河的东岸。在从西向东过渡的任何时候，您都可以通过选择适当的表示形式来查看求解路径。这个问题有两个约束条件：船在任何时候最多只能容纳两个人，任何岸上的食人族永远都不能超过传教士的人数。

##### 基于逻辑的解决方案

人工智能研究人员已将基于逻辑的方法用于知识表示和问题解决技术。特里·温诺格拉德（Terry Winograd）的《积木世界》（Blocks World）（1972）是使用逻辑为此目的开创性的例子，其中机械臂与桌面上的积木互动。该计划涵盖了语言理解和场景分析以及AI其他方面的问题。

此外，生产规则和生产系统用于构建许多成功的专家系统。生产规则和专家系统的吸引力在于清晰，简洁地表示启发式方法的可行性。结合这种方法，已经建立了成千上万的专家系统。

##### 语义网

语义网络是知识的另一种图形表示（尽管很复杂）。语义网络先于使用继承的面向对象语言（其中来自特定类的对象继承了超类的许多属性）。

使用语义网络的许多工作都集中在表示语言的知识和结构上。例子包括Stuart Shapiro SNePS （语义网处理系统）和Roger Schank 在自然语言处理中的工作。

存在用于知识表示的其他替代方法：图形方法对视觉，空间和运动等感官更具吸引力。最早的图形方法可能是状态空间表示，它显示系统的所有可能状态。

#### 人工智能与游戏

自二十世纪中叶以来，随着计算机的出现，通过培训计算机以玩和掌握复杂的棋盘游戏的挑战，计算机科学和编程技术水平得到了长足的进步。借助AI见解和方法论的应用而受益于计算机游戏的一些示例包括国际象棋，跳棋，围棋和奥赛罗。

游戏激发了人们对人工智能的发展和兴趣。1959年亚瑟·塞缪尔（Arthur Samuel）在跳棋游戏中的努力突显了早期的努力。他的程序基于五十种试探法的表格，并被用来与不同版本的自身进行对抗。在一系列比赛中失败的程序将采用获胜程序的试探法。它发挥了强大的跳棋，但从未掌握过游戏。

几个世纪以来，人们一直在尝试训练机器下棋。对国际象棋机器的痴迷可能源于一种普遍接受的观点，即需要智力才能很好地下棋。

1959年，Newell，Simon和Shaw开发了第一个真正的国际象棋程序，该程序遵循了Shannon-Turing范例。理查德·格林布拉特（Richard Greenblatt）的程序是第一个玩俱乐部级象棋的程序。在1970年代，计算机象棋程序稳步改进，直到该十年末达到专家级水平（相当于国际象棋锦标赛前1％的玩家）。

1983年，肯·汤普森（Ken Thompson）的美女（Belle）是第一个正式达到硕士水平的课程。随后是卡内基-梅隆大学的高科技公司的成功，作为第一个高级硕士（超过2400级）课程，该大学成功地实现了重要的里程碑。此后不久，“深思”程序（也来自卡耐基-梅隆大学）得到了发展，并成为第一个能够定期击败大师大师的程序。

当IBM在1990年代接手该项目时，Deep Thought演变为Deep Blue，而Deep Blue与世界冠军Garry Kasparov进行了六场比赛，后者于1996年在费城赢得一场比赛拯救了人类。1997年，Deeper与Deeper对抗蓝色（Deep Blue的继任者）卡斯帕罗夫（Kasparov）输了，国际象棋界动摇了。

在随后的对阵卡斯帕罗夫，克拉姆尼克和其他世界锦标赛级别的球员的六场比赛中，程序进行得不错，但不是世界锦标赛。尽管人们普遍认为这些程序可能仍然不如最优秀的人，但大多数人还是愿意承认，顶级程序与最有成就的人没有区别（如果有人想到图灵测试）。

1989年，埃德蒙顿艾伯塔大学的乔纳森·舍弗（Jonathan Schaeffer）开始了他的长期目标，即以他的程序“奇努克”（Chinook）征服跳棋比赛。在1992年与长期跳棋世界冠军马里恩·廷斯利（Marion Tinsley）进行的四十场比赛中，奇努克（Chinook）输了四分，平局三十四平。1994年，他们的比赛在经过6场比赛后并列，当时因健康原因，廷斯利不得不放弃比赛。从那时起，Schaeffer和他的团队一直在努力从游戏结束（全八张，结局更少）以及从一开始就解决跳棋。

其他使用AI技术的游戏包括步步高，扑克，桥牌，奥赛罗和围棋（通常称为新果蝇）。

##### AlphaZero的成功

谷歌创建了AlphaZero，这是一个基于AI的软件程序，使用自玩游戏来学习如何玩游戏。AlphaZero是Alpha Go的继任者，后者在2016年击败了世界上最好的人类Go玩家。AlphaZero在Go游戏中 轻松击败了Alpha Go。

此外，在学习了国际象棋规则之后，AlphaZero进行了自我训练（再次使用自玩游戏），并且一天之内就成为了世界上顶级的国际象棋手。AlphaZero可以击败任何人类下象棋者以及任何下象棋的计算机程序。

真正有趣的一点是，AlphaZero制定了自己的下棋策略，这不仅与人类不同，而且还涉及被认为违反直觉的棋步。

不幸的是，AlphaZero无法告诉我们它是如何开发出一种优于以前开发的下棋方法的策略的。由于AlphaZero是100％自学的，并且是世界上排名最高的棋手，因此AlphaZero是否有资格成为聪明人？

#### 专家系统

自从人工智能存在以来，专家系统就是人们研究的领域之一，这是人工智能取得巨大成功的一门学科。专家系统具有许多特征，使其成为AI研究与开发所希望的。其中包括知识库与推理引擎的分离，超过其任何或所有专家的总和，知识与搜索技术的关系，推理和不确定性。

最早且最常被引用的系统之一是启发式DENDRAL。其目的是根据质谱图鉴定未知化合物。DENDRAL是由斯坦福大学开发的，目的是对火星土壤进行化学分析。它是最早说明在特定学科中对领域专家知识进行编码的系统之一。

也许最著名的专家系统是斯坦福大学（1984）的MYCIN。开发了Mycin来促进血液传染病的研究。然而，比其领域更为重要的是Mycin为设计所有后续基于知识的系统而建立的示例。它有400多个规则，最终被用来为斯坦福医院的居民提供培训对话。

1970年代，PROSPECTOR（也在斯坦福大学（Stanford University））被开发用于矿物勘探。PROSPECTOR还是使用推理网络的早期且有价值的例子。

1970年代后出现的其他著名且成功的系统是XCON（具有约10,000条规则），其开发目的是帮助在VAX计算机上配置电路板。GUIDON，一种辅导系统，是Mycin的分支；TEIRESIAS，Mycin的知识获取工具；和HEARSAY I和II，这是使用Blackboard Architecture进行语音理解的主要示例。

道格·列纳特（Doug Lenat）的AM（人工数学家）系统是 1970年代研发工作的另一个重要成果，也是不确定条件下的推理的Dempster-Schafer理论以及Zadeh在模糊逻辑中的工作。

自1980年代以来，==在配置，诊断，指令，监视，计划，预后，补救和控制等领域已开发了成千上万的专家系统。==如今，除了独立的专家系统外，许多专家系统还被嵌入到其他软件系统中来进行控制，包括医疗设备和汽车中的系统（例如，牵引力控制何时应用于汽车中？）。

此外，许多专家系统的外壳，例如Emycin，OPS，EXSYS和CLIPS，已成为行业标准。还开发了许多知识表示语言。如今，许多专家系统在后台进行工作以增强日常体验，例如在线购物车。

#### 神经计算

McCulloch和Pitts进行了神经计算的早期研究，因为他们试图了解动物神经系统的行为。他们的人工神经网络（ANN）模型有一个严重的缺点：它不包含学习机制。

弗兰克·罗森布拉特（Frank Rosenblatt）开发了一种称为“感知器学习规则”的迭代算法，用于在==单层网络==（所有神经元都直接连接到输入的网络）中找到合适的权重。Minsky和Papert声明某些问题无法通过单层感知器解决，例如异或（XOR）功能可能严重阻碍了这一新兴学科的研究。这项声明发布后，联邦政府立即大幅削减了对神经网络研究的资助。

在1980年代初，霍普菲尔德（Hopfield）的工作使该领域再次活跃起来。他的==异步网络模型（Hopfield网络）==使用能量函数来近似解决NP完全问题。

1980年代中期还见证了==反向传播==（通常称为*backprop*）的发现，这是一种适用于多层网络的学习算法。通常使用基于反向传播的网络来预测道琼斯平均值，并在光学字符识别系统中读取印刷的材料。

神经网络也用于控制系统。ALVINN是卡内基梅隆大学的一个项目，在该项目中，后向传播网络可感测高速公路并帮助操纵Navlab车辆。

这项工作的一个即时应用是，当车辆偏离高速公路车道时，警告因缺乏睡眠、过量饮酒或其他情况而受损的司机。展望未来，我们希望，有一天，类似的系统将驾驶车辆，使我们可以自由地阅读报纸和打电话，充分利用额外的空闲时间。

#### 进化计算

==遗传算法==通常被归类为进化计算。遗传算法使用概率和并行性来解决组合问题（也称为优化问题），这是约翰·霍兰德（John Holland）开发的一种方法。

但是，进化计算不仅与优化问题有关。Rodney Brooks曾任麻省理工学院计算机科学和AI实验室的负责人。他成功地创建了人类级人工智能的方法，他恰当地将其称为AI研究的圣杯，他放弃了对基于符号方法的依赖。后一种方法依赖于启发式和代表性范例的使用。

在他看来，==智能系统可以在多层中设计==，在多层中较高层依赖于底层。例如，如果你想建造一个能够避开障碍物的机器人，那么避障程序就会建立在一个较低的层次上，这个层次只负责机器人的移动。

brooks坚持认为，情报是通过特工与其环境的相互作用而产生的。他最为人所知的可能是在他的实验室里制造的类似昆虫的机器人，这些机器人体现了这种智能哲学，在这种哲学中，一群自主的机器人与它们的环境以及彼此之间进行互动。

#### 自然语言处理

如果我们希望构建智能系统，那么很自然地要求我们的系统具有语言理解功能。这是许多早期从业人员都很好理解的公理。Eliza是一个著名的早期应用程序，由麻省理工学院的计算机科学家Joseph Weizenbaum（与斯坦福大学精神病学家Kenneth Colby合作）开发。

Eliza的目的是模仿卡尔·罗杰斯学校的心理医生所扮演的角色。例如，如果用户键入“我感到很累”，则Eliza是向后传播应用程序，可以学习英语文本的正确发音。据称它能以95％的准确度发出英语声音。显然，由于英语单词的发音（例如，*rough*和*through*）与从其他语言（例如，*pizza* 和*fizzy*）派生的单词的内在矛盾而引起了问题。

特里·威诺格拉德（Terry Winograd）编写了另一个著名的程序，该程序以ETAOIN SHRDLU对中的第二组字母命名，这是线性印刷机上英语中最常用的字母。Winograd的程序可能会回答：“您说您感到疲倦。告诉我更多。” “对话”将以这种方式继续进行，而机器就对话的独创性而言几乎没有贡献。一位活着的心理分析师可能会以这种方式行事，希望患者能发现自己的真实（也许是隐藏的）感觉和挫败感。同时，Eliza仅使用模式匹配来伪装类似人的交互。

奇怪的是，维岑鲍姆对他的学生（以及整个公众）对与Eliza互动的强烈兴趣感到不安，尽管他们完全意识到Eliza只是一个程序。同时，Colby仍然致力于该项目，并继续编写了一个成功的程序，称为DOCTOR。

尽管Eliza对自然语言处理（NLP）的贡献很小，但==它是一种假装拥有可能是我们独特之处的最后遗迹的软件==，我们感受情感的能力。当人与机器之间的界线（例如：机器人）之间的界限变得不太清晰（可能在大约五十年之内）并且这些机器人会变得不那么凡人而更像不朽时，会发生什么？

最近，已经开发了包括Cog，Kismet和Paro在内的数种MIT机器人，它们具有伪装人类情感并引起与之互动的人的情感反应的超强能力。Turkle研究了这些机器人与养老院中的儿童和老年人之间的关系。涉及真正情感和关怀的关系。Turkle谈到可能需要重新定义“关系”一词， 以包括人们与这些所谓的“关系工件”的相遇。她仍然有信心，但是，这样的关系永远不会取代只能在每天面对死亡的人类之间发生的联系。

Winograd的Blocks World涉及一个能够实现各种目标的机械臂。例如，如果要求SHRDLU提起上面有一个小绿色块的红色块，它知道必须先移除绿色块，然后才能举起红色块。与Eliza不同，SHRDLU能够理解英语命令并做出适当响应。

HEARSAY是一个雄心勃勃的语音识别程序，它采用了黑板结构，其中用于语言的各个组成部分（如语音和短语）的独立知识源（代理）可以自由地进行交流。语法和语义都用于修剪不可能的单词组合。

HWIM（发音为“ whim”，是“听我的意思”的简称）项目使用增强的过渡网络来理解口语。它涉及旅行预算管理的词汇为1,000个单词。也许这个项目在范围上过于雄心勃勃，因为它的表现不如HEARSAY II。

解析对于这些自然语言程序的成功至关重要。SHRDLU采用了上下文无关的语法来帮助解析英语命令。上下文无关的语法提供了一种用于处理符号字符串的语法结构。但是，为了有效处理自然语言，还必须考虑语义。

一个解析树提供了组成句子的单词之间的关系。例如，许多句子可以分解为主语和谓语。主题可以分解为名词短语，然后是介词短语，依此类推。本质上，解析树给出了语义，即句子的含义。

这些==早期语言处理系统==中的每一个都在某种程度上利用了世界知识。然而，在1980年代后期，NLP取得进展的==最大绊脚石==是常识知识问题。例如，尽管在NLP和AI的特定领域构建了许多成功的程序，但这些程序经常被批评为微型世界，这意味着这些程序没有一般的，现实世界的知识或常识。例如，一个程序可能对特定情况有很多了解，例如在餐厅点菜，但是却不知道服务员是否还活着，或者他们通常是否会穿任何衣服。在过去的25年中，道格拉斯·莱纳特（Douglas Lenat）位于德克萨斯州奥斯汀市的MCC的研究人员正在建立最大的常识性知识库，以解决此问题。

NLP经历了一些有趣的发展。在初始阶段之后（如本节前面所述），NLP依靠统计数据来管理句子的语法分析树。Charniak描述了如何扩展无上下文语法（CFG），以使每个规则都有关联的概率。这些相关的概率可以从Penn树库中获取，该树库包含超过一百万个英语单词，这些单词已被手动解析，大部分来自《华尔街日报》。Charniak演示了这种统计方法如何成功地从《纽约时报》的首页获得了一个句子的解析（即使对于大多数人来说也没有什么小事）。

在NLP的发展的下一步涉及到深层学习架构称为RNNs，LSTMs和双向LSTMs，这是在第5章讨论的最新架构被称为变压器，这是由谷歌在2017年BERT开发基于变压器（以及“注意力”），并且是目前可用于解决NLP任务的最强大的开源系统之一。NLP的另一种方法涉及深度强化学习（在第6章中进行了简要讨论）。

#### 生物信息学

生物信息学是一门新兴学科，涉及计算机科学的算法和技术在分子生物学中的应用。它主要涉及==生物数据的管理和分析==。在结构基因组学中，人们尝试为每种观察到的蛋白质指定结构。自动发现和数据挖掘可以帮助实现这一目标。

Juristica和Glasgow证明了基于案例的推理如何帮助发现每种蛋白质的代表性结构。格拉斯哥，尤里西卡和罗斯特在AAAI关于AI和生物信息学的AAAI专刊中于2004年发表的调查文章中 指出：“可能最近生物信息学活动中增长最快的领域是微阵列数据分析。”

对于微生物学家来说，可获得的数据种类繁多，数量不胜其烦。他们被要求仅基于庞大的数据库来了解分子序列，结构和数据。许多研究人员认为，来自知识表示和机器学习的AI技术也将证明是有益的。

本章的下一部分将快速介绍AI的主要部分，包括机器学习和深度学习。

#### 人工智能的主要部分

本书的后续章节深入探讨了AI的各个重要部分，其中包括：

•  ML（机器学习）

•  DL（深度学习）

•  NLP（自然语言处理）

•  RL（强化学习）

•  DRL（深度强化学习）

传统的AI（二十世纪）基于规则集合，这导致了1980年代的专家系统。传统的AI也涉及LISP，它是由John McCarthy（1956年第一次正式AI会议的成员之一）创建的。

传统的AI主要是==与条件逻辑结合的一组规则==，这对于1980年代开发的功能强大的专家系统也是如此。但是，用于决策的基于规则的系统可能涉及成千上万条规则。甚至简单的对象也需要许多规则：尝试提出一组定义椅子，桌子甚至只是苹果的规则。传统的AI有一些明显的限制，主要是因为需要的规则数量。

##### 机器学习

大约在20世纪中叶，机器学习（人工智能的一个子集）主要依靠数据来优化和“学习”如何执行任务，通常伴随着新的或改进的算法，例如线性回归，k-NN，决策树，随机森林和SVM；除了线性回归，其他所有算法都是分类器。

如您所见，机器学习是一个充满活力的领域，其中包括其他子领域。

由于数据（而非规则）在机器学习中非常重要，因此通常是以下类型之一：

•  监督学习（大量标记数据）

•  半监督学习（大量带有部分标签的数据）

•  无监督学习：大量数据，聚类

•  强化学习：试用，反馈和改进

根据Coursera的联合创始人Ng的说法，“所有机器学习中的99％受监督。”

除了分类数据外，机器学习算法还可以分类为以下主要类型：

•  分类器（用于图像，垃圾邮件，欺诈等）

•  回归（股票价格，房屋价格等）

•  聚类（无监督分类器）

##### 深度学习

机器学习的一个重要子领域是深度学习，它也起源于二十世纪中叶。深度学习架构依赖于感知器作为神经网络的基础，通常涉及大型或海量数据集。这样的体系结构还涉及启发式方法和经验结果。如今，对于某些图像分类，深度学习可以超越人类。

虽然机器学习涉及MLP（多层感知器），但深度学习引入了具有新算法和新架构（例如卷积神经网络，RNN和LSTM）的深度神经网络。

##### 强化学习

强化学习（也是机器学习的一个子集）涉及试错法，以最大程度地提高所谓代理的报酬。深度强化学习将深度学习与强化学习的优势结合在一起。特别是，强化学习中的主体被神经网络取代。

深度强化学习在许多不同领域中都有应用，其中最受欢迎的三个是：

•  游戏（围棋，国际象棋等）

•  机器人技术

•  NLP

在游戏中使用强化学习的一些著名且成功的示例包括：

•  Alpha Go（混合RL）

•  Alpha Zero（完整的RL）

•  通常涉及贪婪算法

•  深度RL：将深度学习和RL相结合

##### 机器人技术

机器人以多种方式进入了我们的个人和职业生活，包括：

•  手术（协助外科医生）

•  放射学（检测癌症）

•  药物管理

•  宗教比较理论

•  法律/房地产/军事/科学

•  喜剧（包括单口相声）

•  音乐（指挥乐队）

•  餐厅（美食）

•  协调的舞蹈队

•  许多其他领域

机器人卡车司机正在转移工作，但它们也有优势：他们唯一的成本就是维护机器。此外，机器人并不会像人类那样分散注意力，他们不会从事会导致事故的活动，也不需要薪水或任何休假时间。尽管机器人取得了令人惊讶的成就，但《星际迷航》的角色数据仍然只是一个梦想。

NLP是计算机科学和AI领域，涉及计算机与人类语言之间的交互。在早期，NLP涉及基于规则的技术或统计技术。NLP和机器学习可以处理/分析大量自然语言数据，其中计算机程序执行该处理。

机器学习技术可以解决许多NLP任务。NLP涉及的一些感兴趣的领域包括：

•  语言之间的翻译

•  从文本中查找有意义的信息

•  文件汇总

•  检测仇恨言论

尽管机器学习具有所有进步和优势，等等，但仍有一些问题需要解决。一个问题是职业偏见：一个人工智能系统推断白人是医生，白人是家庭主妇。另一个问题涉及发现性别偏见。例如，在Wikipedia（大约2018年）中，其18％的传记是女性，而Wikipedia的编辑中84％至90％是男性。

在下一篇文章中分析的另一个问题涉及数据偏差与算法偏差：

*https://www.forbes.com/sites/charlestowersclark/2018/09/19/can-we-*

#### 使人工智慧负责

最后，还有AI与伦理之间的相互作用问题，其中包括一些发人深省的问题（例如失业和机器人权利）。以下文章包含道德问题的详细列表：

*https://www.weforum.org/agenda/2016/10/top-10-ethical-issues-inartificial-intelligence/*



### 第二章 机器学习

本章介绍了机器学习中的众多概念，例如特征选择，特征工程，数据清理，训练集和测试集。

本章的第一部分简要讨论了机器学习和准备数据集通常所需的步骤顺序。这些步骤包括可以使用各种算法执行的“功能选择”或“功能提取”。

第二部分描述了您可能会遇到的数据类型，数据集中的数据可能出现的问题以及如何纠正它们。当您执行训练步骤时，您还将了解“伸出”和“折叠”之间的区别。

本章的第三部分简要讨论了线性回归所涉及的基本概念。尽管线性回归是200年前开发的，但该技术仍然是解决（尽管很简单）统计和机器学习问题的“核心”技术之一。实际上，在Python和TensorFlow中实施了一种称为“均方误差”（MSE）的技术，该技术用于为2D平面（或更高尺寸的超平面）中的数据点找到最合适的线，以最大程度地减少所谓的稍后讨论的“成本”功能。

本章的第四部分包含使用NumPy中的标准技术进行线性回归任务的其他代码示例。因此，如果您熟悉此主题，则可以快速浏览本章的前两节。第三部分介绍如何使用Keras求解线性回归。

要记住的一点是，提到了一些算法而没有深入研究它们的细节。例如，与监督学习有关的部分包含一个算法列表，这些算法将在与分类算法有关的部分的本章后面出现。列表中以粗体显示的算法是本书中更感兴趣的算法。在某些情况下，下一章将详细讨论算法。否则，您可以在线搜索有关本书中未详细讨论的算法的其他信息。

#### 什么是机器学习？

从高级的角度讲，机器学习是AI的一个子集，它可以解决“传统”编程语言无法完成或过于繁琐的任务。电子邮件的垃圾邮件过滤器是机器学习的早期示例。机器学习通常会取代旧算法的准确性。

尽管机器学习算法多种多样，但可以说数据比所选算法更重要。数据可能会出现许多问题，例如数据不足，数据质量差，数据不正确，数据丢失，不相关的数据，重复的数据值等等。在本章的后面，您将看到解决许多与数据有关的问题的技术。

如果您不熟悉机器学习术语，则数据集是数据值的集合，可以采用CSV文件或电子表格的形式。每列称为功能，每行是一个数据点，其中包含每个功能的一组特定值。如果数据集包含有关客户的信息，则每一行都与特定客户有关。

##### 机器学习的类型

您将遇到三种主要的机器学习类型（也可以将它们组合）：

•  监督学习

•  无监督学习

•  半监督学习

==监督学习==意味着数据集中的数据点具有标识其内容的标签。例如，MNIST数据集包含28x28个PNG文件，每个文件都包含一个手绘数字（即0到9，包括0和9）。每个数字为0的图像的标签均为0；每个数字为1的图像都带有标签1；所有其他图像均根据这些图像中显示的数字进行标记。

再举一个例子，泰坦尼克号数据集中的列是有关乘客的特征，例如他们的性别，舱位，机票价格，乘客是否幸存等等。每行包含有关单个乘客的信息，如果该乘客幸存，则包括值1。MNIST数据集和Titanic数据集涉及分类任务：目标是基于训练数据集训练模型，然后预测测试数据集中每一行的类别。

通常，用于分类任务的数据集具有少量可能的值：0到9范围内的九个数字之一，四只动物（狗，猫，马，长颈鹿）之一，两个值（幸存或灭亡）之一，已购买还是未购买）。根据经验，如果结果数量可以在下拉列表中“合理地”显示，则可能是分类任务。

对于包含房地产数据的数据集，每行包含有关特定房屋的信息，例如卧室的数量，房屋的平方英尺，浴室的数量，房屋的价格等。在此数据集中，房屋价格是每一行的标签。请注意，可能的价格范围太大，无法在下拉列表中“合理地容纳”。房地产数据集涉及回归任务：目标是基于训练数据集训练模型，然后预测测试数据集中每个房屋的价格。

==无监督学习==涉及未标记的数据，通常是聚类算法的情况（稍后讨论）。下面列出了一些涉及聚类的重要无监督学习算法：

•  k-Means

•  层次聚类分析（HCA）

•  期望最大化

下面列出了一些涉及降维的重要无监督学习算法（稍后将详细讨论）：

•  PCA（主成分分析）

•  内核PCA

•  LLE（局部线性嵌入）

====•  t-SNE（t分布随机邻居嵌入）

还有另一项非常重要的无监督任务称为==异常检测==。此任务与欺诈检测和检测异常值有关（稍后将详细讨论）。

==半监督学习==是监督学习和无监督学习的结合：有些数据点被标记，有些没有标记。一种技术涉及使用标记的数据对未标记的数据进行分类（即标记），然后可以应用分类算法。

#### 机器学习算法的类型

机器学习算法主要有三种类型：

•  回归（例如：线性回归）

•  分类（例如：k最近邻）

•  聚类（例如：kMeans）

==回归==是一种预测数值量的监督学习技术。回归任务的一个示例是预测特定股票的价值。请注意，此任务不同于预测明天（或其他某个未来时间段）特定股票的价值是否会增加或减少。回归任务的另一个示例涉及预测房地产数据集中的房屋成本。这两个任务都是回归任务的示例。

机器学习中的回归算法包括线性回归和广义线性回归（在传统统计中也称为多元分析）。

==分类==也是一种有监督的学习技术，但它是用于预测分类数量的。分类任务的一个示例是检测垃圾邮件的发生，欺诈或确定PNG文件（例如MNIST数据集）中的数字。在这种情况下，数据已被标记，因此您可以将预测与分配给给定PNG的标签进行比较。

机器学习中的分类算法包括以下算法列表（在下一章中将对其进行详细讨论）：

•  决策树（单个树）

•  随机森林（多棵树）

•   kNN（k最近邻居）

•  Logistic回归（尽管其名称）

•  朴素贝叶斯

•  SVM（支持向量机）

一些机器学习算法（例如SVM，随机森林和kNN）支持回归和分类。对于SVM，此算法的scikit-learn实现提供两个API：用于分类的SVC和用于回归的SVR。

前面的每个算法都涉及一个在数据集上训练的模型，然后使用该模型进行预测。相比之下，随机森林由多个独立的树组成（数目由您指定），并且每个树都对要素的值进行预测。如果特征是数字，则采用均值或众数（或执行其他计算）以确定“最终”预测。如果特征是分类的，则使用模式（即，最频繁出现的类）作为结果；否则，使用默认模式。如果是平局，您可以随机选择其中之一。

==聚类==是一种用于将相似数据分组在一起的无监督学习技术。群集算法将数据点放置在不同的群集中，而不知道数据点的性质。将数据分成不同的群集后，可以使用SVM（支持向量机）算法进行分类。

机器学习中的聚类算法包括以下内容（其中一些是彼此的变体）：

•  k-Means

•  平均移位

•  层次聚类分析（HCA）

•  期望最大化

请记住以下几点。首先，k-Means中的k是一个超参数，并且它通常是一个奇数到两个类之间的关系避免。接下来，均值漂移算法是一个变化的k均值算法，它不要求你指定K值。实际上，均值漂移算法确定了最佳的簇数。

但是，该算法不能很好地用于大型数据集。

##### 机器学习任务

除非您已经清理过数据集，否则您需要检查数据集中的数据以确保其处于适当的状态。数据准备阶段包括1）检查行（“数据清理”）以确保它们包含有效数据（这可能需要特定领域的知识），以及2）检查列（特征选择或特征提取）以确定是否可以保留只有最重要的列。

下面显示了机器学习任务序列的高级列表（可能不需要其中的一些）：

•  获取数据集

•  数据清理

•  功能选择

•  降维

•  算法选择

•  训练与测试数据

•  训练模型

•  测试模型

•  微调模型

•  获取模型的指标

首先，您显然需要获取任务的数据集。在理想情况下，该数据集已经存在；否则，您需要从一个或多个数据源（例如CSV文件，关系数据库，no-SQL数据库，Web服务等）中剔除数据。

其次，您需要执行==数据清理==，可以通过以下技术进行清理：

•  缺失价值率

•  低方差滤波器

•  高相关滤波器

通常，数据清理涉及检查数据集中的数据值，以便解决以下一个或多个问题：

•  修正错误的值

•  解决重复值

•  解决缺失值

•  决定如何处理异常值

如果数据集的缺失值太多，请使用缺失值比率技术。在极端情况下，您可能可以删除具有大量缺失值的要素。使用低方差过滤器技术从数据集中识别和删除具有恒定值的要素。使用“高相关性”过滤器技术查找高度相关的要素，这会增加数据集中的多重共线性：可以从数据集中删除此类要素（但在执行此操作之前，请与您的领域专家联系）。

根据您的背景和数据集的性质，您可能需要与一位领域专家合作，该专家对数据集的内容有深刻的了解。

例如，您可以使用统计值（平均值，众数等）将不正确的值替换为合适的值。重复值可以类似的方式处理。您可以在数字列中用零，最小值，平均值，众数或最大值替换缺失的数值。您可以使用分类列的模式替换缺少的分类值。

如果数据集中的行包含一个离群值，则可以选择三个选项：

•  删除行

•  保持排

•  将异常值替换为其他值（平均值？）

当数据集包含异常值时，您需要基于特定于给定数据集的领域知识做出决策。

假设数据集包含与库存相关的信息。如您所知，1929年股市崩盘，您可以将其视为异常值。这种情况很少见，但可以包含有意义的信息。顺便说一句，财富在20有些家庭源届世纪是基于购买的股票大量是大萧条时期非常低的价格。

#### 特征工程，选择和提取

除了创建数据集并“清理”其值外，您还需要检查该数据集中的要素，以确定是否可以减少该数据集的维数（即列数）。这样做的过程涉及三种主要技术：

•  特征工程

•  特征选择

•  特征提取（又称特征投影）

==特征工程==是根据现有特征的组合确定一组新特征的过程，以便为给定任务创建有意义的数据集。即使在相对简单的数据集的情况下，此过程通常也需要领域专家。特征工程可能是乏味且昂贵的，并且在某些情况下，您可能会考虑使用自动特征学习。创建数据集后，最好执行特征选择或特征提取（或同时执行这两项操作）以确保您拥有高质量的数据集。

==特征选择==也称为变量选择，属性选择或变量子集选择。特征选择涉及选择数据集中相关特征的子集。本质上，特征选择涉及选择数据集中的“最重要”特征，这提供了以下优点：

•  减少训练时间

•  更简单的模型更易于解释

•  避免维数的影响

•  由于减少了过拟合（“减少差异”），因此泛化效果更好

特征选择技术通常用于特征较多且样本（或数据点）相对较少的域中。请记住，低价值功能可能是多余的，也可能是无关的，这是两个不同的概念。例如，一个相关的功能与另一个高度相关的功能结合使用时可能是多余的。

特征选择可以涉及三种策略： 过滤器策略（例如，信息增益），包装器策略（例如，以准确性为指导的搜索）和嵌入式策略（预测误差用于确定在开发模型时是否包含或排除特征）。另一个有趣的一点是，特征选择对于回归以及分类任务也可能有用。

==特征提取==从产生原始特征组合的函数中创建新特征。相反，特征选择涉及确定现有特征的子集。

特征选择和特征提取均会导致给定数据集的降维，这是下一部分的主题。

#### 降维

降维指的是减少数据集中特征数量的算法：因此称为“降维”。正如您将看到的，有许多可用的技术，它们涉及特征选择或特征提取。

此处列出了使用特征选择来执行降维的算法：

•  后向功能消除

•  转发功能选择

•  因素分析

•  独立成分分析

此处列出了使用特征提取执行降维的算法：

•  主成分分析（PCA）

•  非负矩阵分解（NMF）

•  内核PCA

•  基于图的内核PCA

•  线性判别分析（LDA）

•  广义判别分析（GDA）

•  自动编码器

以下算法结合了特征提取和降维功能：

•  主成分分析（PCA）

•  线性判别分析（LDA）

•  典型相关分析（CCA）

•  非负矩阵分解（NMF）

这些算法可以在数据集上使用聚类或其他算法（例如kNN）之前的预处理步骤中使用。

另一组算法涉及基于投影的方法，其中包括t分布随机邻居嵌入（t-SNE）以及UMAP。

本章讨论PCA，您可以执行在线搜索以查找有关其他算法的更多信息。

##### PCA

主成分是数据集中初始变量的线性组合的新成分。此外，这些组件是不相关的，最有意义或最重要的信息包含在这些新组件中。

PCA有两个优点：1）由于功能少得多而减少了计算时间； 2）最多有三个组件时可以绘制组件图形。如果您有四个或五个组件，则将无法直观地显示它们，但是您可以选择三个组件的子集进行可视化，并可能对数据集有更多了解。

==PCA使用方差作为信息的度量==：方差越大，组成部分越重要。实际上，只是稍微向前跳一下：PCA确定协方差矩阵的特征值和特征向量（稍后讨论），并构造一个新矩阵，其列为特征向量，根据最左边列中的最大特征值从左到右排序，直到最右边的特征向量也具有最小的特征值。

##### 协方差矩阵

提醒一下，称为随机变量X方差的统计量定义如下：
$$
variance(X)=[\sum(X-Xbar)^2]/n
$$
协方差矩阵C是$n*n$矩阵，其主对角线上的值是变量X1，X2，...的方差。。。，Xn。C的其他值是每对变量Xi和Xj的协方差值。 

变量X和Y的协方差公式是变量方差的概括，公式如下所示：
$$
covariance(X,Y)=[\sum(X-Xbar)*(Y-Ybar)]/n
$$
注意，您可以反转项乘积的顺序（乘法是可交换的），因此协方差矩阵C是对称矩阵：
$$
covariance(X,Y)=covariance(Y,X)
$$
PCA计算协方差矩阵A的特征值和特征向量。

#### 使用数据集

除了清除数据外，还需要执行其他几个步骤，例如选择训练数据与测试数据，以及决定在训练过程中使用“保留”还是交叉验证。

在后续部分中提供了更多详细信息。

##### 训练数据与测试数据

在完成了本章前面介绍的任务（即数据清理和降维）之后，您就可以将数据集分为两部分了。第一部分是==训练集==，用于训练模型，第二部分是==测试集==，用于“推理”（进行预测的另一个术语）。确保您符合以下测试集准则：

•  集合足够大，可以产生具有统计意义的结果

•  代表整个数据集

•  切勿训练测试数据

•  不要测试训练数据

##### 什么是交叉验证？

==交叉验证==的目的是使用不重叠的测试集测试模型，该测试集的执行方式如下：

•  步骤1）将数据拆分为大小相等的k个子集

•  步骤2）选择一个子集进行测试，其余子集用来训练

•  步骤3）对其他k-1个子集重复步骤2

此过程称为==k交叉验证==，总的误差估计是误差估计的平均值。一种评估的标准方法是十倍交叉验证。大量实验表明，10个子集是获得准确估计的最佳选择。实际上，您可以重复十次交叉验证十次，然后计算结果的平均值，这有助于减少差异。

下一部分讨论正则化，如果您对TF 2代码主要感兴趣，则正则化是一个重要但可选的主题。如果您打算精通机器学习，则需要学习正则化。

#### 什么是正则化？

正则化有助于解决==过拟合问题==，当模型在训练数据上表现良好但在验证或测试数据上表现不佳时，就会发生过拟合问题。

正则化通过将==惩罚项==添加到成本函数来解决此问题，从而使用该惩罚项控制模型的复杂性。

正则化通常可用于：

•  大量变量

•  观测值/变量数的比率低

•  高多重共线性

正则化主要有两种类型：L1正则化（与MAE或差的绝对值有关）和L2正则化（与MSE或差的平方有关）。通常，L2的性能优于L1，并且在计算方面非常有效。

##### 机器学习和特征缩放

特征缩放标准化了数据功能范围。该步骤在数据预处理步骤中执行，部分原因是梯度下降得益于特征缩放。

假设数据符合标准正态分布，==标准化==包括减去平均值并除以每个数据点的标准差，从而得出==N（0,1）正态分布。==

##### 数据规范化与标准化

数据标准化是一种线性缩放技术。假设数据集具有值{X1，X2，。。。，Xn }以及以下术语：

Minx = Xi值的最小值

Maxx = Xi值的最大值

现在，如下计算一组新的Xi值：

Xi =（Xi – Minx）/ [Maxx – Minx]

现在新的Xi值被缩放到0到1之间。

#### 偏差-方差权衡

机器学习中的==偏差==可能是由于学习算法中的错误假设导致的错误。高偏差可能导致算法错过特征与目标输出之间的相关关系（拟合不足）。由于“嘈杂”的数据，不完整的功能集或有偏差的训练样本，可能会导致预测偏差。

偏差引起的误差是模型的预期（或平均）预测与要预测的正确值之间的差。多次重复模型构建过程，每次都收集新数据，并进行分析以生成新模型。由于基础数据集具有一定程度的随机性，因此所得模型具有一定范围的预测。偏差衡量了这些模型从正确值得出的预测的程度。

机器学习中的==方差==是平均值的平方偏差的期望值。高方差可能导致算法对训练数据中的随机噪声进行建模，而不是对预期的输出进行建模（又称过度拟合）。

==向模型添加参数会增加模型的复杂性，增加方差并减少偏差。处理偏差和方差就是处理拟合不足和拟合过度。==

由方差引起的误差是给定数据点的模型预测的方差。和以前一样，重复整个模型构建过程，方差是模型的不同“实例”之间给定点的预测变化的程度。

#### 测量模型的指标

R方是最常用的指标之一，它衡量数据与拟合的回归线（回归系数）的接近程度。R方值始终是0到100%之间的百分比。值0％表示该模型无法解释响应数据均值附近的变化。值100％表示该模型解释了响应数据均值附近的所有可变性。通常，==较高的R平方值表示较好的模型==。

##### R方的局限性

尽管较高的R方值是首选，但不一定总是好的值。同样，低R方值并不总是坏的。例如，用于预测人类行为的R方值通常小于50％。而且，R方不能确定系数估计和预测是否有偏差。另外，R方值不表示回归模型是否足够。因此，对于一个好的模型，可能有一个较低的R方值，而对于一个拟合程度较差的模型，可能有一个较高的R方值。结合残差图，其他模型统计信息和主题领域知识来评估R方值。

##### 混淆矩阵

最简单的形式是，混淆矩阵（也称为错误矩阵）是一种列式表，具有两行两列，其中包含错误肯定，错误否定，正确肯定和正确否定。2x2混淆矩阵中的四个条目可以标记如下：

TP：正确肯定

FP：错误肯定

TN：正确否定

FN：错误否定

混淆矩阵的对角线值是正确的预测，而反对角线值是不正确的预测。通常，较低的FP值优于FN值。例如，FP指示健康的人被错误地诊断为疾病，而FN指示不健康的人被错误地诊断为健康。

##### 精度与准确度与召回率

2x2混淆矩阵具有四个条目，分别代表正确和错误分类的各种组合。根据上一节中的定义，精度，准确度和召回率的定义由以下公式给出：(==精度的公式书中给错了？precision = TP/(TN + FP)==)
$$
精度：precision=TP/(TP+FP)\\
准确度：accuracy=(TP+TN)/(P+N)\\
召回率：recall=TP/(TP+FN)
$$
准确度可能是不可靠的指标，因为它会在不平衡的数据集中产生误导性的结果。当不同类别中的观察数显着不同时，它==对错误肯定和错误否定分类都具有同等的重要性==。例如，将癌症声明为良性要比错误地告知患者他们正在患癌症要糟糕。不幸的是，这两种情况的准确度并没有区别。

请记住，混淆矩阵可以是$n*n$矩阵，而不仅仅是2x2矩阵。例如，如果一个类有5个可能的值，则混淆矩阵为5x5矩阵，主对角线上的数字为“真正”结果。

##### ROC曲线

ROC（接收机工作特性）曲线是绘制TPR（即正确肯定率（即召回率）与FPR（即错误肯定率））的曲线。请注意，TNR（真实阴性率）也称为特异性。

以下链接包含使用SKLearn和Iris数据集的Python代码示例，以及用于绘制ROC的代码：

*https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html*

以下链接包含用于绘制ROC的各种Python代码示例：

*https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-*

#### 其他有用的统计术语

机器学习依靠大量统计量来评估模型的有效性，此处列出其中一些：

•  RSS

•  TSS

•  $R^2$

•  F1-score

•  p值

RSS，TSS和$R^2$的定义如下所示，其中$y_{1}$是最佳拟合线上的点的y坐标，而$y_{2}$是数据集中这些点的y值的平均值：

残差平方和$RSS =(y-y_{1})^2$

托尔平方和 $TSS=(y-y_{2})^2 $

$R ^ 2 = 1 - RSS / TSS$

##### 什么是F1-score？

F1-score是测试准确性的量度，被定义为==准确性和召回率的调和平均值==。以下是相关公式，其中p是精度，r是召回率，具体定义前文有交代。

$F1-score=\frac{2}{\frac{1}{r}+\frac{1}{p}} = \frac{2*p*r}{p+r}$

==？？？The best value of an F1 score is 0 and the worse value is 0.==一般来说F1-score越大越好。请记住，F1分数倾向于用于分类问题，而$R^2$值通常用于回归任务（例如线性回归）。

##### 什么是p值？

如果p值足够小（<0.005），则p值用于拒绝无效假设，这表示更高的显著性。回想一下，零假设声明一个因变量（比如y）和一个独立变量（比如x）之间没有相关性。p的阈值通常为1%或5%。

没有一个简单的公式来计算p值，p值总是介于0和1之间。事实上，p值是用来评估所谓“零假设”的统计量，它们是通过p值表或电子表格/统计软件计算出来的。

#### 什么是线性回归？

线性回归的目标是找到“代表”数据集的最佳拟合线。请记住两个关键点。首先，==最佳拟合线不一定穿过数据集中的所有（或大部分）点==。最佳拟合线的目的是使该线与数据集中的点的垂直距离最小。其次，==线性回归并不能确定最适合的多项式==：后者需要找到通过数据集中许多点的更高阶多项式。

而且，平面中的数据集可以包含两个或多个位于同一垂直线上的点，也就是说，这些点具有相同的x值。然而，一个函数不能穿过这样的一对点：如果两个点（X1，Y1）和（X2，Y2）具有相同的x值，则它们必须具有相同的Y值（即，Y1 = Y2）。另一方面，一个函数可以在同一水平线上有两个或多个点。

现在考虑一个散点图，该散点图上有许多点，这些点被“聚集”成细长的云状形状：最合适的线可能只与有限数量的点相交（实际上，最合适的线可能不相交相交的任何点）。

要记住的另一种情况：假设数据集包含位于同一条线上的一组点。例如，假设x值在集合{1,2,3，...，10}中，而y值在集合{2,4,6，...，20}中 。那么，最拟合线的方程为$y = 2*x + 0 $。在这种情况下，所有点都是共线的，也就是说它们位于同一条线上。

##### 线性回归与曲线拟合

假设数据集由（x，y）形式的n个数据点组成，并且这些数据点中没有两个具有相同的x值。然后根据一个著名的数学结果，有一个小于或等于n-1的度多项式经过这n个点（如果您真的很感兴趣，可以在在线文章中找到此陈述的数学证明） ）。例如，一条线是一个次数为1的多项式，它可以与平面中的任意一对非垂直点相交。对于平面中任何三点（并非全部在同一条线上），都有一个通过这些点的二次方程。

另外，有时可以使用低阶多项式。例如，考虑100个点的集合，其中x值等于y值：在这种情况下，线y = x（是一阶多项式）穿过所有100个点。

然而，请记住，到其上线“表示”的程度在平面中的点的集合取决于如何紧密这些点可以通过一条线，这是由测量来近似方差的点的（方差是一个统计数量）。点越共线，方差越小。相反，点越“分散”，方差越大。

##### 什么时候解是精确值？

尽管基于统计的解决方案为线性回归提供了封闭形式的解决方案，但神经网络却提供了近似的解决方案。这是由于以下事实：用于线性回归的机器学习算法包含一系列“收敛”到最佳值的近似值，这意味着机器学习算法会生成精确值的估计值。例如，对于2D平面的一组点而言，最佳拟合线的斜率m和y截距b在统计上具有封闭形式的解决方案，但只能通过机器学习算法来近似（确实存在例外，但他们很少见）。

请记住，即使“传统”线性回归的封闭式解决方案提供了m和b的精确值，有时您也只能使用精确值的近似值。例如，假设最佳拟合线的斜率m等于3的平方根，y截距b是2的平方根。如果您打算在源代码中使用这些值，则只能使用这两个数字的近似值。在相同的情况下，神经网络计算近似值m和b，而不管是否确切值的m和b是无理数，有理数或整数值。但是，机器学习算法更适合于复杂的非线性多维数据集，这超出了线性回归的能力。

举一个简单的例子，假设线性回归问题的闭式解产生m和b的整数或有理值。具体来说，让我们假设闭合形式的解分别得出最佳拟合线的斜率和y截距的值2.0和1.0。该行的等式如下所示：

$y = 2.0 * x + 1.0$

==但是==，来自训练神经网络的相应解决方案可能分别为斜率m和y轴截距b产生值2.0001和0.9997 ，作为最佳拟合线的m和b值。请始终牢记这一点，尤其是在训练神经网络时。

##### 什么是多元分析？

多元分析将欧氏平面中线的方程式推广到更高的维度，称为超平面而不是线。广义方程具有以下形式：

$y = w_{1} * x_{1} + w_{2} * x_{2} +。。。+ w_{n} * x_{n} + b$

在2D线性回归的情况下，您只需要找到斜率m和y轴截距b的值，而在多变量分析中，您需要找到$w_{1}$，$w_{2}$，...，$w_{n}$。请注意，多元分析是统计中的一个术语，在机器学习中，它通常被称为“广义线性回归”。

请记住，本书中与线性回归有关的大多数代码示例都涉及欧几里得平面中的2D点。

#### 其他类型的回归

线性回归找到了“代表”数据集的最佳拟合线，但是如果平面中的线与数据集的拟合度不高怎么办？使用数据集时，这是一个相关的问题。

线性回归的一些替代方法包括==二次方程，三次方程或高阶多项式==。但是，这些替代方案需要权衡取舍，我们将在后面讨论。

另一种可能性是一种涉及==分段线性函数==的混合方法，该函数包括一组线段。如果连接了连续的线段，则它是分段线性连续函数；否则，它是分段线性不连续函数。

因此，给定平面中的一组点，回归涉及解决以下问题：

•  哪种类型的曲线很好地拟合了数据？我们怎么知道？

•  其他类型的曲线是否更适合数据？

•  “最合适”是什么意思？

检查线是否适合数据的一种方法涉及视觉检查，但是这种方法不适用于二维以上的数据点。而且，这是一个主观决定，本章稍后将显示一些示例数据集。通过对数据集的直观检查，您可能会认为二次多项式或三次多项式（甚至更高阶）的多项式有可能更适合数据。但是，目视检查可能仅限于2D平面或三维上的点。

让我们推迟非线性场景，让我们假设一条线将非常适合数据。有一种众所周知的技术可以找到此类数据集的“最佳拟合”线，该技术涉及==最小化均方误差==（MSE），我们将在本章稍后讨论。

下一节将快速回顾平面中的线性方程，并提供一些图像说明线性方程的示例。



#### 在平面中处理线（可选）

这一节介绍如何在二维平面中绘制直线，由于过于简单，这里不做赘述。

#### 使用NumPy和Matplotlib绘制散点图（1）

Listing 2.1显示了np_plot1.py的内容，它使用Numpy randn（）API生成一个数据集，然后Matplotlib中的scatter（）API绘制数据集中的点。需要注意的一个细节是，所有相邻的水平值都是相等的间距，而垂直值基于线性方程加上“扰动”值。本章的其他代码示例中使用了这种“==扰动技术==”（不是标准术语），以便在绘制点时添加稍微随机的效果。该技术的优势在于预先知道了m和b的最佳拟合值，因此我们无需猜测它们的值。

**Listing 2.1: np_plot1.py**

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.random.randn(15,1)
y = 2.5*x + 5 + 0.2*np.random.randn(15,1)
print("x:",x)
print("y:",y)
plt.scatter(x,y)
plt.show()
```

Listing 2.1包含两个import语句，然后使用0到1之间的15个随机数初始化数组变量x。

接下来，将数组变量y分为两部分：第一部分是线性方程2.5 * x + 5，第二部分是基于随机数的“扰动”值。因此，数组变量y模拟了一组非常接近线段的值。

此技术用于模拟线段的代码样本中，然后训练为最适合的线近似m和b的值。显然，我们已经知道最佳拟合线的方程式，该技术的目的是将斜率m和y截距b的训练值与已知值（在本例中为2.5和5）进行比较。Listing 2.1的部分输出在这里：

![image-20201113131524595](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113131524595.png)

![image-20201113131550320](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113131550320.png)

##### 为什么“扰动技术”有用

您已经了解了如何使用“扰动技术”，考虑了具有以下定义在Python数组变量X和Y中点的数据集：

X = [0,0.12,0.25,0.27,0.38,0.42,0.44,0.55,0.92,1.0]

Y = [0,0.15,0.54,0.51,0.34,0.1,0.19,0.53,1.0,0.58]

如果您需要为上述数据集找到最佳拟合线，您如何猜测斜率m和y截距b的值？在大多数情况下，您可能无法猜测它们的值。不过，“扰动技术”使您可以“晃动”预先指定了斜率m的值（以及可选的y截距b的值）的线上的点。

请记住，“扰动技术”仅在引入小的随机值（不会导致m和b的值不同）时起作用。

#### 使用NumPy和Matplotlib绘制散点图（2）

Listing 2.1中的代码将随机值分配给变量x，而硬编码值分配给斜率m。而y值是x值的硬编码倍数，加上通过“扰动技术”计算的随机值。因此，我们不知道y截距b的值。

在本节中，trainX的值基于np.linspace（）API，而trainY的值涉及上一节中介绍的“扰动技术”。

本示例中的代码仅打印trainX和trainY的值，它们对应于欧几里得平面中的数据点。Listing 2.2显示了np_plot2.py的内容，该内容说明了如何在NumPy中模拟线性数据集。

**Listing 2.2: np_plot2.py**

```python
import numpy as np

trainX = np.linspace(-1, 1, 11)
trainY = 4*trainX + np.random.randn(*trainX.shape)*0.5
print("trainX: ",trainX)
print("trainY: ",trainY)
```

Listing 2.2通过NumPy linspace（）API初始化NumPy数组变量trainX，然后是由两部分定义的数组变量trainY。第一部分是线性项4 * trainX，第二部分涉及“扰动技术”，它是随机生成的数字。Listing 2.2的输出在这里：

![image-20201113133019958](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113133019958.png)

#### 使用NumPy和Matplotlib的二次散点图

Listing 2.3显示了np_plot_quadratic.py的内容，该内容说明了如何在平面上绘制二次函数。

**Listing 2.3: np_plot_quadratic.py**

```python
import numpy as np
import matplotlib.pyplot as plt
#see what happens with this set of values:
#x = np.linspace(-5,5,num=100)
x = np.linspace(-5,5,num=100)[:,None]
y = -0.5 + 2.2*x +0.3*x**2 + 2*np.random.randn(100,1)
print("x:",x)
plt.plot(x,y)
plt.show()
```

Listing 2.3使用通过np.linspace（） API生成的值来初始化数组变量x，在这种情况下，它是一组在-5到5之间的100个等间隔的十进制数字。注意在初始化x时的代码片段[：，None]，这会产生一个元素数组，每个元素都是一个由单个数字组成的数组。

变量y分为两部分：第一部分是$-0.5 + 2.2 * x + 0.3 * x^2$的二次方程，第二部分是基于随机数的“扰动”值。因此，数组变量y模拟一组近似于二次方程的值。Listing 2.3的输出在这里：

![image-20201113133716602](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113133716602.png)

![image-20201113133759383](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113133759383.png)

#### 均方差（MSE）公式

简单来说，==MSE是实际y值与预测y值之差的平方和除以点的个数==。注意，预测的y值是每个点实际位于最拟合线上时所具有的y值。

尽管MSE在线性回归中很流行，但还有其他误差类型可用，其中一些误差类型将在下一节中进行简要讨论。

##### 误差类型列表

尽管本书中我们仅讨论用于线性回归的MSE，但是还有其他类型的误差公式可用于线性回归，其中一些在此处列出：

•  MSE

•  RMSE

•  RMSPROP

•  MAE

MSE是上述误差类型的基础。例如，RMSE是“==均方根误差==”，它是MSE的平方根。

MAE是“==绝对误差均值==”，它是每个y值的差的绝对值的总和（而非每个y值的差的平方），然后除以项数。

RMSProp优化器利用最近渐变的大小来规范化渐变。具体来说，RMSProp在RMS（均方根）梯度上保持一个移动平均值，然后将该项除以当前梯度。

虽然计算MSE的导数更容易，但MSE更容易受到异常值的影响，而MAE则不太容易受到异常值的影响。原因很简单：平方项显著大于项的绝对值。例如，如果差异项为10，则将平方项100添加到MSE，而仅将10添加到MAE。同样，如果差项为-20，则将平方项400添加到MSE，而仅将20（这是-20的绝对值）添加到MAE。

##### 非线性最小二乘法

在预测房价这样包含大量数据的数据集时，诸如线性回归或随机森林之类的技术可能会使模型==过拟合==来满足最大值以减少像平均绝对误差这样的数值。

在这种情况下，您可能需要一个误差度量（例如相对误差），以降低使用最大值拟合样本的权重。这项技术称为==非线性最小二乘法==，它可以使用基于对数的标签和预测值的转换。

下一部分包含几个代码示例，其中第一个涉及手动计算MSE，然后是使用NumPy公式执行计算的示例。

#### 手动计算MSE

本节包含两个折线图，两个折线图都包含一条近似散点图中一组点的线。

![image-20201113135628436](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113135628436.png)

图2.7显示的线段近似于点的散点图（其中一些与线段相交）。图2.7中的行的MSE计算如下：

$MSE =(1*1+(-1)*(-1)+(-1)*(-1)+1*1)= 4/7$

![image-20201113140038295](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113140038295.png)

图2.8显示了一组点和一条线，该线是最适合线的潜在候选者。图2.8中的行的MSE计算如下：

$MSE =((-2)*(-2)+2*2)= 8/7$

因此，图2.7中的行的MSE比图2.8中的行小，这可能使您感到惊讶（或您猜对了吗？）

在这两个图中，我们可以轻松快速地计算出MSE，==但总的来说计算MSE要困难得多==。例如，如果我们在欧几里得平面中绘制10个不很接近直线的点，并且各个项涉及非整数值，则可能需要计算器。

更好的解决方案涉及NumPy函数，例如np.linspace（）API，这将在下一节中讨论。

#### 用np.linspace逼近线性数据（）

Listing 2.4显示了np_linspace1.py的内容，该内容说明了如何使用np.linspace（） API结合“扰动技术”来生成一些数据。

**Listing 2.4: np_linspace1.py**

```python
import numpy as np
trainX = np.linspace(-1, 1, 6)
trainY = 3*trainX+ np.random.randn(*trainX.shape)*0.5
print("trainX: ", trainX)
print("trainY: ", trainY)
```

此代码示例的目的仅是生成和显示一组随机生成的数字。在本章的后面，我们将使用此代码作为实际线性回归任务的起点。

Listing 2.4从数组变量trainX的定义开始，该数组变量是通过np.linspace（）API初始化的 。接下来，通过在前面的代码样本中看到的“扰动技术”定义数组变量trainY。Listing 2.4的输出在这里：

![image-20201113140646202](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113140646202.png)

现在我们知道了如何为线性方程式生成（x，y）值，让我们学习如何计算MSE，这将在下一节中讨论。

下一个示例使用np.linspace（）方法和np.random.randn（）方法生成一组数据值，以便在数据点中引入一些随机性。

#### 使用np.linspace（） API计算MSE

本节中的代码示例与本章中的许多早期代码示例不同：它使用一组X和Y的硬编码的值代替“扰动”技术。因此，您将不知道斜率和y截距的正确值（并且您可能无法猜测它们的正确值）。Listing 2.5显示了plain_ linreg1.py的内容，该内容说明了如何使用模拟数据来计算MSE。

**Listing 2.5: plain_linreg1.py**

```python
import numpy as np
import matplotlib.pyplot as plt
X = [0,0.12,0.25,0.27,0.38,0.42,0.44,0.55,0.92,1.0]
Y = [0,0.15,0.54,0.51,0.34,0.1,0.19,0.53,1.0,0.58]
costs = []
#Step 1: Parameter initialization
W = 0.45
b = 0.75
for i in range(1, 100):
     #Step 2: Calculate Cost
     Y_pred = np.multiply(W, X) + b
     Loss_error = 0.5 * (Y_pred - Y)**2
     cost = np.sum(Loss_error)/10
     #Step 3: Calculate dW and db
     db = np.sum((Y_pred - Y))
     dw = np.dot((Y_pred - Y), X)
     costs.append(cost)
     #Step 4: Update parameters:
     W = W - 0.01*dw
     b = b - 0.01*db
     if i%10 == 0:
     	print("Cost at", i,"iteration = ", cost)
#Step 5: Repeat via a for loop with 100 iterations
#Plot cost versus # of iterations
print("W = ", W,"& b = ", b)
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per tens)')
plt.show()
```

Listing 2.5使用硬编码值初始化数组变量X和Y，然后初始化标量变量W和b。Listing 2.5的下一部分包含一个重复100次的for循环。在循环的每次迭代之后，都会计算变量Y_pred，Loss_error和cost 。接下来，分别基于数组Y_pred -Y中各项的总和以及Y_pred -y和X的内积来计算dw和db的值。

请注意W和b的更新方式：它们的值分别递减0.01 * dw和0.01 * db。这种计算应该看起来有点熟悉：代码正在以编程方式计算W和b的梯度的近似值，二者均乘以学习率（硬编码值0.01），并且所得项相对于W和b的当前值，以便产生W和b的新近似值。尽管此技术非常简单，但确实可以计算出W和b的合理值。

Listing 2.5中的最后代码块显示W和b的中间近似值，以及成本（垂直轴）与迭代次数（水平轴）的关系图。Listing 2.5的输出在这里：

![image-20201113152031830](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113152031830.png)

### 第三章 机器学习分类器

本章介绍了机器学习中的众多分类算法。这包括诸如kNN（k近邻）算法，逻辑回归（尽管其名称是分类器），决策树，随机森林，SVM和贝叶斯分类器之类的算法。旨在向您介绍机器学习，其中包括依赖于scikit-learn的基于树的代码示例。本章的后半部分包含用于标准数据集的基于Keras的代码示例。

由于篇幅所限，本章不涉及其他知名算法，例如线性判别分析和kMeans算法（用于无监督学习和聚类）。但是，有许多在线教程可以讨论机器学习中的这些算法和其他算法。

考虑到这些要点，本章的第一部分简要讨论了这些分类器。本章的第二部分概述了激活函数，如果您决定学习深度神经网络，这将非常有用。在本节中，您将学习如何以及为什么在神经网络中使用它们。本部分还包含用于激活功能的TensorFlow API列表，其后是一些优点的描述。

第三部分介绍了基于激活函数的逻辑回归，该函数也用于RNN（递归神经网络）和LSTM（长期短期记忆）中。本章的第四部分包含一个涉及Logistic回归和MNIST数据集的代码示例。

这里提供一些背景信息，分类器是三种主要算法类型之一：回归算法（如第4章中的线性回归），分类算法（本章中讨论过）和聚类算法（如kMeans，在本书中讨论）。

另一点：关于激活函数的部分确实涉及对神经网络中隐藏层的基本理解。根据您的程度，在深入阅读本节之前，您可能会从阅读一些准备材料中受益（在线上有很多文章）。

#### 什么是分类

给定一个包含已知类成员关系的观测值的数据集，==分类就是确定新数据点所属的类的任务==。类是指类别，也称为目标或标签。例如，电子邮件服务提供商中的垃圾邮件检测涉及二元分类（仅两个类别）。所述MNIST数据集包含的一组图像，其中每个图像是一个单一的数字，这意味着有10标签。分类中的一些应用程序包括信用批准，医学诊断和目标市场。

##### 什么是分类器

在上一章中，您了解了线性回归将监督学习与数值数据结合使用：目标是训练一个可以进行数值预测的模型（例如，明天的股票价格，系统的温度，其大气压力以及等等）。相比之下，分类器结合各种数据类别使用监督学习：目标是训练可以做出分类预测的模型。

例如，假设数据集中的每一行都是特定的葡萄酒，并且每一列都与特定的葡萄酒功能（单宁，酸度等）有关。进一步假设数据集中有五类葡萄酒：为简单起见，我们将它们分别标记为A，B，C，D和E。给定一个新的数据点，即新的数据行，为此的分类器数据集尝试确定此酒的标签。

本章中的某些分类器可以执行分类分类并进行数值预测（即，它们可以用于回归和分类）。

##### 常见分类器

以下列出了一些最流行的机器学习分类器（不分先后）：

•  线性分类器

•  kNN

•  Logistic回归

•  决策树

•  随机森林

•  支持向量机

•  贝叶斯分类器

•  CNN（深度学习）

请记住，不同的分类器具有不同的优缺点，这通常涉及复杂性和准确性之间的权衡，类似于AI以外领域的算法。

在深度学习的情况下，CNN（卷积神经网络）执行图像分类，这使它们成为分类器（它们也可以用于音频和文本处理）。

下一节将简要描述这些ML分类器。

##### 二元分类与多元分类

二元分类器可用于具有两个类别的数据集，而多元分类器可区分两个以上的分类。随机森林分类器和朴素贝叶斯分类器支持多个类，而SVM和线性分类器只能用作二元分类器（但存在SVM的多类扩展名）。

此外，还有一些基于二元分类器的多类分类技术：==一对多（OvA）和一对一（OvO）==。

OvA技术涉及多个二元分类器，分类器数量等于类的数量。例如，如果数据集具有五个类别，则OvA使用五个二元分类器，每个分类器都检测这五个类别之一。为了在此数据集中对数据点进行分类，请选择输出最高得分的二元分类器。

OvO技术也涉及到多个二元分类器，但是在这种情况下的二元分类器被用于在一对类训练。例如，如果类是A，B，C，D和E，则需要十个二元分类器：一个用于A和B，一个用于A和C，一个用于A和D，依此类推，直到最后一个D和E的二元分类器。

通常，如果有n个类，则需要n *（n-1）/ 2个二元分类器。尽管OvO技术比OvA技术需要更多的二元分类器，但是OvO技术的优势在于每个二元分类器==只在属于其两个所选类的数据集部分上进行训练==。

##### 多标签分类

多标签分类涉及将多个标签分配给数据集中的一个实例。因此，多标签分类概括了多类分类（在上一节中讨论过），其中后者涉及将单个标签分配给属于具有多个类的数据集的实例。

您也可以在线搜索涉及SKLearn 或PyTorch的文章，以进行多标签分类任务。

#### 什么是线性分类器

线性分类器将数据集分为两类。线性分类器是2D点的线，3D点的平面和高维点的超平面（平面的概括）。

线性分类器通常是最快的分类器，因此在分类速度非常重要时经常使用它们。当输入向量稀疏（即，大多数为零值）或维数较大时，线性分类器通常可以很好地工作。

#### 什么是kNN

所谓的kNN （K近邻）算法是一种分类算法。简而言之，将彼此靠近的数据点归为同一类。引入新点后，会将其添加到其最近邻居的大多数类别中。例如，假设k等于3，并引入了一个新的数据点。看一下它的三个最近邻居的类：假设它们是A，A和B。然后以多数表决，新数据点被标记为A类的数据点。

该k近邻算法本质上是一种==启发式的==，而不是用复杂的数学基础的技术，但它仍然是一个有效的和有用的算法。

如果您想使用简单的算法，或者您认为数据集的性质高度非结构化，请尝试使用kNN算法。在k近邻尽管是非常简单的算法可以产生高度非线性的决策。您可以在要搜索类似项目的搜索应用程序中使用kNN。

通过创建项目的矢量表示来测量相似度，然后使用适当的距离度量（例如欧几里得距离）比较矢量。

kNN搜索的一些具体示例包括搜索语义相似的文档。

##### 如何处理kNN的平局

k的奇数不太可能导致平局，但并非不可能。例如，假设k等于7，并且当引入新的数据点时，它的七个最近邻居属于集合{A，B，A，B，A，B，C}。如您所见，没有多数票，因为A类有3分，B类有3分，C类有1分。

在kNN中有几种处理平局的技术：

•  将较高的权重分配给较近的点

•  增加k的值，直到确定获胜者

•  减小k的值，直到确定获胜者

•  随机选择一个分类

如果将k减小到等于1，仍然可以进行平局表决：可能有两个点与新点的距离相等，因此需要一种机制来确定选择这两个点中的哪一个作为1-邻居。

如果A类和B类之间存在平局，则随机选择A类或B类。另一种办法是跟踪平局票数，并采用交替循环的方式以确保分配更均匀。

#### 什么是决策树

决策树是涉及树状结构的另一种分类算法。在通用树中，数据点的位置由简单的条件逻辑确定。作为简单说明，假设数据集包含一组代表人们年龄的数字，并且还假设第一个数字是50。选择该数字作为树的根，所有小于50的数字在树的左分支上添加，而所有大于50的数字都在树的右分支上添加。

例如，假设我们的数字序列为{50，25，70，40}。然后我们可以构造一个树，如下所示：50是根节点；25是50的左孩子；70是50的右孩子；40是20的右孩子。我们添加到此数据集中的每个附加数值都将被处理，以确定树中每个节点的前进方向（左或右）。

Listing 3.1显示了sklearn_tree2.py的内容，该内容定义了欧几里得平面中的一组2D点及其标签，然后预测了欧几里得平面中其他几个2D点的标签（即类）。

**Listing 3.1  sklearn_tree.py**

```python
from sklearn import tree
# X = pairs of 2D points and Y = the class of each point
X = [[0, 0], [1, 1], [2,2]]
Y = [0, 1, 1]
tree_clf = tree.DecisionTreeClassifier()
tree_clf = tree_clf.fit(X, Y)
#predict the class of samples:
print("predict class of [-1., -1.]:")
print(tree_clf.predict([[-1., -1.]]))
print("predict class of [2., 2.]:")
print(tree_clf.predict([[2., 2.]]))
# the percentage of training samples of the same class
# in a leaf note equals the probability of each class
print("probability of each class in [2.,2.]:")
print(tree_clf.predict_proba([[2., 2.]]))
```

Listing 3.1从sklearn导入树类，然后用数据值初始化数组X和y。接下来，将变量tree_clf初始化为DecisionTreeClassifier类的实例，然后通过调用具有X和y值的fit()方法对其进行训练。

现在启动Listing 3.1中的代码，您将看到以下输出：

![image-20201113160006949](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113160006949.png)

如您所见，点[-1，-1]和[2,2]分别正确地标记了值0和1，这可能正是您所期望的。

Listing 3.2显示了sklearn_tree3.py的内容，该内容扩展了Listing 3.1中的代码，方法是添加第三个标签，并通过预测欧氏平面中三个点而不是两个点的标签。

**Listing 3.2: sklearn_tree3.py**

```python
from sklearn import tree
# X = pairs of 2D points and Y = the class of each point
X = [[0, 0], [1, 1], [2,2]]
Y = [0, 1, 2]
tree_clf = tree.DecisionTreeClassifier()
tree_clf = tree_clf.fit(X, Y)
#predict the class of samples:
print("predict class of [-1., -1.]:")
print(tree_clf.predict([[-1., -1.]]))
print("predict class of [0.8, 0.8]:")
print(tree_clf.predict([[0.8, 0.8]]))
print("predict class of [2., 2.]:")
print(tree_clf.predict([[2., 2.]]))
# the percentage of training samples of the same class
# in a leaf note equals the probability of each class
print("probability of each class in [2.,2.]:")
print(tree_clf.predict_proba([[2., 2.]]))
```

现在启动Listing 3.2中的代码，您将看到以下输出：

![image-20201113160246066](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113160246066.png)

如您所见，点[-1，-1]，[0.8、0.8]和[2,2]分别正确地标记了值0、1和2，这可能又是您期望的值。

Listing 3.3显示了数据集partial_wine.csv的一部分，该数据集包含两个功能和一个标签列（共有三个类）。该数据集的总行数为178。

![image-20201113160410681](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113160410681.png)

Listing 3.4显示了使用决策树的tree_classifier.py的内容，以在数据集partial_wine.csv上训练模型。

**Listing 3.4: tree_classifier.py**

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('partial_wine.csv')
X = dataset.iloc[:, [0, 1]].values
y = dataset.iloc[:, 2].values
# split the dataset into a training set and a test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
# ====> INSERT YOUR CLASSIFIER CODE HERE <====
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion='entropy',random_state=0)
classifier.fit(X_train, y_train)
# ====> INSERT YOUR CLASSIFIER CODE HERE <====
# predict the test set results
y_pred = classifier.predict(X_test)
# generate the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print("confusion matrix:")
print(cm)
```

Listing 3.4包含一些导入语句，然后使用CSV文件partial_ wine.csv的内容填充Pandas DataFrame数据集。接下来，变量X用数据集的前两列初始化，变量y用数据集的第三列初始化。

接下来，变量X_train，X_test，y_train，y_test使用来自X和y的数据以75/25的分割比例填充。注意，变量sc（是StandardScalar类的实例）对变量X_train和X_test执行缩放操作。

Listing 3.4中以粗体显示的代码块是我们创建DecisionTreeClassifier类的实例，然后使用变量X_train和X_test中的数据训练该实例的地方。

Listing 3.4的下一部分用从X_test变量中的数据生成的一组预测填充变量y_pred。Listing 3.4的最后部分创建一个基于在数据中的混淆矩阵y_test和在预测数据y_pred。

请记住，混淆矩阵的所有对角元素都是正确的预测（例如正确肯定和正确否定）。所有其他单元格都包含一个数值，该数值指定了不正确的预测数（例如错误肯定性和错误否定）。

现在启动Listing 3.4中的代码，您将看到以下混淆矩阵的输出，其中有36个正确的预测和9个错误的预测（准确度为80％）：

![image-20201113161015724](C:\Users\12178\AppData\Roaming\Typora\typora-user-images\image-20201113161015724.png)

在前面的3x3矩阵中，总共有45个条目，对角线条目是正确标识的标签。因此，精度为36/45 = 0.80。

#### 什么是随机森林

随机森林是决策树的概括：此分类算法涉及多个树（数目由您指定）。如果数据涉及进行数字预测，则计算树的预测平均值。如果数据涉及分类预测，则确定树木的预测模式。

以此类推，随机森林的运作方式类似于金融投资组合的多元化：目标是在损失与收益之间取得平衡。随机森林使用==多数表决来进行预测==，这种假设是在假设多数表决比从一棵树上进行任何单个预测更正确（且更频繁）的前提下进行的。

通过用以下代码替换粗体显示的两行代码，您可以轻松地修改Listing 3.4中的代码以使用随机森林：

```python
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10,criterion='entropy', random_state = 0)
```

更改此代码，启动代码，并检查混淆矩阵，以将其准确性与Listing 3.4中决策树的准确性进行比较。

#### 什么是SVM

支持向量机涉及监督的ML算法，可用于分类或回归问题。SVM既可以处理==线性可分离数据==，也可以处理==非线性可分离数据==。SVM使用一种称为核技巧的技术来转换数据，然后找到一个最佳边界，这种转换涉及到更高的维数。该技术可将转换后的数据分离，然后可以找到将数据分为两类的超平面。

与回归任务相比，SVM在分类任务中更为常见。SVM的一些用例包括：

•  文本分类任务：类别分配

•  检测垃圾邮件/情感分析

•  图像识别：基于方面的识别，基于颜色的分类

•  手写数字识别（邮政自动化）

##### SVM的权衡

尽管SVM非常强大，但仍存在一些权衡。SVM的一些优点是：

•  精度高

•  适用于较小的清洁数据集

•  可以提高效率，因为它使用了一部分训练点

•  在数据集有限的情况下可以替代CNN

•  捕获数据点之间更复杂的关系

尽管SVM功能强大，但SVM仍存在一些缺点：

•  不适合较大的数据集：训练时间可能很长

•  对于重叠类的噪声较大的数据集，效果较差

与决策树和随机森林相比，SVM涉及更多的参数

建议：修改Listing 3.4以使用SVM，方法是将以下两行替换为以粗体显示的两行：

```python
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear',random_state = 0)
```

现在，只需进行先前的代码更新，您便有了基于SVM的模型！进行代码更改，然后启动代码并检查混淆矩阵，以便将其准确性与本章前面的决策树模型和随机森林模型的准确性进行比较。

#### 什么是贝叶斯推理？

贝叶斯推断是统计学中的一项重要技术，涉及==统计推断和贝叶斯定理==，以随着更多信息的出现而更新假设的可能性。贝叶斯推理通常被称为贝叶斯概率，它在顺序数据的动态分析中很重要。

##### 贝叶斯定理

给定两组A和B，让我们定义以下数值（它们都在0和1之间）：

P（A）= 在集合A的概率

P（B）= 在集合B的概率

P（Both）= 既在A又在B的概率

P（A | B）= 在A中的概率（假设您在B中）

P（B | A）= 在B中的概率（假设您在A中）

则以下公式也成立：

P（A | B）= P（Both）/ P（B）（＃1）

P（B | A）= P（Both）/ P（A）（＃2）

将前面的方程对与分母中出现的项相乘，我们得到这些方程：

P（B）* P（A | B）= P（Both）（＃3）

P（A）* P（B | A）= P（Both）（＃4）

现在，将方程式＃3和＃4的左侧设置为彼此相等，这样就得出了该方程式：

P（B）* P（A | B）= P（A）* P（B | A）（＃5）

将＃5的两边除以P（B），我们得到这个著名的方程：

P（A | B）= P（A）* P（A | B）/ P（B）（＃6）

##### 一些贝叶斯术语

在上一节中，我们得出以下关系：

P（h | d）=（P（d | h）* P（h））/ P（d）

前面的等式中的四个术语都有一个名称：

首先，后验概率为P（h | d），即给定数据d的假设h的概率。

其次，P（d | h）是假设h为真时数据d的概率。

第三，h的先验概率是P（h），这是假设h为真的概率（不管数据如何）。。

最后，P（d）是数据的概率（与假设无关）

我们感兴趣的是根据先验概率p（h）与P（d）和P（d | h）计算出P（h | d）的后验概率。

##### 什么是MAP

最大后验概率（MAP）假设是概率最大的假设，即最大概率假设。可以这样写：

MAP（h）= max（P（h | d））或：

MAP（h）= max（（P（d | h）* P（h））/ P（d））或：

MAP（h）= max（P（d | h）* P（h））

##### 为什么使用贝叶斯定理？

贝叶斯定理基于可能与事件相关的条件的==先验知识来描述事件的概率==。如果知道条件概率，则可以使用贝叶斯规则找出逆概率。前面的陈述是贝叶斯规则的一般表示。

#### 什么是贝叶斯分类器

朴素贝叶斯分类器是受贝叶斯定理启发的概率分类器。朴素贝叶斯分类器假定属性是==条件独立==的，即使假设不成立，它也可以正常工作。这种假设大大降低了计算成本，并且这是一种仅需线性时间即可实施的简单算法。此外，朴素贝叶斯分类器可以轻松扩展到更大的数据集，并且在大多数情况下都可以获得良好的结果。朴素贝叶斯分类器的其他优点包括：

•  可用于二元和多元分类

•  提供不同类型的朴素贝叶斯算法

•  是文本分类问题的不错选择

•  是垃圾邮件分类的流行选择

•  可以在小型数据集上轻松训练

如您所料，朴素贝叶斯分类器确实存在一些缺点，例如：

•  所有功能均假定无关

•  它无法学习要素之间的关系

•  可能遭受“零概率问题”的困扰

“==零概率问题==”指的是当一个属性的条件概率为零时，它无法给出有效的预测。但是，可以使用Laplacian估计器显式修复。

##### 朴素贝叶斯分类器的类型

朴素贝叶斯分类器分为三种主要类型：

•   高斯朴素贝叶斯

•   MultinomialNB 朴素贝叶斯

•   伯努利朴素贝叶斯

这些分类器的详细信息不在本章范围之内，但是您可以执行在线搜索以获取更多信息。

#### 训练分类器

训练分类器的一些常用技术是：

•  holdout方法

•  k折交叉验证

holdout方法是最常用的方法，首先将数据集分为两个分区，分别称为训练和测试（分别为80％和20％）。训练集用于训练模型，测试数据测试其预测能力。

k折交叉验证技术来验证该模型没有过拟合。数据集被随机分为k个互斥子集，其中每个分区的大小均相等。一个分区用于测试，其他分区用于训练。遍历整个k个子集。

#### 评估分类器

每当为数据集选择分类器时，评估该分类器的准确性显然很重要。评估分类器的一些常用技术是：

•  精确度和召回率

•  ROC曲线

精度和召回率在第2章中进行了讨论，为方便起见，在此处进行了复制。让我们定义以下变量：

TP：正确肯定

FP：错误肯定

TN：正确否定

FN：错误否定

然后，以下公式给出精度，准确度和召回率的定义：
$$
精度：precision=TP/(TP+FP)\\
准确度：accuracy=(TP+TN)/(P+N)\\
召回率：recall=TP/(TP+FN)
$$
==ROC曲线==用于分类模型的可视化比较，显示了正确肯定率和错误肯定率之间的权衡。ROC曲线下的面积是模型精度的度量。当一个模型更接近于对角线时，它的精度会降低，并且具有完美精度的模型的面积将为1.0。

ROC曲线绘制的是正确肯定率与错误肯定率。另一种类型的曲线是==PR曲线==，它绘制了精度与召回率的关系。当处理高度偏斜的数据集（严重的类不平衡）时，Precision-Recall（PR）曲线可提供更好的结果。

到此为止，本章的有关统计术语和用于测量数据集有效性的技术的部分到此结束。下面让我们看一下机器学习中的激活函数。

#### 什么是激活函数

单句描述：激活函数（通常）是将非线性引入神经网络的非线性函数，从而防止了神经网络中隐藏层的“合并”。具体来说，假设神经网络中的每一对相邻层都只涉及矩阵变换而没有激活函数。这样的网络是线性系统，这意味着可以将其层合并为一个更小的系统。

首先，连接输入层和第一个隐藏层的边的权重可以用矩阵表示：我们称其为W1。接下来，连接第一隐藏层和第二隐藏层的边缘的权重也可以用矩阵表示：我们称其为 W2。重复此过程，直到到达将最终隐藏层与输出层连接起来的边缘为止：我们将此矩阵称为Wk。由于我们没有激活函数，因此我们可以简单地将矩阵W1，W2，…，Wk相乘在一起，得到一个矩阵：我们称它为W。现在，我们用等效神经网络替换了原始神经网络，该神经网络包含一个输入层，一个权重W的矩阵和一个输出层。换句话说，我们不再拥有原始的多层神经网络！

幸运的是，当我们在每对相邻层之间指定激活函数时，可以防止发生前面的情况。换句话说，每一层的激活功能都可以防止这种“矩阵合并”。因此，我们可以在训练神经网络的过程中维护所有中间隐藏层。

为简单起见，我们假设在每对相邻的层之间具有相同的激活函数（我们将很快删除此假设）。在神经网络中使用激活函数的过程*分为两个步骤*，如下所述：

•  步骤1.从数字的输入向量x1开始

•  步骤2。将x1乘以权重矩阵W1，该矩阵代表将输入层与第一个隐藏层相连的边：结果是一个新的向量x2

•  步骤3.将激活函数应用于x2的每个元素以创建另一个向量x3

现在重复步骤2和3，不同之处在于我们使用起始向量x3和权重矩阵W2作为连接第一个隐藏层和第二个隐藏层（或者只有输出层，如果只有一个隐藏层）的边。

完成上述过程后，我们保留了神经网络，这意味着可以在数据集上对其进行训练。另一件事：可以在每个步骤中使用不同的激活功能来代替每个激活功能，而不必在每个步骤中使用相同的激活功能（由您选择）。

##### 为什么需要激活功能

上一节概述了以下过程：先从输入层转换输入向量，然后再转换隐藏层，直到到达输出层。激活函数在神经网络中的用途至关重要，因此在这里值得重复：激活函数“保持”了神经网络的结构，并防止它们被简化为输入层和输出层。换句话说，如果我们在每对连续的层之间指定一个非线性激活函数，那么除非您明确删除它们，否则无法用包含较少层的神经网络替换神经网络。

如果没有非线性激活函数，我们只需将给定一对连续层的权重矩阵乘以从先前一对连续层产生的输出矢量即可。我们重复这个简单的乘法，直到到达神经网络的输出层。到达输出层后，我们已有效地用单个矩阵替换了多个矩阵，该矩阵将输入层与输出层“连接”在一起。

##### 激活功能如何工作？

如果这是您第一次遇到激活功能的概念，那么它可能会令人困惑，因此这里提供一个类比可能会有所帮助。假设您在深夜开车，高速公路上没有其他人。只要没有障碍物（停车标志，交通信号灯等），您就可以恒定速度行驶。另一方面，假设您开车进入一家大型杂货店的停车场。接近减速带时，您必须减速，越过减速带并再次提高速度，并对每个减速带重复此过程。

将神经网络中的非线性激活函数视为减速带的对应部分：您根本无法保持恒定的速度，（通过类推）这意味着您不能先将所有权重矩阵相乘并将它们“折叠”成一个单一的权重矩阵。另一个比喻涉及一条有多个收费站的道路：您必须放慢速度，支付通行费，然后继续行驶，直到到达下一个收费站。这些只是类推（因此是不完善的），可以帮助您了解非线性激活函数的需求。

#### 常用的激活功能

尽管激活功能很多（如果您知道如何激活，则可以定义自己的激活功能），以下是常用激活功能的列表，并附有简短说明：

•	 Sigmoid 

•	 Tanh 

•	 ReLU 

•	 ReLU6 

•	 ELU 

•	 SELU

Sigmoid激活功能是基于欧拉常数e，其范围在0和1，其化学式之间的数值如下所示：

1/[1+e^(-x)]

Tanh激活功能也是基于欧拉常数e，其化学式如下所示：

[ e ^ x – e ^（-x）] / [ e ^ x + e ^（-x）]

记住前面公式的一种方法是，注意分子和分母具有相同的术语对：分子之间用“-”号隔开，分母中用“ +”号隔开。所述的tanh函数有-1和1之间的值的范围

所述RELU（整流线性单位）激活函数是直接的：如果x为负，那么RELU（x）为0; 对于x的所有其他值，ReLU（x）等于x。ReLU6特定于TensorFlow，它是ReLU（x）的变体：附加约束是x> = 6时ReLU（x）等于6（因此得名）。

ELU是指数线性单元和它的的指数“信封” RELU，它取代的两个直线段RELU具有指数激活函数是可微的对于x的所有值（包括x = 0时）。

SELU是“比例指数线性单位”的首字母缩写，它比其他激活函数稍微复杂一些（并且使用频率较低）。有关这些和其他激活功能（以及描述其形状的图形）的完整说明，请导航至以下Wikipedia链接：

[1]: https://zh.wikipedia.org/wiki/Activation_function

此链接提供了一长串激活函数及其派生函数。

##### Python中的激活函数

listing 3.5显示了文件activations.py的内容，其中包含各种激活函数的公式。

#### Listing 3.5: activations.py

```python
import numpy as np
# Python sigmoid example:
z = 1/(1 + np.exp(-np.dot(W, x)))
# Python tanh example:
z = np.tanh(np.dot(W,x))
# Python ReLU example:
z = np.maximum(0, np.dot(W, x))
```

listing 3.5包含使用NumPy方法的Python代码，以定义S型函数，tanh函数和ReLU函数。注意，您需要为x和W指定值，以便启动 listing 3.5中的代码。

#### Keras激活功能

TensorFlow（和许多其他框架）提供了许多激活功能的实现，从而节省了您编写自己的激活功能实现的时间和精力。

这是tf.keras.layers命名空间中的TF 2 / Keras API激活函数的列表：

- tf.keras.layers.leaky_relu
- tf.keras.layers.relu
- tf.keras.layers.relu6
- tf.keras.layers.selu
- tf.keras.layers.sigmoid
- tf.keras.layers.sigmoid_cross_entropy_with_logits
- tf.keras.layers.softmax
- tf.keras.layers.softmax_cross_entropy_with_logits_v2
- tf.keras.layers.softplus
- tf.keras.layers.softsign
- tf.keras.layers.softmax_cross_entropy_with_logits
- tf.keras.layers.tanh
- tf.keras.layers.weighted_cross_entropy_with_logits

以下小节提供了有关前面列表中的某些激活功能的其他信息。请牢记以下几点：对于简单的神经网络，将**ReLU**作为首选。

#### 该RELU和ELU激活功能

目前，ReLU 通常是“首选”激活功能：以前首选的激活功能是tanh（在tanh之前是sigmoid）。ReLU的表现接近线性单位，并提供最佳的训练精度和验证精度。

ReLU就像是线性开关：如果您不需要它，则将其“关闭”，并且在激活时其导数为1，这使得ReLU是当前所有激活函数中最简单的。请注意，该函数的二阶导数在任何地方都为0：这是一个非常简单的函数，可简化优化过程。另外，只要需要大的值，梯度就很大，并且永远不会“饱和”（即在正水平轴上不会缩小到零）。

校正后的线性单位和广义版本均基于以下原则：线性模型更易于优化。使用ReLU 激活功能或其相关替代方法之一（稍后讨论）

##### ReLU的优缺点

下表包含ReLU激活功能的优点：

•  在正区域不饱和

•  在计算方面非常高效

•  具有ReLU的模型通常会收敛于具有其他激活功能的模型

但是， 当ReLU 神经元的激活值变为0时，ReLU确实有一个缺点：在反向传播期间，神经元的梯度也将为0。您可以通过明智地分配初始权重值和学习率来减轻这种情况。

##### ELU

ELU是基于 ReLU的*指数线性单位*的首字母缩写：关键区别在于ELU在原点是可微的（ReLU是连续函数但在原点是*不可*微的）。但是，请记住几点。首先，ELU用计算效率来换取*永生性* （对死亡的免疫力）：有关详细信息，请阅读以下文章：arxiv.org/abs/1511.07289。其次，由于ELU的使用引入了新的超参数，因此RELU仍然比ELU更为流行和首选。

#### Sigmoid，Softmax和Hardmax相似之处

sigmoid的激活函数在（0,1）的范围内，并且是饱和的和梯度消失。与tanh激活功能不同，sigmoid输出不是零中心的。此外，不建议使用Sigmoid和softmax（稍后讨论）来实现普通前馈（请参见在线书籍《 Deep Learning》第6章 ，by Ian Goodfellow et al）。但是，sigmoid激活函数仍用于LSTM（具体用于遗忘门，输入门和输出门），GRU（门控循环单元）和概率模型中。此外，某些自动编码器还有其他要求，这些要求会阻止使用分段线性激活函数。

##### Softmax

所述SOFTMAX激活函数在一个数据集中的值映射到另一组是0和1，并且其总和等于1。因此SOFTMAX创建了一个概率分布。在使用卷积神经网络（CNN）进行图像分类的情况下，softmax激活函数会将最终隐藏层中的值映射到输出层中的十个神经元。在输入图像的一键编码中，包含最大概率的位置的索引与数字1的索引匹配。如果索引值相等，则表示图像已分类，否则视为不匹配

##### Softplus

所述softplus激活函数是平滑的（即，可微的）逼近RELU 激活函数。回想一下，原点是ReLU函数的唯一不可微分点，可通过softmax激活对其进行平滑，其等式为：

f（x）= ln（1 + e ^ x）

##### Tanh

所述的tanh激活函数具有范围（-1,1），而sigmoid函数具有范围（0,1）。这两个激活都饱和，但是与sigmoid神经元不同，tanh输出为零中心。因此，实际上，tanh非线性总是比sigmoid非线性更可取。

在LSTMs中出现了sigmoid和tanh激活函数(sigmoid表示三门，tanh表示细胞内部状态)如GRUs(门控周期性单位)在计算期间用于修饰或说明输入门、遗忘门和输出门(在下一章)。

#### Sigmoid，Softmax和HardMax差异

本节简要讨论这三个功能之间的一些区别。首先，在逻辑回归模型中以及在LSTM和GRU中的门，Sigmoid函数用于二进制分类。sigmoid函数用作激活功能，同时建立神经网络，但要记住的是，概率之和并不一定等于1。

其次，softmax函数概括了sigmoid函数：它用于logistic回归模型中的多分类。softmax函数是CNNs中全连接层的激活函数，为最右边的隐层和输出层。与sigmoid函数不同，概率之和 *必须*等于1。您可以将sigmoid函数或softmax用于二元（n = 2）分类。

第三，所谓的hardmax函数将0或1分配给输出值（类似于步进函数）。例如，假设我们有三个类

{c1，c2，c3}的得分分别为[ 1，7，2 ] 。hardmax概率是[0，1，0] ，而SOFTMAX概率是[0.1，0.7，0.2] 。注意，hardmax概率的总和为1，softmax概率的总和也是如此。但是，hardmax概率全有或全无，而softmax概率类似于获得“部分信用”。

#### 什么是逻辑回归？

尽管逻辑回归的名称如此，但**逻辑回归是一个分类器和具有二元输出的线性模型**。逻辑回归与多个自变量一起工作，并且涉及用于计算概率的sigmoid函数。逻辑回归本质上是将sigmoid激活函数应用于线性回归以执行二元分类的结果。

逻辑回归在许多无关领域中很有用。这些领域包括机器学习，各种医学领域和社会科学。基于患者的各种观察到的特征，逻辑回归可以用来预测患上给定疾病的风险。使用逻辑回归的其他领域包括工程，市场营销和经济学。

逻辑回归可以是二项式的（因变量只有两个结果），多项式的（因变量三个或更多结果）或有序的（因变量是有序的）。例如，假设数据集由属于A类或B类的数据组成。如果为您提供了一个新的数据点，则逻辑回归预测该新数据点是属于A类还是属于B类。相比之下，线性回归预测数值，例如股票的第二天价值。

##### 设置阈值

该阈值是确定哪些数据点属于类别A和哪些点属于类别B的数值。例如，通过/失败阈值可能为0.70。在加利福尼亚州，通过写作驾驶员考试的通过/失败阈值为0.85。

作为另一个示例，假设p = 0.5是截止概率。然后，我们可以将A类分配给概率大于0.5的数据点，将B类分配给概率小于等于0.5的数据点。由于只有两个类，因此我们有一个分类器。

类似（但略有不同）的情况涉及扔出平衡良好的硬币。我们知道，有50％的机会扔头（让我们将此结果标记为A类）和50％的机会扔尾巴（让我们将该结果标记为B类）。如果我们有一个包含标记结果的数据集，则可以预期其中约有50％是A类和B类。

另一方面，我们无法预先确定要通过书面笔试的人数比例，或者不能通过考试的人数比例。包含针对这些类型场景的结果的数据集需要进行培训，逻辑回归可以是适合的技术

##### 逻辑回归：重要假设

逻辑回归要求观测值彼此独立。此外，逻辑回归要求自变量之间几乎没有多重共线性。逻辑回归处理数字变量，分类变量和连续变量，并且还假设自变量和log odds为线性关系，其定义为：

odds= p /（1-p）和  logit=log（odds）

该分析不需要因变量和自变量线性相关。但是，另一个要求是自变量与对数几率线性相关。

在存在多个解释变量的情况下，使用逻辑回归获得比值比。该过程与多元线性回归非常相似，只是响应变量是二项式的。结果是每个变量对观察到的感兴趣事件的优势比的影响。

##### 线性可分离数据

线性可分离数据是可以由一条线（2D），一个平面（3D）或一个超平面（较大尺寸）分离的数据。线性不可分离的数据是不能由线或超平面分开的数据（簇）。例如，XOR函数涉及无法用行分隔的数据点。如果使用两个输入为XOR函数创建真值表，则点（0,0）和（1,1）属于类0，而点（0,1）和（1,0）属于类1 （在2D平面上绘制这些点以说服自己）。解决方案包括以更高的维度转换数据，以使数据变得线性可分离，这是SVMS中使用的技术（在本章前面讨论过）。

#### keras，逻辑回归，iris数据集

listing3.6显示的内容tf2-keras-iris.py限定Keras为基础的模型来执行逻辑回归

##### Listing 3.6: tf2-keras-iris.py

```python
import tensorflow as tf
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder,StandardScaler
    
iris = load_iris()
X = iris['data']
y = iris['target']

#you can view the data and the labels:
#print("iris data:",X)
#print("iris target:",y)

# scale the X values so they are between 0 and 1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_
	split(X_scaled, y, test_size = 0.2)
    
model = tf.keras.models.Sequential()
	model.add(tf.keras.layers.
		Dense(activation='relu', input_dim=4,units=4, kernel_initializer='uniform'))
    
model.add(tf.keras.layers.
	Dense(activation='relu', units=4,kernel_initializer='uniform'))

model.add(tf.keras.layers.
	Dense(activation='sigmoid', units=1,kernel_initializer='uniform'))
#model.add(tf.keras.layers.Dense(1,activation='softmax'))
    
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
              
model.fit(X_train, y_train, batch_size=10,epochs=100)
              
# Predicting values from the test set
y_pred = model.predict(X_test)

# scatter plot of test values-vs-predictions
fig, ax = plt.subplots()
ax.scatter(y_test, y_pred)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r*--')
ax.set_xlabel('Calculated')
ax.set_ylabel('Predictions')
plt.show()
```

listing3.6从各种各样的import语句开始，然后用Iris数据集初始化变量iris。变量X包含Iris数据集的前三列(和所有行)，变量y包含的第四列(以及所有行)Iris数据集

listing3.6的下一部分初始化训练集和测试集使用80/20数据分割。接下来，基于keras的模型包含三个密集的层，前两层指定relu激活函数，第三层指定sigmoid激活函数。

listing3.6的下一部分将编译模型，训练模型，然后通过测试数据计算模型的准确性。启动listing3.6中的代码，您将看到以下输出：

![image-20201204123353478](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204123353478.png)

<img src="C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204123406256.png" alt="image-20201204123406256"  />

![image-20201204171905962](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204171905962.png)

上面显示了基于测试值和这些测试值的预测的点的散点图。

它的准确性很差，但你很有可能会遇到这种情况。用不同数量的隐藏层进行实验，用一个指定softmax激活函数或其他激活函数的全连接层替换最后的隐藏层，看看这个改变是否提高了准确性

#### 总结

本章首先说明分类和分类器，然后简要解释机器学习中常用的分类器。

接下来，您了解了激活函数，为什么它们在神经网络中很重要，以及如何在神经网络中使用它们。然后，您看到了用于各种激活功能的TensorFlow / Keras API的列表，然后描述了它们的一些优点。

您还了解了涉及sigmoid激活函数的逻辑回归，以及有关逻辑回归的基于Keras的代码示例。



# 第四章 深度学习介绍

本章向您介绍深度学习，其中包括MLP（多层感知器），CNN（卷积神经网络）。第5章将讨论其他深度学习架构，例如RNN（循环神经网络）和LSTM （长短期记忆网络）。

本章中的大多数材料都是描述性内容，以及一些基于Keras的代码示例，这些示例假定您已阅读前面各章中的Keras材料。*本章旨在粗略地介绍各种主题，并提供指向其他信息的适当链接。*

如果您不熟悉深度学习，则本章中的许多主题可能需要进一步学习才能适应它们：将本章视为通向您精通深度学习的温和步骤。

本章的第一部分简要讨论了深度学习，它可以解决的问题以及未来的挑战。

本章的第二部分简要介绍了感知器，它本质上是神经网络的核心构建块。实际上，ANN，MLP，RNN，LSTM，VAE都是基于包含多个感知器的多层，以及附加的处理步骤。

本章第三部分将介绍的CNN，然后列举了Keras基于CNN与MNIST数据集的例子：如果你有阅读关于激活函数在第5章节，此代码示例会更有意义。

#### Keras和XOR函数

XOR函数是众所周知的函数，不是线性的平面分离。XOR（“异或”）函数的真值表很简单：给定两个二元输入，如果最多一个输入为1，则输出为1；否则，为0。否则，输出为0。如果我们将XOR视为具有两个二元输入的函数的名称，则输出为：

XOR（0,0）= 0

XOR（1,0）= 1

XOR（0,1）= 1

XOR（1,1）= 0

我们可以将输出值视为与输入值关联的标签。具体来说，点（0,0）和（1,1）在类0中，点（1,0）和（0,1）在类1中。在平面上绘制这些点，您将获得以左下角顶点为原点的单位正方形的四个顶点。此外，每对对角线元素属于同一类，因此无法在欧几里得平面中用直线将0类中的点与1类中的点分开。因此，XOR函数在平面中不可线性分离。如果您对此表示怀疑，请尝试为欧几里得平面中的XOR函数找到一个线性分隔符。

listing4.1显示了tf2_keras_xor.py的内容，该内容说明了如何创建基于Keras的神经网络来训练XOR函数

##### listing4.1: tf2_keras_xor.py

```python
import tensorflow as tf
import numpy as np

# Logical XOR operator and "truth" values:
x = np.array([[0., 0.],[0., 1.],[1., 0.],[1.,1.]])
y = np.array([[0.], [1.], [1.], [0.]])

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_dim=2,activation='relu'))
model.add(tf.keras.layers.Dense(1))
print("compiling model...")
model.compile(loss='mean_squared_error',optimizer='adam')
print("fitting model...")
model.fit(x,y,verbose=0,epochs=1000)
pred = model.predict(x)

# Test final prediction
print("Testing XOR operator")
p1 = np.array([[0., 0.]])
p2 = np.array([[0., 1.]])
p3 = np.array([[1., 0.]])
p4 = np.array([[1., 1.]])

print(p1,":", model.predict(p1))
print(p2,":", model.predict(p2))
print(p3,":", model.predict(p3))
print(p4,":", model.predict(p4))
```

listing4.1使用4对数字(0和1的四种组合)初始化NumPy数组x，然后是NumPy数组y，它包含x中每对数字的逻辑或。

listing4.1的下一部分定义了一个基于keras的模型，它有两个全连接层。接下来，编译模型，训练模型，然后是变量pred由一组基于训练模型的预测填充。

下一个代码块初始化点p1、p2、p3和p4，然后显示这些点的预测值。

启动listing4.1中的代码输出如下：

![image-20201204181725352](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204181725352.png)

![image-20201204180725208](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204180725208.png)

用不同的epochs进行实验，看看它是如何影响预测的。使用listing4.1中的代码作为其他逻辑函数的模板。需要对listing4.1进行的惟一修改是用变量y替换listing4.1中的变量y，该变量被指定为下面列出的其他几个逻辑门的标签。

**The labels for the NOR function:**
y = np.array([[1.], [0.], [0.], [1.]])
**The labels for the OR function:**
y = np.array([[0.], [1.], [1.], [1.]])
**The labels for the XOR function:**
y = np.array([[0.], [1.], [1.], [0.]])
**The labels for the ANDR function:**
y = np.array([[0.], [0.], [0.], [1.]])
mnist = tf.keras.datasets.mnist

既然您已经看到了具有单个隐藏层的神经网络的局限性示例，那么具有多个隐藏层的体系结构的实用性就更加有意义了，这将在下一部分中进行讨论

#### 什么是深度学习

深度学习是机器学习的一个子集，它专注于神经网络和用于训练神经网络的算法。正如您在本章简介中所了解的那样，深度学习包括许多类型的经网络，例如CNN，RNN，LSTM，GRU，变分自动编码器（VAE）和GAN。深度学习模型要求神经网络中至少有两个隐藏层（非常深度的学习涉及具有至少十个隐藏层的神经网络）。

从高层的角度来看，带监督学习的深度学习涉及定义模型（又称神经网络）：

•  估算数据点

•  计算每个估计的损失或误差

•  通过梯度下降减少误差

在第3章中，您学习了机器学习中的线性回归，该线性回归从m和b的初始值开始

m = tf.Variable(0.) 

b = tf.Variable(0.)

训练过程涉及在以下等式中找到m和b的最佳值：

y = m * x + b

我们要计算给定自变量x值的因变量y。在这种情况下，计算由以下Python函数处理

```python
def predict(x): 

y = m*x + b 

return y
```

损失是当前估计误差的另一个名称，可以通过以下确定MSE值的Python函数计算得出

```python
def squared_error(y_pred, y_actual):
 return tf.reduce_mean(tf.square(y_pred-y_actual)) 
```

我们还需要初始化训练数据（通常命名为x_train和y_train）和与测试相关的数据（通常命名为x_test和y_test）的变量，通常在训练集和测试集之间以80/20或75/25的比例进行分配数据。然后，训练过程以以下方式调用前面的Python函数

```python
loss = squared_error(predict(x_train), y_train)
print("Loss:", loss.numpy())
```

尽管本节中的Python函数很简单，但是可以将它们通用化以处理复杂的模型，例如本章稍后介绍的模型。

您还可以通过深度学习来解决线性回归问题，其中涉及到的代码与您在本节前面看到的代码相同

##### 什么是超参数

深度学习涉及超参数，它们类似于旋钮和转盘，其值在实际训练过程之前由您初始化。例如，隐藏层的数量和隐藏层中神经元的数量是超参数的示例。您将在深度学习模型中遇到许多超参数，此处列出了其中的一些：

•  隐藏层数

•  隐藏层中的神经元数量

•  权重初始化

•  激活函数

•  代价函数

•  优化器

•  学习率

•  dropout

初始列表中的前三个超参数对于神经网络的初始设置是必需的。前向传播需要第四个超参数。接下来的三个超参数（即代价函数，优化器和学习率）是必需的，以便在监督学习任务期间执行向后错误传播（通常简称为backprop）。此步骤计算一组数字，这些数字用于更新神经网络中的权重值，以提高神经网络的准确性。如果您需要减少模型的过度拟合，则最终的超参数很有用。通常，代价函数是所有这些超参数中最复杂的。

在反向传播期间， 可能会出现消失的梯度问题（即，梯度值非常接近零），此后不再更新某些权重，在这种情况下，神经网络本质上是惰性的（调试此问题通常是不重要的） 。另一个要考虑的问题是：确定局部最小值是否“足够好”，并且比花费额外的时间和精力来寻找绝对最小值更为可取。

##### 深度学习架构

如前所述，深度学习支持各种架构，包括MLP，CNN，RNN和LSTM。尽管这些架构可以解决的任务类型有所重叠，但是每个架构都有其创建的特定原因。随着您从MLP升级到LSTM，架构变得越来越复杂。有时，这些架构的组合非常适合解决任务。例如，捕获视频并进行预测通常涉及CNN（用于处理视频序列中的每个输入图像），LSTM，以预测视频流中对象的位置。

此外，对于NLP神经网络可以包含一个或多个CNN ，RNN ，LSTM 和双LSTM （双向LSTM）。特别地，强化学习与这些架构的结合被称为深度强化学习。

尽管MLP很久以来一直很流行，但是它们有两个缺点：它们不能扩展到计算机视觉任务，并且很难训练。另一方面，CNN不需要相邻层完全连接。CNN的另一个优点是所谓的平移不变性，这意味着图像（例如数字，猫，狗等）都可以被这样识别，而不管其在位图中的位置如何。

##### 深度学习可以解决的问题

如您所知，反向传播涉及更新连续层之间的边缘权重，该权重以从右到左的方式执行（即，从输出层到输入层）。更新涉及链规则（计算导数的规则）以及参数和梯度值的算术乘积。可能会发生两种异常结果：项的乘积接近零（这称为消失梯度问题）或项的乘积变得任意大（称为爆炸梯度 问题）。sigmoid激活函数会产生这些问题。

深度学习可以通过LSTM缓解这两个问题。深度学习模型通常用ReLU激活函数代替sigmoid激活函数。ReLU是一个非常简单的连续函数，除了原点外，其他地方都是可微分的（y轴右侧为1值，y轴左侧为-1值）。因此，有必要进行一些调整以使事物在原点处正常工作。

##### 深度学习中的挑战

尽管深度学习功能强大并且在许多领域都取得了令人瞩目的成果，但仍在探索一些重要的持续挑战，包括：

•    算法偏差

•    容易受到对抗性攻击（Susceptibility to adversarial attacks）

•    概括能力有限

•    缺乏解释性

•    相关但非因果关系

算法可能包含意外偏差，即使消除了偏差，数据中也可能存在意外偏差。例如，例如，一个神经网络在包含高加索人男性和女性图片的数据集上进行训练。培训过程的结果“确定”男性是医生，女性是家庭主妇，而且这样做的可能性很高。原因很简单：数据集几乎只描绘了这两个角色中的男性和女性。以下文章包含有关算法偏差的更多信息：

*https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix*

深度学习专注于在数据集中查找模式，而将这些结果归纳起来则是一项更加困难的任务。有一些举措试图为神经网络的结果提供可解释性，但是这种工作仍处于起步阶段。深度学习可以找到模式并可以确定相关性，但是它无法确定因果关系。

现在，您已经对深度学习有了一个总体的了解，让我们回顾并讨论机器学习的一个重要基石，即感知器，这是下一节的主题

#### 什么是感知器

回顾第四章，线性回归模型涉及一个包含单个神经元的输出层，而一个多神经元输出层用于分类器（在第3章中讨论过）。DNN（深层神经网络）至少包含两个隐藏层，它们可以解决逻辑回归问题以及分类问题。实际上，用于分类问题的模型的输出层由一组总和等于1的概率（数据集中的每个类别一个）组成

图4.1显示了一个带有数字权重的输入边的感知器。

![image-20201204162629342](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204162629342.png)

##### 感知器功能的定义

一个感知器包括函数f(x) ，其中下式成立：

f(x)= 1， if W * X + B> 0（否则f(x)= 0）

在前面的表达式中，w是权重的向量，x是输入向量，b是偏差向量。乘积w * x是向量w和x的内积，激活感知器是全有或全无的决定（例如，灯泡是打开还是关闭，没有中间状态）。

注意，函数f（x）检查线性项w * x + b的值，该值也在Sigmoid函数中指定以进行逻辑回归。相同的术语作为S型值计算的一部分出现，如下所示：

1 / [1 + e ^（w * x + b）]

给定w * x + b的值，前面的表达式生成一个数值。但是，一般情况下，W是权重矩阵，x和b是向量。

下一节将稍微介绍一下以描述人工神经网络，之后我们将讨论MLP。

##### 感知器的详细视图

神经元本质上是神经网络的构建块。通常，每个神经元接收多个输入（它们是数值），每个输入来自属于神经网络中前一层的神经元。计算输入的加权总和，并将其分配给神经元。

具体地，假设神经元N'（N“素数”）接收权重在集合{ w1，w2，w3，....., wn }，其中这些数字指定连接到神经元N'的边缘的权重。由于正向传播涉及从左到右方式的数据流，因此这意味着边缘的左端点连接到神经元{ N1，N2，.....，Nk }在上一层中，这些边的右端点为N'。加权和的计算方法如下：x1 * w1 + x2 * w2 +。。。+ xn * wn

计算出加权总和后，将其输入到激活函数中，该函数将计算第二个值。人工神经网络需要执行此步骤，本章稍后将对此进行说明。对于给定层中的每个神经元，都将重复计算加权总和的过程，然后在神经网络的下一层中的神经元上，将重复相同的过程。

整个过程称为正向传播，它由反向误差传播步骤（也称为backprop）进行补充。在反向误差传播步骤中，将为整个神经网络计算新的权重值。对于每个数据点（例如，CSV文件中的每一行数据）都重复使用前向道具和后向道具的组合。目标是完成此训练过程，以使最终的神经网络（也称为模型）准确地表示数据集中的数据，并且还可以准确地预测测试数据的值。当然，神经网络的准确性取决于所讨论的数据集，并且准确性可以高于99％。

#### 人工神经网络（ANN）的解剖

一个ANN由一个输入层，一个输出层和一个或多个隐藏层组成。对于ANN中的每对相邻层，左层中的神经元通过具有数字权重的边与右层中的神经元相连。如果左侧层中的所有神经元都连接到右侧层中的所有神经元，则称为MLP（稍后讨论）。

请记住，ANN中的感知器是“无状态的”：它们不保留有关先前处理的数据的任何信息。此外，ANN不包含循环（因此ANN是非循环的）。相比之下，RNN 和LSTM保持状态，他们确实有周期类似的行为，因为你将在本章后面看到。

顺便说一句，如果您有数学背景，则可能会倾向于将ANN视为一组连续的二部图，其中数据从输入层（认为“多个源”）流向输出层（“接收器”） 。不幸的是，这种观点对理解ANN并没有帮助。理解ANN的更好方法是将其结构视为以下列表中超参数的组合：

•  隐藏层数

•  每个隐藏层中的神经元数量

•  连接神经对对的边的初始权重

•  激活功能

•  代价（又称亏损）功能

•  优化器（与代价函数一起使用）

•  学习率（少数）

•  dropout（可选）

![image-20201204164630085](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201204164630085.png)

由于图4.2中ANN的输出层包含多个神经元，因此我们知道它是用于分类任务的模型

##### 初始化模型的超参数

上一部分中的项目符号列表中的前三个参数是初始化神经网络所必需的。隐藏层是中间计算层，每个计算层都由神经元组成。每对相邻层之间的边数是灵活的，由您确定。有关网络初始化的更多信息在这里：

http://www.deeplearning.ai/ai-notes/initialization/
连接每对相邻层（包括输入层和输出层）中的神经元的边具有数字权重。这些权重的初始值通常是介于0和1之间的小的随机数。请记住，相邻层之间的连接会影响模型的复杂性。训练过程的目的是微调边缘权重，以提高模型的准确性。

ANN不一定完全连接，也就是说，相邻层神经元对之间的某些边缘可能会丢失。相比之下，诸如CNN的神经网络共享边（及其权重），这可以使它们在计算上更加可行（但即使是CNN也可能需要大量的训练时间）。注意，Keras tf.keras.layers.Dense（）类处理的完全连接两个相邻层的任务。如稍后所讨论的，MLP是完全连接的，这可以大大增加这种神经网络的训练时间。

##### 激活超参数

第四个参数是激活函数，该函数应用于每对连续层之间的权重。具有许多层的神经网络通常涉及不同的激活功能。例如，CNN在特征图上使用ReLU激活功能（通过对图像应用滤镜来创建），而倒数第二层则通过softmax函数（这是sigmoid函数的概括）连接到输出层

#### 损失函数超参数

第五，第六和第七超参数是从输出层开始并向右向左移向输入层的反向误差传播所必需的。这些超级参数极大地提高了机器学习框架的强度：它们计算神经网络中边缘权重的更新。

损失函数是多维欧几里得空间的函数。例如，MSE损失函数是具有全局最小值的碗形损失函数。通常，目标是最小化MSE函数以最小化损失，这反过来将帮助我们最大化模型的准确性（但这不能保证其他损失函数）。但是，有时将局部最小值视为“足够好”，而不是找到全局最小值：您必须做出此决策（即，这不是纯粹的程序化决策）。

大型数据集的损失函数往往非常复杂，这对于检测数据集中的潜在模式是必需的。另一个损失函数是交叉熵函数，它涉及最大化似然函数（将此与MSE进行对比）。搜索在线文章（例如Wikipedia）以获取有关损失函数的更多详细信息。

#### 优化器超参数

一个优化的是，被选择在具有损失函数结合的算法，并且其目的是为了收敛到在训练阶段期间的成本函数的最小值（见前面部分中关于一个局部最小值的注释）。不同的优化器对训练过程中计算新近似值的方式做出不同的假设。一些优化器仅涉及最新近似值，而其他优化器则使用 考虑了多个先前近似值的滚动平均值。

有几种著名的优化器，包括SGD，RMSprop，Adagrad，Adadelta和Adam 。在线检查有关这些优化器的优点和折衷方法的详细信息

##### 学习率超参数

所述学习率是一个小数目，通常在0.001和0.05之间，这会影响其被添加到的边缘的当前权重，以便将模型与这些更新的权重训练数的大小。学习率具有某种限制作用。如果该值太大，则新的近似值可能会超出最佳点。如果太小，训练时间会大大增加。打个比方，假设您乘坐的是客机，而距机场100英里。当您接近机场时，飞机的速度会降低，这对应于神经网络中学习率的降低。

##### dropout

所述dropout rate是第八超参数，它是在0和1之间的小数值，典型地在0.2和0.5之间。将此十进制值乘以100，可以确定在训练过程中每次向前通过时要忽略的随机选择的神经元的百分比。例如，如果dropout rate为0.2，则将在向前传播的每个步骤中随机选择20％的神经元并忽略它们。每当在神经网络中处理新的数据点时，都会随机选择一组不同的神经元。请注意，神经元并未从神经网络中删除：它们仍然存在，并且在正向传播过程中忽略它们会导致神经网络变薄。在TF 2中，tf.keras.layers.Dropout 类执行细化神经网络的任务。

您可以指定其他超级参数，但是它们是可选的，对于理解ANN来说不是必需的

#### 什么是反向传播算法？

ANN通常以从左到右的方式绘制，其中最左边的层是输入层。每层的输出将成为下一层的输入。术语“前向传播”是指将值提供给输入层，并通过隐藏层向输出层发展。输出层包含模型的正向通过的结果（它们是估计的数值）。

这是一个关键点：向后误差传播涉及用于更新神经网络中边缘权重的数字的计算。更新过程是通过损失函数（以及优化程序和学习率）执行的，从输出层（最右边的层）开始，然后从右向左移动以便更新连续图层之间的边缘权重。该过程训练了神经网络，这涉及减少输出层的估计值和真实值之间的损失（在监督学习的情况下）。对数据集训练部分中的每个数据点重复此过程。处理数据集被称为一个时期，很多时候神经网络都是通过多个时期来训练的。

上一段没有解释损失函数是什么或如何选择：这是因为损失函数和优化器以及学习率是上一节中讨论的超参数。但是，两个常用的损失函数是MSE和交叉熵。常用的优化器是Adam优化器（以及SGD和RMSprop等）；学习率的通用值为0.01。

#### 什么是多层感知器（MLP）?

多层感知器（MLP）是一种前馈人工神经网络，由至少三层节点组成：输入层，隐藏层和输出层。一个MLP完全连接：给定一对相邻层的，在左边的层的每一个节点被连接到在右侧层的每个节点。除了输入层中的节点外，每个节点都是神经元，神经元的每一层都涉及非线性激活函数。另外，MLP使用反向传播算法（或简称为back prop）进行训练，这对于CNN（卷积神经网络）也适用

图4.3显示了具有两个隐藏层的MLP的内容。

需要牢记的一点是：MLP的非线性激活功能将MLP与线性感知器区分开来。实际上， MLP可以处理不可线性分离的数据。例如，OR函数和AND函数涉及线性可分离的数据，因此可以通过线性感知器来表示它们。另一方面，XOR函数涉及不可线性分离的数据，因此需要神经网络（例如MLP）

![image-20201215182724922](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201215182724922.png)

##### 激活功能

在任何相邻对层之间没有激活功能的MLP是线性系统：在每一层，只需将前一层的向量与当前矩阵（将当前层连接到下一层）相乘即可生成另一个向量。

另一方面，将一组矩阵相乘以生成单个矩阵很简单。由于没有激活函数的神经网络是线性系统，因此我们可以将这些矩阵（每一对相邻层的矩阵）相乘以生成单个矩阵：原始神经网络因此被简化为由以下组成的两层神经网络：输入层和输出层，这破坏了具有多层神经网络的目的。

为了防止神经网络层的这种减少，MLP必须在相邻层之间包括非线性激活函数（这对于任何其他深度神经网络也是如此）。非线性激活函数的选择通常为S型，tanh（双曲正切函数）或ReLU（线性化校正单元）。

sigmoid函数的输出范围是0到1，具有“压缩”数据值的作用。类似地，tanh函数的输出范围是-1到1。但是，ReLU激活函数（或其变体之一）对于ANN和CNN来说是首选，而Sigmoid和tanh在LSTM中使用。

接下来的几节包含构造MLP的详细信息，例如如何初始化MLP的权重，如何存储权重和偏差以及如何通过向后错误传播来训练神经网络。

#### **数据点如何正确分类？**

作为参考点：数据点是指数据集中的一行数据，可以是房地产数据集，缩略图数据集或其他某种类型的数据集。假设我们要为包含四个类（也称为标签）的数据集训练MLP。在这种情况下，输出层还必须包含四个神经元，其中神经元的索引值为0、1、2和3（十个神经元输出层的索引值为0到9（包括0和9）。由于从倒数第二层过渡到输出层时使用了softmax激活函数，因此输出层中的概率总和始终等于1 。

该索引具有最大概率值与比较索引 的当前数据点的从数据集的标签值的一个热编码。如果索引值相等，则NN已正确分类了当前数据点（否则不匹配）。

例如， MNIST数据集包含从0到9（含0）到9的手绘数字图像，这意味着MNIST数据集的NN在最后一层有十个输出，每个数字一个。假设当前正在通过NN传递包含数字3的图像。 3的onehot编码为[0,0,0,1,0,0,0,0,0,0] ，并且onehot编码中具有最大值的索引值也为3。现在假设输出处理数字3后的神经网络层是以下向量

值的取值：[0.05,0.05,0.2,0.6,0.2,0.2,0.1,0.1,0.238] 。如您所见，具有最大值（0.6）的索引值也是3。在这种情况下，神经网络已正确识别输入图像。

二元分类器用于处理任务，例如确定的垃圾邮件/不垃圾邮件，欺诈/不欺诈，股票增加/减少（或温度，或气压）两种结果，等等。预测股票价格的未来价值是一项回归任务，而预测股票价格将上涨还是下跌是一项分类任务。

在机器学习中，多层感知器是一个用于二元分类器（是线性分类器的一种）的监督学习的神经网络。但是，单层感知器只能学习线性可分离的模式。实际上，由Marvin Minsky和Seymour Papert撰写的著名著作Perceptrons（写于1969年）表明，这些类的网络不可能学习XOR函数。但是，两层感知器可以“学习” XOR函数。

#### CNN高级视图

CNN是很深的NN（具有一个或多个卷积层），非常适合图像分类以及其他用例，例如音频和NLP （自然语言处理）。

尽管MLP已成功用于图像识别，但它们却无法很好地缩放，因为每一对相邻层都完全连接在一起，这反过来又会导致大规模的神经网络。对于大图像（或其他大输入），复杂度变得很明显，并且会对性能产生不利影响

![image-20201215183638570](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201215183638570.png)

##### 简约的CNN

高质量的CNN可能非常复杂，包含许多隐藏层。但是，在本节中，我们将研究一个简约的CNN （本质上是一个“玩具”神经网络），它由以下几层组成：

•     Conv2D（卷积层）

•     ReLU（激活函数）

•    最大池化层（池化技术）

•    全连接（FC）层

•     Softmax激活函数

接下来的小节简要说明了前面的项目列表中每个项目符号的目的。

##### 卷积层（Conv2D）

卷积层通常在Python和TF代码中标记为Conv2D。所述Conv2D层包括一组过滤器，这是小的方阵，其尺寸通常是3×3，但也可以是5×5，7×7，或甚至1×1。每个滤镜都会在图像上进行扫描（认为是《星际迷航》电影中的三阶），并且在每个步骤中，都会使用滤镜和当前在滤镜下方的部分计算出内积。该扫描过程的结果称为包含实数的特征图。

![image-20201216205810612](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216205810612.png)

##### RELU激活函数

创建每个要素图之后，要素图中的某些值可能为负。ReLU激活函数的目的是将负值（如果有）替换为零。回忆一下ReLU函数的定义：

RELU（X）= X，如果x> = 0

RELU（X）= 0，如果x <0

如果绘制ReLU的2D图，则它由两部分组成：x的水平轴小于零，x的第一个象限中的标识函数（是一条线）大于或等于0。

##### 最大池化层

第三步涉及最大池化，该池易于执行：在上一步中使用ReLU激活功能处理了特征图后，将更新后的特征图划分为2x2矩形，然后从每个矩形中选择最大值。结果是一个较小的数组，其中包含25％的要素图（即，丢弃了75％的数字）。您可以使用几种算法来执行此提取：每个正方形中数字的平均值；每个平方中数字平方和的平方根；或每个正方形中的最大数量。

对于CNN，最大池化算法从每个2x2矩形中选择最大数量。图4.6显示了CNN中最大池化的结果

![image-20201216210003730](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216210003730.png)

如您所见，结果是一个很小的正方形数组，其大小仅为前一个要素图的25％。对在Conv2D层中选择的一组过滤器中的每个过滤器执行此序列。该集合可以具有8、16、32或更多个过滤器。

如果您对此技术感到困惑或怀疑，请考虑涉及压缩算法的类比，可将其分为两种类型：有损和无损。如果您还不知道，JPEG是一种有损算法（即，数据在压缩过程中丢失了），但它对于压缩图像还是很好的。如果有帮助，可以考虑将最大池作为有损压缩算法的对等物，也许可以说服您该算法的功效。

同时，您的怀疑是有效的。实际上，杰弗里·欣顿（Geoffrey Hinton）（通常被称为深度学习的教父）提出了一种称为胶囊网络的最大池替代方案。这种体系结构更复杂，更难训练，并且超出了本书的范围（您可以找到详细讨论胶囊网络的在线教程）。但是，胶囊网络倾向于更具有对抗性的GAN（生成对抗网络）。

重复前面的步骤序列（如LeNet中所述），然后执行一个相当不直观的操作：展平所有这些小数组，使其成为一维向量，并将这些向量连接成一个（很长）向量。然后，生成的向量与输出层完全连接，后者由10个“存储桶”组成。在MNIST的情况下，这些占位符表示0到9之间的数字。请注意，Keras类tf.keras.layers.Flatten执行该平整处理。

SOFTMAX激活函数以填充输出层的10桶施加到数字的长向量。结果：10个存储桶中填充了一组总和等于1的非零（非负）数字。查找包含最大数字的存储桶的索引，并将该数字与与刚刚处理的图像相关联的单热编码标签的索引进行比较。如果索引值相等，则图像被成功识别。

更为复杂的CNN涉及多个Conv2D层，多个FC（完全连接）层，不同的过滤器大小，以及用于组合先前的层（例如ResNet）以增强数据值的当前层的技术。有关CNN的其他信息，请参见：https：//en.wikipedia.org/wiki/Convolutional_neural_network。

现在，您已经对CNN有了一个高级的了解，让我们看一个代码示例，该示例说明MNIST数据集中的图像（以及该图像的像素值），然后是两个使用Keras在其上训练模型的代码示例。该MNIST数据集。

#### 在MNIST数据集中显示图像

listing4.2显示了tf2_keras_mnist_digit.py的内容，该内容说明了如何在TensorFlow中创建一个处理MNIST数据集的神经网络

##### listing 4.2：tf2_keras_mnist_digit.py

```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train.shape:",X_train.shape)
print("X_test.shape: ",X_test.shape)

first_img = X_train[0]

# uncomment this line to see the pixel values
#print(first_img)

import matplotlib.pyplot as plt
plt.imshow(first_img, cmap='gray')
plt.show()
```

listing 4.2从一些导入语句开始，然后从MNIST数据集中填充训练数据和测试数据。变量first_img被初始化为X_train数组中的第一项，它是训练数据集中的第一张图像。清单中的最后代码块显示第一个图像的像素值。输出为

![image-20201216210728672](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216210728672.png)

![image-20201216210812435](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216210812435.png)

#### Keras和MNIST数据集

你读到包含代码样本Keras为基础的使用模型MNIST数据集，模型输入层中使用不同的API。

具体地说，不是CNN的模型通过tf.keras.layers.Flatten（）API将输入图像展平为一维向量，此处是一个示例（有关详细信息，请参见listing 4.3）：

tf.keras.layers.Flatten（input_shape =（28,28））

另一方面，CNN使用tf.keras.layers.Conv2D（）API，此处是一个示例（有关详细信息，请参见listing 4.4）：

tf.keras.layers.Conv2D（32，（3,3），activation =' relu '，input_shape =（28,28,1））

 listing 4.3显示的内容keras_mnist.py示出如何创建Keras的基于在TensorFlow神经网络处理该MNIST数据集。

##### listing 4.3：keras_mnist.py

```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()

x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),
                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model.summary()

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',
              metrics=[‘accuracy'])
model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test
```

listing 4.3从一些import语句开始，然后初始化变量mnist作为对内置MNIST数据集的引用。接下来，使用MNIST数据集的相应部分初始化与训练相关和与测试相关的变量，然后对x_train和x_test进行比例转换。

listing 4.3的下一部分定义了一个非常简单的基于keras的模型，它有四个层，这些层是从tf.keras.layers包中的类创建的。下一个代码段显示模型定义的摘要，如下所示：

![image-20201216215516941](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216215516941.png)

listing 4.3的其余部分将编译，拟合和评估模型，该模型将产生以下输出

![image-20201216215606325](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216215606325.png)

如您所见，此模型的最终精度为98.6％，这是一个可观的值。

#### Keras，CNN和MNIST数据集

listing 4.4显示的内容keras_cnn_mnist.py示出如何创建Keras的基于在TensorFlow神经网络处理该MNIST数据集。

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
(train_images, train_labels), (test_images, test_labels)=tf.keras.datasets.mnist.load_data()

train_images = train_images.reshape((60000, 28, 28,1))
test_images = test_images.reshape((10000, 28, 28, 1))

# Normalize pixel values: from the range 0-255 to the range 0-1
train_images, test_images = train_images/255.0,test_images/255.0

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3),activation='relu', input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3),activation='relu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3),activation='relu'))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(10,activation='softmax'))

model.summary()

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=[‘accuracy'])
                                                                               
model.fit(train_images, train_labels, epochs=1)
test_loss, test_acc = model.evaluate(test_images,test_labels)
print(test_acc)
                                                                               
# predict the label of one image
test_image = np.expand_dims(test_images[300],axis = 0)
plt.imshow(test_image.reshape(28,28))
plt.show()
                                                                               
result = model.predict(test_image)
print("result:", result)
print("result.argmax():", result.argmax())
```

listing 4.4通过load_data（）函数初始化了训练数据和标签，以及测试数据和标签。接下来，将图像重塑为28x28图像，然后将像素值从0-255（所有整数）范围缩放到0-1（小数值）范围。

listing 4.4用途的下一部分Keras Sequential() API来定义一个基于keras的模型，它包括两对的Conv2D和MaxPooling2D层，其次是扁平层，然后两个连续的全连接层。

接下来，分别通过compile（），fit（）和valuate（）方法对模型进行编译，训练和评估。listing 4.4的最后部分成功地预测了标签为4的图像，然后通过Matplotlib显示了该图像。启动listing 4.4中的代码，您将在命令行上看到以下输出：

![image-20201216230442825](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216230442825.png)

您可能会问自己清单4.4中的模型是如何实现的。当每个输入图像都被平化成一个一维向量时，这样的高精度，它失去了二维图像中可用的邻接信息。在cnn流行之前，有一项技术使用mlp和另一种技术涉及支持向量机作为模型图像。事实上，如果你没有足够的图片来训练，你可以仍然使用SVM。另一个选项是使用GAN生成合成数据(这是它最初的目的)

![image-20201216231048112](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201216231048112.png)

#### 使用CNN分析音频信号

除了图像分类，您还可以使用音频信号训练CNN，音频信号可以从模拟转换为数字。音频信号具有各种数字参数（例如，分贝电平和电压电平），在此进行描述：

#### https://zh.wikipedia.org/wiki/Audio_signal

如果您有一组音频信号，则其关联参数的数值将成为CNN的数据集。请记住，CNN不能理解数字输入值：无论数字值的来源如何，都以相同的方式处理数字值。

一个用例涉及建筑物外部的麦克风检测并识别各种声音。显然，识别车辆的回火声和枪声是很重要的。在后一种情况下，将通知警方潜在的犯罪。有些公司使用CNN识别不同类型的声音。其他公司正在探索使用RNN和LSTM代替CNN 。

#### 总结

在本章中，您简要介绍了深度学习，它与机器学习的区别以及它可以解决的一些问题。您了解了深度学习中存在的挑战，其中包括算法的偏见，对抗性攻击的易感性，概括能力有限 ，神经网络缺乏可解释性以及缺乏因果关系。

接下来，您了解了XOR函数，该函数是平面中四个点的非线性可分离集合的示例。尽管在2D情况下它很简单，但XOR函数无法用单层浅层网络解决：相反，需要两个隐藏层。接下来，您了解了Perceptrons，它本质上是神经网络的核心构建块。你也看到了Keras基础的代码样本在训练神经网络MNIST数据集。此外，您了解了细胞神经网络是如何构建的，基于keras的代码示例用于训练CNN与MNIST数据集：阅读完第3章中与激活功能有关的部分后，此代码示例将更有意义。



### 第五章 深度学习：RNN和LSTM

本章通过讨论RNN（循环神经网络）和LSTM（长短期记忆神经网络）扩展了第4章的介绍。

本章的第一部分向您介绍RNN的体系结构，BPTT（反向传播）以及一个简短的基于Keras的代码示例。正如您将看到的，RNN可以跟踪较早时间段的信息，这使它们对于包括NLP任务在内的各种任务都非常有用。

本章的第二部分向您介绍LSTM的体系结构，该体系结构比RNN更复杂。具体而言，LSTM包括遗忘门，输入门和输出门，以及长期存储单元。您还将了解LSTM相对于RNN的优势。此外，您还将接触到在某些与NLP相关的著名模型中使用的双向LSTM（请参见第6章）。

本章的第三部分向您介绍自动编码器的体系结构和使用它们的原理，并介绍变型自动编码器。

#### 什么是RNN

RNN是循环神经网络，这是20世纪80年代发明的模型。RNN适用于包含序列数据的数据集以及NLP任务，例如语言建模，文本生成或句子的自动完成。实际上，您可能甚至可以通过RNN执行图像分类（例如MNIST）。

除了简单的RNN，还有更强大的结构，如LSTM和GRU。基本的RNN具有最简单类型的反馈机制（稍后描述），并且包含Sigmoid激活函数。

RNN （包括LSTM和GRU）在一些重要方面与ANN不同，如下所示：

•  状态性（所有RNN）

•  反馈机制（所有RNN）

•  sigmoid或tanh激活函数

•  多个门（LSTM和GRU）

•  BPTT（反向传播）

•  截断的BPTT（简单的RNN）

ANN和CNN本质上是无状态的，而RNN是有状态的。因此，RNN可以处理更复杂的输入序列，这使其适合于诸如手写识别或语音识别之类的任务。

##### 解剖RNN

假设输入序列被标记为x1，x2，x3，...，x（t），....并且隐藏状态的序列被标记为h1，h2，h3，...，h（t ）。请注意，每个输入序列和隐藏状态序列都是1*n向量，其中n是特征数量。

在时间段t，输入基于h（t-1）和x（t）的组合，此后将激活函数应用于该组合（这也可能涉及添加偏差矢量）。

另一个区别是连续时间间隔之间发生的RNN反馈机制。具体而言，将先前时间段的输出与当前时间段的新输入组合在一起，以计算新的内部状态。让我们用序列{H（0） ，H（1） ，H（2） ，。 。。H（T-1） ，H（t）的}来表示该组的内部状态的RNN的时间段期间{0 ，1 ，2 ，... ，T-1 ，T}和让我们还假设序列 {x（0），x（1），x（2），... ，x（t-1），x（t）}是相同时间段内的输入。

在时间段t ，RNN的基本关系如下：
$$
h(t)=f(W*x(t)+U*h(t-1))
$$
在上式中，W和U是权重矩阵，f通常是tanh激活函数。

##### 什么是BPTT

RNN中的BPTT（通过时间的反向传播）与CNN的反向传播相对应。在BPTT期间更新RNN的权重矩阵来训练神经网络。

但是，在RNN中可能存在爆炸梯度的问题，也就是说，梯度变得任意大（而在所谓的消失梯度情况下，梯度变得任意小）。解决爆炸梯度问题的一种方法是使用截断的BPTT，这意味着仅针对少量步长而不是所有时间步长计算BPTT。另一种技术是为梯度指定最大值，这涉及简单的条件逻辑。

还有另一种克服爆炸梯度和消失梯度问题的方法，其中涉及LSTM，本章稍后将对此进行讨论。

#### 什么是LSTM

LSTM是RNN的一种特殊类型，非常适合许多用例，包括NLP，语音识别和手写识别。LSTM非常适合处理称为长期依赖的问题，长期依赖是指相关信息与需要该信息的位置之间的距离差距。当文档某一部分中的信息需要链接到文档中较远位置的信息时，就会出现这种情况。

LSTM于1997年开发，并不断超越最新算法的准确性。LSTM也开始革新语音识别（大约在2007年）。然后在2009年，LSTM赢得了模式识别竞赛，在2014年，百度使用RNN超过了语音识别记录。

##### 解剖LSTM

LSTM是有状态的，它们包含涉及S型函数的三个门（忘记门，输入门和输出门）以及涉及tanh激活函数的单元状态。在时间段t，LSTM的输入基于两个向量h（t-1）和x（t）的组合。在忘记门，输入门和输出门的情况下，将这对输入进行组合，然后将Sigmoid激活函数应用于该组合（还可以包括偏置矢量）。

在时间步t处发生的处理是LSTM的短期记忆。LSTM的内部单元状态保持长期记忆。如上一段所述，更新内部单元状态涉及到tanh激活函数，而其他门则使用Sigmoid激活函数。

##### 双向LSTM

除了单向LSTM，您还可以定义 由两个常规LSTM组成的双向LSTM ：一个LSTM用于正向，一个LSTM位于反向或相反。您可能会惊讶地发现双向LSTM非常适合解决NLP任务。

例如，ELMo是使用双向LSTM的NLP任务的深层词表示。NLP领域中有一个较新的架构transformer。BERT是一个非常著名的系统（由Google在2018年发布），它使用双向transformer，可以解决复杂的NLP问题。

##### LSTM公式

LSTM的公式比简单RNN的更新公式更复杂，但是有些模式可以帮助您理解这些公式。

公式显示了在时间步长t期间，如何为忘记门，输入门和输出门计算新的权重。此外，前面的链接显示了如何计算新的内部状态和隐藏状态（在时间步t处）。

请注意忘记门，输入门和输出门的模式：它们都计算两个项的和，每个项都是x（t）和h（t）的乘积，然后将Sigmoid函数应用于该和。具体来说，以下是时间t的“遗忘门”的公式：
$$
f(t)=\sigma(W(f)*x(t)+U(f)*h(t)+b(f))
$$
在上式中，W（f），U（f）和b（f） 分别是与x（t）关联的权重矩阵，与h（t）关联的权重矩阵以及忘记门的偏置向量。

请注意，输入门和输出门的计算与遗忘门的计算具有相同的模式。区别在于输入门具有矩阵W（i）和U（i），而输出门具有矩阵W（o）和U（o）。

用于计算的C（T） ，i（T） ，和H（t）的均基于该值F（T） ，i（T） ，和O（T） ，如下所示：
$$
c(t)=f(t)*c(t-1)+i(t)*tanh(c'(t))\\
c'(t)=\sigma(W(c)*x(t)+U(c)*h(t-1))\\
h(t)=o(t)*tanh(c(t))
$$
LSTM的最终状态是一维向量，其中包含LSTM中所有其他层的输出。如果您的模型包含多个LSTM，则给定LSTM的最终状态向量将成为该模型中下一个LSTM的输入。

##### LSTM超参数调整

LSTM也容易过度拟合，如果您正在为LSTM手动优化超参数，则这里列出了要考虑的事项：

•  过拟合（使用L1或L2正则化）

•  较大的网络更可能过拟合

•  更多数据可以减少过拟合

•  在多个时段训练网络

•  学习率至关重要

•  堆叠层可能会有所帮助

•  对LSTM使用softsign而非softmax

•  RMSprop，AdaGrad或动量是不错的选择

•  Xavier权重初始化

#### 什么是GRU

GRU（门控重复单元）是一类RNN也是简化的LSTM。GRU和LSTM之间的主要区别在于：GRU具有两个门（重置和更新门），而LSTM具有三个门（输入，输出和遗忘门）。GRU中的重置门执行LSTM的输入门和遗忘门的功能。

请记住，GRU和LSTM都具有有效跟踪长期依赖关系的目标，并且都解决了梯度消失和梯度爆炸的问题。

#### 什么是AE

自编码器（AE）是类似于MLP的神经网络，其中输出层与输入层相同。最简单的AE类型包含单个隐藏层，该隐藏层的神经元少于输入层或输出层。但是，有许多不同类型的AE，其中有多个隐藏层，有时包含比输入层更多的神经元（有时包含更少的神经元）。

AE使用无监督学习和反向传播来学习有效的数据编码。它们的目的是降低维数：AE将输入值设置为等于输入值，然后尝试查找恒等函数。图5.2显示了一个涉及单个隐藏层的简单AE。

本质上，基本的AE将输入压缩为尺寸小于输入数据的“中间”向量，然后将其转换为具有与输入相同形状的张量。下面列出了AE的几种用例：

•  文件检索

•  分类

•  异常检测

•  对抗式自动编码器

•  图像降噪（生成清晰图像）

AE也可以用于特征提取，因为它们可以产生比PCA更好的结果。请记住，AE是特定于数据的，这意味着它们仅适用于相似的数据。但是，它们不同于图像压缩（并且对于数据压缩而言是中等的）。例如，在脸部受过训练的自动编码器在树木的图片上效果不佳。概括而言，AE涉及：

•  将输入“压缩”到较小的图层

•  学习一组数据的表示形式

•  通常用于降维（PCA）

•  仅保留中间的“压缩”层

请考虑一个10x10的图像（100像素），以及一个具有100个神经元（10x10像素）的AE，一个具有50个神经元的隐藏层以及一个具有100个神经元的输出层。因此，AE将100个神经元压缩为50个神经元。

如您先前所见，AE有多种变体，下面列出其中一些：

•  LSTM自动编码器

•  去噪自动编码器

•  压缩自动编码器

•  稀疏自动编码器

•  堆叠式自动编码器

•  深度自动编码器

•  线性自动编码器

##### 自编码器和PCA

如果自动编码器涉及线性激活或仅包含一个Sigmoid隐藏层，则自动编码器的最佳解决方案与主成分分析（PCA）密切相关。

具有单个隐藏层（大小为p）（其中p小于输入的大小）的自动编码器的权重与由前p个主分量所覆盖的向量子空间跨度相同。

自动编码器的输出是对该子空间的正交投影。自动编码器权重不等于主分量，并且通常不正交，但是可以使用奇异值分解从主分量中恢复主分量。

##### 什么是变分自编码器

简而言之，变分自动编码器是一种增强的常规自编码器，其中左侧充当编码器，右侧充当解码器。双方都有与编码和解码过程相关的概率分布。

另外，编码器和解码器实际上都是神经网络。编码器的输入是数值的向量x，其输出是具有权重和偏差的隐藏表示z。解码器输入了a（即编码器的输出），其输出是数据的概率分布的参数，该参数也具有权重和偏差。注意，编码器和解码器的概率分布是不同的。

#### 什么是GAN

GAN为对抗生成网络，其最初目的是生成合成数据，典型地用于增强小的数据集或数据集不平衡的缩写。一个用例与失踪人员有关：将这些人员的可用图像提供给GAN，以生成这些人员今天的外观的图像。GAN还有许多其他用例，其中一些在这里列出：

•  艺术创作

•  创造时尚风格

•  改善低质量的图像

•  创建“人造”面孔

•  重建不完整/损坏的图像

Ian Goodfellow（蒙特利尔大学机器学习博士）在2014年创建GAN，Yann LeCun（Facebook的AI研究总监）呼吁对抗训练“在过去的10年中ML最有趣的想法。” 顺便说一句，Yann LeCun 与Yoshua Bengio和Geoffrey Hinton一起是2019年图灵奖的三位获奖者之一。

GAN变得越来越普遍，人们正在发现它们的创造性（意想不到的？）用途。GAN已经被用于邪恶的目的，例如规避图像识别系统。GAN可以通过更改像素值从有效图像生成伪造图像，以欺骗神经网络。由于这些系统依赖于像素模式，因此可以通过对抗图像（它们的像素值已更改的图像）来欺骗它们。

##### 对抗性攻击可以停止么

不幸的是，对抗性攻击没有长期解决方案，而且鉴于其性质，可能永远无法完全防御。尽管正在开发各种技术来对抗对抗性攻击，但是它们的效果往往是短暂的：创建了可以胜过这些技术的新GAN。

#### 创建GAN

GAN具有两个主要部分：生成器和鉴别器。生成器可以具有类似于CNN的体系结构以生成图像，而鉴别器可以具有类似于CNN的体系结构，以便检测（生成器提供的）图像是真实的还是伪造的。通过类比，生成器类似于赚取假币的人，而鉴别器类似于试图区分有效货币和假币的执法人员。

生成器（先前已初始化）将伪图像发送到鉴别器（已经训练但不再可更新）以进行分析。如果鉴别器在检测真实和伪图像方面非常准确，则需要修改生成器，以提高所产生伪图像的质量。对生成器的修改是通过向后误差传播来执行的。另一方面，如果鉴别器性能较差，则生成器将生成高质量的伪图像，因此生成器不需要进行重大修改。

# 第六章 NLP和强化学习

本章对NLP（自然语言处理）和强化学习（RL）进行了简要介绍。这两个主题都可以轻松填满整本书，通常涉及复杂的主题，这意味着本章将对这些主题进行有限的介绍。如果您想全面了解BERT（在本章的后面部分简要讨论），则需要了解注意事项和transformer架构。同样，如果您想深入了解深度强化学习，则需要了解深度学习架构。在阅读完本章中有关NLP和RL的粗略介绍之后，您可以找到有关您感兴趣的NLP或RL方面的其他在线信息。

第一部分讨论了NLP，以及Keras中的一些代码示例。本节还讨论了NLU（自然语言理解）和NLG（自然语言生成）。

第二部分介绍强化学习，并描述非常适合RL的任务类型。您将了解nchain任务和epsilon-greedy算法，它们可以解决使用纯贪婪算法无法解决的问题。在本节中，您还将学习Bellman方程，它是强化学习的基石。

第三部分讨论了Google的TF-Agents工具包，深度强化学习（结合强化学习的深度学习）和 Google Dopamine toolkit。

#### 使用NLP（自然语言处理）

本节重点介绍了NLP中的一些概念，根据您的背景，您可能需要执行在线搜索以了解有关这些概念的更多信息（尝试Wikipedia）。尽管以非常肤浅的方式对待这些概念，但是您将知道该如何追求以进一步学习NLP

当前，NLP是机器学习社区中非常感兴趣的焦点。以下列出了NLP的一些用例：

•  聊天机器人

•  搜索（文本和音频）

•  文字分类

•  情绪分析

•  推荐系统

•  问题解答

•  语音识别

•  NLU（自然语言理解）

•  NLG（自然语言生成）

在日常生活中，您会遇到许多这些用例：当您访问网页或在线搜索书籍或有关电影的建议时。

##### NLP技术

解决NLP任务的最早方法是基于规则的方法，该方法在整个行业中占据了主导地位。使用基于规则的方法的技术示例包括正则表达式（RegExs）和上下文无关文法（CFG）。有时会使用RegExs来从已从网页上抓取的文本中删除HTML标签，或从文档中删除不需要的特殊字符。

第二种方法涉及使用一些基于某些用户定义特征的数据来训练机器学习模型。该技术需要大量的特征工程（一项艰巨的任务），并且包括分析文本以删除不想要的和多余的内容（包括停用词），以及对单词进行转换（例如，将大写转换为小写）。

最新的方法涉及深度学习，其中神经网络学习特征而不是依靠人类来执行特征工程。关键思想之一涉及将单词映射到数字，这使我们能够将句子映射到数字的向量。将文档转换为向量后，我们可以对这些向量执行大量操作。例如，我们可以使用向量空间的概念来定义向量空间模型，其中两个向量之间的距离可以通过它们之间的角度（与余弦相似性相关）来度量。如果两个向量彼此接近，则对应的句子在含义上可能相似。它们的相似性基于分布假设，该假设断言在相同上下文中的单词往往具有相似的含义。

一篇不错的文章讨论了单词的矢量表示以及代码示例的链接：

https://www.tensorflow.org/tutorials/representation/word2vec

##### The Transformer Architecture and NLP

2017年，谷歌推出了Transformer神经网络架构，该架构是基于自注意力机制，非常适合语言理解。

Google显示，在将RNN和CNN转换为学术英语到德语以及英语到法语的过程中，transformer的性能均优于早期的基准测试。此外，transformer需要较少的计算来进行训练，并且将训练时间缩短了一个数量级。

transformer能够处理“I arrived at the bank after crossing the river”这句话，并正确判断“bank”一词指的是河岸，而不是金融机构。Transformer通过在bank和river之间建立关联，一步就可以确定。再举一个例子，transformer可以判断这两句话的不同含义：

“The horse did not cross the street because it was too tired.” 

“The horse did not cross the street because it was too narrow.”

transformer通过将单词与句子中的每一个单词进行比较，计算给定单词的下一个表示，从而为句子中的单词获得一个注意分值。transformer使用这些分数来确定其他单词对给定单词的下一个表示的影响程度。

这些比较的结果是句子中每个其他单词的注意力得分。结果，在计算bank的含义时，river获得了很高的关注度。

尽管LSTM和双向LSTM在NLP任务中大量使用，但是Transformer在AI社区中获得了很大的吸引力，不仅在语言之间进行翻译，而且在某些任务上它都胜过RNN和CNN。该transformer的架构需要更少的计算时间以训练模型，这也解释了为什么有些人认为，变transformer已经开始取代 RNN和LSTM。

以下链接包含您可以在Google Colaboratory中启动的Transformer神经网络的TF 2代码示例：

[]: https : //www.tensorflow.org/alpha/tutorials/text/transformer

另一个有趣且最新的架构称为“注意力增强卷积网络”，它是CNN与自注意力机制的结合。这种组合比纯CNN具有更好的准确性，您可以在本文中找到更多详细信息：

[]: https : //arxiv.org/abs/1904.09925

##### Transformer-XL体系结构

transformer-XL结合了transformer架构和段级递归机制和相对位置编码两种技术来获得比transformer更好的结果。Transformer-XL可用于单词级和字符级语言建模

ransform - xl和Transformer都处理第一部分的token，前者还保留隐藏层的输出。因此，每个隐含层接收来自前一个隐含层的两个输入，然后将它们连接起来，以向神经网络提供额外的信息。

根据以下文章，Transformer-XL的性能明显优于Transformer，并且其依赖性比“普通” RNN长80％：
https://hub.packtpub.com/transformer-xl-a-google-architecture-with80-longer-dependency-than-rnns/

##### reformer 架构

最近，Reformer体系结构发布了，该体系结构使用两种技术来提高Transformer体系结构的效率（即，在长序列上具有更低的内存和更快的性能）。结果，Reformer架构的复杂度也比Transformer低。有关reformer的更多详细信息，请点击此处：

https://openreview.net/pdf?id=rkgNKkHtvB
一些与reformer相关的代码在这里：

https : //pastebin.com/62r5FuEW

##### NLP和深度学习

使用深度学习的NLP模型可以包括CNN，RNN，LSTM和双向LSTM。例如，谷歌于2018年发布了BERT，这是NLP极其强大的框架。BERT非常复杂，涉及双向transformer和所谓的注意力（在本章稍后简要讨论）。

NLP的深度学习通常比其他技术具有更高的准确性，但请记住，有时它不如基于规则的学习和传统的机器学习方法那么快。如果您有兴趣，可以在以下代码示例中使用TensorFlow和RNN进行文本分类：

https://www.tensorflow.org/alpha/tutorials/text/text_classification_rnn 

使用TensorFlow和RNN进行文本生成的代码示例如下：

https : //www.tensorflow.org/alpha/tutorials/text/text_generation

##### NLP中的数据预处理任务

对文档执行一些常见的预处理任务，如下所示：

•    [1]转换小写（Lowercasing）

•    [1]除噪（ Noise removal）

•    [2]标准化（Normalization）

•    [3]文字丰富（Text enrichment）

•    [3]停用词删除（Stopword removal）

•    [3]词干（Stemming）

•    [3]合法化（Lemmatization）

前面的任务可以分类如下：

•    [1]：强制性任务（Mandatory tasks）

•    [2]：推荐任务（Recommended tasks）

•    [3]：任务依赖型（Task dependent）

简而言之，预处理任务至少涉及到删除多余的单词（a，the等），删除单词的结尾（running， runs和ran与run相同），以及将文本从大写转换为小写。

#### 流行的NLP算法

下面列出了一些流行的NLP算法，在某些情况下，它们是更复杂的NLP工具包的基础

•	 BoW: Bag of Words
•	 n-grams and skip-grams
•	 TF-IDF: basic algorithm in extracting keywords
•	 Word2Vector (Google): O/S project to describe text
•	 GloVe (Stanford NLP Group)
•	 LDA: text classification
•	 CF (collaborative filtering协作过滤): an algorithm in news recommend system(Google News and Yahoo News)

在前面的列表的上半部分中的主题将在后面的部分中进行简要讨论

##### 什么是n-grams

n-gram是一种创建词汇表的技术，它基于分组在一起的相邻单词。这种技术保留了一些单词位置（与BoW不同）。您需要指定“ n”值，该值指定组的大小。

这个想法很简单：对于句子中的每个单词，构造一个词汇术语，其中要在给定单词的左侧包含n个单词，在给定单词的右侧包含n个单词。举一个简单的例子，“this is a sentence”具有以下2-grams语法：

（this，is），（is，a），（a，sentence）

再举一个例子，我们可以使用相同的句子“this is a sentence”来确定它的3-grams：

（this， is，a），（is，a，sentence）

n-gram的概念出奇的强大，在预训练模型时，它们在流行的开源工具包（例如ELMo和BERT）中大量使用。

##### 什么是skip-gram

给定句子中的一个单词，skip-gram通过构建一个列表来创建词汇表，该列表在给定单词的两面都包含n个单词，然后是单词本身。例如，考虑以下句子：

the quick brown fox jumped over the lazy dog

size为1的skip-gram会产生如下词汇

([the,brown], quick),  ([quick,fox], brown),  ([brown,jumped], fox),  ...

size为2的skip-gram产生如下的

([the,quick,fox,jumped], brown)   ,   ([quick,brown,jumped,over], fox)   ,    

([brown,fox,over,the], jumped)  ,   ...

##### 什么是BoW

BoW（Bag of Words）为句子中的每个单词分配一个数值，并将这些单词视为一个集合（或包）， BoW不会跟踪相邻单词，因此它是一种非常简单的算法。

listing 6.1显示了Python脚本bow_to_vector的内容

```python
VOCAB = ['dog', 'cheese', 'cat', 'mouse']
TEXT1 = 'the mouse ate the cheese'
TEXT2 = 'the horse ate the hay'

def to_bow(text):
	words = text.split(" ")
	return [1 if w in words else 0 for w in VOCAB]
print("VOCAB: ",VOCAB)
print("TEXT1:",TEXT1)
print("BOW1: ",to_bow(TEXT1)) # [0, 1, 0, 1]
print("")

print("TEXT2:",TEXT2)
print("BOW2: ",to_bow(TEXT2)) # [0, 0, 0, 0]
```

listing 6.6初始化了一个VOCAB列表和两个文本字符串TEXT1和TEXT2 。listing 6.6的下一部分定义了Python函数to_bow（），该函数返回包含0和1的数组：如果当前句子中的单词出现在词汇表中，则返回1（否则返回0）。listing 6.6的最后一部分使用两个不同的句子调用Python函数,输出如下：

![image-20201217160043485](C:\Users\12772\AppData\Roaming\Typora\typora-user-images\image-20201217160043485.png)

##### 什么是词频（TF）

词频是单词在文档中出现的次数，在不同的文档中可能有所不同。考虑下面的简单示例，该示例由两个“文档” Doc1和Doc2组成

Doc1 = "This is a short sentence"
Doc2 = "yet another short sentence"

is和short的词频如下：

tf(is) = 1/5 for doc1 

tf(is) = 0 for doc

tf(short) = 1/5 for doc1 

tf(short) = 1/4 for doc2

前面的值将用于tf-idf的计算中，稍后将对此进行说明。

##### 什么是逆文本频率指数（IDF）

给定一组N个文档，并给定文档中的一个单词，让我们定义每个单词的idf和dc，如下所示：

dc = # of documents containing a given word 

idf = log(N/dc)

现在，让我们使用上一部分中相同的两个文档Doc1和Doc2

Doc1 = "This is a short sentence" 

Doc2 = "yet another short sentence"

显示单词is和单词short的idf值的计算

idf（is）= log（2/1）= log（2）

idf（short）= log（2/2）= 0

##### 什么是tf-idf

tf-idf是term Frequency, Inverse Document Frequency的缩写，是tf值和idf值的乘积

tf-idf = tf * idf

高频词的tf值较高， 而idf值较低。通常，稀有的词比常用的词更为相关，因此有助于提取相关性。例如，假设您有十个文档的集合（真实文档，而不是我们之前使用的玩具文档）。“ the ”一词在英语句子中经常出现，但未在任何文档中提供任何有关主题的指示。另一方面，如果您确定单词Universe 在单个文档中多次出现，则此信息可以提供对该文档主题的某种指示，并借助NLP技术帮助确定主题（或多个主题）在那个文件中

#### 什么是Word Embeddings（词向量）

Word Embeddings是一种固定长度的向量，用于编码和表示一个实体(文档、句子、单词、图)。每个单词都由一个实值向量表示，它可以产生数百个维度。此外，这种编码可能会产生稀疏向量:一个例子是独热编码（one-hot encoding），其中一个位置的值是1，而所有其他位置的值是0

流行的三个Word Embeddings算法是Word2vec，GloVe，和FastText。请记住，这三种算法都涉及无监督方法。它们也基于分布假设：相同上下文中的单词往往具有相似的含义

除了前面流行的算法外，还有一些流行的嵌入模型，下面列出了其中一些：

•	 Baseline Averaged Sentence Embeddings 

•	 Doc2Vec 

•	 Neural-Net Language Models 

•	 Skip-Thought Vectors 

•	 Quick-Thought Vectors 

•	 InferSent 

•	 Universal Sentence Encoder

#### ELMo, ULMFit, OpenAI, BERT, and ERNIE 2.0

在2018年期间，与NLP相关的研究取得了一些重大进展，产生了以下工具包和框架

•     ELMo：   于02/2018发布

•     ULMFit：  于05/2018发布

•     OpenAI：  于06/2018发布

•    BERT：   于10/2018发布

•    MT-DNN：  发布于01/2019

•    ERNIE 2.0：发布于08/2019

ELMo是“嵌入语言模型”的首字母缩写，它提供了深度上下文化词表示（ Deep Contextualized Word Representations）和最新的上下文词向量（state-of-the-art contextual word vectors），从而显着改善了Word Embeddings。

杰里米·霍华德（Jeremy Howard）和塞巴斯蒂安·鲁德（Sebastian Ruder）创建了通用语言模型微调（ULMFit），这是一种迁移学习方法，可以应用于NLP中的任何任务。ulmfit显著优于目前最先进的六个文本分类任务，在大多数数据集上减少了18-24%的错误。

OpenAI开发了GPT-2（GPT的后继产品），该模型经过训练可以预测40GB的互联网文本中的下一个单词。 由于担心其技术的恶意应用，OpenAI选择不发布经过训练的模型

GPT-2是一个基于transformer的大型语言模型，具有15亿个参数，在800万个网页（由人类策划）的数据集上进行了训练，并着重于内容的多样性。给定某些文本中的所有previous words，GPT-2经过训练可以预测下一个单词。数据集的多样性导致此目标包含跨不同领域的许多任务的自然发生的演示。GPT-2是GPT的直接扩展，具有超过10倍的参数，并接受了超过10倍的数据量训练。

BERT是“Bidirectional Encoder Representations from Transformers”的首字母缩写。BERT可以通过简单的英语考试（即BERT可以在多项选择中确定正确的选择）：

On stage, a woman takes a seat at the piano. She:
a) sits on a bench as her sister plays with the doll.
b) smiles with someone as the music plays.
c) is in the crowd, watching the dancers.
d) nervously sets her fingers on the keys.

##### 什么是translatotron

Translatotron是一种端到端的语音到语音翻译模型（来自Google），其输出保留了原始讲话者的声音；此外，它使用更少的数据进行训练。

语音翻译系统在过去的几十年中得到了发展，其目标是帮助说不同语言的人相互交流。这样的系统包括三个部分：

•    自动语音识别，可将源语音转录为文本

•    机器翻译，可将转录的文字翻译成目标语言

•    文本到语音合成（TTS），从翻译后的文本中生成目标语言的语音

前面的方法在商业产品（包括Google Translate）中已经成功。但是，Translatatron不需要单独的阶段，因此具有以下优点：

•    更快的推理速度

•    避免识别和翻译之间的复合错误

•    翻译后更容易保留原始说话者的声音

•    更好地处理未翻译的单词（名称和专有名词）总结了本章与NLP有关的部分。在AI社区中，另一个引起人们极大兴趣的领域是强化学习，本章稍后将对此进行介绍。